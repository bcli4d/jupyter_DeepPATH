{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook fine tunes the Inception v3 model to classify normal tissue/LUAD/LUSC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We first define some basic constants,  the location of the DeepPATH repo, as well as base paths where results and other data are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_cpus: 8, num_gpus: 1, batch_size: 30\n",
      "/mnt/disks/deeppath-data exists\n",
      "/mnt/disks/deeppath-data/Data exists\n",
      "/mnt/disks/deeppath-data/Data/Raw exists\n",
      "/mnt/disks/deeppath-data/inception_checkpoints exists\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, shutil\n",
    "from os.path import join\n",
    "import psutil\n",
    "from utilities import completed_batches\n",
    "\n",
    "# If we are doing tumor classification: \n",
    "# Set classifiation to \"Tu\"\n",
    "# ...and training_type to \"FT\" for finetuning the model \n",
    "# If we are doing mutation classification: \n",
    "# Set calssification to \"Mu\"\n",
    "# ...and training_type to \"TS\" to train the model from scratch \n",
    "\n",
    "# We are doing mutation classification\n",
    "classification = \"TU\"\n",
    "training_type = \"FT\"\n",
    "\n",
    "\n",
    "num_cpus = psutil.cpu_count()\n",
    "num_gpus = !nvidia-smi -L | wc -l\n",
    "num_gpus = int(num_gpus[0])\n",
    "batch_size_per_gpu = 30\n",
    "batch_size = num_gpus * batch_size_per_gpu\n",
    "print(\"num_cpus: {}, num_gpus: {}, batch_size: {}\".format(num_cpus, num_gpus, batch_size))\n",
    "\n",
    "\n",
    "deeppath_code = join(os.environ[\"HOME\"],'DeepPATH/DeepPATH_code')\n",
    "deeppath_data = '/mnt/disks/deeppath-data'\n",
    "data_base = join(deeppath_data,'Data')\n",
    "raw_images = join(data_base,'Raw')\n",
    "data_labels = join(data_base,'data_labels')\n",
    "#tilings = join(data_base,'tilings')\n",
    "inception_checkpoints = join(deeppath_data,'inception_checkpoints')\n",
    "\n",
    "for d in (deeppath_data, data_base, raw_images, inception_checkpoints):\n",
    "  try:\n",
    "    os.makedirs(d)\n",
    "    print(\"{} created\".format(d))\n",
    "  except:\n",
    "    print(\"{} exists\".format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various GCS buckets and relevant defines\n",
    "cgc_deeppath_bucket = 'cgc-deeppath' # Bucket containing users credentials\n",
    "deeppath_data_bucket = 'deeppath-data-whc' # Bucket where results are saved\n",
    "\n",
    "svs_images_bucket = 'imaging-west'# Bucket containing TCGA pathology images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify tiling parameters\n",
    "### We place results into a tiling specific directory tree. The first four values below control the tiling. Change these to perform a different tiling. Note that Inception V3 wants 299 pixel tiles; any other size will be scaled by tensor flow before submission to the model. In other words, leave tile_size at 299 and overlap at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiling directory: /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile exists\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/logs exists\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/tiles exists\n"
     ]
    }
   ],
   "source": [
    "tile_size = 299\n",
    "overlap = 0\n",
    "background = 25\n",
    "magnification = 20\n",
    "tiling_params = \"Px{}Ol{}Bg{}Mg{}_Tile\".format(str(tile_size), str(overlap), str(background),str(magnification))\n",
    "tiling = join(data_base, tiling_params)\n",
    "tiling_logs = join(tiling,'logs')\n",
    "# Directory where tiles are stored              \n",
    "tiles = join(tiling,'tiles')\n",
    "if classification == \"TU\":\n",
    "    images_metadata = 'tu_images_metadata.json' # Name of images metadata file  (which will be created below)\n",
    "else:\n",
    "    images_metadata = 'mu_images_metadata.json' # Name of images metadata file  (which will be created below)\n",
    "images_metadata_path = join(tiling, images_metadata) # Path to images metadata file\n",
    "\n",
    "print(\"Tiling directory: {}\".format(tiling))\n",
    "\n",
    "for d in (tiling, tiling_logs, tiles):\n",
    "  try:\n",
    "    os.makedirs(d)\n",
    "    print(\"{} created\".format(d))\n",
    "  except:\n",
    "    print(\"{} exists\".format(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify sorting parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting directory: /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort exists\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/logs exists\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/sorted exists\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/TFRecord_TrainValid exists\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/TFRecord_Test exists\n"
     ]
    }
   ],
   "source": [
    "if classification == \"TU\":\n",
    "    sorting_option = 3\n",
    "    sorting_params = 'So{}Tu_Sort'.format(sorting_option)\n",
    "    sorting = join(tiling, sorting_params)\n",
    "    data_labels_path = join(os.environ[\"HOME\"],'jupyter_DeepPATH','tumor_labels.txt')\n",
    "else:\n",
    "    sorting_option = 10\n",
    "    sorting_params = 'So{}Mu{}_Sort'.format(sorting_option,'NonSilent')\n",
    "    sorting = join(tiling, sorting_params)\n",
    "    mutations_metadata = 'mutations_metadata.txt' # Name of mutation metadata file  (which will be created below)\n",
    "    mutations_metadata_path = join(sorting, mutations_metadata) # Path to metadata file\n",
    "    data_labels_path = join(sorting, 'hugo_symbols.txt')\n",
    "\n",
    "sorting_logs = join(sorting, 'logs')\n",
    "sorted_tiles = join(sorting, 'sorted')\n",
    "\n",
    "print(\"Sorting directory: {}\".format(sorting))\n",
    "\n",
    "# Directories where sharded tensor flow formatted records are stored\n",
    "trainValid_records = join(sorting, 'TFRecord_TrainValid')\n",
    "test_records = join(sorting, 'TFRecord_Test')\n",
    "for d in (sorting, sorting_logs, sorted_tiles, trainValid_records, test_records):\n",
    "  try:\n",
    "    os.makedirs(d)\n",
    "    print(\"{} created\".format(d))\n",
    "  except:\n",
    "    print(\"{} exists\".format(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify training, validation, test, and  parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training directory: /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train exists\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/logs exists\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints exists\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/pretrained_checkpoints exists\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results exists\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/test_results exists\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/roc_curves exists\n"
     ]
    }
   ],
   "source": [
    "# If fine_tune param to TF is True, fine tuning is performed. we do fine tuning for\n",
    "# tumor classification, but training from scratch for mutation classification\n",
    "if training_type == \"FT\":\n",
    "    fine_tune = True \n",
    "    initial_learning_rate = 0.001\n",
    "    # Use softmax for multiclass classification\n",
    "    training_mode = '0_softmax' \n",
    "    class_number = 3 # Number of tumor classes.\n",
    "else:\n",
    "    fine_tune = False \n",
    "    initial_learning_rate = 0.1\n",
    "    # Use sigmoid for multi-class classification\n",
    "    training_mode = '1_sigmoid'\n",
    "    class_number = 10 # Number of mutations.\n",
    "\n",
    "#trainings = join(sorting, \"trainings\")\n",
    "training_params = \"Cl{}Ft{}_Train\".format(str(class_number),str(fine_tune))\n",
    "training = join(sorting, training_params)\n",
    "training_logs = join(training, 'logs')\n",
    "\n",
    "print(\"Training directory: {}\".format(training))\n",
    "\n",
    "# Directory where pretrained checkpoints are stored\n",
    "pretrained_checkpoints = join(training, 'pretrained_checkpoints')\n",
    "# Directory where intermediate training checkpoints are stored\n",
    "intermediate_checkpoints = join(training, 'intermediate_checkpoints')\n",
    "# Directory where validation and test results are stored\n",
    "eval_results = join(training, 'eval_results')\n",
    "test_results = join(training, 'test_results')\n",
    "roc_curves = join(training, 'roc_curves')\n",
    "heatmaps = join(training,'Px'+str(tile_size))\n",
    "\n",
    "for d in (training, training_logs, intermediate_checkpoints, pretrained_checkpoints,\n",
    "         eval_results, test_results, roc_curves):\n",
    "  try:\n",
    "    os.makedirs(d)\n",
    "    print(\"{} created\".format(d))\n",
    "  except:\n",
    "    print(\"{} exists\".format(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get metadata from BQ\n",
    "\n",
    "### Rather than obtain metdata from the GDC, we perform an SQL query to obtain the names of LUAD images and corresponding mutation calls.\n",
    "\n",
    "### In order to perform a BQ query, we must first establish our credentials\n",
    "See https://cloud.google.com/bigquery/docs/quickstarts/quickstart-client-libraries#bigquery_simple_app_query-python for help. Put your credentials in some GCS bucket and modify the \"bucket\" and \"credentials\" variables below as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcs_access import upload_from_GCS\n",
    "bucket = 'cgc-deeppath'                                                                                         \n",
    "credentials = 'GAC.json' \n",
    "upload_from_GCS(bucket, credentials, \"/tmp/GAC.json\")\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"/tmp/GAC.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now perform a BQ query to get metadata. \n",
    "\n",
    "For this purpose, we call query_mutation_metadata(), passing it the SQL query string. query_mutation_metadata() also formats the returned data in the format expected by the tiling and sorting phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_metadata_from_bq import query_mutation_metadata, query_tumor_metadata\n",
    "\n",
    "if classification == \"TU\":\n",
    "    sql_query = \"\"\"\n",
    "        SELECT\n",
    "          t1.svsFileName,\n",
    "          t1.file_gdc_id,\n",
    "          t2.sample_type_name,\n",
    "          t2.project_short_name,\n",
    "          t3.clinical_stage\n",
    "        FROM\n",
    "          `isb-cgc.metadata.TCGA_slide_images` t1\n",
    "        INNER JOIN\n",
    "          `isb-cgc.TCGA_bioclin_v0.Biospecimen` AS t2\n",
    "        ON\n",
    "          t1.sample_barcode = t2.sample_barcode\n",
    "        INNER JOIN\n",
    "          `isb-cgc.TCGA_bioclin_v0.Clinical` t3\n",
    "        ON\n",
    "          t2.case_barcode = t3.case_barcode\n",
    "        WHERE\n",
    "          (t2.project_short_name='TCGA-LUAD'\n",
    "            OR t2.project_short_name='TCGA-LUSC')\n",
    "          AND NOT t1.slide_barcode LIKE '%TCGA-__-____-___-__-D%'\"\"\"\n",
    "    query_tumor_metadata(images_metadata_path, sql_query)\n",
    "else:\n",
    "    sql_query = \"\"\"\n",
    "    WITH\n",
    "      luads AS (\n",
    "      SELECT\n",
    "        t1.file_gdc_id AS file_gdc_id,\n",
    "        t1.svsFilename AS svsFilename,\n",
    "        t1.sample_barcode AS sample_barcode,\n",
    "        t2.callerName AS callerName,\n",
    "        t2.Hugo_Symbol AS Hugo_Symbol,\n",
    "        t2.Variant_Classification AS Variant_Classification\n",
    "      FROM\n",
    "        `isb-cgc.metadata.TCGA_slide_images` t1\n",
    "      INNER JOIN\n",
    "        `isb-cgc.TCGA_hg38_data_v0.Somatic_Mutation` t2\n",
    "      ON\n",
    "        t1.sample_barcode = t2.sample_barcode_tumor\n",
    "      WHERE\n",
    "        (t2.project_short_name='TCGA-LUAD')\n",
    "        AND NOT t1.slide_barcode LIKE '%TCGA-__-____-___-__-D%'),\n",
    "      mutations AS (\n",
    "      SELECT\n",
    "        file_gdc_id,\n",
    "        svsFilename,\n",
    "        sample_barcode,\n",
    "        Hugo_Symbol,\n",
    "        Variant_Classification\n",
    "      FROM\n",
    "        luads\n",
    "      WHERE\n",
    "        callerName LIKE '%mutect%'\n",
    "        AND Hugo_Symbol IN ('EGFR',\n",
    "          'FAT1',\n",
    "          'FAT4',\n",
    "          'KEAP1',\n",
    "          'KRAS',\n",
    "          'LRP1B',\n",
    "          'NF1',\n",
    "          'SETBP1',\n",
    "          'STK11',\n",
    "          'TP53')\n",
    "        AND Variant_Classification IN ('Missense_Mutation',\n",
    "          'In_Frame_Del',\n",
    "          'In_Frame_Ins',\n",
    "          'Frame_Shift',\n",
    "          'Frame_Shift_Del',\n",
    "          'Frame_Shift_Ins',\n",
    "          'Nonsense_Mutation',\n",
    "          'Nonstop_Mutation' )),\n",
    "      wts AS (\n",
    "      SELECT\n",
    "        file_gdc_id,\n",
    "        svsFilename,\n",
    "        sample_barcode,\n",
    "        'WT' AS Hugo_Symbol,\n",
    "        '' AS Variant_Classification\n",
    "      FROM\n",
    "        luads\n",
    "      WHERE\n",
    "        sample_barcode NOT IN (\n",
    "        SELECT\n",
    "          sample_barcode\n",
    "        FROM\n",
    "          mutations))\n",
    "    SELECT\n",
    "      file_gdc_id,\n",
    "      svsFilename,\n",
    "      sample_barcode,\n",
    "      Hugo_Symbol,\n",
    "      Variant_Classification\n",
    "    FROM\n",
    "      mutations\n",
    "    UNION DISTINCT\n",
    "    SELECT\n",
    "      file_gdc_id,\n",
    "      svsFilename,\n",
    "      sample_barcode,\n",
    "      Hugo_Symbol,\n",
    "      Variant_Classification\n",
    "    FROM\n",
    "      wts\n",
    "    ORDER BY\n",
    "      sample_barcode,\n",
    "      Hugo_Symbol  \"\"\"\n",
    "    query_mutation_metadata(images_metadata_path, mutations_metadata_path, data_labels_path, sql_query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tile the images listed in the manifest\n",
    "Images are uploaded, one at a time, from GCS and tiled according to parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally load previously saved results from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('gs://deeppath-data-whc/Px512Ol0Bg25Mg5_Tile/data.tar.gz', '/mnt/disks/deeppath-data/Data/Px512Ol0Bg25Mg5_Tile/data.tar.gz')\n",
      "Found\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "gcs = join('gs://', deeppath_data_bucket, tiling_params,'data.tar')\n",
    "loc = join(tiling,'data.tar')\n",
    "\n",
    "result = !gsutil ls $gcs\n",
    "if result[0] ==gcs:\n",
    "    !gsutil -m cp $gcs $loc \n",
    "\n",
    "    with tarfile.open(loc) as tar:\n",
    "        tar.extractall()\n",
    "\n",
    "    !rm $loc\n",
    "else:\n",
    "    print('{} not found'.format(loc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform tiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "tiler = join(deeppath_code, '00_preprocessing/0b_tileLoop_deepzoom4.py')\n",
    "out_log_file = join(tiling_logs,'tiler_out.log')\n",
    "err_log_file = join(tiling_logs,'tiler_err.log')\n",
    "!rm $out_log_file\n",
    "!rm $err_log_file\n",
    "!rm $raw_images/*\n",
    "skipped_file_name = \"\"\n",
    "skipped_file_id = \"\"\n",
    "with open(images_metadata_path) as fid:\n",
    "    jdata = json.loads(fid.read())\n",
    "    jdata.sort(key = lambda x: x['file_name'])\n",
    "    for file in jdata:\n",
    "        print(\"\")\n",
    "        print(\"Checking {}\".format(file))\n",
    "        tiled_dziname = join(tiles,file['file_name'].rsplit('.',1)[0]+'.dzi')\n",
    "        tiled_filesname = join(tiles,file['file_name'].rsplit('.',1)[0]+'_files')\n",
    "        \n",
    "        if os.path.exists(tiled_dziname):\n",
    "            # If the .dzi file exists, then we presume that tiling was completed\n",
    "            skipped_file_name = file['file_name']\n",
    "            skipped_file_id = file['file_id']\n",
    "            print(\"Skipping {}\".format(file['file_name']))\n",
    "        else:\n",
    "            print(\"Processing {}\".format(file['file_name']))\n",
    "            if os.path.exists(tiled_filesname):\n",
    "                # Apparently this file was only partially tiled, so delete it completely\n",
    "                print (\"removing {}\".format(tiled_filesname))\n",
    "                shutil.rmtree(tiled_filesname)\n",
    "            GCS_filename = 'gs://'+join(svs_images_bucket,file['file_id'],file['file_name'])\n",
    "            local_filename = join(raw_images,file['file_name'])\n",
    "            oldstderr = sys.stderr\n",
    "            sys.stderr = open(err_log_file, 'a')\n",
    "            oldstdout = sys.stdout\n",
    "            sys.stdout = open(out_log_file, 'a')\n",
    "            !gsutil -m cp $GCS_filename $local_filename\n",
    "            !python $tiler --output=$tiles --Mag=$magnification \\\n",
    "                --size=$tile_size --overlap=$overlap --Background=$background --jobs=$num_cpus \\\n",
    "                $raw_images/*svs\n",
    "            #print(\"gsutil -m cp {} {}\".format(GCS_filename, local_filename))\n",
    "            !rm $local_filename\n",
    "            sys.stderr = oldstderr\n",
    "            sys.stdout = oldstdout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally save tiling results to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# Populate the tar file. Don't try to compress.\n",
    "loc = join(tiling,'data.tar')\n",
    "with tarfile.open(loc, \"w\") as tar:\n",
    "    for name in [tiles, tiling_logs]:\n",
    "        tar.add(name)\n",
    "\n",
    "# Copy it to GCS\n",
    "gcs = join('gs://', deeppath_data_bucket,tiling_params,'data.tar')\n",
    "!gsutil -m cp $loc $gcs \n",
    "\n",
    "# Delete the tar file\n",
    "!rm $loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort tiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally load previously saved sorting results from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://deeppath-data-whc/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/data.tar.gz not found\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "gcs = join('gs://', deeppath_data_bucket, tiling_params, sorting_params, 'data.tar.gz')\n",
    "loc = join(sorting,'data.tar.gz')\n",
    "\n",
    "result = !gsutil ls $gcs\n",
    "if result[0] == gcs:\n",
    "    !gsutil -m cp $gcs $loc \n",
    "    \n",
    "    !ls -l $loc\n",
    "    with tarfile.open(loc) as tar:\n",
    "        tar.extractall('/')\n",
    "\n",
    "    !rm $loc\n",
    "else:\n",
    "    print('{} not found'.format(gcs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done removing sorted_tiles\n",
      "Exception reporting mode: Verbose\n"
     ]
    }
   ],
   "source": [
    "# The sort routine outputs to the cwd. So change cwd as needed.\n",
    "# First, remove existing sort results\n",
    "os.chdir(os.environ[\"HOME\"])\n",
    "try:\n",
    "    shutil.rmtree(sorted_tiles)\n",
    "except:\n",
    "    pass\n",
    "os.mkdir(sorted_tiles)\n",
    "os.chdir(sorted_tiles)\n",
    "\n",
    "func = join(deeppath_code,'00_preprocessing/0d_SortTiles.py' )\n",
    "root = func.rsplit('/',1)[1].split('.')[0]\n",
    "\n",
    "out_log_file = join(sorting_logs,root+'.out.log')\n",
    "err_log_file = join(sorting_logs,root+'.err.log')\n",
    "\n",
    "!python $func --SourceFolder=$tiles --JsonFile=$images_metadata_path --PercentValid=15 --PercentTest=15 \\\n",
    "    --PatientID=12 --Magnification=$magnification --MagDiffAllowed=0.0 --SortingOption=$sorting_option --nSplit=0 \\\n",
    "    > $out_log_file 2> $err_log_file\n",
    "os.chdir(os.environ[\"HOME\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the JPEG tiles into TFRecord format\n",
    "## First format the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Verbose\n",
      "Starting to format training data\n",
      "Starting to format validation data\n",
      "/bin/sh: 1: Syntax error: end of file unexpected\n"
     ]
    }
   ],
   "source": [
    "# Remove existing TFRecord conversion results\n",
    "try:\n",
    "    shutil.rmtree(trainValid_records)\n",
    "except:\n",
    "    pass\n",
    "os.mkdir(trainValid_records)\n",
    "\n",
    "%xmode Verbose\n",
    "\n",
    "if classification == \"TU\":\n",
    "    # Format the training data\n",
    "    print(\"Starting to format training data\")\n",
    "    func = join(deeppath_code,'00_preprocessing/TFRecord_2or3_Classes/build_image_data.py' )\n",
    "    root = func.rsplit('/',1)[1].split('.')[0]\n",
    "    out_log_file = join(sorting_logs,root+'.train.out.log')\n",
    "    err_log_file = join(sorting_logs,root+'.train.err.log')\n",
    "    !python $func --directory=$sorted_tiles --output_directory=$trainValid_records --train_shards=1024 \\\n",
    "        --validation_shards=128 --num_threads=$num_cpus > $out_log_file 2> $err_log_file\n",
    "\n",
    "    # Format the validation data\n",
    "    # NOTE: num_threads must be 1. Using more than one thread causes corruption of the resulting files!\n",
    "    print(\"Starting to format validation data\")\n",
    "    func = join(deeppath_code,'00_preprocessing/TFRecord_2or3_Classes/build_TF_test.py' )\n",
    "    root = func.rsplit('/',1)[1].split('.')[0]\n",
    "    out_log_file = join(sorting_logs,root+'.valid.out.log')\n",
    "    err_log_file = join(sorting_logs,root+'.valid.err.log')\n",
    "    !python $func --directory=$sorted_tiles --output_directory=$trainValid_records --train_shards=1024 \\\n",
    "        --validation_shards=128 --num_threads=1 --one_FT_per_Tile=False --ImageSet_basename=valid \\\n",
    "        > $out_log_file 2> $err_log_file\n",
    "\n",
    "else:\n",
    "    # Format both training and validation data in one step\n",
    "    print(\"Starting to format training and validation data\")\n",
    "\n",
    "    func = join(deeppath_code,'00_preprocessing/TFRecord_multi_Classes/build_image_data_multiClass.py' )\n",
    "    root = func.rsplit('/',1)[1].split('.')[0]\n",
    "    out_log_file = join(sorting_logs,root+'.train.out.log')\n",
    "    err_log_file = join(sorting_logs,root+'.train.err.log')\n",
    "    !python $func --directory=$sorted_tiles/tiles --output_directory=$trainValid_records --train_shards=1024 \\\n",
    "        --validation_shards=128 --num_threads=$num_cpus  --labels_names=$data_labels_path --labels=$mutations_metadata_path \\\n",
    "        --PatientID=12 > $out_log_file 2> $err_log_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now format the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: num_threads must be 1. Using more than one thread causes corruption of the resulting files!\n",
    "# Remove existing TFRecord conversion results\n",
    "try:\n",
    "    shutil.rmtree(test_records)\n",
    "except:\n",
    "    pass\n",
    "os.mkdir(test_records)\n",
    "\n",
    "if classification == \"TU\":\n",
    "    func = join(deeppath_code,'00_preprocessing/TFRecord_2or3_Classes/build_TF_test.py' )\n",
    "    root = func.rsplit('/',1)[1].split('.')[0]\n",
    "    out_log_file = join(sorting_logs,root+'.test.out.log')\n",
    "    err_log_file = join(sorting_logs,root+'.test.err.log')\n",
    "    !python $func --directory=$sorted_tiles --output_directory=$test_records --train_shards=1024 \\\n",
    "        --validation_shards=128 --num_threads=1 --one_FT_per_Tile=False --ImageSet_basename=test \\\n",
    "        > $out_log_file 2> $err_log_file\n",
    "else:\n",
    "    func = join(deeppath_code,'00_preprocessing/TFRecord_multi_Classes/build_TF_test_multiClass.py' )\n",
    "    root = func.rsplit('/',1)[1].split('.')[0]\n",
    "    out_log_file = join(sorting_logs,root+'.test.out.log')\n",
    "    err_log_file = join(sorting_logs,root+'.test.err.log')\n",
    "    !python $func --directory=$sorted_tiles/tiles --output_directory=$test_records --train_shards=1024 \\\n",
    "        --validation_shards=128 --num_threads=1 --one_FT_per_Tile=False --ImageSet_basename='test' \\\n",
    "        --labels_names=$data_labels_path --labels=$mutations_metadata_path \\\n",
    "        --PatientID=12 > $out_log_file 2> $err_log_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optionally save sorting results to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# Populate the tar file\n",
    "loc = join(sorting,'data.tar')\n",
    "with tarfile.open(loc, \"w\") as tar:\n",
    "    #for name in [sorting_logs, sorted_tiles, trainValid_records, test_records]:\n",
    "    for name in [trainValid_records, test_records]:\n",
    "        tar.add(name)\n",
    "\n",
    "# Copy it to GCS\n",
    "gcs = join('gs://', deeppath_data_bucket,tiling_params, sorting_params, 'data.tar')\n",
    "!gsutil -m cp $loc $gcs \n",
    "\n",
    "# Delete the tar file\n",
    "!rm $loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optionally load previously saved training results from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://deeppath-data-whc/Px512Ol0Bg25Mg5_Tile/So3_Sort/Cl3FtTrue_Train/data.tar.gz not found\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "gcs = join('gs://', deeppath_data_bucket, tiling_params, sorting_params, training_params, 'data.tar')\n",
    "loc = join(training,'data.tar')\n",
    "\n",
    "result = !gsutil ls $gcs\n",
    "if result[0] ==gcs:\n",
    "    !gsutil -m cp $gcs $loc \n",
    "\n",
    "    with tarfile.open(loc) as tar:\n",
    "        tar.extractall('/')\n",
    "\n",
    "    !rm $loc\n",
    "else:\n",
    "    print('{} not found'.format(gcs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If training from scratch, first have to install the Bazel build tool, and build the inception model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_type == \"TS\":\n",
    "    !sudo apt-get install -y pkg-config zip g++ zlib1g-dev unzip python\n",
    "    !wget https://github.com/bazelbuild/bazel/releases/download/0.24.0/bazel-0.24.0-installer-linux-x86_64.sh\n",
    "    !chmod +x bazel-0.24.0-installer-linux-x86_64.sh\n",
    "    !./bazel-0.24.0-installer-linux-x86_64.sh --user\n",
    "    os.environ[\"PATH\"] += \":\" + join(os.getcwd(),'bin')\n",
    "    !rm bazel-0.24.0-installer-linux-x86_64.sh\n",
    "\n",
    "    #os.chdir(join(os.environ[\"HOME\"],'DeepPATH/DeepPATH_code/s01_training/xClasses'))\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(join(deeppath_code,'01_training/xClasses'))\n",
    "    print(os.getcwd())\n",
    "    !bazel build inception/imagenet_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-13 22:05:47.885599: step 4450, loss = 1.57 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-13 22:06:00.022545: step 4460, loss = 1.62 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-13 22:06:12.250794: step 4470, loss = 1.43 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-13 22:06:24.369800: step 4480, loss = 1.33 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-13 22:06:36.518571: step 4490, loss = 1.44 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-13 22:06:48.561858: step 4500, loss = 1.61 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-13 22:07:05.173601: step 4510, loss = 1.51 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-13 22:07:17.360991: step 4520, loss = 1.45 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-13 22:07:29.391949: step 4530, loss = 1.73 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-13 22:07:41.441672: step 4540, loss = 1.34 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-13 22:07:53.639594: step 4550, loss = 1.35 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-13 22:08:05.848563: step 4560, loss = 1.41 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-13 22:08:17.991787: step 4570, loss = 1.35 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-13 22:08:30.093961: step 4580, loss = 1.50 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-13 22:08:42.243699: step 4590, loss = 1.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-13 22:08:54.343232: step 4600, loss = 1.20 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-13 22:09:10.892200: step 4610, loss = 1.62 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-06-13 22:09:22.994796: step 4620, loss = 1.19 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-13 22:09:35.038860: step 4630, loss = 1.61 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-13 22:09:47.200485: step 4640, loss = 1.32 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-13 22:09:59.335780: step 4650, loss = 1.64 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-13 22:10:11.481596: step 4660, loss = 1.29 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-13 22:10:23.646442: step 4670, loss = 1.26 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-13 22:10:35.703716: step 4680, loss = 1.32 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-13 22:10:47.841588: step 4690, loss = 1.37 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-13 22:10:59.884670: step 4700, loss = 1.28 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-13 22:11:16.619767: step 4710, loss = 1.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-13 22:11:28.745892: step 4720, loss = 1.54 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-13 22:11:40.885692: step 4730, loss = 1.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-13 22:11:53.030534: step 4740, loss = 1.43 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-13 22:12:05.070125: step 4750, loss = 1.64 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-13 22:12:17.103332: step 4760, loss = 1.33 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-13 22:12:29.166543: step 4770, loss = 1.22 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-13 22:12:41.344589: step 4780, loss = 1.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-13 22:12:53.468682: step 4790, loss = 1.45 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-13 22:13:05.600158: step 4800, loss = 1.35 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-13 22:13:22.397012: step 4810, loss = 1.53 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-13 22:13:34.456358: step 4820, loss = 1.39 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-13 22:13:46.511224: step 4830, loss = 1.40 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-06-13 22:13:58.575587: step 4840, loss = 1.43 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-13 22:14:10.628640: step 4850, loss = 1.60 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-13 22:14:22.771351: step 4860, loss = 1.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-13 22:14:34.777897: step 4870, loss = 1.28 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-13 22:14:47.043553: step 4880, loss = 1.38 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-06-13 22:14:59.124815: step 4890, loss = 1.29 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-13 22:15:11.170736: step 4900, loss = 1.62 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-13 22:15:28.234046: step 4910, loss = 1.37 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-13 22:15:40.286832: step 4920, loss = 1.56 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-13 22:15:52.543351: step 4930, loss = 1.55 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-13 22:16:04.659492: step 4940, loss = 1.38 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-13 22:16:16.823833: step 4950, loss = 1.35 (23.8 examples/sec; 1.263 sec/batch)\n",
      "2019-06-13 22:16:28.912628: step 4960, loss = 1.63 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-13 22:16:41.029761: step 4970, loss = 1.35 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-13 22:16:53.277930: step 4980, loss = 1.44 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-13 22:17:05.326808: step 4990, loss = 1.27 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-13 22:17:17.479955: step 5000, loss = 1.54 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-13 22:17:37.645402: step 5010, loss = 1.36 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-13 22:17:49.820332: step 5020, loss = 1.29 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-13 22:18:02.165532: step 5030, loss = 1.56 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-13 22:18:14.563936: step 5040, loss = 1.45 (24.9 examples/sec; 1.205 sec/batch)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "func = join(deeppath_code,'01_training/xClasses/bazel-bin/inception/imagenet_train' )\n",
    "root = func.rsplit('/',1)[1]\n",
    "\n",
    "# Note that the --train_dir directory appears to be deleted and recreated\n",
    "if training_type == \"TS\":\n",
    "    # Note that the --train_dir directory appears to be deleted and recreated\n",
    "    print(\"Performing training from scratch\")\n",
    "    !python $func \\\n",
    "        --num_gpus=$num_gpus --batch_size=$batch_size --train_dir=$intermediate_checkpoints --data_dir=$trainValid_records \\\n",
    "        --fine_tune=$fine_tune --ClassNumber=$class_number --mode=$training_mode\n",
    "else:\n",
    "    print(\"Performing fine tuning\")\n",
    "    !python $func \\\n",
    "        --num_gpus=$num_gpus --batch_size=$batch_size --train_dir=$intermediate_checkpoints --data_dir=$trainValid_records \\\n",
    "        --pretrained_model_checkpoint_path=$inception_checkpoints/inception-v3/model.ckpt-157585 \\\n",
    "        --fine_tune=$fine_tune --initial_learning_rate=$initial_learning_rate  --ClassNumber=$class_number --mode=$training_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue training\n",
    "After halting initial training, you can resume by executiong the following cell.\n",
    "Note that when you continue training, --fine_tune should be False. If True, the final layer of weights will be randomly set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 140000 batches\n",
      "!!!!!!!!!!!!!!\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/TFRecord_TrainValid/train-*\n",
      "WARNING:tensorflow:From /home/bcliffor/dp/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "!!!!!!!!!!!!!!\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/TFRecord_TrainValid/train-*\n",
      "WARNING:tensorflow:From /home/bcliffor/DeepPATH/DeepPATH_code/01_training/xClasses/bazel-bin/inception/imagenet_train.runfiles/inception/inception/image_processing.py:457: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/bcliffor/dp/lib/python3.5/site-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/bcliffor/dp/lib/python3.5/site-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /home/bcliffor/dp/lib/python3.5/site-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/bcliffor/dp/lib/python3.5/site-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/bcliffor/dp/lib/python3.5/site-packages/tensorflow/python/training/input.py:202: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/bcliffor/DeepPATH/DeepPATH_code/01_training/xClasses/bazel-bin/inception/imagenet_train.runfiles/inception/inception/dataset.py:110: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n",
      "WARNING:tensorflow:From /home/bcliffor/DeepPATH/DeepPATH_code/01_training/xClasses/bazel-bin/inception/imagenet_train.runfiles/inception/inception/image_processing.py:245: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "WARNING:tensorflow:From /home/bcliffor/DeepPATH/DeepPATH_code/01_training/xClasses/bazel-bin/inception/imagenet_train.runfiles/inception/inception/image_processing.py:519: batch_join (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.interleave(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From /home/bcliffor/DeepPATH/DeepPATH_code/01_training/xClasses/bazel-bin/inception/imagenet_train.runfiles/inception/inception/slim/ops.py:419: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "softmax training\n",
      "WARNING:tensorflow:From /home/bcliffor/DeepPATH/DeepPATH_code/01_training/xClasses/bazel-bin/inception/imagenet_train.runfiles/inception/inception/inception_model.py:123: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/bcliffor/dp/lib/python3.5/site-packages/tensorflow/contrib/nn/python/ops/cross_entropy.py:68: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "softmax loss\n",
      "softmax loss\n",
      "WARNING:tensorflow:From /home/bcliffor/dp/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2019-06-16 18:21:29.472653: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-06-16 18:21:31.614513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-06-16 18:21:31.615170: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55fc970f8960 executing computations on platform CUDA. Devices:\n",
      "2019-06-16 18:21:31.615220: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2019-06-16 18:21:31.618800: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-06-16 18:21:31.619258: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55fc97161b10 executing computations on platform Host. Devices:\n",
      "2019-06-16 18:21:31.619319: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-06-16 18:21:31.619929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
      "2019-06-16 18:21:31.619973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2019-06-16 18:21:31.620873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-06-16 18:21:31.620912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2019-06-16 18:21:31.620921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2019-06-16 18:21:31.621235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10802 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "WARNING:tensorflow:From /home/bcliffor/dp/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bcliffor/DeepPATH/DeepPATH_code/01_training/xClasses/bazel-bin/inception/imagenet_train.runfiles/inception/inception/inception_train.py:339: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "2019-06-16 18:22:04.327737: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
      "2019-06-16 18:22:14.469669: step 0, loss = 0.93 (1.2 examples/sec; 24.642 sec/batch)\n",
      "2019-06-16 18:22:46.097524: step 10, loss = 1.14 (23.4 examples/sec; 1.284 sec/batch)\n",
      "2019-06-16 18:22:58.253637: step 20, loss = 1.05 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 18:23:10.764358: step 30, loss = 0.96 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 18:23:23.084213: step 40, loss = 1.10 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 18:23:35.741447: step 50, loss = 1.17 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 18:23:48.013259: step 60, loss = 1.03 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-16 18:24:00.235729: step 70, loss = 1.12 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 18:24:12.586326: step 80, loss = 1.22 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 18:24:24.997664: step 90, loss = 1.11 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-16 18:24:37.326990: step 100, loss = 1.05 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-16 18:24:54.090425: step 110, loss = 1.11 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-16 18:25:06.496978: step 120, loss = 1.06 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-16 18:25:18.843932: step 130, loss = 1.06 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 18:25:31.324890: step 140, loss = 1.08 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-16 18:25:43.635329: step 150, loss = 1.06 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 18:25:55.933241: step 160, loss = 1.10 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 18:26:08.472821: step 170, loss = 1.00 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 18:26:20.857836: step 180, loss = 1.12 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 18:26:33.300440: step 190, loss = 1.07 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 18:26:45.503490: step 200, loss = 1.19 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 18:27:02.609964: step 210, loss = 1.13 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 18:27:14.934422: step 220, loss = 1.03 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-06-16 18:27:27.208337: step 230, loss = 1.08 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-16 18:27:39.392623: step 240, loss = 1.27 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 18:27:51.784936: step 250, loss = 1.14 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 18:28:03.920013: step 260, loss = 1.01 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 18:28:16.453485: step 270, loss = 1.06 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 18:28:28.990238: step 280, loss = 1.11 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 18:28:41.374109: step 290, loss = 0.91 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 18:28:53.905933: step 300, loss = 1.12 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 18:29:10.473362: step 310, loss = 0.93 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 18:29:22.632853: step 320, loss = 1.18 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-16 18:29:34.834808: step 330, loss = 1.05 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 18:29:47.085194: step 340, loss = 1.10 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 18:29:59.674440: step 350, loss = 1.13 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 18:30:12.132350: step 360, loss = 0.94 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-16 18:30:24.420539: step 370, loss = 1.00 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 18:30:36.623403: step 380, loss = 1.00 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 18:30:48.901192: step 390, loss = 1.11 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 18:31:01.222743: step 400, loss = 1.01 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 18:31:18.152773: step 410, loss = 1.31 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-16 18:31:30.600244: step 420, loss = 1.22 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 18:31:42.888116: step 430, loss = 1.24 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-06-16 18:31:55.473528: step 440, loss = 1.03 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-16 18:32:07.837727: step 450, loss = 1.16 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-16 18:32:20.315211: step 460, loss = 1.17 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 18:32:32.572208: step 470, loss = 1.01 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 18:32:44.700855: step 480, loss = 0.90 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 18:32:57.066928: step 490, loss = 1.14 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 18:33:09.165987: step 500, loss = 1.15 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 18:33:26.037605: step 510, loss = 1.10 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-16 18:33:38.195127: step 520, loss = 0.98 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 18:33:50.495705: step 530, loss = 1.18 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 18:34:02.944785: step 540, loss = 1.20 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 18:34:15.184915: step 550, loss = 1.12 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 18:34:27.713218: step 560, loss = 1.08 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-16 18:34:40.175221: step 570, loss = 1.23 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 18:34:52.486972: step 580, loss = 1.09 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 18:35:04.627433: step 590, loss = 1.06 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 18:35:17.056802: step 600, loss = 1.35 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 18:35:34.001516: step 610, loss = 1.37 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 18:35:46.441788: step 620, loss = 1.11 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 18:35:58.813565: step 630, loss = 0.90 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 18:36:11.180497: step 640, loss = 1.15 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 18:36:23.408748: step 650, loss = 1.01 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 18:36:35.742299: step 660, loss = 1.26 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 18:36:48.212155: step 670, loss = 1.03 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 18:37:00.461412: step 680, loss = 1.02 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 18:37:12.817096: step 690, loss = 1.12 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-16 18:37:25.267293: step 700, loss = 1.21 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 18:37:42.208800: step 710, loss = 1.05 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-16 18:37:54.683556: step 720, loss = 1.03 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-16 18:38:06.899774: step 730, loss = 1.21 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-16 18:38:19.090723: step 740, loss = 1.31 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 18:38:31.314636: step 750, loss = 0.97 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-16 18:38:43.833969: step 760, loss = 0.87 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-06-16 18:38:56.090176: step 770, loss = 0.99 (23.0 examples/sec; 1.307 sec/batch)\n",
      "2019-06-16 18:39:08.420954: step 780, loss = 1.13 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 18:39:20.821758: step 790, loss = 1.16 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 18:39:33.227631: step 800, loss = 1.16 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-16 18:39:50.183097: step 810, loss = 1.03 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 18:40:02.318426: step 820, loss = 1.12 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-16 18:40:14.638715: step 830, loss = 0.93 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 18:40:27.075091: step 840, loss = 1.04 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 18:40:39.552958: step 850, loss = 1.23 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 18:40:51.857287: step 860, loss = 1.10 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 18:41:04.318459: step 870, loss = 1.12 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-06-16 18:41:16.587212: step 880, loss = 1.07 (24.8 examples/sec; 1.210 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-16 18:41:28.993341: step 890, loss = 1.06 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-16 18:41:41.337842: step 900, loss = 0.94 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 18:41:57.934767: step 910, loss = 1.31 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 18:42:10.228006: step 920, loss = 1.33 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 18:42:22.533147: step 930, loss = 1.33 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 18:42:34.732100: step 940, loss = 0.97 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-16 18:42:46.891707: step 950, loss = 1.06 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 18:42:59.345787: step 960, loss = 1.14 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-16 18:43:11.731936: step 970, loss = 0.92 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 18:43:24.094725: step 980, loss = 0.97 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-06-16 18:43:36.460391: step 990, loss = 1.10 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 18:43:49.042520: step 1000, loss = 1.17 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-16 18:44:05.746575: step 1010, loss = 1.21 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 18:44:17.874899: step 1020, loss = 1.08 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 18:44:30.280639: step 1030, loss = 1.08 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 18:44:42.535833: step 1040, loss = 1.06 (22.6 examples/sec; 1.330 sec/batch)\n",
      "2019-06-16 18:44:54.852786: step 1050, loss = 1.40 (22.9 examples/sec; 1.313 sec/batch)\n",
      "2019-06-16 18:45:07.297206: step 1060, loss = 1.17 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 18:45:19.437537: step 1070, loss = 1.10 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-16 18:45:31.740863: step 1080, loss = 1.10 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 18:45:44.015796: step 1090, loss = 0.95 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-16 18:45:56.517116: step 1100, loss = 1.34 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-16 18:46:13.572419: step 1110, loss = 1.06 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 18:46:25.894606: step 1120, loss = 1.13 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 18:46:38.123098: step 1130, loss = 1.16 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 18:46:50.753028: step 1140, loss = 1.02 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-16 18:47:03.025140: step 1150, loss = 1.12 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 18:47:15.562619: step 1160, loss = 1.23 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-16 18:47:27.718848: step 1170, loss = 1.02 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 18:47:39.925213: step 1180, loss = 1.00 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 18:47:52.578873: step 1190, loss = 1.10 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 18:48:04.811631: step 1200, loss = 1.02 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 18:48:21.649984: step 1210, loss = 0.95 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 18:48:33.782428: step 1220, loss = 0.98 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 18:48:45.936708: step 1230, loss = 1.23 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-16 18:48:58.292656: step 1240, loss = 1.15 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 18:49:10.535621: step 1250, loss = 1.08 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-16 18:49:22.796528: step 1260, loss = 0.96 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 18:49:35.171521: step 1270, loss = 0.97 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 18:49:47.571704: step 1280, loss = 1.27 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 18:49:59.958562: step 1290, loss = 1.07 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-16 18:50:12.145664: step 1300, loss = 0.98 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 18:50:28.782538: step 1310, loss = 1.29 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 18:50:41.075710: step 1320, loss = 1.09 (23.6 examples/sec; 1.271 sec/batch)\n",
      "2019-06-16 18:50:53.451749: step 1330, loss = 0.88 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 18:51:05.733224: step 1340, loss = 1.27 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 18:51:17.880059: step 1350, loss = 1.18 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-16 18:51:30.081866: step 1360, loss = 1.01 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 18:51:42.249264: step 1370, loss = 1.05 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 18:51:54.941108: step 1380, loss = 1.07 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 18:52:07.419902: step 1390, loss = 1.11 (22.9 examples/sec; 1.307 sec/batch)\n",
      "2019-06-16 18:52:19.615073: step 1400, loss = 0.95 (23.1 examples/sec; 1.298 sec/batch)\n",
      "2019-06-16 18:52:36.417020: step 1410, loss = 1.10 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 18:52:48.790208: step 1420, loss = 1.05 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 18:53:01.218470: step 1430, loss = 0.93 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 18:53:13.533156: step 1440, loss = 1.12 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-16 18:53:25.981239: step 1450, loss = 1.12 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 18:53:38.273338: step 1460, loss = 1.29 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 18:53:50.619018: step 1470, loss = 0.96 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-16 18:54:03.119833: step 1480, loss = 1.13 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-16 18:54:15.379627: step 1490, loss = 1.00 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 18:54:27.710091: step 1500, loss = 0.93 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 18:54:44.394162: step 1510, loss = 0.94 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 18:54:56.567980: step 1520, loss = 1.09 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 18:55:08.872361: step 1530, loss = 1.08 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 18:55:21.056707: step 1540, loss = 1.21 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 18:55:33.896465: step 1550, loss = 1.15 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-16 18:55:46.281983: step 1560, loss = 0.97 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 18:55:58.500270: step 1570, loss = 1.26 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 18:56:10.891610: step 1580, loss = 1.28 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 18:56:23.049886: step 1590, loss = 1.12 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 18:56:35.273740: step 1600, loss = 1.04 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-06-16 18:56:52.076057: step 1610, loss = 1.06 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 18:57:04.404100: step 1620, loss = 0.98 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-06-16 18:57:16.780134: step 1630, loss = 1.18 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 18:57:28.918608: step 1640, loss = 1.08 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 18:57:41.162585: step 1650, loss = 0.96 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 18:57:53.378777: step 1660, loss = 1.27 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-16 18:58:05.889063: step 1670, loss = 0.95 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-16 18:58:18.003941: step 1680, loss = 1.28 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 18:58:30.547957: step 1690, loss = 1.10 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 18:58:42.737794: step 1700, loss = 1.09 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-16 18:58:59.340664: step 1710, loss = 1.21 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-06-16 18:59:11.491396: step 1720, loss = 1.05 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 18:59:23.756444: step 1730, loss = 0.99 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 18:59:36.010221: step 1740, loss = 1.03 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 18:59:48.152759: step 1750, loss = 0.90 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 19:00:00.392396: step 1760, loss = 1.12 (22.6 examples/sec; 1.330 sec/batch)\n",
      "2019-06-16 19:00:12.857588: step 1770, loss = 1.17 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-16 19:00:25.155856: step 1780, loss = 1.00 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 19:00:37.275436: step 1790, loss = 1.01 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 19:00:49.580996: step 1800, loss = 1.23 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-16 19:01:06.713569: step 1810, loss = 1.15 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 19:01:18.963340: step 1820, loss = 1.09 (24.7 examples/sec; 1.215 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-16 19:01:31.293646: step 1830, loss = 1.02 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 19:01:43.816342: step 1840, loss = 1.25 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 19:01:56.180982: step 1850, loss = 1.03 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-16 19:02:08.408319: step 1860, loss = 0.98 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 19:02:20.697828: step 1870, loss = 0.90 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 19:02:33.091678: step 1880, loss = 1.13 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 19:02:45.302708: step 1890, loss = 1.21 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 19:02:57.595991: step 1900, loss = 1.20 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 19:03:14.563000: step 1910, loss = 0.97 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 19:03:26.977671: step 1920, loss = 1.18 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 19:03:39.353039: step 1930, loss = 1.06 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-16 19:03:51.779020: step 1940, loss = 1.08 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 19:04:03.953932: step 1950, loss = 0.87 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 19:04:16.539851: step 1960, loss = 1.13 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-06-16 19:04:28.830971: step 1970, loss = 1.05 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 19:04:41.198079: step 1980, loss = 1.03 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 19:04:53.548360: step 1990, loss = 1.22 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 19:05:05.824072: step 2000, loss = 1.05 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 19:05:22.643142: step 2010, loss = 0.85 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-16 19:05:35.129556: step 2020, loss = 1.28 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 19:05:47.494364: step 2030, loss = 1.11 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-16 19:05:59.842151: step 2040, loss = 1.08 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-16 19:06:12.299169: step 2050, loss = 1.30 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 19:06:24.530916: step 2060, loss = 1.08 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 19:06:36.923802: step 2070, loss = 1.20 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 19:06:49.078803: step 2080, loss = 1.06 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 19:07:01.379320: step 2090, loss = 1.02 (23.7 examples/sec; 1.266 sec/batch)\n",
      "2019-06-16 19:07:13.754630: step 2100, loss = 1.12 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 19:07:30.844129: step 2110, loss = 1.22 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 19:07:43.193111: step 2120, loss = 1.36 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 19:07:55.543091: step 2130, loss = 1.38 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-16 19:08:07.708392: step 2140, loss = 1.27 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-16 19:08:20.138911: step 2150, loss = 1.01 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-16 19:08:32.574372: step 2160, loss = 1.22 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 19:08:44.999378: step 2170, loss = 0.97 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 19:08:57.336159: step 2180, loss = 1.11 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 19:09:09.746991: step 2190, loss = 1.04 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-16 19:09:22.045072: step 2200, loss = 1.05 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 19:09:38.871352: step 2210, loss = 0.84 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 19:09:51.131270: step 2220, loss = 1.31 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 19:10:03.560712: step 2230, loss = 1.15 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 19:10:15.763224: step 2240, loss = 0.89 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 19:10:28.052155: step 2250, loss = 1.09 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-16 19:10:40.386693: step 2260, loss = 1.10 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-16 19:10:52.780928: step 2270, loss = 0.93 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 19:11:05.056520: step 2280, loss = 1.05 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 19:11:17.445551: step 2290, loss = 0.95 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-16 19:11:29.856825: step 2300, loss = 1.08 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 19:11:46.866365: step 2310, loss = 1.02 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-16 19:11:59.186066: step 2320, loss = 1.18 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 19:12:11.615918: step 2330, loss = 1.23 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-16 19:12:23.828140: step 2340, loss = 0.91 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-16 19:12:36.253720: step 2350, loss = 0.86 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-16 19:12:48.536340: step 2360, loss = 1.18 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-06-16 19:13:00.906899: step 2370, loss = 0.99 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-16 19:13:13.547440: step 2380, loss = 1.25 (23.5 examples/sec; 1.275 sec/batch)\n",
      "2019-06-16 19:13:25.860862: step 2390, loss = 1.10 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-16 19:13:38.344850: step 2400, loss = 1.38 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-16 19:13:55.050883: step 2410, loss = 0.99 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-16 19:14:07.205016: step 2420, loss = 1.04 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 19:14:19.613626: step 2430, loss = 1.13 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 19:14:31.767263: step 2440, loss = 1.24 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 19:14:44.164569: step 2450, loss = 1.00 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 19:14:56.399308: step 2460, loss = 0.90 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 19:15:08.565125: step 2470, loss = 1.12 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 19:15:20.892120: step 2480, loss = 0.97 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 19:15:33.407421: step 2490, loss = 0.89 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 19:15:45.665718: step 2500, loss = 1.00 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 19:16:02.122717: step 2510, loss = 1.15 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 19:16:14.466595: step 2520, loss = 0.91 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 19:16:26.604924: step 2530, loss = 1.09 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 19:16:38.925234: step 2540, loss = 0.93 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 19:16:51.109871: step 2550, loss = 1.10 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 19:17:03.566487: step 2560, loss = 1.10 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-16 19:17:15.955852: step 2570, loss = 0.97 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 19:17:28.211310: step 2580, loss = 0.92 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-16 19:17:40.512456: step 2590, loss = 0.93 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-16 19:17:52.722397: step 2600, loss = 1.01 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 19:18:09.765962: step 2610, loss = 1.22 (23.2 examples/sec; 1.294 sec/batch)\n",
      "2019-06-16 19:18:22.258977: step 2620, loss = 1.26 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 19:18:34.657692: step 2630, loss = 1.25 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 19:18:46.921229: step 2640, loss = 1.05 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 19:18:59.072008: step 2650, loss = 1.03 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 19:19:11.414535: step 2660, loss = 0.96 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 19:19:23.641023: step 2670, loss = 1.33 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 19:19:35.919109: step 2680, loss = 1.33 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-16 19:19:48.430726: step 2690, loss = 1.04 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 19:20:00.881498: step 2700, loss = 1.19 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-16 19:20:17.901786: step 2710, loss = 1.09 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 19:20:30.341885: step 2720, loss = 0.92 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 19:20:42.833765: step 2730, loss = 0.91 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 19:20:55.077259: step 2740, loss = 1.29 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-16 19:21:07.428553: step 2750, loss = 1.09 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-16 19:21:19.684958: step 2760, loss = 1.16 (24.5 examples/sec; 1.224 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-16 19:21:31.847804: step 2770, loss = 1.04 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 19:21:44.108377: step 2780, loss = 1.05 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-16 19:21:56.669037: step 2790, loss = 1.26 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 19:22:08.885840: step 2800, loss = 0.95 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 19:22:25.427028: step 2810, loss = 0.99 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 19:22:37.563166: step 2820, loss = 1.05 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 19:22:49.807276: step 2830, loss = 1.17 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 19:23:02.400018: step 2840, loss = 1.21 (22.9 examples/sec; 1.307 sec/batch)\n",
      "2019-06-16 19:23:14.675124: step 2850, loss = 1.18 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 19:23:26.987410: step 2860, loss = 0.91 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 19:23:39.188126: step 2870, loss = 0.86 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 19:23:51.423710: step 2880, loss = 1.07 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 19:24:03.799731: step 2890, loss = 0.96 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-16 19:24:16.245919: step 2900, loss = 1.04 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-16 19:24:32.807247: step 2910, loss = 1.04 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 19:24:45.048815: step 2920, loss = 1.14 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 19:24:57.335879: step 2930, loss = 1.23 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 19:25:09.851984: step 2940, loss = 1.12 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-16 19:25:22.242648: step 2950, loss = 1.19 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 19:25:34.600051: step 2960, loss = 1.13 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 19:25:46.863719: step 2970, loss = 0.98 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-16 19:25:59.117660: step 2980, loss = 0.96 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-16 19:26:11.316297: step 2990, loss = 1.01 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 19:26:23.449262: step 3000, loss = 1.19 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 19:26:40.027884: step 3010, loss = 0.96 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 19:26:52.271941: step 3020, loss = 1.04 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 19:27:04.679189: step 3030, loss = 1.11 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-06-16 19:27:16.936951: step 3040, loss = 1.10 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 19:27:29.138489: step 3050, loss = 0.99 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 19:27:41.518834: step 3060, loss = 1.02 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 19:27:53.697209: step 3070, loss = 1.12 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-16 19:28:06.219591: step 3080, loss = 1.15 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-16 19:28:18.640134: step 3090, loss = 1.26 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-16 19:28:31.254194: step 3100, loss = 1.18 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-16 19:28:48.057342: step 3110, loss = 0.96 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 19:29:00.378116: step 3120, loss = 0.99 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-06-16 19:29:12.764868: step 3130, loss = 1.26 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 19:29:25.004206: step 3140, loss = 0.95 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 19:29:37.488369: step 3150, loss = 1.19 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-16 19:29:49.630741: step 3160, loss = 1.28 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 19:30:02.102119: step 3170, loss = 1.15 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-16 19:30:14.405089: step 3180, loss = 0.95 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-06-16 19:30:26.914876: step 3190, loss = 1.30 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-16 19:30:39.231861: step 3200, loss = 1.06 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 19:30:55.838296: step 3210, loss = 1.24 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 19:31:08.450071: step 3220, loss = 1.18 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 19:31:20.717347: step 3230, loss = 1.01 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 19:31:33.162049: step 3240, loss = 1.08 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-16 19:31:45.407439: step 3250, loss = 1.04 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 19:31:57.822489: step 3260, loss = 0.89 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-16 19:32:10.258348: step 3270, loss = 1.06 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 19:32:22.714339: step 3280, loss = 0.88 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 19:32:35.223061: step 3290, loss = 0.96 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-16 19:32:47.635871: step 3300, loss = 1.21 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 19:33:04.524503: step 3310, loss = 1.09 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 19:33:16.986852: step 3320, loss = 1.15 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 19:33:29.281702: step 3330, loss = 0.96 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 19:33:41.406429: step 3340, loss = 1.05 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 19:33:53.666936: step 3350, loss = 1.27 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 19:34:06.075191: step 3360, loss = 1.10 (22.5 examples/sec; 1.331 sec/batch)\n",
      "2019-06-16 19:34:18.550197: step 3370, loss = 0.96 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-16 19:34:31.129289: step 3380, loss = 0.94 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 19:34:43.389592: step 3390, loss = 1.26 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 19:34:55.708608: step 3400, loss = 1.32 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 19:35:12.536206: step 3410, loss = 1.13 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 19:35:24.805630: step 3420, loss = 1.32 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 19:35:36.925330: step 3430, loss = 0.99 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 19:35:49.032149: step 3440, loss = 1.10 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 19:36:01.302888: step 3450, loss = 0.99 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 19:36:13.535766: step 3460, loss = 1.05 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 19:36:26.178007: step 3470, loss = 1.16 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 19:36:38.339582: step 3480, loss = 0.99 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 19:36:50.800574: step 3490, loss = 1.00 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-06-16 19:37:03.148847: step 3500, loss = 1.03 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 19:37:20.033662: step 3510, loss = 1.12 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 19:37:32.379467: step 3520, loss = 1.07 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-16 19:37:44.766250: step 3530, loss = 1.18 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-16 19:37:57.277244: step 3540, loss = 1.12 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-16 19:38:09.482681: step 3550, loss = 0.94 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 19:38:21.833471: step 3560, loss = 1.04 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 19:38:34.174633: step 3570, loss = 1.00 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-16 19:38:46.402034: step 3580, loss = 1.16 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-06-16 19:38:58.582066: step 3590, loss = 1.02 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 19:39:10.942773: step 3600, loss = 1.06 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-06-16 19:39:27.778157: step 3610, loss = 0.94 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 19:39:40.089237: step 3620, loss = 0.95 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 19:39:52.526352: step 3630, loss = 1.06 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 19:40:04.968811: step 3640, loss = 1.20 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 19:40:17.349028: step 3650, loss = 0.91 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 19:40:29.617556: step 3660, loss = 1.05 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 19:40:41.910974: step 3670, loss = 1.11 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 19:40:54.446543: step 3680, loss = 1.06 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-16 19:41:07.060121: step 3690, loss = 1.15 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 19:41:19.541841: step 3700, loss = 1.03 (24.9 examples/sec; 1.207 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-16 19:41:36.449864: step 3710, loss = 1.03 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-16 19:41:48.971816: step 3720, loss = 1.26 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-16 19:42:01.258853: step 3730, loss = 1.28 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-16 19:42:13.749732: step 3740, loss = 1.02 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 19:42:26.254633: step 3750, loss = 1.04 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 19:42:38.533180: step 3760, loss = 1.10 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 19:42:50.743993: step 3770, loss = 1.39 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 19:43:02.997605: step 3780, loss = 0.90 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 19:43:15.142576: step 3790, loss = 1.10 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 19:43:27.406513: step 3800, loss = 1.01 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 19:43:44.195563: step 3810, loss = 0.99 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 19:43:56.446089: step 3820, loss = 1.02 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 19:44:08.746745: step 3830, loss = 1.28 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-16 19:44:21.154515: step 3840, loss = 0.94 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 19:44:33.328169: step 3850, loss = 1.09 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 19:44:45.647678: step 3860, loss = 0.99 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-16 19:44:57.998075: step 3870, loss = 1.18 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-16 19:45:10.458028: step 3880, loss = 1.17 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 19:45:22.704824: step 3890, loss = 1.06 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 19:45:35.036783: step 3900, loss = 1.10 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 19:45:51.415294: step 3910, loss = 1.09 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 19:46:03.905523: step 3920, loss = 1.08 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 19:46:16.261566: step 3930, loss = 0.91 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-16 19:46:28.525232: step 3940, loss = 1.06 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 19:46:40.944959: step 3950, loss = 0.97 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-16 19:46:53.370963: step 3960, loss = 1.03 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 19:47:05.616089: step 3970, loss = 1.19 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 19:47:17.963434: step 3980, loss = 1.25 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 19:47:30.234834: step 3990, loss = 1.10 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 19:47:42.702603: step 4000, loss = 0.98 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 19:47:59.758045: step 4010, loss = 1.25 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 19:48:12.042549: step 4020, loss = 0.97 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 19:48:24.201637: step 4030, loss = 1.01 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 19:48:36.617955: step 4040, loss = 1.11 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-16 19:48:49.072570: step 4050, loss = 1.33 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 19:49:01.278383: step 4060, loss = 0.99 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 19:49:13.393182: step 4070, loss = 1.07 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 19:49:25.736230: step 4080, loss = 1.20 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 19:49:38.132632: step 4090, loss = 1.05 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-16 19:49:50.417938: step 4100, loss = 0.97 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 19:50:06.990057: step 4110, loss = 0.98 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 19:50:19.428990: step 4120, loss = 1.10 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-16 19:50:31.601332: step 4130, loss = 1.23 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 19:50:44.041006: step 4140, loss = 1.06 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-16 19:50:56.402487: step 4150, loss = 1.21 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-06-16 19:51:08.755596: step 4160, loss = 0.92 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 19:51:21.243581: step 4170, loss = 0.99 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-16 19:51:33.766014: step 4180, loss = 1.02 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 19:51:46.158533: step 4190, loss = 1.22 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 19:51:58.486105: step 4200, loss = 1.25 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 19:52:15.251302: step 4210, loss = 1.04 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 19:52:27.500427: step 4220, loss = 0.97 (23.4 examples/sec; 1.282 sec/batch)\n",
      "2019-06-16 19:52:39.737659: step 4230, loss = 1.22 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 19:52:51.875205: step 4240, loss = 1.12 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-16 19:53:04.241308: step 4250, loss = 1.01 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 19:53:16.732107: step 4260, loss = 1.12 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 19:53:28.981815: step 4270, loss = 1.09 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 19:53:41.434428: step 4280, loss = 1.06 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-16 19:53:53.780104: step 4290, loss = 1.11 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-16 19:54:05.911983: step 4300, loss = 1.09 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-16 19:54:23.060227: step 4310, loss = 1.07 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 19:54:35.266482: step 4320, loss = 1.28 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 19:54:47.532372: step 4330, loss = 1.02 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 19:54:59.780825: step 4340, loss = 1.31 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 19:55:12.086373: step 4350, loss = 1.30 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 19:55:24.437541: step 4360, loss = 1.07 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 19:55:36.699314: step 4370, loss = 1.39 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 19:55:48.991536: step 4380, loss = 1.06 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 19:56:01.520421: step 4390, loss = 1.22 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 19:56:13.900144: step 4400, loss = 1.02 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-16 19:56:30.505892: step 4410, loss = 1.12 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-16 19:56:42.882817: step 4420, loss = 0.93 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 19:56:55.058076: step 4430, loss = 1.12 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 19:57:07.325142: step 4440, loss = 1.17 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 19:57:19.571764: step 4450, loss = 1.03 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-16 19:57:31.776580: step 4460, loss = 1.21 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 19:57:44.132945: step 4470, loss = 1.10 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 19:57:56.520755: step 4480, loss = 1.04 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 19:58:08.904718: step 4490, loss = 1.08 (23.7 examples/sec; 1.268 sec/batch)\n",
      "2019-06-16 19:58:21.359945: step 4500, loss = 1.22 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 19:58:38.029707: step 4510, loss = 1.01 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 19:58:50.462957: step 4520, loss = 0.87 (22.9 examples/sec; 1.307 sec/batch)\n",
      "2019-06-16 19:59:02.606464: step 4530, loss = 1.08 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 19:59:14.847447: step 4540, loss = 1.15 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-16 19:59:27.059360: step 4550, loss = 0.96 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 19:59:39.307001: step 4560, loss = 0.93 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 19:59:51.490405: step 4570, loss = 0.92 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 20:00:03.762617: step 4580, loss = 1.13 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 20:00:16.026587: step 4590, loss = 1.01 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 20:00:28.266077: step 4600, loss = 1.07 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 20:00:44.968944: step 4610, loss = 1.23 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-16 20:00:57.247787: step 4620, loss = 1.12 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 20:01:09.518911: step 4630, loss = 1.00 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 20:01:21.871533: step 4640, loss = 1.02 (24.8 examples/sec; 1.207 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-16 20:01:34.380407: step 4650, loss = 0.98 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 20:01:46.674430: step 4660, loss = 1.09 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 20:01:59.015004: step 4670, loss = 1.08 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 20:02:11.453860: step 4680, loss = 1.15 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 20:02:23.692323: step 4690, loss = 1.14 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 20:02:36.157981: step 4700, loss = 1.02 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 20:02:53.340391: step 4710, loss = 1.06 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-16 20:03:05.608589: step 4720, loss = 1.14 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 20:03:17.945295: step 4730, loss = 1.02 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 20:03:30.360924: step 4740, loss = 1.14 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 20:03:42.575878: step 4750, loss = 1.18 (23.0 examples/sec; 1.307 sec/batch)\n",
      "2019-06-16 20:03:54.936719: step 4760, loss = 1.24 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-06-16 20:04:07.276924: step 4770, loss = 0.95 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 20:04:19.720810: step 4780, loss = 1.09 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 20:04:32.105238: step 4790, loss = 1.00 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 20:04:44.405449: step 4800, loss = 1.08 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 20:05:00.975041: step 4810, loss = 1.04 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 20:05:13.373268: step 4820, loss = 0.99 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 20:05:25.695587: step 4830, loss = 0.94 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-16 20:05:37.954218: step 4840, loss = 1.11 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 20:05:50.285912: step 4850, loss = 1.01 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 20:06:02.458830: step 4860, loss = 1.36 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 20:06:14.913336: step 4870, loss = 1.11 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 20:06:27.197890: step 4880, loss = 1.19 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-06-16 20:06:39.514079: step 4890, loss = 0.94 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 20:06:51.865363: step 4900, loss = 0.94 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 20:07:08.466330: step 4910, loss = 1.26 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 20:07:21.070408: step 4920, loss = 1.36 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-16 20:07:33.364900: step 4930, loss = 1.06 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 20:07:45.802651: step 4940, loss = 1.11 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-06-16 20:07:58.005169: step 4950, loss = 1.25 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 20:08:10.683973: step 4960, loss = 1.03 (22.6 examples/sec; 1.330 sec/batch)\n",
      "2019-06-16 20:08:23.137414: step 4970, loss = 1.08 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-06-16 20:08:35.507463: step 4980, loss = 1.01 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-06-16 20:08:47.892336: step 4990, loss = 1.18 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 20:09:00.246670: step 5000, loss = 1.43 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 20:09:20.430871: step 5010, loss = 1.08 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 20:09:32.679662: step 5020, loss = 1.11 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 20:09:45.034112: step 5030, loss = 1.08 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 20:09:57.377714: step 5040, loss = 0.98 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-16 20:10:09.559044: step 5050, loss = 0.95 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 20:10:21.980847: step 5060, loss = 1.17 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-16 20:10:34.333957: step 5070, loss = 1.18 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 20:10:46.801561: step 5080, loss = 1.16 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 20:10:59.035273: step 5090, loss = 0.98 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-06-16 20:11:11.301136: step 5100, loss = 1.06 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 20:11:28.272216: step 5110, loss = 1.06 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 20:11:40.531259: step 5120, loss = 0.93 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 20:11:52.865563: step 5130, loss = 1.09 (23.3 examples/sec; 1.289 sec/batch)\n",
      "2019-06-16 20:12:05.067222: step 5140, loss = 1.05 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 20:12:17.465183: step 5150, loss = 1.15 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-06-16 20:12:29.737218: step 5160, loss = 1.19 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 20:12:41.883803: step 5170, loss = 1.18 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 20:12:54.193953: step 5180, loss = 1.14 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 20:13:06.752935: step 5190, loss = 1.05 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 20:13:19.051723: step 5200, loss = 1.10 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-06-16 20:13:36.223965: step 5210, loss = 1.14 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 20:13:48.468555: step 5220, loss = 0.97 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 20:14:00.827756: step 5230, loss = 1.06 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 20:14:13.347346: step 5240, loss = 1.29 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-16 20:14:25.822018: step 5250, loss = 0.99 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 20:14:37.989055: step 5260, loss = 1.03 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 20:14:50.151536: step 5270, loss = 1.02 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 20:15:02.596100: step 5280, loss = 1.17 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 20:15:14.735096: step 5290, loss = 0.92 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 20:15:26.937831: step 5300, loss = 1.17 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 20:15:43.516166: step 5310, loss = 1.11 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-06-16 20:15:55.633800: step 5320, loss = 1.30 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 20:16:08.309944: step 5330, loss = 1.01 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 20:16:20.899506: step 5340, loss = 0.92 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-16 20:16:33.167318: step 5350, loss = 1.22 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 20:16:45.713213: step 5360, loss = 1.04 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 20:16:58.147090: step 5370, loss = 0.91 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-06-16 20:17:10.620286: step 5380, loss = 1.18 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 20:17:22.955703: step 5390, loss = 0.92 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 20:17:35.507540: step 5400, loss = 1.20 (23.1 examples/sec; 1.300 sec/batch)\n",
      "2019-06-16 20:17:52.168659: step 5410, loss = 1.01 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-16 20:18:04.568067: step 5420, loss = 1.24 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 20:18:16.745671: step 5430, loss = 0.96 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 20:18:28.964579: step 5440, loss = 0.89 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 20:18:41.531702: step 5450, loss = 1.04 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 20:18:54.081664: step 5460, loss = 0.93 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-16 20:19:06.394524: step 5470, loss = 1.07 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 20:19:18.625858: step 5480, loss = 1.33 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 20:19:30.999680: step 5490, loss = 1.16 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 20:19:43.544375: step 5500, loss = 1.01 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-16 20:20:00.177352: step 5510, loss = 1.22 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-16 20:20:12.506875: step 5520, loss = 0.94 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 20:20:24.794277: step 5530, loss = 0.95 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-16 20:20:37.345119: step 5540, loss = 1.25 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 20:20:49.691897: step 5550, loss = 0.93 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-16 20:21:02.087818: step 5560, loss = 1.15 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-06-16 20:21:14.281696: step 5570, loss = 1.12 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 20:21:26.537992: step 5580, loss = 1.04 (24.8 examples/sec; 1.211 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-16 20:21:38.776794: step 5590, loss = 1.17 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 20:21:51.174640: step 5600, loss = 1.10 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 20:22:07.762526: step 5610, loss = 1.55 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 20:22:20.155512: step 5620, loss = 1.07 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 20:22:32.359762: step 5630, loss = 1.23 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 20:22:44.916100: step 5640, loss = 1.17 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-16 20:22:57.456937: step 5650, loss = 1.03 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 20:23:09.732851: step 5660, loss = 1.07 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 20:23:22.184041: step 5670, loss = 1.01 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-06-16 20:23:34.379830: step 5680, loss = 1.22 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 20:23:46.898154: step 5690, loss = 1.21 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-16 20:23:59.321659: step 5700, loss = 0.91 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-16 20:24:15.767468: step 5710, loss = 0.87 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-06-16 20:24:28.119958: step 5720, loss = 1.07 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 20:24:40.527058: step 5730, loss = 1.09 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-16 20:24:53.092424: step 5740, loss = 1.03 (23.5 examples/sec; 1.278 sec/batch)\n",
      "2019-06-16 20:25:05.535899: step 5750, loss = 1.24 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 20:25:17.841777: step 5760, loss = 1.00 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 20:25:30.048223: step 5770, loss = 1.02 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 20:25:42.352796: step 5780, loss = 1.36 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 20:25:54.711945: step 5790, loss = 1.04 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-06-16 20:26:06.963731: step 5800, loss = 1.02 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 20:26:23.783451: step 5810, loss = 1.20 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 20:26:36.164507: step 5820, loss = 1.14 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 20:26:48.439408: step 5830, loss = 0.97 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 20:27:00.651141: step 5840, loss = 1.15 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 20:27:12.895936: step 5850, loss = 1.09 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 20:27:25.092360: step 5860, loss = 1.01 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 20:27:37.325522: step 5870, loss = 1.02 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 20:27:49.695962: step 5880, loss = 1.17 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 20:28:02.277703: step 5890, loss = 0.89 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 20:28:14.461362: step 5900, loss = 1.01 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 20:28:31.069234: step 5910, loss = 1.29 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 20:28:43.489195: step 5920, loss = 1.00 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 20:28:55.861114: step 5930, loss = 1.17 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 20:29:08.389331: step 5940, loss = 1.11 (22.9 examples/sec; 1.307 sec/batch)\n",
      "2019-06-16 20:29:20.906614: step 5950, loss = 0.98 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 20:29:33.245205: step 5960, loss = 1.26 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 20:29:45.604097: step 5970, loss = 1.09 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 20:29:57.897363: step 5980, loss = 1.06 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-16 20:30:10.468390: step 5990, loss = 1.09 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-16 20:30:22.912533: step 6000, loss = 1.02 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-16 20:30:39.747616: step 6010, loss = 1.33 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 20:30:51.970484: step 6020, loss = 1.15 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 20:31:04.232170: step 6030, loss = 1.06 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 20:31:16.627712: step 6040, loss = 1.39 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 20:31:29.054044: step 6050, loss = 1.17 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 20:31:41.495930: step 6060, loss = 1.17 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 20:31:53.913158: step 6070, loss = 1.17 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 20:32:06.348687: step 6080, loss = 1.19 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 20:32:18.699684: step 6090, loss = 1.25 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-16 20:32:30.888797: step 6100, loss = 1.21 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 20:32:47.318822: step 6110, loss = 1.18 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 20:32:59.575549: step 6120, loss = 1.05 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-16 20:33:11.689479: step 6130, loss = 0.99 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 20:33:23.922954: step 6140, loss = 1.02 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 20:33:36.380258: step 6150, loss = 1.24 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 20:33:48.748951: step 6160, loss = 1.05 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 20:34:01.032036: step 6170, loss = 1.05 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 20:34:13.375661: step 6180, loss = 0.97 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 20:34:25.680992: step 6190, loss = 0.95 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 20:34:38.029184: step 6200, loss = 1.10 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 20:34:55.015967: step 6210, loss = 1.40 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 20:35:07.348522: step 6220, loss = 1.00 (23.5 examples/sec; 1.277 sec/batch)\n",
      "2019-06-16 20:35:19.447474: step 6230, loss = 1.04 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 20:35:31.557447: step 6240, loss = 1.11 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 20:35:44.024542: step 6250, loss = 1.14 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 20:35:56.459466: step 6260, loss = 1.17 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-16 20:36:08.693514: step 6270, loss = 1.00 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 20:36:21.031941: step 6280, loss = 0.98 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 20:36:33.575319: step 6290, loss = 1.18 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 20:36:46.184579: step 6300, loss = 1.03 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 20:37:02.975951: step 6310, loss = 1.24 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 20:37:15.390327: step 6320, loss = 1.24 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-16 20:37:27.634721: step 6330, loss = 0.99 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-16 20:37:39.996738: step 6340, loss = 1.08 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-16 20:37:52.490046: step 6350, loss = 1.02 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 20:38:05.006512: step 6360, loss = 1.11 (23.8 examples/sec; 1.258 sec/batch)\n",
      "2019-06-16 20:38:17.279756: step 6370, loss = 1.13 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 20:38:29.677702: step 6380, loss = 1.04 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-16 20:38:41.809395: step 6390, loss = 1.18 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 20:38:54.177656: step 6400, loss = 1.24 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-16 20:39:11.025084: step 6410, loss = 1.03 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 20:39:23.268102: step 6420, loss = 1.11 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 20:39:35.464687: step 6430, loss = 0.94 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 20:39:47.790106: step 6440, loss = 1.09 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-16 20:40:00.092268: step 6450, loss = 1.25 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 20:40:12.387011: step 6460, loss = 0.99 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-16 20:40:24.722433: step 6470, loss = 1.09 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-16 20:40:36.991313: step 6480, loss = 1.10 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-16 20:40:49.243478: step 6490, loss = 1.04 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 20:41:01.509263: step 6500, loss = 0.95 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 20:41:18.545253: step 6510, loss = 1.07 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 20:41:31.057633: step 6520, loss = 1.18 (24.7 examples/sec; 1.214 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-16 20:41:43.290643: step 6530, loss = 1.25 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 20:41:55.498680: step 6540, loss = 0.92 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-16 20:42:07.606828: step 6550, loss = 1.04 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 20:42:19.765657: step 6560, loss = 1.13 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 20:42:32.147490: step 6570, loss = 0.89 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-16 20:42:44.474803: step 6580, loss = 1.14 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-16 20:42:56.647436: step 6590, loss = 1.24 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 20:43:09.243742: step 6600, loss = 1.01 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 20:43:25.903964: step 6610, loss = 1.13 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 20:43:38.052123: step 6620, loss = 1.12 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 20:43:50.512975: step 6630, loss = 1.17 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 20:44:02.784715: step 6640, loss = 1.06 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 20:44:14.939811: step 6650, loss = 1.28 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 20:44:27.485564: step 6660, loss = 1.25 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-16 20:44:39.847561: step 6670, loss = 1.18 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 20:44:52.078625: step 6680, loss = 1.18 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-06-16 20:45:04.632394: step 6690, loss = 1.11 (22.5 examples/sec; 1.333 sec/batch)\n",
      "2019-06-16 20:45:16.939846: step 6700, loss = 1.23 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 20:45:34.037016: step 6710, loss = 1.06 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 20:45:46.254978: step 6720, loss = 1.24 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-16 20:45:58.670352: step 6730, loss = 1.17 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 20:46:11.115593: step 6740, loss = 1.00 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-16 20:46:23.394679: step 6750, loss = 0.98 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 20:46:35.673993: step 6760, loss = 1.17 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 20:46:47.969876: step 6770, loss = 1.01 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-06-16 20:47:00.259647: step 6780, loss = 1.20 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-16 20:47:12.373517: step 6790, loss = 1.03 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-16 20:47:24.612977: step 6800, loss = 1.11 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-16 20:47:41.432515: step 6810, loss = 1.15 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 20:47:53.889333: step 6820, loss = 1.06 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-16 20:48:06.160230: step 6830, loss = 1.07 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-16 20:48:18.374813: step 6840, loss = 1.12 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 20:48:30.692346: step 6850, loss = 1.19 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 20:48:43.166051: step 6860, loss = 1.19 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-16 20:48:55.478336: step 6870, loss = 1.04 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 20:49:08.169486: step 6880, loss = 0.96 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-06-16 20:49:20.692729: step 6890, loss = 1.02 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 20:49:33.002353: step 6900, loss = 0.99 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 20:49:49.861473: step 6910, loss = 1.06 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 20:50:02.162461: step 6920, loss = 1.14 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 20:50:14.594903: step 6930, loss = 1.20 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 20:50:27.071913: step 6940, loss = 0.97 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-16 20:50:39.523621: step 6950, loss = 1.11 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 20:50:51.772164: step 6960, loss = 1.04 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 20:51:04.033460: step 6970, loss = 1.03 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 20:51:16.368170: step 6980, loss = 1.19 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 20:51:28.772791: step 6990, loss = 1.11 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 20:51:41.110648: step 7000, loss = 1.06 (23.1 examples/sec; 1.298 sec/batch)\n",
      "2019-06-16 20:51:58.284918: step 7010, loss = 1.29 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-16 20:52:10.526397: step 7020, loss = 0.98 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 20:52:22.734925: step 7030, loss = 1.07 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 20:52:34.913237: step 7040, loss = 1.07 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-16 20:52:47.214865: step 7050, loss = 0.99 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 20:52:59.611375: step 7060, loss = 1.07 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 20:53:12.164946: step 7070, loss = 1.04 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 20:53:24.375816: step 7080, loss = 1.11 (23.2 examples/sec; 1.295 sec/batch)\n",
      "2019-06-16 20:53:36.746992: step 7090, loss = 1.05 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 20:53:49.027204: step 7100, loss = 1.18 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 20:54:05.892110: step 7110, loss = 1.05 (22.5 examples/sec; 1.335 sec/batch)\n",
      "2019-06-16 20:54:18.177191: step 7120, loss = 0.92 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 20:54:30.444369: step 7130, loss = 1.00 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-06-16 20:54:42.741284: step 7140, loss = 1.13 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 20:54:54.951857: step 7150, loss = 1.17 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 20:55:07.162677: step 7160, loss = 1.03 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-06-16 20:55:19.496575: step 7170, loss = 0.94 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 20:55:31.871408: step 7180, loss = 0.99 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 20:55:44.089844: step 7190, loss = 0.96 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 20:55:56.482042: step 7200, loss = 1.02 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-16 20:56:13.164025: step 7210, loss = 1.10 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-06-16 20:56:25.628526: step 7220, loss = 1.03 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 20:56:37.999705: step 7230, loss = 0.93 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 20:56:50.328136: step 7240, loss = 1.16 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-16 20:57:02.743497: step 7250, loss = 1.17 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 20:57:15.016405: step 7260, loss = 1.35 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 20:57:27.413711: step 7270, loss = 0.93 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-16 20:57:39.606295: step 7280, loss = 0.99 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-06-16 20:57:51.916615: step 7290, loss = 1.16 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-16 20:58:04.239658: step 7300, loss = 1.16 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 20:58:20.962170: step 7310, loss = 1.21 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 20:58:33.295070: step 7320, loss = 1.07 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 20:58:45.698626: step 7330, loss = 1.20 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-16 20:58:58.101284: step 7340, loss = 1.26 (23.5 examples/sec; 1.277 sec/batch)\n",
      "2019-06-16 20:59:10.648803: step 7350, loss = 1.00 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 20:59:23.038568: step 7360, loss = 0.91 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-16 20:59:35.256917: step 7370, loss = 1.17 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-16 20:59:47.642896: step 7380, loss = 1.34 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-16 20:59:59.951920: step 7390, loss = 1.34 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-06-16 21:00:12.316890: step 7400, loss = 1.08 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-16 21:00:29.035615: step 7410, loss = 1.01 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 21:00:41.311829: step 7420, loss = 0.95 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 21:00:53.447096: step 7430, loss = 1.17 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 21:01:05.632013: step 7440, loss = 0.90 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 21:01:18.136114: step 7450, loss = 0.91 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 21:01:30.332140: step 7460, loss = 1.02 (24.5 examples/sec; 1.223 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-16 21:01:42.665923: step 7470, loss = 0.96 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 21:01:54.917137: step 7480, loss = 1.10 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-16 21:02:07.340500: step 7490, loss = 1.03 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-16 21:02:19.505803: step 7500, loss = 1.04 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 21:02:36.364426: step 7510, loss = 1.16 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 21:02:48.789111: step 7520, loss = 1.15 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 21:03:01.063973: step 7530, loss = 0.96 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 21:03:13.364207: step 7540, loss = 1.02 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 21:03:25.621469: step 7550, loss = 1.16 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 21:03:37.821181: step 7560, loss = 1.12 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-16 21:03:49.969400: step 7570, loss = 1.27 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 21:04:02.347880: step 7580, loss = 0.91 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 21:04:14.611248: step 7590, loss = 1.12 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 21:04:26.763492: step 7600, loss = 1.16 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 21:04:43.451286: step 7610, loss = 0.98 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 21:04:55.876437: step 7620, loss = 1.17 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 21:05:08.167979: step 7630, loss = 0.94 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-06-16 21:05:20.653657: step 7640, loss = 0.93 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 21:05:33.004923: step 7650, loss = 1.36 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-16 21:05:45.424476: step 7660, loss = 1.04 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-16 21:05:57.634506: step 7670, loss = 1.06 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-16 21:06:09.917490: step 7680, loss = 1.07 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 21:06:22.195670: step 7690, loss = 1.21 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 21:06:34.719784: step 7700, loss = 0.87 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-06-16 21:06:51.557260: step 7710, loss = 1.00 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 21:07:03.913961: step 7720, loss = 1.05 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-16 21:07:16.475395: step 7730, loss = 1.08 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 21:07:28.853581: step 7740, loss = 1.01 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 21:07:41.217998: step 7750, loss = 1.14 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 21:07:53.451555: step 7760, loss = 1.16 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 21:08:05.873799: step 7770, loss = 1.08 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 21:08:18.331577: step 7780, loss = 1.28 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-06-16 21:08:30.677439: step 7790, loss = 1.10 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 21:08:43.028010: step 7800, loss = 1.16 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 21:09:00.002598: step 7810, loss = 0.99 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 21:09:12.247181: step 7820, loss = 1.09 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-16 21:09:24.420165: step 7830, loss = 1.06 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 21:09:36.650123: step 7840, loss = 1.04 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-06-16 21:09:48.810451: step 7850, loss = 1.02 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 21:10:01.296042: step 7860, loss = 1.03 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 21:10:13.670188: step 7870, loss = 1.04 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 21:10:25.889079: step 7880, loss = 0.99 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 21:10:38.234735: step 7890, loss = 1.07 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 21:10:50.525402: step 7900, loss = 0.98 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-16 21:11:07.336855: step 7910, loss = 0.94 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 21:11:19.588572: step 7920, loss = 1.05 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 21:11:31.957090: step 7930, loss = 1.24 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-16 21:11:44.140808: step 7940, loss = 1.02 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 21:11:56.404222: step 7950, loss = 1.12 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 21:12:08.796168: step 7960, loss = 0.93 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 21:12:21.317132: step 7970, loss = 0.98 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-06-16 21:12:33.477510: step 7980, loss = 1.18 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 21:12:46.152115: step 7990, loss = 0.99 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-16 21:12:58.347959: step 8000, loss = 1.28 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 21:13:15.005177: step 8010, loss = 1.21 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 21:13:27.456941: step 8020, loss = 1.07 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 21:13:39.796027: step 8030, loss = 0.97 (23.6 examples/sec; 1.272 sec/batch)\n",
      "2019-06-16 21:13:52.151106: step 8040, loss = 0.99 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 21:14:04.351607: step 8050, loss = 0.98 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 21:14:16.660515: step 8060, loss = 1.25 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 21:14:29.008554: step 8070, loss = 0.99 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 21:14:41.194242: step 8080, loss = 1.06 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 21:14:53.598835: step 8090, loss = 1.12 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 21:15:05.913434: step 8100, loss = 1.15 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 21:15:22.429218: step 8110, loss = 0.95 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 21:15:34.584083: step 8120, loss = 0.92 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 21:15:46.927995: step 8130, loss = 1.15 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 21:15:59.142164: step 8140, loss = 1.18 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-16 21:16:11.529730: step 8150, loss = 1.00 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-16 21:16:23.767801: step 8160, loss = 1.42 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-16 21:16:36.259485: step 8170, loss = 1.08 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 21:16:48.392522: step 8180, loss = 1.17 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-16 21:17:00.639546: step 8190, loss = 1.09 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 21:17:12.943925: step 8200, loss = 1.18 (23.4 examples/sec; 1.283 sec/batch)\n",
      "2019-06-16 21:17:29.239193: step 8210, loss = 1.04 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 21:17:41.638252: step 8220, loss = 1.05 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-16 21:17:53.961033: step 8230, loss = 1.28 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 21:18:06.212728: step 8240, loss = 0.91 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-16 21:18:18.473110: step 8250, loss = 1.02 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 21:18:30.869756: step 8260, loss = 1.13 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 21:18:43.428924: step 8270, loss = 1.23 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 21:18:55.731880: step 8280, loss = 1.06 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 21:19:08.061988: step 8290, loss = 0.91 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 21:19:20.379578: step 8300, loss = 0.90 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-16 21:19:36.939876: step 8310, loss = 1.10 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-06-16 21:19:49.381712: step 8320, loss = 0.95 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 21:20:01.803983: step 8330, loss = 1.04 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 21:20:14.181992: step 8340, loss = 0.91 (23.5 examples/sec; 1.279 sec/batch)\n",
      "2019-06-16 21:20:26.494237: step 8350, loss = 0.91 (23.1 examples/sec; 1.297 sec/batch)\n",
      "2019-06-16 21:20:38.926405: step 8360, loss = 1.03 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-16 21:20:51.173558: step 8370, loss = 1.10 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 21:21:03.559025: step 8380, loss = 1.17 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-06-16 21:21:16.060168: step 8390, loss = 1.08 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-16 21:21:28.270580: step 8400, loss = 1.11 (24.8 examples/sec; 1.211 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-16 21:21:44.557347: step 8410, loss = 1.28 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 21:21:56.819200: step 8420, loss = 1.11 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 21:22:09.140157: step 8430, loss = 0.98 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-16 21:22:21.328657: step 8440, loss = 1.05 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 21:22:33.502678: step 8450, loss = 1.23 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 21:22:45.835713: step 8460, loss = 1.06 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 21:22:58.167801: step 8470, loss = 1.06 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-16 21:23:10.424500: step 8480, loss = 0.92 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 21:23:22.776284: step 8490, loss = 1.09 (23.2 examples/sec; 1.295 sec/batch)\n",
      "2019-06-16 21:23:34.927100: step 8500, loss = 1.02 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 21:23:51.877339: step 8510, loss = 1.01 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 21:24:04.437100: step 8520, loss = 1.19 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-16 21:24:16.618194: step 8530, loss = 1.23 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 21:24:28.840827: step 8540, loss = 0.97 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-16 21:24:41.434105: step 8550, loss = 1.23 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-16 21:24:53.884786: step 8560, loss = 1.01 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-16 21:25:06.109641: step 8570, loss = 1.12 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-16 21:25:18.262419: step 8580, loss = 1.24 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 21:25:30.514352: step 8590, loss = 1.05 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 21:25:42.799971: step 8600, loss = 1.04 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 21:25:59.387456: step 8610, loss = 1.23 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 21:26:11.546528: step 8620, loss = 1.05 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 21:26:23.964734: step 8630, loss = 1.08 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 21:26:36.210132: step 8640, loss = 1.02 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-16 21:26:48.475327: step 8650, loss = 1.00 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 21:27:00.833028: step 8660, loss = 1.16 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-06-16 21:27:13.174720: step 8670, loss = 1.14 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-16 21:27:25.430776: step 8680, loss = 0.91 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 21:27:37.768351: step 8690, loss = 1.24 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 21:27:50.078470: step 8700, loss = 1.23 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 21:28:06.814884: step 8710, loss = 0.99 (23.5 examples/sec; 1.278 sec/batch)\n",
      "2019-06-16 21:28:19.066318: step 8720, loss = 0.85 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 21:28:31.456978: step 8730, loss = 1.11 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-06-16 21:28:43.649977: step 8740, loss = 1.20 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-16 21:28:56.097940: step 8750, loss = 1.22 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-16 21:29:08.187598: step 8760, loss = 1.23 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 21:29:20.420579: step 8770, loss = 1.20 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-06-16 21:29:32.568332: step 8780, loss = 1.06 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 21:29:44.810478: step 8790, loss = 1.10 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 21:29:57.084279: step 8800, loss = 1.15 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 21:30:13.705932: step 8810, loss = 1.03 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 21:30:26.215319: step 8820, loss = 1.03 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-16 21:30:38.518911: step 8830, loss = 1.11 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 21:30:50.737107: step 8840, loss = 0.97 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 21:31:02.964433: step 8850, loss = 0.96 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-16 21:31:15.184688: step 8860, loss = 1.09 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 21:31:27.531661: step 8870, loss = 0.91 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-16 21:31:40.060962: step 8880, loss = 1.11 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-16 21:31:52.446127: step 8890, loss = 1.17 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-16 21:32:04.800385: step 8900, loss = 1.15 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-16 21:32:21.198762: step 8910, loss = 1.06 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-16 21:32:33.538010: step 8920, loss = 1.04 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-16 21:32:45.693469: step 8930, loss = 1.11 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-16 21:32:58.044822: step 8940, loss = 1.07 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-16 21:33:10.216570: step 8950, loss = 1.58 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 21:33:22.485762: step 8960, loss = 1.05 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 21:33:34.786762: step 8970, loss = 0.98 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 21:33:47.145645: step 8980, loss = 1.15 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 21:33:59.496078: step 8990, loss = 1.09 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-16 21:34:11.762771: step 9000, loss = 1.03 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 21:34:28.340108: step 9010, loss = 1.20 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 21:34:40.627998: step 9020, loss = 1.19 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-06-16 21:34:52.884232: step 9030, loss = 1.13 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 21:35:05.183485: step 9040, loss = 0.88 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 21:35:17.520172: step 9050, loss = 0.94 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 21:35:29.768721: step 9060, loss = 1.14 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 21:35:41.931931: step 9070, loss = 1.29 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 21:35:54.204007: step 9080, loss = 1.02 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 21:36:06.557869: step 9090, loss = 1.02 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 21:36:18.890479: step 9100, loss = 1.22 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 21:36:35.532640: step 9110, loss = 0.87 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 21:36:47.974060: step 9120, loss = 1.04 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 21:37:00.188725: step 9130, loss = 1.04 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-16 21:37:12.497120: step 9140, loss = 0.92 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 21:37:24.808519: step 9150, loss = 1.07 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 21:37:37.292937: step 9160, loss = 1.10 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-16 21:37:49.855166: step 9170, loss = 1.08 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 21:38:02.251201: step 9180, loss = 1.05 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 21:38:14.515782: step 9190, loss = 0.88 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 21:38:26.803152: step 9200, loss = 1.35 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 21:38:43.602474: step 9210, loss = 0.96 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 21:38:55.974990: step 9220, loss = 1.01 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-16 21:39:08.247488: step 9230, loss = 1.34 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 21:39:20.427359: step 9240, loss = 1.09 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 21:39:32.736833: step 9250, loss = 1.15 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 21:39:45.137067: step 9260, loss = 1.20 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-16 21:39:57.522155: step 9270, loss = 1.08 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-16 21:40:09.844552: step 9280, loss = 1.22 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 21:40:22.296610: step 9290, loss = 1.03 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 21:40:34.643522: step 9300, loss = 1.11 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-16 21:40:51.415036: step 9310, loss = 1.11 (23.1 examples/sec; 1.298 sec/batch)\n",
      "2019-06-16 21:41:03.717188: step 9320, loss = 1.25 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 21:41:16.015090: step 9330, loss = 1.09 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 21:41:28.536313: step 9340, loss = 1.00 (22.7 examples/sec; 1.324 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-16 21:41:40.895367: step 9350, loss = 0.99 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 21:41:53.086835: step 9360, loss = 0.96 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-16 21:42:05.640946: step 9370, loss = 0.93 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 21:42:17.808968: step 9380, loss = 1.12 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 21:42:30.196763: step 9390, loss = 0.93 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 21:42:42.694076: step 9400, loss = 1.11 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 21:42:59.404819: step 9410, loss = 1.06 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 21:43:11.697765: step 9420, loss = 1.27 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 21:43:24.338803: step 9430, loss = 0.93 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 21:43:36.552600: step 9440, loss = 0.98 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 21:43:49.135072: step 9450, loss = 1.16 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-16 21:44:01.503698: step 9460, loss = 1.12 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-16 21:44:14.047523: step 9470, loss = 1.12 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 21:44:26.572125: step 9480, loss = 1.12 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 21:44:38.876090: step 9490, loss = 0.97 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 21:44:51.392257: step 9500, loss = 1.09 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 21:45:08.098030: step 9510, loss = 1.29 (23.5 examples/sec; 1.275 sec/batch)\n",
      "2019-06-16 21:45:20.334042: step 9520, loss = 1.03 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-16 21:45:32.552618: step 9530, loss = 1.06 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 21:45:44.755281: step 9540, loss = 1.03 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 21:45:57.150365: step 9550, loss = 1.08 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-16 21:46:09.387306: step 9560, loss = 1.10 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-06-16 21:46:21.866181: step 9570, loss = 1.07 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 21:46:34.189941: step 9580, loss = 1.14 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 21:46:46.330607: step 9590, loss = 0.99 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 21:46:58.816906: step 9600, loss = 1.04 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-16 21:47:15.683478: step 9610, loss = 1.06 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-16 21:47:27.909788: step 9620, loss = 1.10 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 21:47:40.319480: step 9630, loss = 1.10 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 21:47:52.613047: step 9640, loss = 1.13 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 21:48:04.972543: step 9650, loss = 1.26 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 21:48:17.242164: step 9660, loss = 1.11 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-16 21:48:29.564738: step 9670, loss = 1.04 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 21:48:41.777728: step 9680, loss = 1.13 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 21:48:54.415729: step 9690, loss = 1.02 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-16 21:49:06.771846: step 9700, loss = 0.91 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 21:49:23.777600: step 9710, loss = 1.08 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 21:49:36.120675: step 9720, loss = 1.07 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-16 21:49:48.505536: step 9730, loss = 1.36 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 21:50:00.669674: step 9740, loss = 1.03 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-16 21:50:13.119090: step 9750, loss = 1.24 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 21:50:25.386962: step 9760, loss = 0.96 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 21:50:37.947608: step 9770, loss = 1.13 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-06-16 21:50:50.219578: step 9780, loss = 1.07 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-06-16 21:51:02.469691: step 9790, loss = 1.05 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 21:51:14.605657: step 9800, loss = 1.32 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 21:51:31.568922: step 9810, loss = 1.02 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 21:51:43.836816: step 9820, loss = 1.02 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-16 21:51:56.253685: step 9830, loss = 1.13 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 21:52:08.368610: step 9840, loss = 1.09 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 21:52:20.784112: step 9850, loss = 1.05 (22.5 examples/sec; 1.331 sec/batch)\n",
      "2019-06-16 21:52:33.167415: step 9860, loss = 0.98 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-16 21:52:45.522490: step 9870, loss = 1.40 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 21:52:58.052510: step 9880, loss = 1.04 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-16 21:53:10.403328: step 9890, loss = 0.99 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-16 21:53:22.817149: step 9900, loss = 1.07 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-06-16 21:53:39.771295: step 9910, loss = 0.89 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 21:53:51.951565: step 9920, loss = 0.92 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 21:54:04.158209: step 9930, loss = 0.87 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-06-16 21:54:16.709572: step 9940, loss = 1.08 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 21:54:29.063517: step 9950, loss = 0.97 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 21:54:41.256392: step 9960, loss = 0.89 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 21:54:53.596166: step 9970, loss = 1.00 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-06-16 21:55:06.038418: step 9980, loss = 1.10 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-16 21:55:18.363357: step 9990, loss = 0.99 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 21:55:30.772811: step 10000, loss = 1.01 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-16 21:55:50.674802: step 10010, loss = 1.10 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 21:56:03.108637: step 10020, loss = 1.06 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 21:56:15.349155: step 10030, loss = 1.01 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 21:56:27.683601: step 10040, loss = 0.94 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 21:56:39.993425: step 10050, loss = 1.16 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-06-16 21:56:52.273582: step 10060, loss = 1.08 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 21:57:04.709421: step 10070, loss = 1.25 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 21:57:16.944637: step 10080, loss = 1.08 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 21:57:29.109458: step 10090, loss = 1.43 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 21:57:41.350198: step 10100, loss = 1.29 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 21:57:58.060076: step 10110, loss = 0.96 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-16 21:58:10.476985: step 10120, loss = 0.94 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 21:58:22.792518: step 10130, loss = 1.09 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 21:58:35.052426: step 10140, loss = 0.99 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 21:58:47.470743: step 10150, loss = 1.26 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-06-16 21:58:59.739402: step 10160, loss = 1.15 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-16 21:59:12.127384: step 10170, loss = 1.17 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-06-16 21:59:24.461039: step 10180, loss = 1.10 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 21:59:36.785398: step 10190, loss = 0.92 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 21:59:49.069185: step 10200, loss = 1.05 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 22:00:05.826325: step 10210, loss = 0.97 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 22:00:18.113187: step 10220, loss = 0.91 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 22:00:30.279458: step 10230, loss = 1.12 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-16 22:00:42.631254: step 10240, loss = 1.34 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 22:00:54.915643: step 10250, loss = 1.29 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 22:01:07.172023: step 10260, loss = 1.18 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 22:01:19.498519: step 10270, loss = 1.18 (22.9 examples/sec; 1.312 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-16 22:01:31.872446: step 10280, loss = 1.12 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 22:01:44.377049: step 10290, loss = 1.05 (23.2 examples/sec; 1.295 sec/batch)\n",
      "2019-06-16 22:01:56.821279: step 10300, loss = 0.94 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 22:02:13.661070: step 10310, loss = 1.14 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-06-16 22:02:25.915172: step 10320, loss = 1.05 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 22:02:38.217700: step 10330, loss = 1.28 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 22:02:50.362799: step 10340, loss = 0.97 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 22:03:02.791255: step 10350, loss = 1.25 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 22:03:15.028933: step 10360, loss = 1.02 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-16 22:03:27.225452: step 10370, loss = 1.03 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 22:03:39.566520: step 10380, loss = 1.13 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-06-16 22:03:51.988718: step 10390, loss = 1.23 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 22:04:04.350949: step 10400, loss = 1.41 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 22:04:20.892580: step 10410, loss = 1.15 (22.9 examples/sec; 1.313 sec/batch)\n",
      "2019-06-16 22:04:33.175994: step 10420, loss = 1.18 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 22:04:45.651489: step 10430, loss = 0.96 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 22:04:57.775096: step 10440, loss = 1.14 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 22:05:10.034899: step 10450, loss = 1.04 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 22:05:22.196450: step 10460, loss = 1.10 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 22:05:34.537910: step 10470, loss = 1.02 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 22:05:46.816690: step 10480, loss = 0.96 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 22:05:58.965069: step 10490, loss = 0.93 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 22:06:11.194210: step 10500, loss = 0.90 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 22:06:28.141221: step 10510, loss = 0.91 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-16 22:06:40.400670: step 10520, loss = 0.99 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 22:06:52.730704: step 10530, loss = 1.22 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 22:07:04.888333: step 10540, loss = 1.01 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 22:07:17.165483: step 10550, loss = 1.13 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-16 22:07:29.556153: step 10560, loss = 1.15 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 22:07:41.943052: step 10570, loss = 1.03 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 22:07:54.256781: step 10580, loss = 1.02 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 22:08:06.563385: step 10590, loss = 1.03 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 22:08:18.816297: step 10600, loss = 1.10 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 22:08:35.281296: step 10610, loss = 1.06 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-16 22:08:47.760829: step 10620, loss = 1.06 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-16 22:09:00.043785: step 10630, loss = 0.88 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 22:09:12.573080: step 10640, loss = 1.10 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 22:09:24.850198: step 10650, loss = 1.09 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 22:09:36.990881: step 10660, loss = 1.00 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 22:09:49.656553: step 10670, loss = 1.02 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-16 22:10:02.011960: step 10680, loss = 1.12 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 22:10:14.349943: step 10690, loss = 0.92 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 22:10:26.708853: step 10700, loss = 1.11 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-16 22:10:43.608122: step 10710, loss = 1.20 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-16 22:10:55.831071: step 10720, loss = 0.99 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-06-16 22:11:08.084376: step 10730, loss = 1.16 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 22:11:20.284900: step 10740, loss = 1.04 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-16 22:11:32.601245: step 10750, loss = 0.90 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 22:11:44.896802: step 10760, loss = 1.15 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 22:11:57.158932: step 10770, loss = 1.15 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 22:12:09.560720: step 10780, loss = 0.94 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 22:12:21.949388: step 10790, loss = 1.22 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 22:12:34.205375: step 10800, loss = 1.25 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 22:12:50.815320: step 10810, loss = 0.95 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 22:13:03.030808: step 10820, loss = 1.16 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 22:13:15.408801: step 10830, loss = 1.16 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 22:13:28.037403: step 10840, loss = 1.04 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 22:13:40.226443: step 10850, loss = 1.19 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 22:13:52.332255: step 10860, loss = 1.08 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 22:14:04.582804: step 10870, loss = 1.02 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-16 22:14:16.718912: step 10880, loss = 1.25 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 22:14:29.045658: step 10890, loss = 1.08 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-16 22:14:41.219918: step 10900, loss = 1.12 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-16 22:14:58.108863: step 10910, loss = 0.96 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 22:15:10.431506: step 10920, loss = 1.10 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 22:15:22.752233: step 10930, loss = 1.02 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 22:15:35.116647: step 10940, loss = 1.05 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-16 22:15:47.608689: step 10950, loss = 0.91 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-16 22:15:59.728156: step 10960, loss = 1.06 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 22:16:11.969897: step 10970, loss = 1.00 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 22:16:24.507571: step 10980, loss = 1.04 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-16 22:16:36.727121: step 10990, loss = 1.18 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 22:16:48.889682: step 11000, loss = 1.03 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-16 22:17:05.466805: step 11010, loss = 1.08 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-16 22:17:17.717920: step 11020, loss = 1.06 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-16 22:17:30.052894: step 11030, loss = 1.01 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 22:17:42.235605: step 11040, loss = 1.12 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 22:17:54.798429: step 11050, loss = 1.22 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 22:18:07.067257: step 11060, loss = 1.03 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 22:18:19.429820: step 11070, loss = 1.03 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 22:18:31.572389: step 11080, loss = 1.05 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 22:18:43.877693: step 11090, loss = 1.20 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 22:18:56.172577: step 11100, loss = 1.01 (23.1 examples/sec; 1.300 sec/batch)\n",
      "2019-06-16 22:19:13.664444: step 11110, loss = 1.07 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-16 22:19:25.975051: step 11120, loss = 0.97 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 22:19:38.226050: step 11130, loss = 1.51 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-16 22:19:50.469305: step 11140, loss = 1.25 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 22:20:02.710333: step 11150, loss = 1.21 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-16 22:20:14.992780: step 11160, loss = 0.97 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 22:20:27.282375: step 11170, loss = 1.18 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 22:20:39.541169: step 11180, loss = 1.02 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 22:20:51.980586: step 11190, loss = 0.96 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-16 22:21:04.453894: step 11200, loss = 1.08 (24.7 examples/sec; 1.214 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-16 22:21:21.600670: step 11210, loss = 0.92 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 22:21:34.000388: step 11220, loss = 0.91 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 22:21:46.126760: step 11230, loss = 1.02 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-16 22:21:58.236094: step 11240, loss = 1.27 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 22:22:10.639373: step 11250, loss = 1.12 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 22:22:23.017288: step 11260, loss = 0.98 (23.1 examples/sec; 1.299 sec/batch)\n",
      "2019-06-16 22:22:35.479313: step 11270, loss = 1.07 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 22:22:47.798380: step 11280, loss = 0.93 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-06-16 22:23:00.313949: step 11290, loss = 0.97 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 22:23:12.569206: step 11300, loss = 0.96 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-06-16 22:23:29.478705: step 11310, loss = 1.05 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 22:23:41.709902: step 11320, loss = 1.01 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 22:23:53.958211: step 11330, loss = 0.94 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 22:24:06.472940: step 11340, loss = 0.98 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 22:24:18.847921: step 11350, loss = 0.91 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 22:24:31.053086: step 11360, loss = 1.05 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 22:24:43.358515: step 11370, loss = 1.09 (23.0 examples/sec; 1.307 sec/batch)\n",
      "2019-06-16 22:24:55.656949: step 11380, loss = 1.02 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-06-16 22:25:08.035100: step 11390, loss = 1.15 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-16 22:25:20.161808: step 11400, loss = 1.21 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 22:25:37.310689: step 11410, loss = 1.26 (23.9 examples/sec; 1.258 sec/batch)\n",
      "2019-06-16 22:25:49.514373: step 11420, loss = 1.18 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 22:26:01.737241: step 11430, loss = 1.03 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 22:26:14.101005: step 11440, loss = 1.00 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 22:26:26.453878: step 11450, loss = 1.14 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 22:26:38.860702: step 11460, loss = 1.01 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 22:26:51.100063: step 11470, loss = 0.99 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 22:27:03.346638: step 11480, loss = 1.24 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 22:27:16.010033: step 11490, loss = 1.07 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-16 22:27:28.465059: step 11500, loss = 1.04 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 22:27:44.989812: step 11510, loss = 0.86 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-16 22:27:57.397752: step 11520, loss = 0.92 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-16 22:28:09.878155: step 11530, loss = 1.12 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 22:28:22.064472: step 11540, loss = 1.01 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 22:28:34.397790: step 11550, loss = 1.04 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 22:28:46.747333: step 11560, loss = 0.95 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 22:28:59.160460: step 11570, loss = 1.10 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-16 22:29:11.394991: step 11580, loss = 1.12 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 22:29:23.756904: step 11590, loss = 0.90 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 22:29:36.074791: step 11600, loss = 1.06 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-16 22:29:52.801251: step 11610, loss = 1.02 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 22:30:05.128228: step 11620, loss = 1.29 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 22:30:17.644354: step 11630, loss = 0.89 (22.6 examples/sec; 1.330 sec/batch)\n",
      "2019-06-16 22:30:29.937632: step 11640, loss = 1.09 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-06-16 22:30:42.097519: step 11650, loss = 1.07 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 22:30:54.360214: step 11660, loss = 1.25 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 22:31:06.628986: step 11670, loss = 1.19 (24.1 examples/sec; 1.242 sec/batch)\n",
      "2019-06-16 22:31:18.952645: step 11680, loss = 1.03 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 22:31:31.260563: step 11690, loss = 1.15 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 22:31:43.614654: step 11700, loss = 1.14 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-16 22:32:00.355681: step 11710, loss = 0.97 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-16 22:32:12.726037: step 11720, loss = 1.12 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 22:32:24.981346: step 11730, loss = 1.00 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 22:32:37.528206: step 11740, loss = 0.99 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-16 22:32:49.872436: step 11750, loss = 1.22 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 22:33:02.351543: step 11760, loss = 1.32 (23.4 examples/sec; 1.282 sec/batch)\n",
      "2019-06-16 22:33:14.908483: step 11770, loss = 1.26 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 22:33:27.170886: step 11780, loss = 1.08 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 22:33:39.529369: step 11790, loss = 1.06 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 22:33:52.152589: step 11800, loss = 1.23 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-16 22:34:09.022562: step 11810, loss = 1.23 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-16 22:34:21.205563: step 11820, loss = 1.31 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 22:34:33.587508: step 11830, loss = 0.97 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 22:34:45.854082: step 11840, loss = 0.93 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-16 22:34:58.171782: step 11850, loss = 0.89 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 22:35:10.699805: step 11860, loss = 1.28 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-16 22:35:22.865186: step 11870, loss = 0.93 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 22:35:35.180267: step 11880, loss = 1.08 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 22:35:47.553348: step 11890, loss = 0.95 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 22:35:59.910973: step 11900, loss = 1.12 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-16 22:36:16.856681: step 11910, loss = 1.11 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 22:36:29.071414: step 11920, loss = 1.08 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 22:36:41.484308: step 11930, loss = 1.12 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-16 22:36:53.754641: step 11940, loss = 1.17 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 22:37:06.202202: step 11950, loss = 1.18 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 22:37:18.427538: step 11960, loss = 1.10 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 22:37:30.854034: step 11970, loss = 1.14 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 22:37:43.100010: step 11980, loss = 1.04 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 22:37:55.449482: step 11990, loss = 0.87 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 22:38:07.892755: step 12000, loss = 1.10 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-06-16 22:38:24.778701: step 12010, loss = 1.01 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-16 22:38:37.127727: step 12020, loss = 1.17 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-16 22:38:49.628462: step 12030, loss = 0.88 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-06-16 22:39:01.868443: step 12040, loss = 1.00 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 22:39:14.184099: step 12050, loss = 1.04 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-16 22:39:26.478042: step 12060, loss = 1.36 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-16 22:39:38.611219: step 12070, loss = 1.08 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-16 22:39:50.967007: step 12080, loss = 1.03 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 22:40:03.327552: step 12090, loss = 1.27 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-16 22:40:15.538674: step 12100, loss = 1.05 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-06-16 22:40:32.919079: step 12110, loss = 1.18 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 22:40:45.318415: step 12120, loss = 0.96 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-16 22:40:57.647125: step 12130, loss = 1.41 (24.6 examples/sec; 1.218 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-16 22:41:09.913287: step 12140, loss = 0.91 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-16 22:41:22.258404: step 12150, loss = 1.00 (23.3 examples/sec; 1.288 sec/batch)\n",
      "2019-06-16 22:41:34.490310: step 12160, loss = 1.15 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 22:41:47.015471: step 12170, loss = 1.01 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 22:41:59.359580: step 12180, loss = 1.25 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 22:42:11.761931: step 12190, loss = 1.09 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 22:42:24.431431: step 12200, loss = 1.07 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-16 22:42:41.502173: step 12210, loss = 0.97 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 22:42:53.662664: step 12220, loss = 1.04 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 22:43:06.131857: step 12230, loss = 1.32 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-16 22:43:18.243393: step 12240, loss = 1.11 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 22:43:30.417502: step 12250, loss = 1.07 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 22:43:42.564610: step 12260, loss = 1.27 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 22:43:54.900409: step 12270, loss = 1.08 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 22:44:07.246921: step 12280, loss = 1.25 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 22:44:19.790903: step 12290, loss = 1.26 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 22:44:32.039793: step 12300, loss = 1.11 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 22:44:48.771030: step 12310, loss = 1.02 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 22:45:01.046954: step 12320, loss = 1.25 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 22:45:13.389755: step 12330, loss = 0.98 (22.9 examples/sec; 1.313 sec/batch)\n",
      "2019-06-16 22:45:25.516715: step 12340, loss = 1.05 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-16 22:45:37.866736: step 12350, loss = 1.03 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 22:45:50.106246: step 12360, loss = 1.00 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-16 22:46:02.346807: step 12370, loss = 1.33 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 22:46:14.452600: step 12380, loss = 1.14 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 22:46:27.099424: step 12390, loss = 0.98 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 22:46:39.341041: step 12400, loss = 1.07 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-06-16 22:46:55.849282: step 12410, loss = 1.25 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 22:47:08.184024: step 12420, loss = 1.09 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 22:47:20.680827: step 12430, loss = 1.01 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 22:47:32.926064: step 12440, loss = 0.95 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 22:47:45.279272: step 12450, loss = 1.08 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 22:47:57.650648: step 12460, loss = 0.99 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 22:48:10.356838: step 12470, loss = 1.14 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-06-16 22:48:22.479924: step 12480, loss = 1.29 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 22:48:34.918393: step 12490, loss = 1.12 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 22:48:47.183510: step 12500, loss = 1.05 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 22:49:03.759179: step 12510, loss = 1.11 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 22:49:16.202649: step 12520, loss = 1.06 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 22:49:28.572214: step 12530, loss = 0.94 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-16 22:49:40.880382: step 12540, loss = 1.08 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-06-16 22:49:53.223363: step 12550, loss = 1.05 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 22:50:05.524936: step 12560, loss = 1.08 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-16 22:50:17.913612: step 12570, loss = 1.02 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 22:50:30.252870: step 12580, loss = 0.92 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-16 22:50:42.724874: step 12590, loss = 1.02 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 22:50:55.069708: step 12600, loss = 1.19 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 22:51:11.541643: step 12610, loss = 0.96 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-16 22:51:24.021062: step 12620, loss = 1.10 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 22:51:36.179698: step 12630, loss = 1.21 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 22:51:48.528473: step 12640, loss = 1.20 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 22:52:00.857970: step 12650, loss = 1.02 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-06-16 22:52:12.996403: step 12660, loss = 0.99 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 22:52:25.398647: step 12670, loss = 0.95 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 22:52:37.663723: step 12680, loss = 0.99 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 22:52:50.018751: step 12690, loss = 0.90 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 22:53:02.441367: step 12700, loss = 1.14 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 22:53:19.138802: step 12710, loss = 0.98 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 22:53:31.438570: step 12720, loss = 1.11 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 22:53:43.853693: step 12730, loss = 0.98 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-16 22:53:56.275020: step 12740, loss = 0.95 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-16 22:54:08.693113: step 12750, loss = 1.12 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-16 22:54:21.246115: step 12760, loss = 0.90 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-16 22:54:33.504816: step 12770, loss = 0.90 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 22:54:45.978087: step 12780, loss = 1.28 (22.9 examples/sec; 1.313 sec/batch)\n",
      "2019-06-16 22:54:58.209018: step 12790, loss = 1.10 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 22:55:10.705054: step 12800, loss = 1.19 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-16 22:55:27.957925: step 12810, loss = 1.16 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-06-16 22:55:40.249978: step 12820, loss = 1.14 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-06-16 22:55:52.451137: step 12830, loss = 1.42 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 22:56:04.706568: step 12840, loss = 1.05 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 22:56:17.051297: step 12850, loss = 0.93 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 22:56:29.466165: step 12860, loss = 0.99 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-06-16 22:56:41.741708: step 12870, loss = 1.01 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 22:56:54.197370: step 12880, loss = 1.24 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-16 22:57:06.515848: step 12890, loss = 1.03 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 22:57:18.868977: step 12900, loss = 1.48 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-16 22:57:35.431314: step 12910, loss = 0.96 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 22:57:47.684786: step 12920, loss = 1.09 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-16 22:57:59.940889: step 12930, loss = 1.06 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-16 22:58:12.180927: step 12940, loss = 1.06 (23.0 examples/sec; 1.307 sec/batch)\n",
      "2019-06-16 22:58:24.340624: step 12950, loss = 1.17 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 22:58:36.648845: step 12960, loss = 0.94 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-16 22:58:48.839715: step 12970, loss = 1.01 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 22:59:01.166105: step 12980, loss = 1.20 (23.1 examples/sec; 1.298 sec/batch)\n",
      "2019-06-16 22:59:13.444141: step 12990, loss = 1.01 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 22:59:25.774438: step 13000, loss = 1.16 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-16 22:59:42.899706: step 13010, loss = 0.98 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-16 22:59:55.141119: step 13020, loss = 0.94 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-16 23:00:07.412355: step 13030, loss = 1.05 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 23:00:19.675313: step 13040, loss = 1.04 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 23:00:31.974454: step 13050, loss = 1.00 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-16 23:00:44.459240: step 13060, loss = 1.15 (24.3 examples/sec; 1.236 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-16 23:00:56.858533: step 13070, loss = 1.03 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 23:01:09.040341: step 13080, loss = 1.48 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 23:01:21.351993: step 13090, loss = 0.97 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 23:01:33.757381: step 13100, loss = 1.29 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 23:01:50.561372: step 13110, loss = 0.84 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-06-16 23:02:02.752029: step 13120, loss = 0.97 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-16 23:02:15.080505: step 13130, loss = 0.99 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 23:02:27.477918: step 13140, loss = 0.99 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 23:02:39.653983: step 13150, loss = 1.23 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 23:02:52.000507: step 13160, loss = 0.97 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-16 23:03:04.317496: step 13170, loss = 0.85 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 23:03:16.604745: step 13180, loss = 1.11 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-16 23:03:28.948129: step 13190, loss = 1.00 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 23:03:41.258615: step 13200, loss = 1.23 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 23:03:57.832061: step 13210, loss = 1.19 (23.1 examples/sec; 1.300 sec/batch)\n",
      "2019-06-16 23:04:10.169312: step 13220, loss = 0.92 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 23:04:22.433245: step 13230, loss = 1.10 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 23:04:34.584101: step 13240, loss = 1.11 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 23:04:46.845762: step 13250, loss = 1.12 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-16 23:04:59.152123: step 13260, loss = 0.96 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 23:05:11.371938: step 13270, loss = 0.91 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 23:05:23.860461: step 13280, loss = 1.15 (23.1 examples/sec; 1.296 sec/batch)\n",
      "2019-06-16 23:05:36.175705: step 13290, loss = 1.10 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 23:05:48.457215: step 13300, loss = 1.35 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 23:06:05.165681: step 13310, loss = 0.91 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-16 23:06:17.614620: step 13320, loss = 1.11 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 23:06:29.944035: step 13330, loss = 1.14 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 23:06:42.392226: step 13340, loss = 1.19 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 23:06:54.882541: step 13350, loss = 1.05 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 23:07:07.014137: step 13360, loss = 1.00 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 23:07:19.179509: step 13370, loss = 1.00 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 23:07:31.542042: step 13380, loss = 1.00 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-16 23:07:43.766545: step 13390, loss = 1.37 (23.3 examples/sec; 1.288 sec/batch)\n",
      "2019-06-16 23:07:55.963439: step 13400, loss = 1.15 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 23:08:12.957828: step 13410, loss = 1.02 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-16 23:08:25.267835: step 13420, loss = 1.21 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 23:08:37.388478: step 13430, loss = 1.16 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 23:08:49.673175: step 13440, loss = 1.16 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 23:09:02.191696: step 13450, loss = 1.03 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-16 23:09:14.394466: step 13460, loss = 1.10 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-16 23:09:26.704416: step 13470, loss = 1.07 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-16 23:09:38.891625: step 13480, loss = 1.00 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-16 23:09:51.073384: step 13490, loss = 1.04 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 23:10:03.612443: step 13500, loss = 1.10 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-16 23:10:20.271951: step 13510, loss = 1.15 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-16 23:10:32.496665: step 13520, loss = 1.15 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 23:10:44.806008: step 13530, loss = 1.24 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 23:10:57.288556: step 13540, loss = 0.97 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 23:11:09.547339: step 13550, loss = 1.03 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-06-16 23:11:21.840579: step 13560, loss = 0.93 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 23:11:34.479766: step 13570, loss = 1.16 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 23:11:46.949990: step 13580, loss = 1.10 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-16 23:11:59.237505: step 13590, loss = 1.15 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 23:12:11.519428: step 13600, loss = 1.17 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-16 23:12:27.940581: step 13610, loss = 1.02 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 23:12:40.173596: step 13620, loss = 0.90 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 23:12:52.408241: step 13630, loss = 0.97 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 23:13:04.817962: step 13640, loss = 1.08 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 23:13:16.990392: step 13650, loss = 1.07 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 23:13:29.355393: step 13660, loss = 1.15 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-16 23:13:41.642483: step 13670, loss = 1.08 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 23:13:54.201146: step 13680, loss = 1.20 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-16 23:14:06.552226: step 13690, loss = 1.18 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 23:14:18.767565: step 13700, loss = 0.99 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-16 23:14:35.354200: step 13710, loss = 0.92 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 23:14:47.658455: step 13720, loss = 1.09 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 23:14:59.930920: step 13730, loss = 1.16 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 23:15:12.092679: step 13740, loss = 1.16 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 23:15:24.238462: step 13750, loss = 1.07 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 23:15:36.792946: step 13760, loss = 1.31 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 23:15:49.136071: step 13770, loss = 1.31 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 23:16:01.583632: step 13780, loss = 1.16 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 23:16:13.914904: step 13790, loss = 1.06 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 23:16:26.315436: step 13800, loss = 0.96 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 23:16:42.776890: step 13810, loss = 0.92 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-16 23:16:55.016808: step 13820, loss = 1.25 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 23:17:07.158201: step 13830, loss = 0.96 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 23:17:19.431959: step 13840, loss = 0.96 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-16 23:17:32.062680: step 13850, loss = 1.17 (22.9 examples/sec; 1.313 sec/batch)\n",
      "2019-06-16 23:17:44.564283: step 13860, loss = 1.07 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-16 23:17:56.977668: step 13870, loss = 1.22 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 23:18:09.378449: step 13880, loss = 1.10 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-16 23:18:21.978596: step 13890, loss = 1.11 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 23:18:34.522058: step 13900, loss = 0.99 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-16 23:18:51.487940: step 13910, loss = 0.95 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-16 23:19:03.853081: step 13920, loss = 1.13 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 23:19:16.145656: step 13930, loss = 0.92 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-06-16 23:19:28.379579: step 13940, loss = 0.93 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 23:19:40.791086: step 13950, loss = 1.07 (23.8 examples/sec; 1.263 sec/batch)\n",
      "2019-06-16 23:19:52.939936: step 13960, loss = 0.93 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 23:20:05.082702: step 13970, loss = 0.98 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 23:20:17.320366: step 13980, loss = 1.11 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-06-16 23:20:29.516831: step 13990, loss = 0.94 (24.2 examples/sec; 1.240 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-16 23:20:41.871410: step 14000, loss = 1.11 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 23:20:58.279999: step 14010, loss = 1.02 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-16 23:21:10.737400: step 14020, loss = 0.99 (22.5 examples/sec; 1.331 sec/batch)\n",
      "2019-06-16 23:21:23.252391: step 14030, loss = 0.93 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-06-16 23:21:35.645547: step 14040, loss = 1.00 (22.9 examples/sec; 1.307 sec/batch)\n",
      "2019-06-16 23:21:47.985470: step 14050, loss = 0.98 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-16 23:22:00.414843: step 14060, loss = 1.00 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 23:22:12.749485: step 14070, loss = 1.00 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 23:22:24.986703: step 14080, loss = 1.28 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 23:22:37.184349: step 14090, loss = 1.01 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 23:22:49.466262: step 14100, loss = 0.88 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-16 23:23:05.953307: step 14110, loss = 0.92 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 23:23:18.214039: step 14120, loss = 0.99 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-06-16 23:23:30.566212: step 14130, loss = 1.07 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-16 23:23:43.000359: step 14140, loss = 1.11 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 23:23:55.152421: step 14150, loss = 0.99 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 23:24:07.670048: step 14160, loss = 1.07 (23.0 examples/sec; 1.307 sec/batch)\n",
      "2019-06-16 23:24:19.948450: step 14170, loss = 1.18 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-16 23:24:32.185483: step 14180, loss = 1.12 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 23:24:44.520572: step 14190, loss = 1.23 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-16 23:24:56.762097: step 14200, loss = 1.22 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 23:25:13.715637: step 14210, loss = 1.12 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-16 23:25:26.062860: step 14220, loss = 0.97 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-16 23:25:38.423689: step 14230, loss = 0.96 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 23:25:50.683921: step 14240, loss = 1.03 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 23:26:02.896159: step 14250, loss = 0.93 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 23:26:15.335296: step 14260, loss = 1.08 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 23:26:27.810519: step 14270, loss = 1.13 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-16 23:26:40.020947: step 14280, loss = 1.17 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 23:26:52.304778: step 14290, loss = 0.81 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-16 23:27:04.612999: step 14300, loss = 1.05 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 23:27:21.424036: step 14310, loss = 0.92 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-16 23:27:33.696337: step 14320, loss = 0.97 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-16 23:27:45.885434: step 14330, loss = 1.02 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 23:27:58.031726: step 14340, loss = 1.07 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 23:28:10.378450: step 14350, loss = 0.93 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 23:28:22.802117: step 14360, loss = 1.05 (23.1 examples/sec; 1.297 sec/batch)\n",
      "2019-06-16 23:28:35.100172: step 14370, loss = 0.98 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-16 23:28:47.349421: step 14380, loss = 1.02 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-16 23:28:59.541579: step 14390, loss = 1.04 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 23:29:12.002402: step 14400, loss = 0.98 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 23:29:28.438394: step 14410, loss = 1.15 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 23:29:40.830126: step 14420, loss = 1.05 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 23:29:53.042956: step 14430, loss = 1.06 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 23:30:05.350962: step 14440, loss = 1.07 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-06-16 23:30:17.731563: step 14450, loss = 1.28 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 23:30:30.083204: step 14460, loss = 1.18 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-16 23:30:42.339595: step 14470, loss = 0.98 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 23:30:54.702989: step 14480, loss = 1.17 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-16 23:31:06.973996: step 14490, loss = 1.02 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 23:31:19.354001: step 14500, loss = 0.96 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 23:31:35.864665: step 14510, loss = 1.45 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 23:31:48.262635: step 14520, loss = 0.99 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-16 23:32:00.694213: step 14530, loss = 0.98 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-16 23:32:13.231846: step 14540, loss = 1.10 (22.9 examples/sec; 1.307 sec/batch)\n",
      "2019-06-16 23:32:25.458073: step 14550, loss = 1.11 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-06-16 23:32:37.867267: step 14560, loss = 1.39 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 23:32:50.190272: step 14570, loss = 0.99 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-16 23:33:02.500909: step 14580, loss = 0.99 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 23:33:14.676794: step 14590, loss = 1.13 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 23:33:27.118352: step 14600, loss = 1.06 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 23:33:44.042498: step 14610, loss = 1.20 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 23:33:56.458399: step 14620, loss = 1.22 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-06-16 23:34:08.858274: step 14630, loss = 1.19 (23.2 examples/sec; 1.294 sec/batch)\n",
      "2019-06-16 23:34:21.274943: step 14640, loss = 1.01 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-16 23:34:33.614032: step 14650, loss = 1.22 (23.1 examples/sec; 1.300 sec/batch)\n",
      "2019-06-16 23:34:45.886792: step 14660, loss = 1.12 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 23:34:58.233524: step 14670, loss = 1.07 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 23:35:10.473514: step 14680, loss = 1.04 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 23:35:22.750580: step 14690, loss = 1.13 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-16 23:35:35.097507: step 14700, loss = 1.16 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 23:35:51.656985: step 14710, loss = 0.99 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 23:36:04.137776: step 14720, loss = 0.97 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-16 23:36:16.329875: step 14730, loss = 1.23 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 23:36:28.620344: step 14740, loss = 0.93 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 23:36:40.980208: step 14750, loss = 1.07 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-16 23:36:53.354234: step 14760, loss = 1.18 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-06-16 23:37:05.641231: step 14770, loss = 0.88 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 23:37:17.904729: step 14780, loss = 1.12 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 23:37:30.101424: step 14790, loss = 1.16 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-16 23:37:42.495031: step 14800, loss = 1.08 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-16 23:37:59.490969: step 14810, loss = 1.11 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 23:38:11.784431: step 14820, loss = 1.26 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 23:38:23.961334: step 14830, loss = 0.98 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 23:38:36.694213: step 14840, loss = 1.11 (22.9 examples/sec; 1.313 sec/batch)\n",
      "2019-06-16 23:38:48.929038: step 14850, loss = 1.14 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-16 23:39:01.591652: step 14860, loss = 0.96 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-06-16 23:39:13.838244: step 14870, loss = 1.09 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 23:39:26.233417: step 14880, loss = 1.30 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-06-16 23:39:38.617645: step 14890, loss = 1.06 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 23:39:50.833143: step 14900, loss = 0.95 (23.3 examples/sec; 1.288 sec/batch)\n",
      "2019-06-16 23:40:07.335933: step 14910, loss = 1.01 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 23:40:19.682457: step 14920, loss = 1.13 (24.6 examples/sec; 1.217 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-16 23:40:31.971571: step 14930, loss = 1.16 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 23:40:44.376151: step 14940, loss = 0.98 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-16 23:40:56.981743: step 14950, loss = 1.15 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-16 23:41:09.227121: step 14960, loss = 1.09 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-16 23:41:21.651475: step 14970, loss = 0.96 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 23:41:34.024810: step 14980, loss = 1.24 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-06-16 23:41:46.231998: step 14990, loss = 1.17 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-16 23:41:58.550535: step 15000, loss = 1.11 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 23:42:18.360453: step 15010, loss = 0.96 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 23:42:30.922281: step 15020, loss = 1.03 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-16 23:42:43.325844: step 15030, loss = 1.13 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 23:42:55.786771: step 15040, loss = 1.17 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 23:43:08.017900: step 15050, loss = 1.04 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-16 23:43:20.462971: step 15060, loss = 1.03 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-16 23:43:32.709163: step 15070, loss = 1.04 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 23:43:44.838240: step 15080, loss = 1.18 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 23:43:57.217902: step 15090, loss = 0.95 (22.9 examples/sec; 1.307 sec/batch)\n",
      "2019-06-16 23:44:09.492757: step 15100, loss = 1.13 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 23:44:25.934442: step 15110, loss = 1.04 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 23:44:38.290882: step 15120, loss = 1.31 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-16 23:44:50.461900: step 15130, loss = 1.21 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 23:45:02.603363: step 15140, loss = 1.08 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 23:45:14.749265: step 15150, loss = 0.89 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 23:45:27.121584: step 15160, loss = 0.96 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-16 23:45:39.370115: step 15170, loss = 1.09 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 23:45:51.604539: step 15180, loss = 0.99 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 23:46:04.005976: step 15190, loss = 1.03 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 23:46:16.322661: step 15200, loss = 1.12 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 23:46:33.074015: step 15210, loss = 1.25 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-16 23:46:45.212759: step 15220, loss = 1.14 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 23:46:57.443153: step 15230, loss = 1.20 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-06-16 23:47:09.716250: step 15240, loss = 1.02 (23.4 examples/sec; 1.280 sec/batch)\n",
      "2019-06-16 23:47:21.970074: step 15250, loss = 1.14 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-16 23:47:34.296738: step 15260, loss = 0.90 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-16 23:47:46.539629: step 15270, loss = 1.04 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 23:47:58.703487: step 15280, loss = 1.02 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 23:48:11.011202: step 15290, loss = 0.99 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 23:48:23.453688: step 15300, loss = 1.59 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 23:48:40.262572: step 15310, loss = 1.32 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 23:48:52.605331: step 15320, loss = 0.84 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-06-16 23:49:05.013692: step 15330, loss = 0.99 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 23:49:17.221353: step 15340, loss = 0.94 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-06-16 23:49:29.521939: step 15350, loss = 1.07 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 23:49:41.827697: step 15360, loss = 1.19 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 23:49:54.144086: step 15370, loss = 0.95 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-16 23:50:06.464675: step 15380, loss = 1.18 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 23:50:18.695556: step 15390, loss = 1.21 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 23:50:30.970855: step 15400, loss = 1.23 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 23:50:47.340418: step 15410, loss = 1.12 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-16 23:50:59.656668: step 15420, loss = 0.96 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 23:51:11.995400: step 15430, loss = 1.29 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-16 23:51:24.401743: step 15440, loss = 1.17 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-06-16 23:51:36.650854: step 15450, loss = 1.10 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-16 23:51:48.811732: step 15460, loss = 0.94 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 23:52:01.260806: step 15470, loss = 0.94 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 23:52:13.485449: step 15480, loss = 1.00 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 23:52:25.756111: step 15490, loss = 1.27 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 23:52:38.195194: step 15500, loss = 1.18 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 23:52:54.767382: step 15510, loss = 1.07 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 23:53:06.927287: step 15520, loss = 1.12 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-16 23:53:19.162536: step 15530, loss = 0.95 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-16 23:53:31.299982: step 15540, loss = 0.92 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 23:53:43.512199: step 15550, loss = 1.26 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-06-16 23:53:55.678819: step 15560, loss = 0.99 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 23:54:07.915920: step 15570, loss = 0.91 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 23:54:20.006721: step 15580, loss = 1.07 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-16 23:54:32.332620: step 15590, loss = 1.11 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 23:54:44.479236: step 15600, loss = 1.02 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 23:55:00.917483: step 15610, loss = 1.15 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-16 23:55:13.284670: step 15620, loss = 1.03 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 23:55:25.512502: step 15630, loss = 1.15 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-16 23:55:37.677133: step 15640, loss = 1.63 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-16 23:55:49.987820: step 15650, loss = 1.07 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 23:56:02.411747: step 15660, loss = 0.94 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-16 23:56:14.735126: step 15670, loss = 1.09 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-16 23:56:27.028412: step 15680, loss = 1.03 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-16 23:56:39.524780: step 15690, loss = 1.15 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-16 23:56:52.012201: step 15700, loss = 1.18 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-16 23:57:08.690617: step 15710, loss = 1.03 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-16 23:57:20.822180: step 15720, loss = 0.94 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-16 23:57:32.950314: step 15730, loss = 1.14 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 23:57:45.670175: step 15740, loss = 1.09 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-16 23:57:57.894342: step 15750, loss = 1.29 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-16 23:58:10.281437: step 15760, loss = 0.94 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-16 23:58:22.484939: step 15770, loss = 1.17 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-06-16 23:58:34.724295: step 15780, loss = 1.01 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-16 23:58:47.042694: step 15790, loss = 1.04 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-16 23:58:59.291626: step 15800, loss = 1.09 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-16 23:59:16.078778: step 15810, loss = 1.24 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-16 23:59:28.325740: step 15820, loss = 1.10 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-16 23:59:40.828823: step 15830, loss = 1.02 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-16 23:59:53.111548: step 15840, loss = 1.10 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 00:00:05.499359: step 15850, loss = 1.09 (24.2 examples/sec; 1.237 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 00:00:17.759062: step 15860, loss = 1.08 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 00:00:30.121643: step 15870, loss = 1.15 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 00:00:42.368254: step 15880, loss = 0.93 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 00:00:54.706663: step 15890, loss = 1.04 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 00:01:07.027341: step 15900, loss = 1.12 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 00:01:24.119837: step 15910, loss = 1.11 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 00:01:36.362019: step 15920, loss = 1.14 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 00:01:48.935264: step 15930, loss = 1.13 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 00:02:01.126367: step 15940, loss = 1.03 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 00:02:13.438462: step 15950, loss = 1.10 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-06-17 00:02:25.882811: step 15960, loss = 1.35 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 00:02:38.158683: step 15970, loss = 1.16 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-06-17 00:02:50.407450: step 15980, loss = 1.03 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 00:03:02.881135: step 15990, loss = 0.96 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 00:03:15.292935: step 16000, loss = 1.09 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 00:03:32.267218: step 16010, loss = 1.09 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 00:03:44.476063: step 16020, loss = 1.01 (23.1 examples/sec; 1.298 sec/batch)\n",
      "2019-06-17 00:03:56.691869: step 16030, loss = 1.04 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-06-17 00:04:09.152915: step 16040, loss = 1.01 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 00:04:21.327011: step 16050, loss = 0.92 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 00:04:33.479019: step 16060, loss = 1.00 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 00:04:45.617185: step 16070, loss = 1.10 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 00:04:58.059718: step 16080, loss = 1.19 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 00:05:10.276251: step 16090, loss = 1.15 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 00:05:22.533161: step 16100, loss = 0.89 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 00:05:38.806152: step 16110, loss = 0.97 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 00:05:50.991354: step 16120, loss = 1.00 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 00:06:03.132617: step 16130, loss = 0.89 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 00:06:15.301824: step 16140, loss = 0.92 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 00:06:27.397505: step 16150, loss = 0.87 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 00:06:39.729293: step 16160, loss = 1.03 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-06-17 00:06:52.021416: step 16170, loss = 0.97 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 00:07:04.198667: step 16180, loss = 0.97 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 00:07:16.581553: step 16190, loss = 1.06 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 00:07:28.731487: step 16200, loss = 1.13 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 00:07:45.291618: step 16210, loss = 1.09 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 00:07:57.523693: step 16220, loss = 1.21 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 00:08:09.658185: step 16230, loss = 1.09 (23.1 examples/sec; 1.299 sec/batch)\n",
      "2019-06-17 00:08:21.836348: step 16240, loss = 1.06 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 00:08:34.088882: step 16250, loss = 0.97 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 00:08:46.373268: step 16260, loss = 0.94 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 00:08:58.773644: step 16270, loss = 1.06 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 00:09:11.052086: step 16280, loss = 1.19 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 00:09:23.711281: step 16290, loss = 1.49 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-17 00:09:36.156415: step 16300, loss = 1.15 (22.8 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 00:09:52.727304: step 16310, loss = 1.16 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 00:10:04.851054: step 16320, loss = 1.07 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 00:10:17.043646: step 16330, loss = 1.14 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 00:10:29.361645: step 16340, loss = 1.07 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 00:10:41.586485: step 16350, loss = 1.13 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 00:10:53.682831: step 16360, loss = 1.02 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 00:11:05.952718: step 16370, loss = 1.30 (22.6 examples/sec; 1.330 sec/batch)\n",
      "2019-06-17 00:11:18.394735: step 16380, loss = 0.99 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 00:11:30.592452: step 16390, loss = 1.01 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 00:11:42.883295: step 16400, loss = 1.06 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 00:11:59.587193: step 16410, loss = 0.97 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 00:12:11.987981: step 16420, loss = 1.35 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 00:12:24.083572: step 16430, loss = 1.13 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 00:12:36.443544: step 16440, loss = 1.01 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 00:12:48.726030: step 16450, loss = 0.81 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 00:13:01.103252: step 16460, loss = 1.05 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 00:13:13.642617: step 16470, loss = 1.26 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 00:13:25.842755: step 16480, loss = 1.07 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 00:13:38.162641: step 16490, loss = 1.01 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-17 00:13:50.787998: step 16500, loss = 1.10 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 00:14:07.343282: step 16510, loss = 1.12 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 00:14:19.608890: step 16520, loss = 0.98 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 00:14:32.071263: step 16530, loss = 1.00 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-17 00:14:44.547639: step 16540, loss = 1.08 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 00:14:56.921007: step 16550, loss = 0.97 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 00:15:09.165264: step 16560, loss = 1.25 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 00:15:21.390537: step 16570, loss = 1.05 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 00:15:33.677408: step 16580, loss = 1.09 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 00:15:45.874850: step 16590, loss = 1.21 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 00:15:58.431279: step 16600, loss = 1.10 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 00:16:15.206427: step 16610, loss = 0.98 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 00:16:27.579163: step 16620, loss = 0.92 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 00:16:39.977725: step 16630, loss = 1.20 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-06-17 00:16:52.259968: step 16640, loss = 1.39 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 00:17:04.725969: step 16650, loss = 0.99 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 00:17:17.120369: step 16660, loss = 1.16 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 00:17:29.339822: step 16670, loss = 1.14 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 00:17:41.634649: step 16680, loss = 0.87 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-06-17 00:17:53.894333: step 16690, loss = 0.99 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 00:18:06.185252: step 16700, loss = 0.93 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 00:18:22.904043: step 16710, loss = 1.14 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 00:18:35.373919: step 16720, loss = 1.16 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 00:18:47.585113: step 16730, loss = 0.93 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 00:18:59.992528: step 16740, loss = 1.20 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 00:19:12.319213: step 16750, loss = 1.10 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 00:19:24.564144: step 16760, loss = 1.26 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 00:19:36.727751: step 16770, loss = 1.06 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 00:19:49.058197: step 16780, loss = 1.07 (22.9 examples/sec; 1.309 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 00:20:01.383618: step 16790, loss = 1.21 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 00:20:13.507044: step 16800, loss = 1.08 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 00:20:30.058586: step 16810, loss = 1.29 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 00:20:42.343732: step 16820, loss = 1.30 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 00:20:54.734693: step 16830, loss = 1.00 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-17 00:21:07.054016: step 16840, loss = 1.24 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 00:21:19.406046: step 16850, loss = 0.96 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 00:21:31.839527: step 16860, loss = 0.94 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 00:21:44.328327: step 16870, loss = 1.11 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 00:21:56.672207: step 16880, loss = 0.96 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 00:22:09.023488: step 16890, loss = 1.09 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 00:22:21.358896: step 16900, loss = 1.07 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 00:22:38.212726: step 16910, loss = 1.14 (22.9 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 00:22:50.631277: step 16920, loss = 1.02 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 00:23:03.006173: step 16930, loss = 1.50 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 00:23:15.345937: step 16940, loss = 1.09 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 00:23:27.600294: step 16950, loss = 1.07 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 00:23:39.899951: step 16960, loss = 1.10 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 00:23:52.254908: step 16970, loss = 1.16 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 00:24:04.435169: step 16980, loss = 1.15 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 00:24:16.715846: step 16990, loss = 0.90 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 00:24:29.064908: step 17000, loss = 1.11 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 00:24:45.670639: step 17010, loss = 0.97 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 00:24:57.918319: step 17020, loss = 1.29 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 00:25:10.283414: step 17030, loss = 1.08 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 00:25:22.518220: step 17040, loss = 1.02 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 00:25:34.718446: step 17050, loss = 0.93 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 00:25:47.088631: step 17060, loss = 1.15 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 00:25:59.372012: step 17070, loss = 1.06 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 00:26:11.728142: step 17080, loss = 0.93 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 00:26:23.991470: step 17090, loss = 1.12 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 00:26:36.165320: step 17100, loss = 1.04 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 00:26:53.123659: step 17110, loss = 1.23 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 00:27:05.366588: step 17120, loss = 1.06 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 00:27:17.522216: step 17130, loss = 0.95 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 00:27:30.109889: step 17140, loss = 1.13 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 00:27:42.421555: step 17150, loss = 1.25 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 00:27:55.010844: step 17160, loss = 1.17 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 00:28:07.523571: step 17170, loss = 1.17 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 00:28:19.799295: step 17180, loss = 1.16 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 00:28:32.000789: step 17190, loss = 1.00 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 00:28:44.547797: step 17200, loss = 1.12 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 00:29:01.477632: step 17210, loss = 0.93 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 00:29:13.612422: step 17220, loss = 1.05 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 00:29:26.003107: step 17230, loss = 1.20 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 00:29:38.436929: step 17240, loss = 1.01 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 00:29:50.768499: step 17250, loss = 1.11 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 00:30:03.067921: step 17260, loss = 1.20 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 00:30:15.388725: step 17270, loss = 1.00 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 00:30:28.096060: step 17280, loss = 1.19 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 00:30:40.404088: step 17290, loss = 1.00 (23.5 examples/sec; 1.279 sec/batch)\n",
      "2019-06-17 00:30:52.972489: step 17300, loss = 1.02 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 00:31:09.582734: step 17310, loss = 1.03 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 00:31:22.052411: step 17320, loss = 1.11 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 00:31:34.162710: step 17330, loss = 1.06 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 00:31:46.406421: step 17340, loss = 1.08 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 00:31:58.879313: step 17350, loss = 1.03 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 00:32:11.127111: step 17360, loss = 1.00 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 00:32:23.443927: step 17370, loss = 1.02 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 00:32:35.734373: step 17380, loss = 1.14 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 00:32:48.147107: step 17390, loss = 1.02 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 00:33:00.408164: step 17400, loss = 1.09 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 00:33:16.969053: step 17410, loss = 1.25 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 00:33:29.345712: step 17420, loss = 1.25 (23.2 examples/sec; 1.293 sec/batch)\n",
      "2019-06-17 00:33:41.785398: step 17430, loss = 1.14 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 00:33:54.043038: step 17440, loss = 1.04 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 00:34:06.275024: step 17450, loss = 1.12 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 00:34:18.584577: step 17460, loss = 1.10 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 00:34:30.948114: step 17470, loss = 0.99 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 00:34:43.220053: step 17480, loss = 1.00 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-06-17 00:34:55.547697: step 17490, loss = 1.29 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 00:35:07.837413: step 17500, loss = 1.17 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 00:35:24.695935: step 17510, loss = 1.28 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 00:35:36.995428: step 17520, loss = 0.95 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 00:35:49.250748: step 17530, loss = 1.03 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 00:36:01.743628: step 17540, loss = 1.01 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 00:36:14.120166: step 17550, loss = 1.28 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 00:36:26.300673: step 17560, loss = 1.07 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 00:36:38.737181: step 17570, loss = 1.07 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 00:36:51.268097: step 17580, loss = 1.14 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 00:37:03.654440: step 17590, loss = 1.18 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 00:37:16.014176: step 17600, loss = 1.03 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 00:37:32.769158: step 17610, loss = 1.06 (23.5 examples/sec; 1.277 sec/batch)\n",
      "2019-06-17 00:37:45.014499: step 17620, loss = 1.20 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 00:37:57.440617: step 17630, loss = 1.05 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 00:38:09.850911: step 17640, loss = 1.29 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 00:38:22.307373: step 17650, loss = 0.95 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 00:38:34.676935: step 17660, loss = 1.15 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 00:38:47.099982: step 17670, loss = 1.06 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 00:38:59.399914: step 17680, loss = 1.09 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-06-17 00:39:11.758545: step 17690, loss = 1.29 (23.0 examples/sec; 1.307 sec/batch)\n",
      "2019-06-17 00:39:24.022052: step 17700, loss = 1.22 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 00:39:40.839158: step 17710, loss = 1.06 (24.8 examples/sec; 1.211 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 00:39:53.119154: step 17720, loss = 1.07 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 00:40:05.572948: step 17730, loss = 1.12 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 00:40:17.842363: step 17740, loss = 1.03 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 00:40:30.009888: step 17750, loss = 1.12 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 00:40:42.103260: step 17760, loss = 0.99 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 00:40:54.474514: step 17770, loss = 1.13 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-06-17 00:41:06.680301: step 17780, loss = 0.95 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 00:41:19.020087: step 17790, loss = 1.16 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 00:41:31.364573: step 17800, loss = 1.29 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 00:41:47.971013: step 17810, loss = 0.88 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 00:42:00.299193: step 17820, loss = 1.13 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 00:42:12.616167: step 17830, loss = 1.00 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 00:42:24.925475: step 17840, loss = 1.36 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 00:42:37.179446: step 17850, loss = 0.93 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 00:42:49.510509: step 17860, loss = 1.14 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 00:43:01.870899: step 17870, loss = 1.05 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 00:43:14.267930: step 17880, loss = 1.11 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-06-17 00:43:26.600537: step 17890, loss = 1.11 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 00:43:38.935704: step 17900, loss = 1.03 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 00:43:55.571197: step 17910, loss = 1.13 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 00:44:07.904755: step 17920, loss = 0.94 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 00:44:20.209690: step 17930, loss = 1.05 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 00:44:32.651222: step 17940, loss = 0.86 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 00:44:45.088639: step 17950, loss = 1.07 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 00:44:57.550400: step 17960, loss = 1.04 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 00:45:09.822765: step 17970, loss = 1.12 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 00:45:22.418688: step 17980, loss = 0.97 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 00:45:35.011263: step 17990, loss = 1.00 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 00:45:47.396268: step 18000, loss = 1.02 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 00:46:04.219630: step 18010, loss = 1.25 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-06-17 00:46:16.423474: step 18020, loss = 1.15 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 00:46:28.708403: step 18030, loss = 1.23 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-06-17 00:46:40.900694: step 18040, loss = 0.88 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 00:46:53.115227: step 18050, loss = 1.11 (23.4 examples/sec; 1.282 sec/batch)\n",
      "2019-06-17 00:47:05.462213: step 18060, loss = 1.23 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 00:47:17.809141: step 18070, loss = 1.23 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 00:47:30.051613: step 18080, loss = 1.11 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 00:47:42.435582: step 18090, loss = 1.02 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 00:47:54.698721: step 18100, loss = 1.08 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 00:48:11.374897: step 18110, loss = 1.06 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 00:48:23.508643: step 18120, loss = 1.02 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 00:48:35.741211: step 18130, loss = 1.08 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 00:48:48.044243: step 18140, loss = 1.02 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 00:49:00.407649: step 18150, loss = 1.09 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 00:49:12.727322: step 18160, loss = 1.03 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 00:49:24.944922: step 18170, loss = 1.00 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 00:49:37.387994: step 18180, loss = 0.95 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 00:49:49.725015: step 18190, loss = 1.24 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 00:50:02.064215: step 18200, loss = 1.08 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-06-17 00:50:18.721114: step 18210, loss = 1.02 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 00:50:30.975202: step 18220, loss = 1.19 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 00:50:43.303918: step 18230, loss = 1.21 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 00:50:55.666900: step 18240, loss = 1.12 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 00:51:07.741052: step 18250, loss = 1.01 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 00:51:20.283820: step 18260, loss = 1.09 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 00:51:32.599287: step 18270, loss = 0.97 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 00:51:44.904080: step 18280, loss = 1.06 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 00:51:57.338687: step 18290, loss = 0.94 (23.2 examples/sec; 1.295 sec/batch)\n",
      "2019-06-17 00:52:09.809255: step 18300, loss = 1.07 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 00:52:26.310914: step 18310, loss = 1.13 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 00:52:38.836852: step 18320, loss = 1.01 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 00:52:51.093490: step 18330, loss = 0.99 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 00:53:03.391764: step 18340, loss = 0.95 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 00:53:15.653897: step 18350, loss = 1.18 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 00:53:27.895421: step 18360, loss = 1.19 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 00:53:40.414856: step 18370, loss = 1.09 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-06-17 00:53:52.950348: step 18380, loss = 1.10 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 00:54:05.186560: step 18390, loss = 0.86 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 00:54:17.571915: step 18400, loss = 1.07 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 00:54:34.227755: step 18410, loss = 1.16 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 00:54:46.550264: step 18420, loss = 1.07 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 00:54:58.793934: step 18430, loss = 1.14 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 00:55:11.051977: step 18440, loss = 1.04 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 00:55:23.325889: step 18450, loss = 0.99 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-06-17 00:55:35.443886: step 18460, loss = 0.87 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 00:55:47.802222: step 18470, loss = 1.10 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 00:56:00.145328: step 18480, loss = 1.18 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 00:56:12.560311: step 18490, loss = 1.18 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 00:56:24.780708: step 18500, loss = 1.07 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 00:56:41.479853: step 18510, loss = 1.14 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 00:56:53.700741: step 18520, loss = 1.33 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 00:57:06.140909: step 18530, loss = 1.10 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 00:57:18.283647: step 18540, loss = 1.31 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 00:57:30.445247: step 18550, loss = 1.13 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 00:57:42.847155: step 18560, loss = 1.10 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 00:57:55.199997: step 18570, loss = 0.95 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 00:58:07.893292: step 18580, loss = 0.99 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 00:58:20.051140: step 18590, loss = 1.07 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 00:58:32.543100: step 18600, loss = 1.05 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-06-17 00:58:49.259437: step 18610, loss = 1.10 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-17 00:59:01.620279: step 18620, loss = 0.96 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-17 00:59:13.889419: step 18630, loss = 1.09 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 00:59:26.065301: step 18640, loss = 0.99 (24.6 examples/sec; 1.219 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 00:59:38.324943: step 18650, loss = 1.04 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 00:59:50.750876: step 18660, loss = 1.02 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 01:00:03.293972: step 18670, loss = 1.05 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-17 01:00:15.840217: step 18680, loss = 1.10 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-06-17 01:00:28.265064: step 18690, loss = 0.97 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-06-17 01:00:40.398160: step 18700, loss = 0.99 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 01:00:57.288597: step 18710, loss = 0.91 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 01:01:09.905562: step 18720, loss = 1.12 (23.3 examples/sec; 1.287 sec/batch)\n",
      "2019-06-17 01:01:22.364127: step 18730, loss = 1.15 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 01:01:34.753409: step 18740, loss = 1.21 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 01:01:46.910981: step 18750, loss = 1.17 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 01:01:59.118246: step 18760, loss = 1.14 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 01:02:11.452437: step 18770, loss = 0.92 (22.6 examples/sec; 1.328 sec/batch)\n",
      "2019-06-17 01:02:23.910466: step 18780, loss = 0.93 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 01:02:36.154757: step 18790, loss = 1.31 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 01:02:48.552895: step 18800, loss = 0.88 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 01:03:04.887848: step 18810, loss = 1.02 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 01:03:17.179822: step 18820, loss = 1.07 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 01:03:29.462687: step 18830, loss = 0.99 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 01:03:41.822606: step 18840, loss = 1.11 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 01:03:53.978536: step 18850, loss = 1.21 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 01:04:06.418364: step 18860, loss = 1.02 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 01:04:18.902048: step 18870, loss = 1.18 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 01:04:31.130242: step 18880, loss = 1.04 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 01:04:43.431912: step 18890, loss = 1.23 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 01:04:55.713416: step 18900, loss = 1.02 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 01:05:12.812643: step 18910, loss = 0.95 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 01:05:25.131879: step 18920, loss = 1.09 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 01:05:37.432719: step 18930, loss = 1.22 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 01:05:49.790213: step 18940, loss = 1.13 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 01:06:01.963783: step 18950, loss = 0.92 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-06-17 01:06:14.203418: step 18960, loss = 1.14 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 01:06:26.693993: step 18970, loss = 1.10 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 01:06:39.012443: step 18980, loss = 1.12 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 01:06:51.382452: step 18990, loss = 1.24 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 01:07:03.840455: step 19000, loss = 1.12 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 01:07:20.452101: step 19010, loss = 1.03 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 01:07:32.706798: step 19020, loss = 1.00 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 01:07:45.232884: step 19030, loss = 1.06 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 01:07:57.618230: step 19040, loss = 0.97 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 01:08:10.121480: step 19050, loss = 1.11 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-06-17 01:08:22.454163: step 19060, loss = 1.00 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 01:08:34.801312: step 19070, loss = 1.19 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 01:08:47.071305: step 19080, loss = 1.32 (23.3 examples/sec; 1.290 sec/batch)\n",
      "2019-06-17 01:08:59.285542: step 19090, loss = 1.10 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 01:09:11.521355: step 19100, loss = 1.23 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 01:09:28.455521: step 19110, loss = 1.12 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 01:09:40.683455: step 19120, loss = 1.03 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 01:09:52.885146: step 19130, loss = 1.19 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 01:10:05.431011: step 19140, loss = 1.01 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 01:10:17.820884: step 19150, loss = 1.00 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 01:10:30.089122: step 19160, loss = 0.87 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 01:10:42.483288: step 19170, loss = 1.09 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 01:10:54.813520: step 19180, loss = 1.15 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 01:11:07.097160: step 19190, loss = 1.14 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 01:11:19.460335: step 19200, loss = 1.24 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 01:11:36.544715: step 19210, loss = 1.18 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 01:11:48.905570: step 19220, loss = 1.07 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 01:12:01.025433: step 19230, loss = 1.07 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 01:12:13.313374: step 19240, loss = 1.00 (22.6 examples/sec; 1.328 sec/batch)\n",
      "2019-06-17 01:12:25.424342: step 19250, loss = 1.09 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 01:12:37.671543: step 19260, loss = 1.31 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 01:12:49.945721: step 19270, loss = 1.04 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 01:13:02.349863: step 19280, loss = 1.01 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-06-17 01:13:14.604098: step 19290, loss = 1.21 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 01:13:26.908025: step 19300, loss = 1.05 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 01:13:43.953971: step 19310, loss = 1.00 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 01:13:56.193289: step 19320, loss = 1.04 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 01:14:08.441876: step 19330, loss = 1.02 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 01:14:20.959159: step 19340, loss = 1.14 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 01:14:33.279517: step 19350, loss = 0.96 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 01:14:45.902955: step 19360, loss = 1.18 (23.7 examples/sec; 1.263 sec/batch)\n",
      "2019-06-17 01:14:58.071699: step 19370, loss = 1.18 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 01:15:10.409287: step 19380, loss = 0.98 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-17 01:15:22.662425: step 19390, loss = 1.09 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 01:15:35.061355: step 19400, loss = 1.18 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 01:15:52.117060: step 19410, loss = 1.13 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 01:16:04.642315: step 19420, loss = 0.95 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 01:16:16.784013: step 19430, loss = 1.11 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 01:16:28.998860: step 19440, loss = 1.26 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 01:16:41.499738: step 19450, loss = 1.26 (23.7 examples/sec; 1.266 sec/batch)\n",
      "2019-06-17 01:16:53.806556: step 19460, loss = 1.13 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 01:17:06.174743: step 19470, loss = 1.15 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 01:17:18.442925: step 19480, loss = 1.10 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-17 01:17:30.986275: step 19490, loss = 1.14 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 01:17:43.493154: step 19500, loss = 0.99 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 01:18:00.056628: step 19510, loss = 1.27 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 01:18:12.401616: step 19520, loss = 1.16 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 01:18:24.673508: step 19530, loss = 1.53 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 01:18:37.019544: step 19540, loss = 0.86 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 01:18:49.342159: step 19550, loss = 1.03 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 01:19:01.814168: step 19560, loss = 1.08 (23.1 examples/sec; 1.300 sec/batch)\n",
      "2019-06-17 01:19:14.076801: step 19570, loss = 1.21 (22.6 examples/sec; 1.328 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 01:19:26.463897: step 19580, loss = 1.01 (22.5 examples/sec; 1.330 sec/batch)\n",
      "2019-06-17 01:19:38.747851: step 19590, loss = 0.98 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 01:19:51.173461: step 19600, loss = 1.08 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 01:20:08.039965: step 19610, loss = 0.92 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 01:20:20.361719: step 19620, loss = 1.04 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 01:20:32.536269: step 19630, loss = 1.02 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 01:20:44.940141: step 19640, loss = 0.87 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 01:20:57.428783: step 19650, loss = 1.09 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 01:21:09.576574: step 19660, loss = 1.20 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 01:21:21.800020: step 19670, loss = 1.09 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 01:21:34.208021: step 19680, loss = 0.91 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 01:21:46.497384: step 19690, loss = 1.09 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-06-17 01:21:58.668768: step 19700, loss = 1.14 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 01:22:15.930465: step 19710, loss = 1.14 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-17 01:22:28.177919: step 19720, loss = 1.12 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 01:22:40.339271: step 19730, loss = 0.92 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 01:22:52.715902: step 19740, loss = 1.16 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 01:23:05.164290: step 19750, loss = 1.14 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 01:23:17.350534: step 19760, loss = 0.95 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 01:23:29.859604: step 19770, loss = 0.99 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 01:23:42.458913: step 19780, loss = 1.13 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 01:23:54.814598: step 19790, loss = 1.16 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 01:24:07.165002: step 19800, loss = 1.27 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 01:24:24.092252: step 19810, loss = 1.07 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 01:24:36.247037: step 19820, loss = 1.22 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 01:24:48.633374: step 19830, loss = 1.07 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 01:25:01.128768: step 19840, loss = 1.01 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 01:25:13.529162: step 19850, loss = 1.04 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 01:25:25.879585: step 19860, loss = 1.03 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 01:25:38.123981: step 19870, loss = 1.42 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 01:25:50.422204: step 19880, loss = 1.00 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 01:26:02.738733: step 19890, loss = 1.08 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 01:26:15.255874: step 19900, loss = 1.18 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 01:26:31.840436: step 19910, loss = 0.99 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 01:26:44.091144: step 19920, loss = 1.08 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 01:26:56.599545: step 19930, loss = 1.04 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-06-17 01:27:08.815989: step 19940, loss = 1.32 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 01:27:21.052557: step 19950, loss = 1.04 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 01:27:33.406344: step 19960, loss = 1.32 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 01:27:45.775270: step 19970, loss = 1.02 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 01:27:58.085665: step 19980, loss = 1.21 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 01:28:10.343084: step 19990, loss = 0.97 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 01:28:22.599160: step 20000, loss = 1.17 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 01:28:42.739932: step 20010, loss = 1.02 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 01:28:55.207893: step 20020, loss = 1.03 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 01:29:07.682445: step 20030, loss = 1.17 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 01:29:20.235951: step 20040, loss = 0.93 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 01:29:32.521896: step 20050, loss = 1.22 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 01:29:44.812792: step 20060, loss = 1.08 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 01:29:57.564271: step 20070, loss = 1.61 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 01:30:09.733984: step 20080, loss = 1.03 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 01:30:22.002846: step 20090, loss = 0.98 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 01:30:34.351895: step 20100, loss = 1.29 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 01:30:51.158536: step 20110, loss = 1.06 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 01:31:03.560310: step 20120, loss = 1.00 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 01:31:16.260089: step 20130, loss = 1.05 (22.6 examples/sec; 1.328 sec/batch)\n",
      "2019-06-17 01:31:28.567896: step 20140, loss = 1.08 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 01:31:40.883982: step 20150, loss = 0.93 (23.0 examples/sec; 1.307 sec/batch)\n",
      "2019-06-17 01:31:53.006396: step 20160, loss = 1.05 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 01:32:05.401042: step 20170, loss = 1.13 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 01:32:17.965547: step 20180, loss = 1.01 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 01:32:30.455029: step 20190, loss = 1.05 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 01:32:42.737181: step 20200, loss = 0.94 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 01:32:59.387839: step 20210, loss = 1.07 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 01:33:11.897016: step 20220, loss = 1.05 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 01:33:24.096346: step 20230, loss = 1.32 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 01:33:36.424296: step 20240, loss = 0.96 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 01:33:48.537267: step 20250, loss = 1.03 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 01:34:00.941164: step 20260, loss = 0.96 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 01:34:13.465432: step 20270, loss = 0.96 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 01:34:25.853132: step 20280, loss = 0.90 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 01:34:38.194751: step 20290, loss = 1.20 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 01:34:50.579938: step 20300, loss = 1.12 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 01:35:07.204739: step 20310, loss = 1.10 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 01:35:19.630063: step 20320, loss = 1.54 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 01:35:31.900521: step 20330, loss = 1.14 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 01:35:44.228431: step 20340, loss = 1.01 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 01:35:56.481854: step 20350, loss = 1.02 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-17 01:36:08.761018: step 20360, loss = 1.19 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 01:36:21.139943: step 20370, loss = 1.13 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 01:36:33.535338: step 20380, loss = 1.12 (22.5 examples/sec; 1.332 sec/batch)\n",
      "2019-06-17 01:36:46.162264: step 20390, loss = 1.10 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 01:36:58.422187: step 20400, loss = 0.89 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 01:37:14.995159: step 20410, loss = 1.20 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 01:37:27.230540: step 20420, loss = 1.29 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 01:37:39.465493: step 20430, loss = 0.95 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 01:37:51.796847: step 20440, loss = 1.03 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 01:38:04.159777: step 20450, loss = 0.96 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 01:38:16.710045: step 20460, loss = 1.13 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 01:38:28.979655: step 20470, loss = 1.08 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 01:38:41.280823: step 20480, loss = 1.17 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 01:38:53.694849: step 20490, loss = 1.34 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 01:39:06.149745: step 20500, loss = 1.28 (24.7 examples/sec; 1.214 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 01:39:22.900388: step 20510, loss = 1.05 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 01:39:35.361495: step 20520, loss = 1.09 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 01:39:47.875645: step 20530, loss = 0.95 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 01:40:00.164114: step 20540, loss = 1.04 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 01:40:12.415022: step 20550, loss = 1.03 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 01:40:24.649895: step 20560, loss = 1.11 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 01:40:37.193619: step 20570, loss = 1.12 (23.4 examples/sec; 1.283 sec/batch)\n",
      "2019-06-17 01:40:49.527629: step 20580, loss = 1.09 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 01:41:01.783435: step 20590, loss = 1.12 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 01:41:14.246805: step 20600, loss = 1.29 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 01:41:30.849382: step 20610, loss = 0.95 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 01:41:43.300364: step 20620, loss = 1.15 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 01:41:55.581075: step 20630, loss = 0.97 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 01:42:07.728313: step 20640, loss = 1.04 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 01:42:20.116630: step 20650, loss = 1.09 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-06-17 01:42:32.294584: step 20660, loss = 1.09 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 01:42:44.571846: step 20670, loss = 1.08 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 01:42:56.905511: step 20680, loss = 0.99 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 01:43:09.135611: step 20690, loss = 1.16 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 01:43:21.333548: step 20700, loss = 1.18 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 01:43:38.425671: step 20710, loss = 1.23 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 01:43:50.642184: step 20720, loss = 1.11 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 01:44:02.878470: step 20730, loss = 0.96 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 01:44:15.316134: step 20740, loss = 1.12 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 01:44:27.997767: step 20750, loss = 1.28 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 01:44:40.244807: step 20760, loss = 1.18 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-06-17 01:44:52.621434: step 20770, loss = 1.06 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 01:45:04.888095: step 20780, loss = 1.02 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 01:45:17.115235: step 20790, loss = 1.08 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 01:45:29.459043: step 20800, loss = 1.00 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 01:45:46.205852: step 20810, loss = 0.99 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 01:45:58.494362: step 20820, loss = 1.04 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 01:46:10.637998: step 20830, loss = 1.22 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 01:46:22.905878: step 20840, loss = 1.11 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 01:46:35.347560: step 20850, loss = 0.98 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 01:46:47.781474: step 20860, loss = 1.13 (22.9 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 01:47:00.123368: step 20870, loss = 1.22 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 01:47:12.390935: step 20880, loss = 1.01 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 01:47:24.709576: step 20890, loss = 0.93 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 01:47:37.327750: step 20900, loss = 1.09 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 01:47:54.109863: step 20910, loss = 1.05 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 01:48:06.607979: step 20920, loss = 0.96 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 01:48:19.007338: step 20930, loss = 1.07 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 01:48:31.265431: step 20940, loss = 1.05 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 01:48:43.588382: step 20950, loss = 1.08 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 01:48:55.970423: step 20960, loss = 0.91 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 01:49:08.096410: step 20970, loss = 1.05 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 01:49:20.512466: step 20980, loss = 1.09 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-17 01:49:32.886089: step 20990, loss = 0.90 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 01:49:45.168590: step 21000, loss = 1.14 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-17 01:50:01.935552: step 21010, loss = 1.11 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 01:50:14.400785: step 21020, loss = 1.11 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 01:50:26.537120: step 21030, loss = 1.11 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 01:50:38.889766: step 21040, loss = 1.07 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 01:50:51.044003: step 21050, loss = 1.04 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-17 01:51:03.327089: step 21060, loss = 1.25 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 01:51:15.584766: step 21070, loss = 1.03 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 01:51:28.051317: step 21080, loss = 1.11 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 01:51:40.404693: step 21090, loss = 1.28 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 01:51:52.695006: step 21100, loss = 1.26 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 01:52:09.735473: step 21110, loss = 1.13 (24.1 examples/sec; 1.242 sec/batch)\n",
      "2019-06-17 01:52:22.150175: step 21120, loss = 1.17 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 01:52:34.298302: step 21130, loss = 1.04 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 01:52:46.555626: step 21140, loss = 1.03 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 01:52:58.889219: step 21150, loss = 1.08 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 01:53:11.148944: step 21160, loss = 0.98 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 01:53:23.469187: step 21170, loss = 1.01 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 01:53:35.959951: step 21180, loss = 1.13 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 01:53:48.117769: step 21190, loss = 0.87 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 01:54:00.561591: step 21200, loss = 1.17 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 01:54:17.329284: step 21210, loss = 1.13 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 01:54:29.871849: step 21220, loss = 1.02 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 01:54:42.112153: step 21230, loss = 1.07 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 01:54:54.557226: step 21240, loss = 0.96 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 01:55:07.025754: step 21250, loss = 1.23 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 01:55:19.586162: step 21260, loss = 1.17 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 01:55:31.919981: step 21270, loss = 1.12 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 01:55:44.242837: step 21280, loss = 1.00 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 01:55:56.411824: step 21290, loss = 1.04 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 01:56:08.745497: step 21300, loss = 1.44 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 01:56:25.627483: step 21310, loss = 1.10 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 01:56:37.882261: step 21320, loss = 1.18 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 01:56:50.353941: step 21330, loss = 1.13 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 01:57:02.694377: step 21340, loss = 1.07 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 01:57:14.920893: step 21350, loss = 0.95 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 01:57:27.227252: step 21360, loss = 0.90 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 01:57:39.772315: step 21370, loss = 1.09 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 01:57:52.160502: step 21380, loss = 1.25 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-17 01:58:04.554467: step 21390, loss = 1.09 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 01:58:16.809905: step 21400, loss = 1.01 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 01:58:33.685881: step 21410, loss = 0.97 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 01:58:45.962090: step 21420, loss = 0.94 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 01:58:58.217740: step 21430, loss = 0.85 (24.1 examples/sec; 1.243 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 01:59:10.399925: step 21440, loss = 1.11 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 01:59:22.777478: step 21450, loss = 1.01 (23.8 examples/sec; 1.258 sec/batch)\n",
      "2019-06-17 01:59:35.240343: step 21460, loss = 1.13 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 01:59:47.663099: step 21470, loss = 1.04 (22.4 examples/sec; 1.339 sec/batch)\n",
      "2019-06-17 02:00:00.011030: step 21480, loss = 0.92 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-17 02:00:12.312093: step 21490, loss = 1.05 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 02:00:24.634959: step 21500, loss = 1.07 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 02:00:41.818270: step 21510, loss = 1.10 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 02:00:54.276677: step 21520, loss = 1.17 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-06-17 02:01:06.648206: step 21530, loss = 1.07 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 02:01:19.078433: step 21540, loss = 1.11 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 02:01:31.305321: step 21550, loss = 0.91 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 02:01:43.542678: step 21560, loss = 1.05 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-06-17 02:01:55.872017: step 21570, loss = 0.99 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 02:02:08.385293: step 21580, loss = 1.31 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 02:02:20.555678: step 21590, loss = 0.89 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 02:02:33.147820: step 21600, loss = 1.23 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 02:02:50.254772: step 21610, loss = 1.11 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-17 02:03:02.629993: step 21620, loss = 1.22 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 02:03:14.862002: step 21630, loss = 0.83 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 02:03:27.275111: step 21640, loss = 1.05 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 02:03:39.483347: step 21650, loss = 1.03 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 02:03:51.851349: step 21660, loss = 1.11 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 02:04:04.310610: step 21670, loss = 1.02 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 02:04:16.482343: step 21680, loss = 1.11 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 02:04:28.937412: step 21690, loss = 1.07 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 02:04:41.253533: step 21700, loss = 1.11 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 02:04:58.796098: step 21710, loss = 1.07 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 02:05:11.317312: step 21720, loss = 1.03 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-06-17 02:05:23.573441: step 21730, loss = 1.10 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 02:05:36.042824: step 21740, loss = 1.15 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 02:05:48.504063: step 21750, loss = 1.12 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 02:06:01.050627: step 21760, loss = 1.00 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 02:06:13.392706: step 21770, loss = 0.99 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 02:06:25.765277: step 21780, loss = 1.19 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 02:06:38.368653: step 21790, loss = 0.99 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 02:06:50.823860: step 21800, loss = 1.31 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 02:07:07.802138: step 21810, loss = 1.06 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 02:07:20.236878: step 21820, loss = 1.10 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-17 02:07:32.485903: step 21830, loss = 1.12 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 02:07:44.843664: step 21840, loss = 1.10 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 02:07:57.132602: step 21850, loss = 1.10 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 02:08:09.464819: step 21860, loss = 1.14 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 02:08:21.882389: step 21870, loss = 1.13 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 02:08:34.326355: step 21880, loss = 1.18 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 02:08:46.799244: step 21890, loss = 1.01 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 02:08:59.062404: step 21900, loss = 1.14 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 02:09:15.807013: step 21910, loss = 1.33 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 02:09:28.050642: step 21920, loss = 1.12 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 02:09:40.439270: step 21930, loss = 1.05 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 02:09:52.734227: step 21940, loss = 0.94 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 02:10:05.044619: step 21950, loss = 0.96 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 02:10:17.283607: step 21960, loss = 1.17 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 02:10:29.520326: step 21970, loss = 0.97 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 02:10:41.711429: step 21980, loss = 1.12 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 02:10:54.021787: step 21990, loss = 0.95 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 02:11:06.283523: step 22000, loss = 0.98 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 02:11:23.147388: step 22010, loss = 1.33 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 02:11:35.633679: step 22020, loss = 1.13 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 02:11:48.024166: step 22030, loss = 1.05 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-17 02:12:00.319223: step 22040, loss = 1.09 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 02:12:12.610205: step 22050, loss = 0.96 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 02:12:24.813877: step 22060, loss = 1.08 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 02:12:37.372290: step 22070, loss = 1.10 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 02:12:49.590234: step 22080, loss = 1.04 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 02:13:02.278420: step 22090, loss = 1.18 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 02:13:14.622239: step 22100, loss = 1.42 (24.0 examples/sec; 1.247 sec/batch)\n",
      "2019-06-17 02:13:31.326239: step 22110, loss = 0.97 (22.5 examples/sec; 1.332 sec/batch)\n",
      "2019-06-17 02:13:43.697765: step 22120, loss = 1.22 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 02:13:55.854010: step 22130, loss = 1.09 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 02:14:08.302612: step 22140, loss = 0.95 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 02:14:20.514787: step 22150, loss = 1.17 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 02:14:33.229229: step 22160, loss = 1.16 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 02:14:45.487759: step 22170, loss = 1.13 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 02:14:57.733400: step 22180, loss = 1.02 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 02:15:10.149918: step 22190, loss = 1.02 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 02:15:22.814588: step 22200, loss = 0.88 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 02:15:39.528916: step 22210, loss = 1.23 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 02:15:51.974752: step 22220, loss = 1.15 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 02:16:04.373934: step 22230, loss = 0.97 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 02:16:16.741461: step 22240, loss = 0.95 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 02:16:29.159377: step 22250, loss = 0.91 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 02:16:41.537463: step 22260, loss = 1.19 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 02:16:54.024378: step 22270, loss = 1.01 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 02:17:06.310445: step 22280, loss = 1.01 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 02:17:18.828210: step 22290, loss = 1.32 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 02:17:31.141105: step 22300, loss = 1.14 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 02:17:48.195993: step 22310, loss = 1.12 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 02:18:00.552289: step 22320, loss = 0.95 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 02:18:13.044648: step 22330, loss = 0.96 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 02:18:25.392004: step 22340, loss = 1.09 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 02:18:37.902875: step 22350, loss = 1.02 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 02:18:50.232161: step 22360, loss = 1.14 (24.9 examples/sec; 1.205 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 02:19:02.417698: step 22370, loss = 1.11 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 02:19:14.734283: step 22380, loss = 1.24 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 02:19:26.991870: step 22390, loss = 0.97 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 02:19:39.478433: step 22400, loss = 1.22 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-17 02:19:56.545615: step 22410, loss = 1.22 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 02:20:08.782728: step 22420, loss = 1.12 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 02:20:21.152531: step 22430, loss = 1.06 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-06-17 02:20:33.619493: step 22440, loss = 1.07 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 02:20:45.889942: step 22450, loss = 1.05 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 02:20:58.214615: step 22460, loss = 1.15 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 02:21:10.478618: step 22470, loss = 1.22 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 02:21:22.729372: step 22480, loss = 1.12 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 02:21:35.137117: step 22490, loss = 1.10 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 02:21:47.461670: step 22500, loss = 1.17 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 02:22:04.459043: step 22510, loss = 1.01 (23.4 examples/sec; 1.283 sec/batch)\n",
      "2019-06-17 02:22:16.670563: step 22520, loss = 1.18 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 02:22:29.035929: step 22530, loss = 1.25 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-06-17 02:22:41.537591: step 22540, loss = 1.08 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 02:22:53.748161: step 22550, loss = 1.16 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-06-17 02:23:06.136491: step 22560, loss = 1.08 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 02:23:18.537676: step 22570, loss = 0.96 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 02:23:30.947241: step 22580, loss = 0.97 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 02:23:43.171905: step 22590, loss = 0.94 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 02:23:55.652962: step 22600, loss = 1.07 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 02:24:12.450994: step 22610, loss = 1.04 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 02:24:24.655541: step 22620, loss = 0.91 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 02:24:36.971506: step 22630, loss = 1.09 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-06-17 02:24:49.196344: step 22640, loss = 1.21 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 02:25:01.455202: step 22650, loss = 1.21 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 02:25:13.920625: step 22660, loss = 1.04 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 02:25:26.170556: step 22670, loss = 1.05 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 02:25:38.472497: step 22680, loss = 0.98 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-06-17 02:25:50.726092: step 22690, loss = 1.03 (24.0 examples/sec; 1.247 sec/batch)\n",
      "2019-06-17 02:26:03.280524: step 22700, loss = 0.89 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 02:26:20.372615: step 22710, loss = 1.05 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 02:26:32.725150: step 22720, loss = 1.10 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 02:26:45.188898: step 22730, loss = 1.26 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 02:26:57.398231: step 22740, loss = 1.14 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-06-17 02:27:09.803092: step 22750, loss = 1.16 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 02:27:22.166105: step 22760, loss = 1.01 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 02:27:34.611332: step 22770, loss = 0.98 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 02:27:47.063840: step 22780, loss = 1.09 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-17 02:27:59.466564: step 22790, loss = 1.11 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 02:28:11.724031: step 22800, loss = 1.15 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 02:28:28.532342: step 22810, loss = 0.95 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 02:28:40.974285: step 22820, loss = 1.10 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 02:28:53.220592: step 22830, loss = 0.99 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 02:29:05.754571: step 22840, loss = 0.96 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 02:29:18.234999: step 22850, loss = 1.01 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 02:29:30.447949: step 22860, loss = 0.98 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 02:29:42.899465: step 22870, loss = 1.26 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 02:29:55.324041: step 22880, loss = 0.91 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-17 02:30:07.631148: step 22890, loss = 1.34 (23.0 examples/sec; 1.307 sec/batch)\n",
      "2019-06-17 02:30:20.126649: step 22900, loss = 0.97 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 02:30:37.072239: step 22910, loss = 1.01 (23.6 examples/sec; 1.269 sec/batch)\n",
      "2019-06-17 02:30:49.254922: step 22920, loss = 1.07 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 02:31:01.830947: step 22930, loss = 1.00 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 02:31:14.388648: step 22940, loss = 1.01 (23.8 examples/sec; 1.263 sec/batch)\n",
      "2019-06-17 02:31:26.887740: step 22950, loss = 1.01 (22.6 examples/sec; 1.328 sec/batch)\n",
      "2019-06-17 02:31:39.240853: step 22960, loss = 0.98 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-06-17 02:31:51.729743: step 22970, loss = 1.13 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 02:32:04.198943: step 22980, loss = 1.05 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 02:32:16.653717: step 22990, loss = 1.00 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 02:32:28.909320: step 23000, loss = 0.87 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 02:32:45.604478: step 23010, loss = 1.12 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 02:32:58.090464: step 23020, loss = 1.19 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 02:33:10.750041: step 23030, loss = 1.11 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 02:33:23.070634: step 23040, loss = 1.07 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 02:33:35.553309: step 23050, loss = 1.21 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 02:33:48.020379: step 23060, loss = 0.99 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 02:34:00.304828: step 23070, loss = 1.05 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 02:34:12.584526: step 23080, loss = 1.17 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 02:34:24.837286: step 23090, loss = 1.20 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 02:34:37.244756: step 23100, loss = 1.19 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-06-17 02:34:54.156352: step 23110, loss = 1.01 (23.5 examples/sec; 1.276 sec/batch)\n",
      "2019-06-17 02:35:06.456237: step 23120, loss = 1.03 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-17 02:35:18.844755: step 23130, loss = 1.10 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 02:35:31.112841: step 23140, loss = 0.96 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 02:35:43.515844: step 23150, loss = 1.05 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 02:35:55.659870: step 23160, loss = 1.07 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 02:36:08.003582: step 23170, loss = 1.02 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 02:36:20.191532: step 23180, loss = 0.94 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 02:36:32.457018: step 23190, loss = 1.05 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-06-17 02:36:45.087158: step 23200, loss = 1.05 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 02:37:02.043612: step 23210, loss = 1.04 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 02:37:14.292884: step 23220, loss = 1.08 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 02:37:26.527521: step 23230, loss = 1.03 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 02:37:39.019017: step 23240, loss = 1.01 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 02:37:51.257504: step 23250, loss = 0.98 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 02:38:03.664564: step 23260, loss = 0.89 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 02:38:15.833066: step 23270, loss = 1.27 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 02:38:28.211807: step 23280, loss = 1.11 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 02:38:40.597102: step 23290, loss = 1.00 (23.8 examples/sec; 1.263 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 02:38:53.043765: step 23300, loss = 1.12 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 02:39:09.839673: step 23310, loss = 0.96 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 02:39:22.033311: step 23320, loss = 1.01 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 02:39:34.279276: step 23330, loss = 0.92 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 02:39:46.529513: step 23340, loss = 0.97 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 02:39:59.078630: step 23350, loss = 0.86 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 02:40:11.493663: step 23360, loss = 1.21 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 02:40:23.783077: step 23370, loss = 0.96 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-17 02:40:35.982028: step 23380, loss = 0.94 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 02:40:48.293100: step 23390, loss = 1.23 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 02:41:00.580416: step 23400, loss = 1.14 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 02:41:17.738645: step 23410, loss = 1.01 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-17 02:41:30.167925: step 23420, loss = 1.25 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 02:41:42.478735: step 23430, loss = 1.00 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 02:41:54.732212: step 23440, loss = 1.10 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 02:42:07.025420: step 23450, loss = 1.04 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 02:42:19.339919: step 23460, loss = 1.35 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 02:42:31.562949: step 23470, loss = 1.04 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 02:42:43.814473: step 23480, loss = 1.12 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 02:42:56.272203: step 23490, loss = 1.00 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 02:43:08.695461: step 23500, loss = 1.16 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 02:43:25.462463: step 23510, loss = 1.00 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 02:43:37.817338: step 23520, loss = 0.97 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 02:43:50.120744: step 23530, loss = 1.27 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-17 02:44:02.330338: step 23540, loss = 1.01 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 02:44:14.545446: step 23550, loss = 1.07 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 02:44:26.749128: step 23560, loss = 1.04 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 02:44:39.189474: step 23570, loss = 1.17 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 02:44:51.449262: step 23580, loss = 1.15 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 02:45:03.922060: step 23590, loss = 1.37 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 02:45:16.172107: step 23600, loss = 1.13 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 02:45:33.104296: step 23610, loss = 1.06 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 02:45:45.388584: step 23620, loss = 1.02 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 02:45:57.747259: step 23630, loss = 1.06 (23.4 examples/sec; 1.281 sec/batch)\n",
      "2019-06-17 02:46:10.133722: step 23640, loss = 0.83 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 02:46:22.571466: step 23650, loss = 0.86 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 02:46:35.026333: step 23660, loss = 0.98 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 02:46:47.160014: step 23670, loss = 0.94 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 02:46:59.444111: step 23680, loss = 0.98 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 02:47:11.734013: step 23690, loss = 1.30 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 02:47:24.287369: step 23700, loss = 1.14 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 02:47:40.789048: step 23710, loss = 0.87 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 02:47:53.000172: step 23720, loss = 0.99 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 02:48:05.216586: step 23730, loss = 0.95 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-06-17 02:48:17.459420: step 23740, loss = 1.18 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-17 02:48:29.600658: step 23750, loss = 1.00 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 02:48:41.943921: step 23760, loss = 0.95 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-06-17 02:48:54.091769: step 23770, loss = 1.17 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 02:49:06.250456: step 23780, loss = 1.03 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-06-17 02:49:18.718343: step 23790, loss = 0.95 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-06-17 02:49:31.068715: step 23800, loss = 1.12 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 02:49:47.701834: step 23810, loss = 0.88 (23.8 examples/sec; 1.258 sec/batch)\n",
      "2019-06-17 02:50:00.098207: step 23820, loss = 1.25 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 02:50:12.386042: step 23830, loss = 1.21 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 02:50:24.796596: step 23840, loss = 1.19 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 02:50:37.089973: step 23850, loss = 1.04 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 02:50:49.541918: step 23860, loss = 1.24 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 02:51:01.977240: step 23870, loss = 0.94 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 02:51:14.279375: step 23880, loss = 1.21 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 02:51:26.636676: step 23890, loss = 1.03 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 02:51:39.014870: step 23900, loss = 1.10 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 02:51:55.511112: step 23910, loss = 1.12 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 02:52:07.883829: step 23920, loss = 1.00 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 02:52:20.339129: step 23930, loss = 1.32 (23.1 examples/sec; 1.300 sec/batch)\n",
      "2019-06-17 02:52:33.042771: step 23940, loss = 0.99 (23.0 examples/sec; 1.307 sec/batch)\n",
      "2019-06-17 02:52:45.638688: step 23950, loss = 0.90 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 02:52:58.127568: step 23960, loss = 1.00 (22.5 examples/sec; 1.333 sec/batch)\n",
      "2019-06-17 02:53:10.409938: step 23970, loss = 1.13 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 02:53:22.739483: step 23980, loss = 1.08 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 02:53:35.248505: step 23990, loss = 1.19 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 02:53:47.602950: step 24000, loss = 1.02 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-17 02:54:04.376909: step 24010, loss = 1.01 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 02:54:16.821105: step 24020, loss = 0.99 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 02:54:29.129987: step 24030, loss = 0.93 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 02:54:41.595618: step 24040, loss = 1.36 (23.2 examples/sec; 1.295 sec/batch)\n",
      "2019-06-17 02:54:53.816741: step 24050, loss = 1.22 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 02:55:06.018938: step 24060, loss = 1.10 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 02:55:18.325023: step 24070, loss = 1.01 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 02:55:30.514460: step 24080, loss = 1.06 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 02:55:43.073614: step 24090, loss = 1.06 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 02:55:55.398040: step 24100, loss = 0.98 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 02:56:12.359257: step 24110, loss = 1.19 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 02:56:24.726142: step 24120, loss = 0.95 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 02:56:36.828016: step 24130, loss = 1.02 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 02:56:49.255093: step 24140, loss = 1.33 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-17 02:57:01.473875: step 24150, loss = 0.91 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 02:57:13.831577: step 24160, loss = 1.11 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 02:57:26.070758: step 24170, loss = 1.13 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 02:57:38.439339: step 24180, loss = 0.93 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 02:57:50.928361: step 24190, loss = 1.06 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 02:58:03.175608: step 24200, loss = 1.17 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 02:58:19.655008: step 24210, loss = 0.91 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 02:58:32.092661: step 24220, loss = 1.18 (24.7 examples/sec; 1.215 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 02:58:44.402962: step 24230, loss = 1.22 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 02:58:56.841757: step 24240, loss = 1.03 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 02:59:09.138288: step 24250, loss = 1.12 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 02:59:21.528572: step 24260, loss = 1.06 (23.6 examples/sec; 1.271 sec/batch)\n",
      "2019-06-17 02:59:33.851288: step 24270, loss = 0.91 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-17 02:59:46.102647: step 24280, loss = 1.02 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 02:59:58.684111: step 24290, loss = 1.00 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 03:00:11.091478: step 24300, loss = 1.10 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 03:00:27.887414: step 24310, loss = 0.95 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 03:00:40.054356: step 24320, loss = 0.95 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 03:00:52.313928: step 24330, loss = 1.13 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 03:01:04.582830: step 24340, loss = 0.95 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-06-17 03:01:16.772014: step 24350, loss = 1.18 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 03:01:29.116127: step 24360, loss = 1.27 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 03:01:41.601219: step 24370, loss = 1.09 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 03:01:54.003076: step 24380, loss = 0.99 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 03:02:06.189861: step 24390, loss = 1.00 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 03:02:18.573507: step 24400, loss = 0.96 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-06-17 03:02:35.623112: step 24410, loss = 0.98 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 03:02:47.851243: step 24420, loss = 1.08 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 03:03:00.085459: step 24430, loss = 1.26 (22.5 examples/sec; 1.333 sec/batch)\n",
      "2019-06-17 03:03:12.385350: step 24440, loss = 1.04 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 03:03:24.629122: step 24450, loss = 1.16 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 03:03:36.833987: step 24460, loss = 1.19 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 03:03:49.313115: step 24470, loss = 1.01 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 03:04:01.610945: step 24480, loss = 1.01 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 03:04:13.845417: step 24490, loss = 0.89 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 03:04:26.302769: step 24500, loss = 1.02 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 03:04:42.854926: step 24510, loss = 1.01 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 03:04:55.239609: step 24520, loss = 1.06 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 03:05:07.684247: step 24530, loss = 0.99 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 03:05:19.848390: step 24540, loss = 1.03 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 03:05:32.098931: step 24550, loss = 1.03 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 03:05:44.279443: step 24560, loss = 1.28 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 03:05:56.517718: step 24570, loss = 0.93 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 03:06:08.761774: step 24580, loss = 0.96 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 03:06:21.113874: step 24590, loss = 1.12 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 03:06:33.313649: step 24600, loss = 1.04 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 03:06:50.598972: step 24610, loss = 1.09 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 03:07:03.042392: step 24620, loss = 1.06 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 03:07:15.407979: step 24630, loss = 1.14 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 03:07:27.709583: step 24640, loss = 1.23 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-17 03:07:40.111359: step 24650, loss = 1.00 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 03:07:52.459673: step 24660, loss = 1.19 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 03:08:04.659203: step 24670, loss = 1.00 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-06-17 03:08:16.986794: step 24680, loss = 1.08 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 03:08:29.147753: step 24690, loss = 1.05 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 03:08:41.394547: step 24700, loss = 0.91 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 03:08:57.901451: step 24710, loss = 1.01 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 03:09:10.192935: step 24720, loss = 1.04 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 03:09:22.618987: step 24730, loss = 0.99 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 03:09:35.046547: step 24740, loss = 0.90 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 03:09:47.220693: step 24750, loss = 1.21 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 03:09:59.717128: step 24760, loss = 1.04 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 03:10:12.071653: step 24770, loss = 0.99 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 03:10:24.552826: step 24780, loss = 1.14 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 03:10:36.856815: step 24790, loss = 1.03 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 03:10:49.317884: step 24800, loss = 1.36 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 03:11:06.173371: step 24810, loss = 1.18 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 03:11:18.532447: step 24820, loss = 1.09 (22.9 examples/sec; 1.307 sec/batch)\n",
      "2019-06-17 03:11:30.780744: step 24830, loss = 0.96 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 03:11:43.079863: step 24840, loss = 0.95 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-17 03:11:55.538838: step 24850, loss = 0.98 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-06-17 03:12:07.941965: step 24860, loss = 1.03 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 03:12:20.286797: step 24870, loss = 0.99 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 03:12:32.660006: step 24880, loss = 1.16 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 03:12:44.784012: step 24890, loss = 1.07 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 03:12:57.138871: step 24900, loss = 1.21 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 03:13:13.574705: step 24910, loss = 0.97 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 03:13:25.817402: step 24920, loss = 1.02 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-17 03:13:38.046616: step 24930, loss = 1.03 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 03:13:50.412100: step 24940, loss = 1.01 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 03:14:02.625670: step 24950, loss = 1.16 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 03:14:15.229005: step 24960, loss = 0.99 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 03:14:27.382403: step 24970, loss = 1.06 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 03:14:39.646856: step 24980, loss = 1.12 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 03:14:51.872589: step 24990, loss = 0.97 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 03:15:04.101607: step 25000, loss = 1.03 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-17 03:15:24.316884: step 25010, loss = 0.98 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 03:15:36.651400: step 25020, loss = 1.25 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 03:15:48.977628: step 25030, loss = 1.14 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 03:16:01.139853: step 25040, loss = 1.08 (23.6 examples/sec; 1.271 sec/batch)\n",
      "2019-06-17 03:16:13.492551: step 25050, loss = 0.89 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 03:16:26.066700: step 25060, loss = 0.90 (24.1 examples/sec; 1.242 sec/batch)\n",
      "2019-06-17 03:16:38.555270: step 25070, loss = 1.32 (23.5 examples/sec; 1.276 sec/batch)\n",
      "2019-06-17 03:16:50.698861: step 25080, loss = 1.02 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 03:17:03.126979: step 25090, loss = 1.32 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-17 03:17:15.548746: step 25100, loss = 1.04 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 03:17:32.200237: step 25110, loss = 1.02 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 03:17:44.597230: step 25120, loss = 0.95 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 03:17:57.079589: step 25130, loss = 0.90 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 03:18:09.343660: step 25140, loss = 1.17 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 03:18:21.682753: step 25150, loss = 1.05 (24.8 examples/sec; 1.208 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 03:18:33.809887: step 25160, loss = 1.25 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 03:18:46.057203: step 25170, loss = 0.89 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 03:18:58.473972: step 25180, loss = 0.97 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 03:19:10.645257: step 25190, loss = 1.26 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 03:19:22.993111: step 25200, loss = 1.25 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 03:19:39.782268: step 25210, loss = 1.30 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 03:19:52.156845: step 25220, loss = 0.94 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 03:20:04.361090: step 25230, loss = 1.07 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 03:20:16.726031: step 25240, loss = 1.20 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 03:20:29.083015: step 25250, loss = 1.11 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 03:20:41.454017: step 25260, loss = 1.04 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 03:20:54.026492: step 25270, loss = 1.15 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 03:21:06.187784: step 25280, loss = 1.09 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 03:21:18.556598: step 25290, loss = 1.07 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 03:21:30.764739: step 25300, loss = 1.10 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 03:21:47.490388: step 25310, loss = 1.22 (23.6 examples/sec; 1.271 sec/batch)\n",
      "2019-06-17 03:21:59.819540: step 25320, loss = 1.02 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 03:22:12.238427: step 25330, loss = 0.95 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 03:22:24.397697: step 25340, loss = 1.38 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 03:22:36.718741: step 25350, loss = 1.05 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 03:22:48.885669: step 25360, loss = 1.21 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 03:23:01.086298: step 25370, loss = 1.22 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 03:23:13.533525: step 25380, loss = 1.12 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 03:23:25.928891: step 25390, loss = 1.12 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 03:23:38.175564: step 25400, loss = 1.09 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 03:23:54.993735: step 25410, loss = 1.26 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 03:24:07.481153: step 25420, loss = 1.11 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 03:24:19.748685: step 25430, loss = 0.97 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 03:24:31.949439: step 25440, loss = 1.21 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 03:24:44.254401: step 25450, loss = 1.02 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 03:24:56.582988: step 25460, loss = 0.99 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 03:25:09.086692: step 25470, loss = 1.06 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 03:25:21.496249: step 25480, loss = 0.96 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 03:25:33.967767: step 25490, loss = 0.84 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 03:25:46.498521: step 25500, loss = 1.22 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 03:26:03.099506: step 25510, loss = 1.11 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 03:26:15.494132: step 25520, loss = 1.01 (22.5 examples/sec; 1.335 sec/batch)\n",
      "2019-06-17 03:26:27.686329: step 25530, loss = 1.00 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 03:26:39.830378: step 25540, loss = 1.03 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 03:26:52.275882: step 25550, loss = 1.16 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 03:27:04.624218: step 25560, loss = 1.26 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 03:27:16.874831: step 25570, loss = 0.97 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 03:27:29.179660: step 25580, loss = 1.38 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 03:27:41.541487: step 25590, loss = 1.04 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 03:27:53.861795: step 25600, loss = 1.15 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 03:28:10.591810: step 25610, loss = 1.19 (22.8 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 03:28:23.009192: step 25620, loss = 1.08 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 03:28:35.348798: step 25630, loss = 1.21 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 03:28:47.629829: step 25640, loss = 1.09 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 03:28:59.826694: step 25650, loss = 1.05 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 03:29:12.119102: step 25660, loss = 1.12 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 03:29:24.439728: step 25670, loss = 0.95 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 03:29:36.723866: step 25680, loss = 1.09 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 03:29:49.164011: step 25690, loss = 1.11 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 03:30:01.434961: step 25700, loss = 0.92 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 03:30:18.119402: step 25710, loss = 1.16 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 03:30:30.516796: step 25720, loss = 0.91 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 03:30:42.735836: step 25730, loss = 1.17 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 03:30:55.260222: step 25740, loss = 1.18 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 03:31:07.732577: step 25750, loss = 0.95 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 03:31:20.015540: step 25760, loss = 1.12 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 03:31:32.412567: step 25770, loss = 0.94 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 03:31:44.647542: step 25780, loss = 1.00 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 03:31:56.799830: step 25790, loss = 1.09 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 03:32:09.061542: step 25800, loss = 1.13 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 03:32:25.603794: step 25810, loss = 1.09 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 03:32:38.090647: step 25820, loss = 1.25 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 03:32:50.362947: step 25830, loss = 1.34 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 03:33:02.703134: step 25840, loss = 0.92 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 03:33:14.967056: step 25850, loss = 0.96 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 03:33:27.491047: step 25860, loss = 1.08 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 03:33:40.074402: step 25870, loss = 1.07 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 03:33:52.295298: step 25880, loss = 1.18 (23.3 examples/sec; 1.285 sec/batch)\n",
      "2019-06-17 03:34:04.566639: step 25890, loss = 1.14 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 03:34:16.840725: step 25900, loss = 1.13 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 03:34:33.576426: step 25910, loss = 1.01 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 03:34:45.855206: step 25920, loss = 1.08 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 03:34:58.209670: step 25930, loss = 1.13 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 03:35:10.359559: step 25940, loss = 0.95 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 03:35:22.695393: step 25950, loss = 1.26 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 03:35:35.152210: step 25960, loss = 1.11 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 03:35:47.736942: step 25970, loss = 0.92 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-17 03:36:00.000989: step 25980, loss = 1.10 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-06-17 03:36:12.603862: step 25990, loss = 1.08 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 03:36:25.075543: step 26000, loss = 1.05 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 03:36:41.965011: step 26010, loss = 0.93 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 03:36:54.486940: step 26020, loss = 1.04 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 03:37:06.835805: step 26030, loss = 0.98 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 03:37:19.104962: step 26040, loss = 1.00 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 03:37:31.557911: step 26050, loss = 1.30 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 03:37:43.787901: step 26060, loss = 1.02 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 03:37:56.174774: step 26070, loss = 1.21 (22.5 examples/sec; 1.334 sec/batch)\n",
      "2019-06-17 03:38:08.621723: step 26080, loss = 1.28 (24.6 examples/sec; 1.222 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 03:38:20.981179: step 26090, loss = 1.00 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 03:38:33.235912: step 26100, loss = 1.03 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 03:38:50.013249: step 26110, loss = 1.01 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 03:39:02.250952: step 26120, loss = 0.94 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 03:39:14.581086: step 26130, loss = 1.11 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 03:39:27.023483: step 26140, loss = 1.29 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 03:39:39.388575: step 26150, loss = 1.08 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 03:39:51.901239: step 26160, loss = 0.91 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 03:40:04.286581: step 26170, loss = 1.02 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 03:40:16.483065: step 26180, loss = 1.09 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-06-17 03:40:28.948807: step 26190, loss = 1.06 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 03:40:41.334106: step 26200, loss = 0.95 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 03:40:57.900044: step 26210, loss = 1.15 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 03:41:10.259177: step 26220, loss = 1.08 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 03:41:22.797419: step 26230, loss = 1.04 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 03:41:35.492461: step 26240, loss = 0.96 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 03:41:47.746905: step 26250, loss = 1.08 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 03:42:00.053952: step 26260, loss = 1.11 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 03:42:12.517912: step 26270, loss = 1.14 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 03:42:24.833738: step 26280, loss = 0.97 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 03:42:37.182666: step 26290, loss = 1.12 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 03:42:49.634603: step 26300, loss = 0.98 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 03:43:06.363898: step 26310, loss = 1.10 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 03:43:18.718412: step 26320, loss = 1.08 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 03:43:31.052893: step 26330, loss = 0.99 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 03:43:43.464026: step 26340, loss = 0.92 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 03:43:56.233833: step 26350, loss = 1.22 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 03:44:08.431953: step 26360, loss = 1.05 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 03:44:20.661298: step 26370, loss = 1.06 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 03:44:32.839997: step 26380, loss = 1.11 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 03:44:45.084487: step 26390, loss = 1.08 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 03:44:57.367103: step 26400, loss = 0.97 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 03:45:14.099276: step 26410, loss = 1.01 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 03:45:26.517059: step 26420, loss = 0.97 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 03:45:38.995058: step 26430, loss = 1.15 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 03:45:51.570297: step 26440, loss = 1.14 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 03:46:03.939926: step 26450, loss = 1.00 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 03:46:16.358972: step 26460, loss = 1.04 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 03:46:28.705178: step 26470, loss = 1.09 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 03:46:41.080300: step 26480, loss = 0.96 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 03:46:53.391609: step 26490, loss = 1.24 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 03:47:05.906482: step 26500, loss = 1.23 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 03:47:22.706042: step 26510, loss = 0.91 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 03:47:34.899894: step 26520, loss = 1.18 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 03:47:47.534584: step 26530, loss = 1.07 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 03:47:59.912286: step 26540, loss = 1.05 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 03:48:12.464604: step 26550, loss = 1.22 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 03:48:24.921906: step 26560, loss = 0.97 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 03:48:37.297083: step 26570, loss = 0.88 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-06-17 03:48:49.577491: step 26580, loss = 1.06 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 03:49:01.994654: step 26590, loss = 1.01 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-06-17 03:49:14.330761: step 26600, loss = 1.12 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 03:49:31.276235: step 26610, loss = 0.99 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 03:49:43.693802: step 26620, loss = 1.21 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 03:49:56.385153: step 26630, loss = 1.03 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 03:50:08.613669: step 26640, loss = 1.18 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 03:50:20.942911: step 26650, loss = 1.12 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 03:50:33.416674: step 26660, loss = 0.95 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 03:50:45.917514: step 26670, loss = 1.13 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 03:50:58.174158: step 26680, loss = 1.03 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 03:51:10.585051: step 26690, loss = 1.05 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 03:51:22.979459: step 26700, loss = 0.97 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 03:51:39.821750: step 26710, loss = 1.31 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 03:51:52.108940: step 26720, loss = 1.08 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 03:52:04.549956: step 26730, loss = 1.06 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 03:52:16.880585: step 26740, loss = 0.91 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 03:52:29.258685: step 26750, loss = 1.35 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 03:52:41.666122: step 26760, loss = 1.03 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 03:52:54.244088: step 26770, loss = 1.26 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 03:53:06.685118: step 26780, loss = 1.13 (22.5 examples/sec; 1.333 sec/batch)\n",
      "2019-06-17 03:53:18.988936: step 26790, loss = 0.94 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 03:53:31.457191: step 26800, loss = 0.91 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 03:53:48.380695: step 26810, loss = 1.01 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 03:54:00.813835: step 26820, loss = 1.07 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 03:54:13.500937: step 26830, loss = 1.13 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 03:54:25.860757: step 26840, loss = 1.02 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-06-17 03:54:38.308563: step 26850, loss = 0.95 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 03:54:50.992360: step 26860, loss = 0.96 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 03:55:03.273730: step 26870, loss = 1.05 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 03:55:15.674288: step 26880, loss = 1.19 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 03:55:27.915928: step 26890, loss = 1.04 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 03:55:40.126715: step 26900, loss = 0.98 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 03:55:57.059228: step 26910, loss = 0.99 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 03:56:09.524601: step 26920, loss = 1.02 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 03:56:21.759039: step 26930, loss = 1.16 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 03:56:33.934454: step 26940, loss = 1.15 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 03:56:46.431430: step 26950, loss = 1.15 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-06-17 03:56:58.706890: step 26960, loss = 1.10 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 03:57:11.027289: step 26970, loss = 0.97 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 03:57:23.424699: step 26980, loss = 1.08 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 03:57:35.664333: step 26990, loss = 1.29 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 03:57:47.955038: step 27000, loss = 1.04 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 03:58:04.678070: step 27010, loss = 1.19 (24.8 examples/sec; 1.211 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 03:58:16.951026: step 27020, loss = 0.88 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 03:58:29.118689: step 27030, loss = 1.03 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 03:58:41.465874: step 27040, loss = 0.86 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 03:58:53.738664: step 27050, loss = 1.14 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 03:59:06.742034: step 27060, loss = 1.06 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 03:59:19.020500: step 27070, loss = 1.09 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 03:59:31.437483: step 27080, loss = 1.07 (23.6 examples/sec; 1.274 sec/batch)\n",
      "2019-06-17 03:59:43.782317: step 27090, loss = 1.22 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 03:59:56.244430: step 27100, loss = 1.13 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-17 04:00:12.771475: step 27110, loss = 1.11 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 04:00:25.163712: step 27120, loss = 1.18 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 04:00:37.446107: step 27130, loss = 1.27 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 04:00:49.797583: step 27140, loss = 0.97 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 04:01:02.057020: step 27150, loss = 1.02 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 04:01:14.539901: step 27160, loss = 0.97 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 04:01:26.897108: step 27170, loss = 0.89 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 04:01:39.187604: step 27180, loss = 1.16 (23.4 examples/sec; 1.279 sec/batch)\n",
      "2019-06-17 04:01:51.536220: step 27190, loss = 0.83 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 04:02:03.841322: step 27200, loss = 1.02 (23.2 examples/sec; 1.290 sec/batch)\n",
      "2019-06-17 04:02:20.639293: step 27210, loss = 1.21 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 04:02:33.034809: step 27220, loss = 1.06 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 04:02:45.499168: step 27230, loss = 1.24 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 04:02:58.009297: step 27240, loss = 1.09 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 04:03:10.303141: step 27250, loss = 1.03 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 04:03:22.628236: step 27260, loss = 1.14 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 04:03:34.814006: step 27270, loss = 1.08 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 04:03:47.084500: step 27280, loss = 0.94 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 04:03:59.494199: step 27290, loss = 0.93 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 04:04:11.679191: step 27300, loss = 1.15 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 04:04:28.897400: step 27310, loss = 1.16 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 04:04:41.070695: step 27320, loss = 0.93 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 04:04:53.584031: step 27330, loss = 0.89 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-17 04:05:06.006957: step 27340, loss = 1.25 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 04:05:18.353901: step 27350, loss = 1.06 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 04:05:30.691402: step 27360, loss = 1.17 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-17 04:05:42.939361: step 27370, loss = 1.27 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 04:05:55.265719: step 27380, loss = 0.97 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 04:06:07.514956: step 27390, loss = 0.98 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 04:06:19.872031: step 27400, loss = 1.27 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 04:06:36.479826: step 27410, loss = 1.11 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 04:06:48.961204: step 27420, loss = 1.01 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 04:07:01.277944: step 27430, loss = 1.07 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 04:07:13.644431: step 27440, loss = 0.95 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-06-17 04:07:26.053083: step 27450, loss = 1.20 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 04:07:38.428375: step 27460, loss = 1.07 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 04:07:50.780350: step 27470, loss = 1.15 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 04:08:03.088453: step 27480, loss = 0.94 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 04:08:15.424957: step 27490, loss = 0.94 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 04:08:27.703841: step 27500, loss = 1.23 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-06-17 04:08:44.583715: step 27510, loss = 1.03 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 04:08:57.017380: step 27520, loss = 1.12 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 04:09:09.389622: step 27530, loss = 0.94 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-06-17 04:09:21.671376: step 27540, loss = 1.12 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 04:09:34.241841: step 27550, loss = 1.07 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 04:09:46.465769: step 27560, loss = 1.00 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 04:09:58.691529: step 27570, loss = 0.89 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 04:10:11.104618: step 27580, loss = 0.99 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 04:10:23.693500: step 27590, loss = 1.03 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 04:10:35.974444: step 27600, loss = 1.01 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 04:10:52.983646: step 27610, loss = 1.30 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-17 04:11:05.387825: step 27620, loss = 1.03 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 04:11:17.657161: step 27630, loss = 0.88 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 04:11:29.814866: step 27640, loss = 1.07 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 04:11:42.102565: step 27650, loss = 1.08 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 04:11:54.513671: step 27660, loss = 1.23 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 04:12:06.820617: step 27670, loss = 1.14 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-06-17 04:12:19.054958: step 27680, loss = 0.93 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 04:12:31.377280: step 27690, loss = 0.99 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 04:12:43.735163: step 27700, loss = 1.07 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 04:13:00.896928: step 27710, loss = 0.96 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 04:13:13.441527: step 27720, loss = 1.04 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 04:13:25.824572: step 27730, loss = 1.09 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 04:13:38.101537: step 27740, loss = 0.98 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 04:13:50.461759: step 27750, loss = 0.99 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 04:14:03.029362: step 27760, loss = 0.95 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 04:14:15.331504: step 27770, loss = 1.18 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 04:14:27.608791: step 27780, loss = 1.31 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 04:14:39.811822: step 27790, loss = 1.14 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-17 04:14:52.151653: step 27800, loss = 1.14 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 04:15:09.265319: step 27810, loss = 1.17 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 04:15:21.420404: step 27820, loss = 1.13 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 04:15:33.861279: step 27830, loss = 1.08 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 04:15:46.148140: step 27840, loss = 0.95 (22.5 examples/sec; 1.334 sec/batch)\n",
      "2019-06-17 04:15:58.695382: step 27850, loss = 0.99 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 04:16:11.116336: step 27860, loss = 0.90 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 04:16:23.431891: step 27870, loss = 1.00 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 04:16:35.980956: step 27880, loss = 1.00 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 04:16:48.117739: step 27890, loss = 1.03 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 04:17:00.497852: step 27900, loss = 1.10 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 04:17:17.293171: step 27910, loss = 1.00 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 04:17:29.642931: step 27920, loss = 1.09 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 04:17:42.358547: step 27930, loss = 1.22 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 04:17:54.488674: step 27940, loss = 1.06 (24.7 examples/sec; 1.217 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 04:18:06.765027: step 27950, loss = 1.03 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 04:18:18.927549: step 27960, loss = 0.92 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 04:18:31.274010: step 27970, loss = 1.14 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 04:18:43.495455: step 27980, loss = 0.99 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 04:18:55.860147: step 27990, loss = 1.07 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 04:19:08.236680: step 28000, loss = 1.04 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 04:19:25.256751: step 28010, loss = 1.02 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 04:19:37.637489: step 28020, loss = 1.09 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-17 04:19:50.063258: step 28030, loss = 1.07 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 04:20:02.527880: step 28040, loss = 1.00 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 04:20:14.916229: step 28050, loss = 0.93 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 04:20:27.147780: step 28060, loss = 1.25 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 04:20:39.474555: step 28070, loss = 0.95 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 04:20:51.943894: step 28080, loss = 1.20 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 04:21:04.163640: step 28090, loss = 1.05 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 04:21:16.587583: step 28100, loss = 1.33 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 04:21:33.819267: step 28110, loss = 1.07 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 04:21:46.042151: step 28120, loss = 0.89 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 04:21:58.476023: step 28130, loss = 1.15 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 04:22:11.044254: step 28140, loss = 0.98 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-17 04:22:23.174046: step 28150, loss = 1.06 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 04:22:35.352053: step 28160, loss = 0.94 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 04:22:47.720376: step 28170, loss = 1.10 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 04:23:00.263550: step 28180, loss = 1.04 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 04:23:12.821921: step 28190, loss = 1.08 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 04:23:25.332060: step 28200, loss = 0.95 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 04:23:42.557207: step 28210, loss = 1.14 (23.8 examples/sec; 1.258 sec/batch)\n",
      "2019-06-17 04:23:54.897373: step 28220, loss = 0.93 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 04:24:07.194315: step 28230, loss = 1.11 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 04:24:19.557092: step 28240, loss = 1.05 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 04:24:31.936999: step 28250, loss = 1.01 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 04:24:44.214599: step 28260, loss = 1.09 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 04:24:56.550640: step 28270, loss = 1.35 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 04:25:08.746919: step 28280, loss = 1.04 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 04:25:21.081228: step 28290, loss = 1.05 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 04:25:33.425440: step 28300, loss = 1.18 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 04:25:50.144502: step 28310, loss = 0.91 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 04:26:02.389134: step 28320, loss = 1.16 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 04:26:14.735276: step 28330, loss = 1.05 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-06-17 04:26:27.102321: step 28340, loss = 1.01 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 04:26:39.487344: step 28350, loss = 1.12 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 04:26:51.809649: step 28360, loss = 0.96 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 04:27:04.403919: step 28370, loss = 0.92 (22.5 examples/sec; 1.335 sec/batch)\n",
      "2019-06-17 04:27:16.927941: step 28380, loss = 0.96 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 04:27:29.223015: step 28390, loss = 1.08 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 04:27:41.456757: step 28400, loss = 1.08 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-06-17 04:27:58.710388: step 28410, loss = 1.10 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 04:28:11.138180: step 28420, loss = 1.17 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 04:28:23.442013: step 28430, loss = 0.97 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 04:28:35.797141: step 28440, loss = 1.20 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 04:28:48.009852: step 28450, loss = 1.19 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 04:29:00.380292: step 28460, loss = 1.08 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 04:29:12.838799: step 28470, loss = 1.08 (23.5 examples/sec; 1.276 sec/batch)\n",
      "2019-06-17 04:29:25.252836: step 28480, loss = 1.12 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 04:29:37.659601: step 28490, loss = 1.07 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 04:29:49.890191: step 28500, loss = 1.00 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 04:30:06.726114: step 28510, loss = 1.10 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 04:30:18.897460: step 28520, loss = 1.04 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 04:30:31.465382: step 28530, loss = 1.08 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 04:30:43.937689: step 28540, loss = 1.38 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 04:30:56.192619: step 28550, loss = 1.09 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 04:31:08.571886: step 28560, loss = 1.11 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 04:31:20.767888: step 28570, loss = 1.05 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 04:31:33.116666: step 28580, loss = 1.32 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 04:31:45.264111: step 28590, loss = 1.08 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 04:31:57.621579: step 28600, loss = 0.95 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 04:32:14.516506: step 28610, loss = 1.03 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 04:32:26.810484: step 28620, loss = 1.06 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 04:32:39.045184: step 28630, loss = 1.11 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 04:32:51.291415: step 28640, loss = 1.33 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 04:33:03.597160: step 28650, loss = 1.10 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 04:33:16.047631: step 28660, loss = 0.87 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 04:33:28.298681: step 28670, loss = 1.00 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 04:33:40.586398: step 28680, loss = 1.21 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 04:33:53.013237: step 28690, loss = 1.05 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 04:34:05.162711: step 28700, loss = 1.28 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 04:34:21.931393: step 28710, loss = 1.13 (22.8 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 04:34:34.292673: step 28720, loss = 0.93 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 04:34:46.570307: step 28730, loss = 1.14 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 04:34:59.001895: step 28740, loss = 1.10 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 04:35:11.435589: step 28750, loss = 1.13 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 04:35:23.641521: step 28760, loss = 0.88 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 04:35:35.829481: step 28770, loss = 0.89 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 04:35:48.098010: step 28780, loss = 1.04 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 04:36:00.609052: step 28790, loss = 0.90 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 04:36:13.019064: step 28800, loss = 1.02 (22.5 examples/sec; 1.336 sec/batch)\n",
      "2019-06-17 04:36:29.665492: step 28810, loss = 1.36 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 04:36:42.009628: step 28820, loss = 1.11 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 04:36:54.083642: step 28830, loss = 1.10 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 04:37:06.385889: step 28840, loss = 1.04 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 04:37:18.822208: step 28850, loss = 0.91 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 04:37:31.163929: step 28860, loss = 1.07 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 04:37:43.356746: step 28870, loss = 0.99 (24.5 examples/sec; 1.225 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 04:37:55.522229: step 28880, loss = 1.28 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 04:38:07.897354: step 28890, loss = 1.03 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 04:38:20.497037: step 28900, loss = 1.12 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 04:38:37.482413: step 28910, loss = 1.14 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 04:38:49.777899: step 28920, loss = 0.98 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 04:39:02.036368: step 28930, loss = 1.25 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 04:39:14.188655: step 28940, loss = 0.94 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 04:39:26.520657: step 28950, loss = 1.18 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 04:39:38.878824: step 28960, loss = 1.24 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 04:39:51.137478: step 28970, loss = 1.23 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 04:40:03.403334: step 28980, loss = 1.11 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 04:40:15.804791: step 28990, loss = 0.95 (23.4 examples/sec; 1.284 sec/batch)\n",
      "2019-06-17 04:40:28.175624: step 29000, loss = 1.00 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 04:40:45.365574: step 29010, loss = 1.14 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 04:40:57.644127: step 29020, loss = 0.97 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 04:41:10.278500: step 29030, loss = 0.95 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 04:41:22.446327: step 29040, loss = 1.11 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 04:41:34.711026: step 29050, loss = 0.92 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 04:41:46.923004: step 29060, loss = 1.30 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 04:41:59.176588: step 29070, loss = 1.16 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 04:42:11.440331: step 29080, loss = 1.10 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 04:42:23.613544: step 29090, loss = 0.92 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 04:42:35.819029: step 29100, loss = 1.17 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-06-17 04:42:52.565553: step 29110, loss = 1.02 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 04:43:04.831716: step 29120, loss = 1.13 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 04:43:17.398101: step 29130, loss = 0.97 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 04:43:29.646408: step 29140, loss = 1.03 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 04:43:41.833344: step 29150, loss = 0.95 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 04:43:54.197531: step 29160, loss = 0.96 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 04:44:06.660747: step 29170, loss = 1.00 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 04:44:19.197566: step 29180, loss = 1.00 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-17 04:44:31.587987: step 29190, loss = 0.91 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-17 04:44:44.145722: step 29200, loss = 1.22 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-17 04:45:01.161274: step 29210, loss = 1.07 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 04:45:13.552744: step 29220, loss = 0.98 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 04:45:25.831229: step 29230, loss = 0.98 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 04:45:38.415873: step 29240, loss = 1.08 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 04:45:50.784989: step 29250, loss = 0.97 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 04:46:03.260569: step 29260, loss = 1.28 (23.2 examples/sec; 1.295 sec/batch)\n",
      "2019-06-17 04:46:15.452656: step 29270, loss = 1.06 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 04:46:27.549718: step 29280, loss = 0.95 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 04:46:39.792214: step 29290, loss = 1.32 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 04:46:52.067822: step 29300, loss = 1.15 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 04:47:09.011321: step 29310, loss = 0.91 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 04:47:21.302934: step 29320, loss = 1.24 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 04:47:33.498183: step 29330, loss = 1.18 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 04:47:46.060621: step 29340, loss = 1.07 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 04:47:58.389752: step 29350, loss = 0.94 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 04:48:10.776011: step 29360, loss = 1.00 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 04:48:22.980553: step 29370, loss = 1.47 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 04:48:35.392998: step 29380, loss = 1.11 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 04:48:47.650810: step 29390, loss = 1.03 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 04:48:59.954265: step 29400, loss = 1.04 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 04:49:17.015021: step 29410, loss = 1.19 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 04:49:29.341849: step 29420, loss = 1.11 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 04:49:41.713188: step 29430, loss = 1.07 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-06-17 04:49:54.189601: step 29440, loss = 0.97 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 04:50:06.536700: step 29450, loss = 1.18 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 04:50:18.983046: step 29460, loss = 1.26 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 04:50:31.247231: step 29470, loss = 1.01 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 04:50:43.628720: step 29480, loss = 0.96 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 04:50:56.093388: step 29490, loss = 0.96 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 04:51:08.566583: step 29500, loss = 1.04 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 04:51:25.824897: step 29510, loss = 1.20 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 04:51:38.357902: step 29520, loss = 1.07 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 04:51:50.807615: step 29530, loss = 0.99 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 04:52:03.102328: step 29540, loss = 1.04 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 04:52:15.210978: step 29550, loss = 1.17 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 04:52:27.454341: step 29560, loss = 1.15 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 04:52:39.821890: step 29570, loss = 1.00 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 04:52:52.221313: step 29580, loss = 1.24 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-17 04:53:04.453737: step 29590, loss = 0.89 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 04:53:16.886496: step 29600, loss = 1.05 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 04:53:34.215036: step 29610, loss = 0.87 (23.2 examples/sec; 1.292 sec/batch)\n",
      "2019-06-17 04:53:46.456805: step 29620, loss = 1.30 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 04:53:58.689605: step 29630, loss = 1.21 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 04:54:10.972757: step 29640, loss = 0.94 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 04:54:23.378437: step 29650, loss = 0.91 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 04:54:35.519612: step 29660, loss = 1.18 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-17 04:54:47.776100: step 29670, loss = 0.96 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 04:55:00.037467: step 29680, loss = 1.15 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-17 04:55:12.469794: step 29690, loss = 0.87 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 04:55:24.657216: step 29700, loss = 0.93 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 04:55:41.700661: step 29710, loss = 0.98 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 04:55:53.991869: step 29720, loss = 1.14 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-06-17 04:56:06.275578: step 29730, loss = 1.10 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 04:56:18.886764: step 29740, loss = 1.16 (23.4 examples/sec; 1.280 sec/batch)\n",
      "2019-06-17 04:56:31.179468: step 29750, loss = 1.13 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 04:56:43.489791: step 29760, loss = 1.19 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-06-17 04:56:55.923779: step 29770, loss = 1.00 (24.1 examples/sec; 1.242 sec/batch)\n",
      "2019-06-17 04:57:08.045411: step 29780, loss = 0.95 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 04:57:20.511781: step 29790, loss = 1.00 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 04:57:32.841822: step 29800, loss = 1.23 (22.6 examples/sec; 1.329 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 04:57:49.711113: step 29810, loss = 1.26 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 04:58:01.915663: step 29820, loss = 1.16 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 04:58:14.199978: step 29830, loss = 1.22 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 04:58:26.469686: step 29840, loss = 1.01 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 04:58:38.685914: step 29850, loss = 1.02 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 04:58:51.100738: step 29860, loss = 1.06 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 04:59:03.550135: step 29870, loss = 1.05 (23.1 examples/sec; 1.298 sec/batch)\n",
      "2019-06-17 04:59:15.765690: step 29880, loss = 0.88 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 04:59:27.971364: step 29890, loss = 1.13 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 04:59:40.295613: step 29900, loss = 1.01 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 04:59:57.208206: step 29910, loss = 0.91 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 05:00:09.489272: step 29920, loss = 1.32 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 05:00:21.914327: step 29930, loss = 1.11 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 05:00:34.131300: step 29940, loss = 1.18 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-06-17 05:00:46.494409: step 29950, loss = 0.96 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-17 05:00:59.024532: step 29960, loss = 1.06 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 05:01:11.473916: step 29970, loss = 1.06 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 05:01:23.874518: step 29980, loss = 1.01 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 05:01:36.174107: step 29990, loss = 1.14 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 05:01:48.580815: step 30000, loss = 1.11 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 05:02:08.952683: step 30010, loss = 1.20 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 05:02:21.351254: step 30020, loss = 1.08 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 05:02:33.577468: step 30030, loss = 1.21 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 05:02:45.820604: step 30040, loss = 1.03 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 05:02:58.081649: step 30050, loss = 1.04 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 05:03:10.537383: step 30060, loss = 0.91 (23.4 examples/sec; 1.285 sec/batch)\n",
      "2019-06-17 05:03:22.883068: step 30070, loss = 0.90 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 05:03:35.183563: step 30080, loss = 1.01 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 05:03:47.457492: step 30090, loss = 0.97 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 05:03:59.648652: step 30100, loss = 0.93 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 05:04:17.272847: step 30110, loss = 1.28 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 05:04:29.569391: step 30120, loss = 0.91 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-06-17 05:04:41.786840: step 30130, loss = 1.02 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 05:04:54.109213: step 30140, loss = 1.15 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 05:05:06.380866: step 30150, loss = 0.87 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 05:05:18.722410: step 30160, loss = 1.11 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 05:05:31.111639: step 30170, loss = 0.96 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 05:05:43.483812: step 30180, loss = 1.11 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 05:05:55.737854: step 30190, loss = 1.06 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 05:06:07.928756: step 30200, loss = 1.01 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 05:06:24.611931: step 30210, loss = 1.36 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-06-17 05:06:37.160037: step 30220, loss = 1.14 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 05:06:49.325339: step 30230, loss = 1.12 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 05:07:01.588312: step 30240, loss = 1.18 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 05:07:13.751535: step 30250, loss = 1.12 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 05:07:26.064582: step 30260, loss = 1.06 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 05:07:38.444100: step 30270, loss = 1.06 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 05:07:50.676635: step 30280, loss = 0.95 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 05:08:03.020081: step 30290, loss = 0.99 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 05:08:15.563718: step 30300, loss = 1.26 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 05:08:32.319098: step 30310, loss = 1.07 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 05:08:44.632513: step 30320, loss = 1.21 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 05:08:57.063940: step 30330, loss = 0.93 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 05:09:09.505950: step 30340, loss = 1.15 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 05:09:21.947951: step 30350, loss = 1.08 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 05:09:34.300659: step 30360, loss = 0.98 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 05:09:46.637442: step 30370, loss = 1.02 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 05:09:58.952666: step 30380, loss = 0.94 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 05:10:11.257696: step 30390, loss = 0.92 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 05:10:23.530263: step 30400, loss = 1.21 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 05:10:40.465661: step 30410, loss = 1.08 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 05:10:52.918997: step 30420, loss = 0.99 (23.1 examples/sec; 1.300 sec/batch)\n",
      "2019-06-17 05:11:05.269831: step 30430, loss = 1.07 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 05:11:17.616575: step 30440, loss = 0.99 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-06-17 05:11:30.300037: step 30450, loss = 1.01 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 05:11:42.638650: step 30460, loss = 1.05 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 05:11:54.824014: step 30470, loss = 0.98 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 05:12:07.378699: step 30480, loss = 0.95 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 05:12:20.061650: step 30490, loss = 1.19 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 05:12:32.362430: step 30500, loss = 1.41 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 05:12:49.466311: step 30510, loss = 1.19 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 05:13:01.990218: step 30520, loss = 1.00 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 05:13:14.238234: step 30530, loss = 1.26 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 05:13:26.520328: step 30540, loss = 1.03 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 05:13:38.838268: step 30550, loss = 0.95 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 05:13:51.086714: step 30560, loss = 1.17 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 05:14:03.195028: step 30570, loss = 1.06 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 05:14:15.493723: step 30580, loss = 1.03 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 05:14:27.880351: step 30590, loss = 1.08 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 05:14:40.309059: step 30600, loss = 1.18 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 05:14:56.990814: step 30610, loss = 0.99 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 05:15:09.276609: step 30620, loss = 1.04 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 05:15:21.647243: step 30630, loss = 0.87 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 05:15:34.315384: step 30640, loss = 1.04 (23.3 examples/sec; 1.286 sec/batch)\n",
      "2019-06-17 05:15:46.598342: step 30650, loss = 1.08 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 05:15:58.809863: step 30660, loss = 0.95 (23.7 examples/sec; 1.266 sec/batch)\n",
      "2019-06-17 05:16:11.284069: step 30670, loss = 1.23 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 05:16:23.597592: step 30680, loss = 1.16 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 05:16:36.065052: step 30690, loss = 0.96 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 05:16:48.560289: step 30700, loss = 1.04 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 05:17:05.440319: step 30710, loss = 0.90 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 05:17:17.759490: step 30720, loss = 0.99 (23.0 examples/sec; 1.307 sec/batch)\n",
      "2019-06-17 05:17:30.189109: step 30730, loss = 0.97 (23.7 examples/sec; 1.265 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 05:17:42.620060: step 30740, loss = 1.09 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 05:17:54.771656: step 30750, loss = 1.07 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 05:18:07.279011: step 30760, loss = 0.97 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 05:18:19.813416: step 30770, loss = 1.15 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 05:18:32.336233: step 30780, loss = 1.11 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 05:18:44.722374: step 30790, loss = 1.19 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 05:18:57.021186: step 30800, loss = 0.99 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 05:19:13.787890: step 30810, loss = 1.04 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 05:19:26.129626: step 30820, loss = 0.95 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 05:19:38.572024: step 30830, loss = 0.92 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 05:19:51.079083: step 30840, loss = 1.04 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 05:20:03.356574: step 30850, loss = 1.01 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 05:20:15.584714: step 30860, loss = 0.99 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 05:20:27.791686: step 30870, loss = 1.32 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 05:20:40.119132: step 30880, loss = 0.98 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 05:20:52.230753: step 30890, loss = 1.06 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 05:21:04.529491: step 30900, loss = 1.10 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 05:21:22.052260: step 30910, loss = 1.01 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 05:21:34.391103: step 30920, loss = 0.97 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 05:21:46.918324: step 30930, loss = 1.06 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-17 05:21:59.191934: step 30940, loss = 1.16 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 05:22:11.542592: step 30950, loss = 1.09 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 05:22:23.711437: step 30960, loss = 0.93 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 05:22:36.281265: step 30970, loss = 1.04 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 05:22:48.724055: step 30980, loss = 1.09 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 05:23:01.133297: step 30990, loss = 1.00 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 05:23:13.349634: step 31000, loss = 0.90 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 05:23:30.348044: step 31010, loss = 1.03 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 05:23:42.592954: step 31020, loss = 1.10 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 05:23:55.133779: step 31030, loss = 1.13 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-06-17 05:24:07.282495: step 31040, loss = 1.04 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 05:24:19.668716: step 31050, loss = 1.05 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 05:24:32.017826: step 31060, loss = 1.01 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 05:24:44.330001: step 31070, loss = 0.96 (23.4 examples/sec; 1.280 sec/batch)\n",
      "2019-06-17 05:24:56.622414: step 31080, loss = 1.04 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 05:25:09.004414: step 31090, loss = 1.13 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 05:25:21.393046: step 31100, loss = 1.09 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-06-17 05:25:38.398134: step 31110, loss = 1.17 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 05:25:50.655545: step 31120, loss = 1.13 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 05:26:02.899929: step 31130, loss = 1.08 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-06-17 05:26:15.149698: step 31140, loss = 1.03 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 05:26:27.473088: step 31150, loss = 1.17 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 05:26:39.725702: step 31160, loss = 1.09 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 05:26:51.842824: step 31170, loss = 0.88 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 05:27:04.188473: step 31180, loss = 1.01 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-06-17 05:27:16.673371: step 31190, loss = 1.15 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-06-17 05:27:28.967286: step 31200, loss = 0.94 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 05:27:45.852650: step 31210, loss = 1.08 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 05:27:58.198336: step 31220, loss = 1.18 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 05:28:10.466599: step 31230, loss = 1.13 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 05:28:22.693289: step 31240, loss = 1.06 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 05:28:34.987910: step 31250, loss = 1.11 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 05:28:47.329079: step 31260, loss = 0.95 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 05:28:59.572370: step 31270, loss = 0.98 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 05:29:12.222631: step 31280, loss = 0.98 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 05:29:24.398609: step 31290, loss = 0.92 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 05:29:36.853706: step 31300, loss = 1.08 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 05:29:53.778598: step 31310, loss = 1.03 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-06-17 05:30:06.144339: step 31320, loss = 0.95 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 05:30:18.479290: step 31330, loss = 1.11 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 05:30:30.857648: step 31340, loss = 0.90 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 05:30:43.380123: step 31350, loss = 0.94 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 05:30:55.606430: step 31360, loss = 1.13 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 05:31:08.191930: step 31370, loss = 1.01 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 05:31:20.531724: step 31380, loss = 1.11 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 05:31:33.094197: step 31390, loss = 1.22 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 05:31:45.566954: step 31400, loss = 1.04 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 05:32:02.364287: step 31410, loss = 1.17 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 05:32:14.804430: step 31420, loss = 0.94 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 05:32:27.049889: step 31430, loss = 1.27 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 05:32:39.206633: step 31440, loss = 1.15 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 05:32:51.646621: step 31450, loss = 0.98 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 05:33:03.781747: step 31460, loss = 1.04 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-17 05:33:16.085161: step 31470, loss = 0.98 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 05:33:28.700405: step 31480, loss = 0.93 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 05:33:41.144959: step 31490, loss = 1.12 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 05:33:53.810858: step 31500, loss = 1.12 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 05:34:10.765626: step 31510, loss = 1.00 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 05:34:23.053124: step 31520, loss = 1.04 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 05:34:35.297803: step 31530, loss = 0.92 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 05:34:47.560217: step 31540, loss = 1.02 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 05:34:59.833687: step 31550, loss = 1.28 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 05:35:12.169034: step 31560, loss = 1.07 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 05:35:24.417247: step 31570, loss = 1.00 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 05:35:36.669170: step 31580, loss = 0.94 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 05:35:49.030309: step 31590, loss = 1.00 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 05:36:01.474001: step 31600, loss = 1.15 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 05:36:18.142598: step 31610, loss = 0.95 (23.4 examples/sec; 1.280 sec/batch)\n",
      "2019-06-17 05:36:30.520005: step 31620, loss = 1.31 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 05:36:42.964792: step 31630, loss = 0.98 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 05:36:55.785489: step 31640, loss = 1.00 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 05:37:08.059838: step 31650, loss = 1.34 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 05:37:20.508800: step 31660, loss = 0.96 (24.6 examples/sec; 1.219 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 05:37:32.790247: step 31670, loss = 1.11 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-06-17 05:37:45.069907: step 31680, loss = 1.09 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 05:37:57.399469: step 31690, loss = 0.95 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 05:38:09.877595: step 31700, loss = 1.10 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 05:38:26.809911: step 31710, loss = 1.14 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-17 05:38:39.007363: step 31720, loss = 1.07 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-06-17 05:38:51.488555: step 31730, loss = 1.01 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 05:39:03.980731: step 31740, loss = 1.13 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 05:39:16.351770: step 31750, loss = 0.98 (22.6 examples/sec; 1.330 sec/batch)\n",
      "2019-06-17 05:39:28.525586: step 31760, loss = 1.00 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 05:39:40.817951: step 31770, loss = 1.20 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 05:39:52.970756: step 31780, loss = 1.09 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 05:40:05.270858: step 31790, loss = 1.07 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 05:40:17.640437: step 31800, loss = 1.14 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-06-17 05:40:34.410016: step 31810, loss = 1.02 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 05:40:46.683972: step 31820, loss = 0.90 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 05:40:59.100341: step 31830, loss = 0.93 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 05:41:11.485056: step 31840, loss = 1.15 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 05:41:23.863497: step 31850, loss = 1.10 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 05:41:36.033809: step 31860, loss = 1.06 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 05:41:48.145016: step 31870, loss = 1.10 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 05:42:00.310504: step 31880, loss = 1.14 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 05:42:13.027076: step 31890, loss = 0.94 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 05:42:25.228367: step 31900, loss = 1.05 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 05:42:41.867453: step 31910, loss = 1.22 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 05:42:54.283990: step 31920, loss = 1.05 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 05:43:06.450810: step 31930, loss = 0.95 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 05:43:18.658557: step 31940, loss = 1.23 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-17 05:43:30.965211: step 31950, loss = 1.28 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 05:43:43.285400: step 31960, loss = 0.91 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 05:43:55.569748: step 31970, loss = 1.03 (23.1 examples/sec; 1.298 sec/batch)\n",
      "2019-06-17 05:44:07.967634: step 31980, loss = 1.01 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 05:44:20.359781: step 31990, loss = 0.91 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-06-17 05:44:32.655789: step 32000, loss = 1.15 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 05:44:49.525525: step 32010, loss = 1.01 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 05:45:01.842139: step 32020, loss = 1.06 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 05:45:14.093378: step 32030, loss = 1.11 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 05:45:26.350870: step 32040, loss = 0.94 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 05:45:38.827347: step 32050, loss = 1.23 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 05:45:51.250016: step 32060, loss = 1.04 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-17 05:46:03.560533: step 32070, loss = 1.22 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 05:46:15.711054: step 32080, loss = 1.05 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 05:46:28.184561: step 32090, loss = 0.96 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-06-17 05:46:40.517071: step 32100, loss = 1.29 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 05:46:57.243923: step 32110, loss = 1.00 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 05:47:09.575711: step 32120, loss = 1.15 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 05:47:21.783409: step 32130, loss = 1.17 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 05:47:34.252515: step 32140, loss = 1.20 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 05:47:46.687847: step 32150, loss = 1.13 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 05:47:59.095812: step 32160, loss = 1.12 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 05:48:11.347904: step 32170, loss = 0.94 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 05:48:23.777085: step 32180, loss = 1.02 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 05:48:36.459670: step 32190, loss = 1.22 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 05:48:48.824665: step 32200, loss = 1.09 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 05:49:05.726217: step 32210, loss = 0.99 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 05:49:18.181321: step 32220, loss = 1.07 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 05:49:30.539642: step 32230, loss = 0.91 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 05:49:42.982339: step 32240, loss = 1.17 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 05:49:55.334733: step 32250, loss = 0.90 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 05:50:07.603841: step 32260, loss = 1.10 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 05:50:19.840126: step 32270, loss = 1.07 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-06-17 05:50:31.961771: step 32280, loss = 0.92 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 05:50:44.625888: step 32290, loss = 1.15 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 05:50:57.050258: step 32300, loss = 1.06 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 05:51:13.727881: step 32310, loss = 1.09 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 05:51:25.995584: step 32320, loss = 1.18 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 05:51:38.234046: step 32330, loss = 0.96 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 05:51:50.714798: step 32340, loss = 1.34 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 05:52:03.033376: step 32350, loss = 1.01 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 05:52:15.597107: step 32360, loss = 1.07 (23.0 examples/sec; 1.307 sec/batch)\n",
      "2019-06-17 05:52:27.952145: step 32370, loss = 1.03 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 05:52:40.324045: step 32380, loss = 1.02 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 05:52:52.537537: step 32390, loss = 1.13 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 05:53:05.022199: step 32400, loss = 1.14 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 05:53:21.931650: step 32410, loss = 1.18 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 05:53:34.267730: step 32420, loss = 0.99 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 05:53:46.694455: step 32430, loss = 1.09 (22.5 examples/sec; 1.332 sec/batch)\n",
      "2019-06-17 05:53:58.885232: step 32440, loss = 0.94 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 05:54:11.151398: step 32450, loss = 1.06 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 05:54:23.521428: step 32460, loss = 0.99 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 05:54:35.811631: step 32470, loss = 1.02 (23.8 examples/sec; 1.263 sec/batch)\n",
      "2019-06-17 05:54:48.120753: step 32480, loss = 0.92 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 05:55:00.713850: step 32490, loss = 1.03 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 05:55:13.263203: step 32500, loss = 1.13 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 05:55:30.273672: step 32510, loss = 0.91 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 05:55:42.542164: step 32520, loss = 0.97 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 05:55:54.830960: step 32530, loss = 1.02 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 05:56:07.294077: step 32540, loss = 1.13 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 05:56:19.827317: step 32550, loss = 1.07 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 05:56:31.991690: step 32560, loss = 1.10 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 05:56:44.096438: step 32570, loss = 0.84 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 05:56:56.470011: step 32580, loss = 1.28 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 05:57:08.595736: step 32590, loss = 1.08 (24.7 examples/sec; 1.215 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 05:57:21.184114: step 32600, loss = 1.14 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 05:57:37.998717: step 32610, loss = 1.09 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 05:57:50.415608: step 32620, loss = 1.24 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 05:58:02.725025: step 32630, loss = 1.00 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 05:58:15.154192: step 32640, loss = 1.13 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-17 05:58:27.492972: step 32650, loss = 1.02 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 05:58:39.743092: step 32660, loss = 0.86 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 05:58:52.105744: step 32670, loss = 1.05 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 05:59:04.424152: step 32680, loss = 1.09 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 05:59:16.842162: step 32690, loss = 0.85 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 05:59:29.368607: step 32700, loss = 0.90 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 05:59:46.267606: step 32710, loss = 0.94 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-17 05:59:58.835459: step 32720, loss = 1.22 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 06:00:11.050126: step 32730, loss = 1.09 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-17 06:00:23.180938: step 32740, loss = 1.09 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 06:00:35.444490: step 32750, loss = 0.88 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 06:00:47.930258: step 32760, loss = 0.97 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 06:01:00.337422: step 32770, loss = 1.04 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-06-17 06:01:12.634750: step 32780, loss = 0.98 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 06:01:24.778809: step 32790, loss = 1.02 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 06:01:37.117780: step 32800, loss = 1.16 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 06:01:53.874409: step 32810, loss = 1.04 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 06:02:06.046545: step 32820, loss = 1.05 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 06:02:18.178036: step 32830, loss = 1.07 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-06-17 06:02:30.389841: step 32840, loss = 1.20 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 06:02:42.653670: step 32850, loss = 1.25 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 06:02:55.106550: step 32860, loss = 1.10 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 06:03:07.403523: step 32870, loss = 1.07 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-06-17 06:03:19.845927: step 32880, loss = 0.95 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 06:03:32.094862: step 32890, loss = 1.12 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 06:03:44.231563: step 32900, loss = 1.41 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-06-17 06:04:01.176088: step 32910, loss = 1.01 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 06:04:13.741013: step 32920, loss = 1.09 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 06:04:26.030289: step 32930, loss = 1.07 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 06:04:38.380442: step 32940, loss = 1.13 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 06:04:50.599521: step 32950, loss = 0.97 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 06:05:02.868787: step 32960, loss = 0.99 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 06:05:15.133585: step 32970, loss = 0.98 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 06:05:27.434639: step 32980, loss = 1.25 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 06:05:39.600303: step 32990, loss = 1.08 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 06:05:52.073683: step 33000, loss = 1.02 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 06:06:08.941862: step 33010, loss = 0.97 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 06:06:21.375445: step 33020, loss = 1.55 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-06-17 06:06:33.591133: step 33030, loss = 0.92 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 06:06:45.851601: step 33040, loss = 1.04 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 06:06:58.088235: step 33050, loss = 1.10 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 06:07:10.462017: step 33060, loss = 0.97 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 06:07:22.731324: step 33070, loss = 0.91 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 06:07:34.929772: step 33080, loss = 0.99 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 06:07:47.203786: step 33090, loss = 0.99 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 06:07:59.587202: step 33100, loss = 1.06 (22.5 examples/sec; 1.331 sec/batch)\n",
      "2019-06-17 06:08:17.025666: step 33110, loss = 1.02 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 06:08:29.378959: step 33120, loss = 1.00 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 06:08:41.776332: step 33130, loss = 1.06 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 06:08:54.132733: step 33140, loss = 0.88 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 06:09:06.469261: step 33150, loss = 1.11 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 06:09:18.933615: step 33160, loss = 1.13 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 06:09:31.497035: step 33170, loss = 1.17 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 06:09:43.728480: step 33180, loss = 0.95 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 06:09:56.145840: step 33190, loss = 0.95 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 06:10:08.448768: step 33200, loss = 1.23 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 06:10:25.376583: step 33210, loss = 0.92 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 06:10:37.670217: step 33220, loss = 1.05 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 06:10:50.068227: step 33230, loss = 0.91 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 06:11:02.412107: step 33240, loss = 0.93 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 06:11:14.733608: step 33250, loss = 1.08 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 06:11:27.050780: step 33260, loss = 1.04 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 06:11:39.259897: step 33270, loss = 1.08 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 06:11:51.488828: step 33280, loss = 1.05 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 06:12:03.945595: step 33290, loss = 1.04 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 06:12:16.297929: step 33300, loss = 1.14 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 06:12:33.066422: step 33310, loss = 1.08 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 06:12:45.599355: step 33320, loss = 0.96 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 06:12:57.797796: step 33330, loss = 0.99 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 06:13:10.249291: step 33340, loss = 0.92 (22.6 examples/sec; 1.330 sec/batch)\n",
      "2019-06-17 06:13:22.503202: step 33350, loss = 1.13 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 06:13:34.651280: step 33360, loss = 1.24 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 06:13:47.048813: step 33370, loss = 1.02 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-06-17 06:13:59.361410: step 33380, loss = 1.05 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 06:14:11.690748: step 33390, loss = 0.90 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 06:14:24.041708: step 33400, loss = 1.07 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 06:14:40.906638: step 33410, loss = 1.09 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 06:14:53.472721: step 33420, loss = 1.05 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 06:15:05.921255: step 33430, loss = 0.92 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 06:15:18.466028: step 33440, loss = 1.15 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 06:15:30.944717: step 33450, loss = 1.12 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-06-17 06:15:43.158231: step 33460, loss = 1.06 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 06:15:55.516366: step 33470, loss = 0.98 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 06:16:07.793905: step 33480, loss = 1.16 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 06:16:20.056076: step 33490, loss = 1.11 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 06:16:32.541465: step 33500, loss = 1.06 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 06:16:49.375843: step 33510, loss = 1.17 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 06:17:01.499985: step 33520, loss = 1.18 (24.5 examples/sec; 1.225 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 06:17:13.722863: step 33530, loss = 1.09 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 06:17:26.079588: step 33540, loss = 1.19 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 06:17:38.490439: step 33550, loss = 0.97 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 06:17:50.938089: step 33560, loss = 1.14 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-06-17 06:18:03.199291: step 33570, loss = 1.04 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 06:18:15.745337: step 33580, loss = 0.98 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 06:18:28.066057: step 33590, loss = 1.21 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 06:18:40.383776: step 33600, loss = 1.00 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 06:18:57.229041: step 33610, loss = 0.97 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 06:19:09.675209: step 33620, loss = 1.09 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 06:19:22.114140: step 33630, loss = 1.08 (22.0 examples/sec; 1.362 sec/batch)\n",
      "2019-06-17 06:19:34.623329: step 33640, loss = 1.04 (22.5 examples/sec; 1.333 sec/batch)\n",
      "2019-06-17 06:19:46.978622: step 33650, loss = 1.09 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 06:19:59.462774: step 33660, loss = 0.91 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 06:20:11.962169: step 33670, loss = 1.11 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 06:20:24.384347: step 33680, loss = 1.00 (23.2 examples/sec; 1.292 sec/batch)\n",
      "2019-06-17 06:20:36.880521: step 33690, loss = 0.88 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 06:20:49.235873: step 33700, loss = 0.95 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 06:21:06.175532: step 33710, loss = 0.84 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 06:21:18.533457: step 33720, loss = 1.21 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 06:21:30.857839: step 33730, loss = 1.15 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-17 06:21:43.276915: step 33740, loss = 1.32 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 06:21:55.410346: step 33750, loss = 1.33 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 06:22:07.797813: step 33760, loss = 1.05 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 06:22:20.060989: step 33770, loss = 1.31 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 06:22:32.334291: step 33780, loss = 1.18 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 06:22:44.845468: step 33790, loss = 0.91 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 06:22:57.065014: step 33800, loss = 1.18 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 06:23:14.076231: step 33810, loss = 0.93 (22.6 examples/sec; 1.328 sec/batch)\n",
      "2019-06-17 06:23:26.712142: step 33820, loss = 1.03 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 06:23:39.002203: step 33830, loss = 0.96 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 06:23:51.199792: step 33840, loss = 1.12 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 06:24:03.685352: step 33850, loss = 1.11 (22.6 examples/sec; 1.328 sec/batch)\n",
      "2019-06-17 06:24:15.894515: step 33860, loss = 1.26 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-06-17 06:24:28.035966: step 33870, loss = 1.17 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 06:24:40.557826: step 33880, loss = 1.33 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 06:24:52.919240: step 33890, loss = 1.07 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 06:25:05.356492: step 33900, loss = 1.13 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 06:25:21.861484: step 33910, loss = 0.96 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 06:25:33.991486: step 33920, loss = 1.04 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 06:25:46.424282: step 33930, loss = 1.00 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 06:25:58.779892: step 33940, loss = 1.03 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 06:26:11.345584: step 33950, loss = 1.02 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 06:26:23.621098: step 33960, loss = 1.06 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 06:26:35.773473: step 33970, loss = 1.01 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 06:26:48.106395: step 33980, loss = 1.01 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 06:27:00.455080: step 33990, loss = 1.09 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 06:27:12.963722: step 34000, loss = 1.26 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 06:27:29.697749: step 34010, loss = 0.99 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 06:27:42.202194: step 34020, loss = 1.13 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 06:27:54.497334: step 34030, loss = 1.21 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 06:28:06.812311: step 34040, loss = 1.23 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 06:28:18.983335: step 34050, loss = 1.00 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 06:28:31.437373: step 34060, loss = 1.06 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 06:28:43.998556: step 34070, loss = 0.96 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 06:28:56.173655: step 34080, loss = 1.12 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 06:29:08.365819: step 34090, loss = 1.15 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 06:29:20.919262: step 34100, loss = 1.26 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-06-17 06:29:37.683682: step 34110, loss = 1.03 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 06:29:50.026553: step 34120, loss = 0.97 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 06:30:02.186212: step 34130, loss = 1.03 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 06:30:14.577425: step 34140, loss = 1.22 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-17 06:30:26.757348: step 34150, loss = 0.95 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 06:30:39.186974: step 34160, loss = 0.96 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 06:30:51.712377: step 34170, loss = 0.87 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 06:31:04.167784: step 34180, loss = 1.19 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-17 06:31:16.503910: step 34190, loss = 0.99 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 06:31:28.870817: step 34200, loss = 0.99 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 06:31:45.970918: step 34210, loss = 1.22 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 06:31:58.245224: step 34220, loss = 0.96 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 06:32:10.424866: step 34230, loss = 1.07 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 06:32:23.049712: step 34240, loss = 1.11 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 06:32:35.257511: step 34250, loss = 1.02 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 06:32:47.410526: step 34260, loss = 1.08 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 06:32:59.794083: step 34270, loss = 0.96 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 06:33:12.191466: step 34280, loss = 0.88 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 06:33:24.441189: step 34290, loss = 1.12 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 06:33:36.677505: step 34300, loss = 0.97 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 06:33:53.429002: step 34310, loss = 0.97 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 06:34:05.808875: step 34320, loss = 0.97 (22.5 examples/sec; 1.332 sec/batch)\n",
      "2019-06-17 06:34:18.174994: step 34330, loss = 1.12 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 06:34:30.656013: step 34340, loss = 1.27 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 06:34:43.057404: step 34350, loss = 1.24 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 06:34:55.328246: step 34360, loss = 0.89 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 06:35:07.656899: step 34370, loss = 1.06 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 06:35:19.887338: step 34380, loss = 0.94 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 06:35:32.508045: step 34390, loss = 0.99 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 06:35:44.797026: step 34400, loss = 1.07 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 06:36:01.596069: step 34410, loss = 0.96 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 06:36:13.971136: step 34420, loss = 1.26 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 06:36:26.351018: step 34430, loss = 1.11 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 06:36:38.611604: step 34440, loss = 0.95 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 06:36:51.253119: step 34450, loss = 0.98 (24.6 examples/sec; 1.220 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 06:37:03.532205: step 34460, loss = 1.15 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-06-17 06:37:15.737371: step 34470, loss = 1.10 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 06:37:28.020469: step 34480, loss = 0.97 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 06:37:40.362266: step 34490, loss = 1.20 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 06:37:52.633771: step 34500, loss = 1.15 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 06:38:09.809538: step 34510, loss = 1.24 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-06-17 06:38:22.046845: step 34520, loss = 1.03 (23.4 examples/sec; 1.284 sec/batch)\n",
      "2019-06-17 06:38:34.627102: step 34530, loss = 1.10 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 06:38:46.896883: step 34540, loss = 1.22 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 06:38:59.220776: step 34550, loss = 1.13 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 06:39:11.439033: step 34560, loss = 1.17 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 06:39:23.736140: step 34570, loss = 1.11 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-17 06:39:36.207908: step 34580, loss = 1.13 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 06:39:48.503345: step 34590, loss = 0.95 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 06:40:00.931569: step 34600, loss = 1.11 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 06:40:18.363557: step 34610, loss = 1.27 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 06:40:31.029028: step 34620, loss = 1.30 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-06-17 06:40:43.164804: step 34630, loss = 0.87 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 06:40:55.519450: step 34640, loss = 1.18 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 06:41:07.776394: step 34650, loss = 1.04 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 06:41:19.996682: step 34660, loss = 1.00 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 06:41:32.363077: step 34670, loss = 1.04 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 06:41:44.752250: step 34680, loss = 0.99 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 06:41:57.063617: step 34690, loss = 0.89 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 06:42:09.553251: step 34700, loss = 1.04 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 06:42:26.529121: step 34710, loss = 1.06 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-06-17 06:42:38.942316: step 34720, loss = 1.01 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 06:42:51.421775: step 34730, loss = 1.10 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 06:43:03.709888: step 34740, loss = 1.33 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 06:43:16.090174: step 34750, loss = 1.10 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 06:43:28.498318: step 34760, loss = 0.94 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 06:43:40.839366: step 34770, loss = 1.19 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 06:43:53.160998: step 34780, loss = 1.13 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 06:44:05.357420: step 34790, loss = 1.21 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 06:44:17.637072: step 34800, loss = 1.08 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 06:44:34.434629: step 34810, loss = 1.19 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 06:44:46.734458: step 34820, loss = 1.19 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 06:44:59.237552: step 34830, loss = 1.03 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 06:45:11.478845: step 34840, loss = 1.01 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 06:45:23.785301: step 34850, loss = 1.07 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 06:45:36.075090: step 34860, loss = 0.93 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-06-17 06:45:48.814146: step 34870, loss = 1.14 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 06:46:00.964281: step 34880, loss = 1.12 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 06:46:13.311229: step 34890, loss = 0.92 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 06:46:25.588930: step 34900, loss = 1.16 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 06:46:42.178351: step 34910, loss = 1.01 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 06:46:54.655969: step 34920, loss = 1.18 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 06:47:07.030214: step 34930, loss = 1.06 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 06:47:19.222054: step 34940, loss = 1.15 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 06:47:31.526719: step 34950, loss = 1.17 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 06:47:43.662761: step 34960, loss = 1.26 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 06:47:55.939011: step 34970, loss = 1.01 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 06:48:08.235733: step 34980, loss = 1.08 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 06:48:20.601711: step 34990, loss = 0.99 (22.3 examples/sec; 1.343 sec/batch)\n",
      "2019-06-17 06:48:32.787622: step 35000, loss = 1.05 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 06:48:53.275176: step 35010, loss = 0.97 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 06:49:05.475460: step 35020, loss = 1.15 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 06:49:17.633317: step 35030, loss = 1.04 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 06:49:29.785402: step 35040, loss = 0.98 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 06:49:42.383289: step 35050, loss = 0.89 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 06:49:54.632147: step 35060, loss = 1.18 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-06-17 06:50:06.932383: step 35070, loss = 1.01 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 06:50:19.079289: step 35080, loss = 1.20 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 06:50:31.435691: step 35090, loss = 1.10 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 06:50:44.159944: step 35100, loss = 0.98 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 06:51:00.962608: step 35110, loss = 0.88 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-06-17 06:51:13.069376: step 35120, loss = 0.92 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 06:51:25.573552: step 35130, loss = 1.21 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 06:51:37.794483: step 35140, loss = 1.25 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 06:51:50.127223: step 35150, loss = 1.03 (22.6 examples/sec; 1.328 sec/batch)\n",
      "2019-06-17 06:52:02.399990: step 35160, loss = 1.20 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 06:52:14.926196: step 35170, loss = 1.06 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 06:52:27.156249: step 35180, loss = 1.06 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 06:52:39.729093: step 35190, loss = 1.01 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 06:52:52.142449: step 35200, loss = 0.96 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 06:53:09.121247: step 35210, loss = 0.96 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 06:53:21.451148: step 35220, loss = 1.09 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 06:53:33.848877: step 35230, loss = 0.97 (23.1 examples/sec; 1.300 sec/batch)\n",
      "2019-06-17 06:53:46.185551: step 35240, loss = 1.01 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 06:53:58.539412: step 35250, loss = 0.94 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 06:54:10.753695: step 35260, loss = 0.97 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 06:54:23.027715: step 35270, loss = 1.10 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 06:54:35.474383: step 35280, loss = 1.01 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 06:54:48.040421: step 35290, loss = 1.34 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 06:55:00.590497: step 35300, loss = 0.91 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 06:55:17.372338: step 35310, loss = 1.09 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-17 06:55:29.788907: step 35320, loss = 1.06 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 06:55:42.008486: step 35330, loss = 1.05 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 06:55:54.239797: step 35340, loss = 1.18 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 06:56:06.553548: step 35350, loss = 1.12 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 06:56:18.798088: step 35360, loss = 1.14 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 06:56:31.043696: step 35370, loss = 1.15 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 06:56:43.545390: step 35380, loss = 1.12 (22.9 examples/sec; 1.312 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 06:56:55.783288: step 35390, loss = 1.12 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 06:57:08.233411: step 35400, loss = 1.00 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 06:57:25.136870: step 35410, loss = 1.14 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 06:57:37.597488: step 35420, loss = 1.13 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 06:57:49.963145: step 35430, loss = 1.07 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 06:58:02.741575: step 35440, loss = 0.89 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 06:58:15.057924: step 35450, loss = 0.96 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 06:58:27.508273: step 35460, loss = 1.16 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 06:58:39.592292: step 35470, loss = 1.03 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 06:58:51.797848: step 35480, loss = 1.27 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 06:59:04.273861: step 35490, loss = 1.19 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 06:59:16.739721: step 35500, loss = 1.14 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 06:59:34.196349: step 35510, loss = 0.85 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 06:59:46.391283: step 35520, loss = 0.97 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 06:59:59.220433: step 35530, loss = 1.01 (22.3 examples/sec; 1.347 sec/batch)\n",
      "2019-06-17 07:00:11.454295: step 35540, loss = 0.89 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 07:00:23.869360: step 35550, loss = 0.94 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 07:00:36.011377: step 35560, loss = 1.20 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 07:00:48.318547: step 35570, loss = 1.01 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-06-17 07:01:01.209540: step 35580, loss = 0.99 (22.6 examples/sec; 1.328 sec/batch)\n",
      "2019-06-17 07:01:13.562904: step 35590, loss = 1.01 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 07:01:25.814667: step 35600, loss = 0.98 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 07:01:42.531132: step 35610, loss = 0.95 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 07:01:54.798063: step 35620, loss = 0.91 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 07:02:06.994150: step 35630, loss = 0.90 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 07:02:19.204727: step 35640, loss = 1.20 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 07:02:31.609648: step 35650, loss = 1.03 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 07:02:44.005471: step 35660, loss = 1.12 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 07:02:56.254654: step 35670, loss = 0.91 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 07:03:08.649845: step 35680, loss = 0.94 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 07:03:21.260646: step 35690, loss = 0.92 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 07:03:33.770632: step 35700, loss = 0.95 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-17 07:03:50.541142: step 35710, loss = 0.97 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 07:04:02.935697: step 35720, loss = 1.23 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 07:04:15.345864: step 35730, loss = 1.11 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 07:04:27.930253: step 35740, loss = 1.21 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 07:04:40.273610: step 35750, loss = 1.11 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 07:04:52.525176: step 35760, loss = 1.19 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 07:05:04.779203: step 35770, loss = 1.07 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 07:05:17.095506: step 35780, loss = 1.03 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 07:05:29.274747: step 35790, loss = 1.00 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 07:05:41.461383: step 35800, loss = 0.95 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 07:05:58.184046: step 35810, loss = 1.15 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 07:06:10.557088: step 35820, loss = 1.25 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 07:06:22.931180: step 35830, loss = 0.98 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 07:06:35.329042: step 35840, loss = 1.07 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 07:06:47.804741: step 35850, loss = 0.89 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 07:07:00.178066: step 35860, loss = 1.18 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 07:07:12.410764: step 35870, loss = 0.94 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-06-17 07:07:24.869618: step 35880, loss = 1.34 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 07:07:37.417825: step 35890, loss = 0.93 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 07:07:49.951032: step 35900, loss = 0.93 (23.0 examples/sec; 1.307 sec/batch)\n",
      "2019-06-17 07:08:06.661088: step 35910, loss = 1.09 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 07:08:19.026618: step 35920, loss = 0.97 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 07:08:31.362818: step 35930, loss = 0.96 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 07:08:43.688813: step 35940, loss = 1.09 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 07:08:55.957294: step 35950, loss = 1.17 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 07:09:08.380032: step 35960, loss = 1.15 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 07:09:20.611975: step 35970, loss = 1.05 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 07:09:32.783138: step 35980, loss = 0.93 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 07:09:45.161795: step 35990, loss = 0.91 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 07:09:57.840090: step 36000, loss = 1.02 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 07:10:14.618459: step 36010, loss = 1.15 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 07:10:27.010824: step 36020, loss = 1.16 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-06-17 07:10:39.318146: step 36030, loss = 1.17 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 07:10:51.668247: step 36040, loss = 1.05 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 07:11:04.030760: step 36050, loss = 1.03 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 07:11:16.295070: step 36060, loss = 1.27 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 07:11:28.622440: step 36070, loss = 1.04 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 07:11:40.812870: step 36080, loss = 1.13 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 07:11:53.272573: step 36090, loss = 0.99 (23.1 examples/sec; 1.297 sec/batch)\n",
      "2019-06-17 07:12:05.653012: step 36100, loss = 1.12 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 07:12:22.528539: step 36110, loss = 1.11 (23.1 examples/sec; 1.300 sec/batch)\n",
      "2019-06-17 07:12:34.793861: step 36120, loss = 1.08 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 07:12:47.213303: step 36130, loss = 1.26 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 07:12:59.580776: step 36140, loss = 0.96 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 07:13:11.906414: step 36150, loss = 0.90 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 07:13:24.315568: step 36160, loss = 1.17 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-17 07:13:36.651727: step 36170, loss = 0.90 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 07:13:48.874464: step 36180, loss = 1.25 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 07:14:01.122925: step 36190, loss = 1.18 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 07:14:13.716730: step 36200, loss = 0.96 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 07:14:30.157395: step 36210, loss = 1.07 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 07:14:42.419328: step 36220, loss = 0.94 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 07:14:54.647689: step 36230, loss = 1.08 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 07:15:06.926219: step 36240, loss = 1.05 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 07:15:19.491114: step 36250, loss = 0.91 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 07:15:31.929376: step 36260, loss = 1.41 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 07:15:44.332457: step 36270, loss = 1.00 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 07:15:56.777089: step 36280, loss = 1.08 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 07:16:09.281206: step 36290, loss = 1.25 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 07:16:21.798102: step 36300, loss = 1.01 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 07:16:38.637518: step 36310, loss = 1.04 (22.9 examples/sec; 1.308 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 07:16:51.099525: step 36320, loss = 1.06 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 07:17:03.302028: step 36330, loss = 1.17 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 07:17:15.763453: step 36340, loss = 0.97 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-17 07:17:28.144871: step 36350, loss = 0.99 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 07:17:40.556083: step 36360, loss = 1.06 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 07:17:52.862885: step 36370, loss = 0.95 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 07:18:05.117512: step 36380, loss = 1.04 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 07:18:17.412132: step 36390, loss = 1.08 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 07:18:29.687258: step 36400, loss = 1.07 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 07:18:46.430612: step 36410, loss = 1.08 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 07:18:59.020005: step 36420, loss = 0.95 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 07:19:11.283304: step 36430, loss = 0.88 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 07:19:23.737751: step 36440, loss = 1.24 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 07:19:36.054206: step 36450, loss = 1.11 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 07:19:48.499850: step 36460, loss = 0.92 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 07:20:00.846551: step 36470, loss = 0.94 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 07:20:12.953992: step 36480, loss = 1.08 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 07:20:25.137084: step 36490, loss = 1.10 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 07:20:37.542490: step 36500, loss = 1.21 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 07:20:54.384487: step 36510, loss = 1.03 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 07:21:06.793145: step 36520, loss = 1.07 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 07:21:19.043089: step 36530, loss = 1.03 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 07:21:31.524841: step 36540, loss = 1.11 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 07:21:43.941509: step 36550, loss = 1.08 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 07:21:56.222075: step 36560, loss = 1.06 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 07:22:08.536700: step 36570, loss = 0.99 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-06-17 07:22:20.913541: step 36580, loss = 1.07 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 07:22:33.091152: step 36590, loss = 1.03 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 07:22:45.262814: step 36600, loss = 0.94 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 07:23:02.374677: step 36610, loss = 0.91 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 07:23:14.795814: step 36620, loss = 1.03 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 07:23:27.584313: step 36630, loss = 1.12 (23.4 examples/sec; 1.283 sec/batch)\n",
      "2019-06-17 07:23:39.940957: step 36640, loss = 1.12 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-06-17 07:23:52.224703: step 36650, loss = 1.16 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 07:24:04.440584: step 36660, loss = 1.04 (22.9 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 07:24:16.692106: step 36670, loss = 1.07 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 07:24:28.990750: step 36680, loss = 0.96 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 07:24:41.301724: step 36690, loss = 1.02 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 07:24:53.732164: step 36700, loss = 1.22 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 07:25:11.200123: step 36710, loss = 1.00 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-06-17 07:25:23.565971: step 36720, loss = 0.90 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 07:25:35.846374: step 36730, loss = 1.20 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 07:25:48.237701: step 36740, loss = 1.06 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 07:26:00.594604: step 36750, loss = 0.97 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 07:26:12.895354: step 36760, loss = 1.10 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-17 07:26:25.110571: step 36770, loss = 1.02 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-17 07:26:37.343865: step 36780, loss = 1.17 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 07:26:49.684586: step 36790, loss = 0.86 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 07:27:02.026481: step 36800, loss = 1.19 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 07:27:18.796905: step 36810, loss = 1.13 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 07:27:31.349233: step 36820, loss = 1.08 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 07:27:43.617716: step 36830, loss = 0.90 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 07:27:55.806675: step 36840, loss = 0.98 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 07:28:07.983716: step 36850, loss = 1.38 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 07:28:20.461487: step 36860, loss = 1.13 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 07:28:32.733962: step 36870, loss = 1.02 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 07:28:44.901913: step 36880, loss = 1.07 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 07:28:57.283219: step 36890, loss = 1.04 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 07:29:09.623217: step 36900, loss = 1.10 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 07:29:26.504913: step 36910, loss = 0.98 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 07:29:39.134510: step 36920, loss = 0.93 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 07:29:51.733237: step 36930, loss = 1.09 (23.3 examples/sec; 1.289 sec/batch)\n",
      "2019-06-17 07:30:04.055924: step 36940, loss = 1.18 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-17 07:30:16.406615: step 36950, loss = 1.24 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 07:30:28.706741: step 36960, loss = 0.99 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 07:30:40.964351: step 36970, loss = 1.21 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 07:30:53.435554: step 36980, loss = 1.17 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-17 07:31:05.685452: step 36990, loss = 0.84 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 07:31:18.077533: step 37000, loss = 1.01 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 07:31:35.154287: step 37010, loss = 1.07 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 07:31:47.399286: step 37020, loss = 0.97 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 07:31:59.637896: step 37030, loss = 0.94 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 07:32:12.022583: step 37040, loss = 1.09 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 07:32:24.256079: step 37050, loss = 1.06 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 07:32:36.726165: step 37060, loss = 1.17 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 07:32:49.023392: step 37070, loss = 1.08 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 07:33:01.188140: step 37080, loss = 0.98 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 07:33:13.548960: step 37090, loss = 1.06 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 07:33:25.905933: step 37100, loss = 1.10 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 07:33:42.542022: step 37110, loss = 1.02 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 07:33:54.902344: step 37120, loss = 0.95 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 07:34:07.168261: step 37130, loss = 0.93 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 07:34:19.602543: step 37140, loss = 1.22 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 07:34:32.059997: step 37150, loss = 1.27 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 07:34:44.390402: step 37160, loss = 0.95 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 07:34:56.767141: step 37170, loss = 1.10 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-17 07:35:08.980131: step 37180, loss = 0.95 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 07:35:21.350092: step 37190, loss = 0.99 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 07:35:33.825031: step 37200, loss = 1.05 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 07:35:50.525222: step 37210, loss = 1.11 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 07:36:02.626512: step 37220, loss = 1.13 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 07:36:14.926614: step 37230, loss = 1.27 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 07:36:27.487617: step 37240, loss = 0.95 (22.8 examples/sec; 1.313 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 07:36:39.982186: step 37250, loss = 1.04 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 07:36:52.331950: step 37260, loss = 1.08 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 07:37:04.831613: step 37270, loss = 0.97 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 07:37:17.017342: step 37280, loss = 0.90 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 07:37:29.267746: step 37290, loss = 1.21 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 07:37:41.542071: step 37300, loss = 1.15 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 07:37:58.698184: step 37310, loss = 1.13 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 07:38:11.172088: step 37320, loss = 1.10 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 07:38:23.828794: step 37330, loss = 1.01 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 07:38:36.101849: step 37340, loss = 1.09 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 07:38:48.571785: step 37350, loss = 0.96 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 07:39:00.805402: step 37360, loss = 1.10 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 07:39:13.067218: step 37370, loss = 1.03 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 07:39:25.320156: step 37380, loss = 0.95 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 07:39:37.569091: step 37390, loss = 1.40 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 07:39:50.122095: step 37400, loss = 1.06 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 07:40:07.091828: step 37410, loss = 1.05 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-06-17 07:40:19.457098: step 37420, loss = 0.93 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 07:40:31.632307: step 37430, loss = 1.26 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 07:40:43.876702: step 37440, loss = 0.90 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 07:40:56.170326: step 37450, loss = 0.87 (23.3 examples/sec; 1.287 sec/batch)\n",
      "2019-06-17 07:41:08.462708: step 37460, loss = 0.98 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 07:41:20.817446: step 37470, loss = 1.07 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-17 07:41:33.062285: step 37480, loss = 1.20 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 07:41:45.526064: step 37490, loss = 1.09 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 07:41:57.822720: step 37500, loss = 0.98 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 07:42:14.727290: step 37510, loss = 1.39 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 07:42:27.064306: step 37520, loss = 1.21 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 07:42:39.705697: step 37530, loss = 0.96 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 07:42:51.967904: step 37540, loss = 1.12 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 07:43:04.543946: step 37550, loss = 1.18 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 07:43:16.877666: step 37560, loss = 1.15 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 07:43:29.140671: step 37570, loss = 0.93 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 07:43:41.228057: step 37580, loss = 1.25 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 07:43:53.760607: step 37590, loss = 1.11 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 07:44:06.179397: step 37600, loss = 0.99 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-06-17 07:44:23.310969: step 37610, loss = 1.10 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 07:44:35.647526: step 37620, loss = 0.95 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-17 07:44:47.966438: step 37630, loss = 1.45 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 07:45:00.104896: step 37640, loss = 1.10 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 07:45:12.403828: step 37650, loss = 1.00 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 07:45:24.557215: step 37660, loss = 0.86 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 07:45:36.874062: step 37670, loss = 0.91 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 07:45:49.363450: step 37680, loss = 1.06 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 07:46:01.783138: step 37690, loss = 0.87 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 07:46:14.354226: step 37700, loss = 0.98 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 07:46:31.115492: step 37710, loss = 0.89 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 07:46:43.457949: step 37720, loss = 0.91 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 07:46:55.629495: step 37730, loss = 1.10 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 07:47:07.899113: step 37740, loss = 0.89 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 07:47:20.111055: step 37750, loss = 1.10 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 07:47:32.624236: step 37760, loss = 1.08 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 07:47:45.137809: step 37770, loss = 1.06 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 07:47:57.720833: step 37780, loss = 1.18 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 07:48:09.937361: step 37790, loss = 1.07 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 07:48:22.301140: step 37800, loss = 0.97 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-06-17 07:48:39.179334: step 37810, loss = 0.98 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 07:48:51.493501: step 37820, loss = 1.28 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 07:49:03.871084: step 37830, loss = 1.08 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 07:49:16.162702: step 37840, loss = 0.96 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 07:49:28.481991: step 37850, loss = 0.98 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 07:49:40.746340: step 37860, loss = 1.00 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 07:49:53.135851: step 37870, loss = 1.07 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 07:50:05.648804: step 37880, loss = 1.11 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 07:50:18.042577: step 37890, loss = 1.08 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 07:50:30.316397: step 37900, loss = 1.04 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 07:50:48.180690: step 37910, loss = 1.25 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 07:51:00.394819: step 37920, loss = 1.14 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 07:51:12.870257: step 37930, loss = 0.93 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 07:51:25.322845: step 37940, loss = 0.99 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 07:51:37.436206: step 37950, loss = 1.26 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 07:51:49.694104: step 37960, loss = 1.01 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 07:52:01.890427: step 37970, loss = 0.91 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 07:52:14.029656: step 37980, loss = 1.08 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 07:52:26.268910: step 37990, loss = 1.26 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 07:52:38.500478: step 38000, loss = 0.88 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 07:52:55.116003: step 38010, loss = 0.90 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 07:53:07.347137: step 38020, loss = 1.16 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 07:53:19.664445: step 38030, loss = 1.10 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 07:53:31.991936: step 38040, loss = 0.96 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-17 07:53:44.331575: step 38050, loss = 1.03 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 07:53:56.640082: step 38060, loss = 0.94 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 07:54:08.807941: step 38070, loss = 1.11 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 07:54:21.168339: step 38080, loss = 1.07 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 07:54:33.362611: step 38090, loss = 1.03 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 07:54:45.583977: step 38100, loss = 0.98 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 07:55:02.845429: step 38110, loss = 1.16 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 07:55:15.272317: step 38120, loss = 1.00 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-06-17 07:55:27.434942: step 38130, loss = 0.96 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 07:55:39.850306: step 38140, loss = 1.26 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 07:55:52.106453: step 38150, loss = 0.95 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 07:56:04.608857: step 38160, loss = 1.15 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 07:56:17.114759: step 38170, loss = 0.96 (25.1 examples/sec; 1.196 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 07:56:29.371221: step 38180, loss = 0.96 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 07:56:41.525768: step 38190, loss = 1.01 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 07:56:53.949848: step 38200, loss = 1.15 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 07:57:10.453767: step 38210, loss = 1.06 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 07:57:22.755368: step 38220, loss = 0.92 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 07:57:35.147425: step 38230, loss = 1.09 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 07:57:47.502706: step 38240, loss = 1.13 (22.5 examples/sec; 1.331 sec/batch)\n",
      "2019-06-17 07:57:59.833054: step 38250, loss = 1.00 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 07:58:12.133605: step 38260, loss = 0.98 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 07:58:24.531346: step 38270, loss = 0.95 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 07:58:36.784370: step 38280, loss = 1.18 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 07:58:49.120735: step 38290, loss = 1.06 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 07:59:01.376634: step 38300, loss = 1.09 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 07:59:18.257551: step 38310, loss = 0.94 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 07:59:30.395242: step 38320, loss = 1.00 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 07:59:43.048580: step 38330, loss = 1.28 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 07:59:55.240647: step 38340, loss = 1.12 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 08:00:07.442996: step 38350, loss = 1.24 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 08:00:19.963065: step 38360, loss = 0.92 (23.1 examples/sec; 1.296 sec/batch)\n",
      "2019-06-17 08:00:32.178038: step 38370, loss = 1.08 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 08:00:44.380508: step 38380, loss = 1.15 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 08:00:56.574450: step 38390, loss = 1.41 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 08:01:09.088643: step 38400, loss = 0.99 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 08:01:26.066803: step 38410, loss = 1.01 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 08:01:38.326930: step 38420, loss = 1.10 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 08:01:50.703775: step 38430, loss = 1.06 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 08:02:02.966392: step 38440, loss = 1.15 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 08:02:15.141573: step 38450, loss = 1.05 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 08:02:27.610814: step 38460, loss = 1.06 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 08:02:39.946488: step 38470, loss = 0.93 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 08:02:52.272147: step 38480, loss = 1.10 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 08:03:04.747949: step 38490, loss = 0.86 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 08:03:17.261435: step 38500, loss = 1.06 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 08:03:33.792143: step 38510, loss = 0.98 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 08:03:46.043248: step 38520, loss = 1.18 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 08:03:58.163665: step 38530, loss = 1.01 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 08:04:10.367013: step 38540, loss = 1.02 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 08:04:23.022563: step 38550, loss = 1.05 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 08:04:35.311758: step 38560, loss = 0.95 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-06-17 08:04:47.732038: step 38570, loss = 1.02 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 08:05:00.280852: step 38580, loss = 1.12 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 08:05:12.399007: step 38590, loss = 0.93 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 08:05:24.611212: step 38600, loss = 1.10 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 08:05:41.369181: step 38610, loss = 1.16 (23.0 examples/sec; 1.307 sec/batch)\n",
      "2019-06-17 08:05:53.613723: step 38620, loss = 0.94 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 08:06:05.821892: step 38630, loss = 1.10 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 08:06:18.270526: step 38640, loss = 1.30 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 08:06:30.881584: step 38650, loss = 1.19 (23.8 examples/sec; 1.258 sec/batch)\n",
      "2019-06-17 08:06:43.300758: step 38660, loss = 1.03 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 08:06:55.645463: step 38670, loss = 1.07 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 08:07:07.867690: step 38680, loss = 1.12 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 08:07:20.143285: step 38690, loss = 1.21 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 08:07:32.463008: step 38700, loss = 1.08 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 08:07:49.474546: step 38710, loss = 0.96 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-17 08:08:01.718210: step 38720, loss = 1.12 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 08:08:13.948861: step 38730, loss = 1.02 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 08:08:26.242110: step 38740, loss = 1.14 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 08:08:38.525527: step 38750, loss = 1.06 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 08:08:50.956845: step 38760, loss = 0.97 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 08:09:03.279917: step 38770, loss = 1.52 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-06-17 08:09:15.546928: step 38780, loss = 0.93 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 08:09:27.995422: step 38790, loss = 0.97 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-06-17 08:09:40.498360: step 38800, loss = 1.05 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 08:09:57.449194: step 38810, loss = 0.95 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 08:10:09.819641: step 38820, loss = 1.09 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 08:10:22.163428: step 38830, loss = 1.22 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 08:10:34.481519: step 38840, loss = 1.11 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 08:10:46.776074: step 38850, loss = 0.96 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 08:10:59.086646: step 38860, loss = 1.11 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-17 08:11:11.448581: step 38870, loss = 1.26 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 08:11:23.679840: step 38880, loss = 1.12 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 08:11:36.246033: step 38890, loss = 1.17 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 08:11:48.585173: step 38900, loss = 0.93 (22.6 examples/sec; 1.330 sec/batch)\n",
      "2019-06-17 08:12:05.624687: step 38910, loss = 1.10 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 08:12:17.987840: step 38920, loss = 1.00 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 08:12:30.471377: step 38930, loss = 0.94 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 08:12:42.873391: step 38940, loss = 1.04 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 08:12:55.055912: step 38950, loss = 1.30 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 08:13:07.397156: step 38960, loss = 1.01 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 08:13:19.879436: step 38970, loss = 0.89 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 08:13:32.567274: step 38980, loss = 1.24 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-17 08:13:44.732613: step 38990, loss = 1.09 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 08:13:57.087503: step 39000, loss = 1.03 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 08:14:14.140883: step 39010, loss = 1.03 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 08:14:26.391440: step 39020, loss = 1.24 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 08:14:38.568058: step 39030, loss = 1.23 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 08:14:50.819112: step 39040, loss = 1.17 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 08:15:03.257917: step 39050, loss = 1.13 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 08:15:15.620972: step 39060, loss = 0.93 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 08:15:27.955260: step 39070, loss = 1.09 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 08:15:40.293585: step 39080, loss = 1.12 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 08:15:52.634176: step 39090, loss = 1.19 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 08:16:04.934564: step 39100, loss = 0.86 (24.7 examples/sec; 1.216 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 08:16:21.544408: step 39110, loss = 1.12 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 08:16:33.781840: step 39120, loss = 1.07 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 08:16:46.310525: step 39130, loss = 1.26 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 08:16:58.545736: step 39140, loss = 1.07 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 08:17:10.690537: step 39150, loss = 1.08 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 08:17:22.964506: step 39160, loss = 1.23 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 08:17:35.329116: step 39170, loss = 1.24 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 08:17:47.782752: step 39180, loss = 1.04 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 08:18:00.101971: step 39190, loss = 1.22 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 08:18:12.620503: step 39200, loss = 0.95 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 08:18:29.412294: step 39210, loss = 1.13 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 08:18:41.595603: step 39220, loss = 1.04 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 08:18:54.108543: step 39230, loss = 1.16 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 08:19:06.227323: step 39240, loss = 1.12 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 08:19:18.563324: step 39250, loss = 1.02 (23.5 examples/sec; 1.277 sec/batch)\n",
      "2019-06-17 08:19:30.803001: step 39260, loss = 1.15 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 08:19:43.061389: step 39270, loss = 1.06 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 08:19:55.393755: step 39280, loss = 0.94 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 08:20:07.786071: step 39290, loss = 1.01 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 08:20:20.274788: step 39300, loss = 1.09 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-06-17 08:20:37.083187: step 39310, loss = 1.05 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 08:20:49.476437: step 39320, loss = 1.06 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 08:21:01.934184: step 39330, loss = 1.17 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 08:21:14.192477: step 39340, loss = 1.08 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 08:21:26.602553: step 39350, loss = 1.10 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 08:21:39.010605: step 39360, loss = 1.08 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 08:21:51.399389: step 39370, loss = 0.94 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 08:22:03.639296: step 39380, loss = 1.18 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 08:22:16.004127: step 39390, loss = 0.93 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 08:22:28.305739: step 39400, loss = 1.09 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 08:22:45.373161: step 39410, loss = 0.94 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 08:22:57.559469: step 39420, loss = 0.94 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 08:23:09.883954: step 39430, loss = 0.95 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 08:23:22.085194: step 39440, loss = 1.12 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 08:23:34.371472: step 39450, loss = 0.95 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 08:23:46.631102: step 39460, loss = 1.10 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 08:23:59.025924: step 39470, loss = 1.08 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 08:24:11.357844: step 39480, loss = 0.88 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 08:24:23.610298: step 39490, loss = 1.38 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 08:24:36.000257: step 39500, loss = 1.00 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 08:24:53.217071: step 39510, loss = 0.95 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 08:25:05.691252: step 39520, loss = 1.02 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 08:25:17.992558: step 39530, loss = 1.19 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 08:25:30.364504: step 39540, loss = 1.22 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 08:25:42.720860: step 39550, loss = 1.23 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 08:25:55.111699: step 39560, loss = 0.96 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 08:26:07.346113: step 39570, loss = 1.28 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 08:26:19.800887: step 39580, loss = 1.01 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 08:26:32.125962: step 39590, loss = 0.91 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 08:26:44.464217: step 39600, loss = 1.13 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 08:27:01.405823: step 39610, loss = 1.13 (23.4 examples/sec; 1.283 sec/batch)\n",
      "2019-06-17 08:27:13.699365: step 39620, loss = 1.07 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 08:27:25.866723: step 39630, loss = 1.01 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 08:27:38.120621: step 39640, loss = 0.93 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-17 08:27:50.573374: step 39650, loss = 0.98 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 08:28:02.837482: step 39660, loss = 1.07 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 08:28:15.245612: step 39670, loss = 1.13 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 08:28:27.535931: step 39680, loss = 1.06 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 08:28:39.694505: step 39690, loss = 1.24 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 08:28:52.080130: step 39700, loss = 0.97 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 08:29:09.320987: step 39710, loss = 0.93 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 08:29:21.671380: step 39720, loss = 1.15 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 08:29:33.858750: step 39730, loss = 0.95 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 08:29:46.315792: step 39740, loss = 1.16 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 08:29:58.587411: step 39750, loss = 1.05 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 08:30:10.951321: step 39760, loss = 0.98 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 08:30:23.495189: step 39770, loss = 1.08 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 08:30:35.849115: step 39780, loss = 1.06 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 08:30:48.118492: step 39790, loss = 1.04 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 08:31:00.588231: step 39800, loss = 1.03 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 08:31:17.458476: step 39810, loss = 1.22 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 08:31:29.740358: step 39820, loss = 1.04 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 08:31:42.119614: step 39830, loss = 1.43 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 08:31:54.249315: step 39840, loss = 1.17 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 08:32:06.550071: step 39850, loss = 1.26 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 08:32:18.646371: step 39860, loss = 0.93 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 08:32:30.923242: step 39870, loss = 1.17 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 08:32:43.190660: step 39880, loss = 0.88 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 08:32:55.423174: step 39890, loss = 1.05 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-06-17 08:33:07.616105: step 39900, loss = 1.04 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 08:33:24.494672: step 39910, loss = 0.93 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 08:33:36.772816: step 39920, loss = 1.03 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 08:33:49.139010: step 39930, loss = 0.99 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 08:34:01.597456: step 39940, loss = 1.02 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 08:34:14.041321: step 39950, loss = 0.84 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 08:34:26.440173: step 39960, loss = 1.15 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 08:34:38.611859: step 39970, loss = 1.07 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 08:34:50.947810: step 39980, loss = 1.05 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 08:35:03.370051: step 39990, loss = 1.25 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-17 08:35:15.534385: step 40000, loss = 1.16 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 08:35:35.546196: step 40010, loss = 1.25 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 08:35:48.010135: step 40020, loss = 1.09 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 08:36:00.517321: step 40030, loss = 0.89 (25.0 examples/sec; 1.202 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 08:36:12.976738: step 40040, loss = 1.25 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 08:36:25.288038: step 40050, loss = 1.33 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 08:36:37.413649: step 40060, loss = 1.16 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 08:36:49.639682: step 40070, loss = 1.26 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 08:37:02.063880: step 40080, loss = 0.87 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 08:37:14.321955: step 40090, loss = 0.90 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 08:37:26.845575: step 40100, loss = 1.10 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 08:37:43.960944: step 40110, loss = 1.07 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 08:37:56.567071: step 40120, loss = 0.85 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-06-17 08:38:08.820639: step 40130, loss = 0.90 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 08:38:21.056516: step 40140, loss = 1.00 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 08:38:33.408595: step 40150, loss = 0.93 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-06-17 08:38:45.763285: step 40160, loss = 1.01 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 08:38:58.120414: step 40170, loss = 1.04 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-17 08:39:10.390601: step 40180, loss = 1.08 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 08:39:22.735726: step 40190, loss = 1.10 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 08:39:35.076573: step 40200, loss = 0.97 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 08:39:52.022631: step 40210, loss = 1.09 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 08:40:04.606188: step 40220, loss = 1.18 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 08:40:16.807382: step 40230, loss = 1.09 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 08:40:29.031376: step 40240, loss = 1.21 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 08:40:41.350184: step 40250, loss = 1.01 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 08:40:53.514552: step 40260, loss = 1.04 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 08:41:05.981643: step 40270, loss = 0.96 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 08:41:18.307308: step 40280, loss = 1.00 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 08:41:30.682938: step 40290, loss = 0.93 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 08:41:43.031509: step 40300, loss = 0.93 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 08:41:59.796345: step 40310, loss = 1.02 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 08:42:12.386750: step 40320, loss = 1.14 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 08:42:24.573592: step 40330, loss = 1.02 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 08:42:36.952801: step 40340, loss = 1.14 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 08:42:49.690171: step 40350, loss = 0.96 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 08:43:02.054470: step 40360, loss = 1.08 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 08:43:14.246684: step 40370, loss = 1.16 (23.6 examples/sec; 1.273 sec/batch)\n",
      "2019-06-17 08:43:26.558804: step 40380, loss = 1.13 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 08:43:38.743877: step 40390, loss = 0.93 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 08:43:51.198673: step 40400, loss = 0.97 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 08:44:08.728375: step 40410, loss = 1.38 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 08:44:21.152687: step 40420, loss = 1.11 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 08:44:33.525234: step 40430, loss = 1.04 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 08:44:45.876555: step 40440, loss = 1.08 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 08:44:58.305137: step 40450, loss = 1.04 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 08:45:10.836368: step 40460, loss = 1.02 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 08:45:22.966043: step 40470, loss = 1.00 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 08:45:35.456571: step 40480, loss = 1.29 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 08:45:47.618268: step 40490, loss = 1.18 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 08:45:59.851121: step 40500, loss = 0.91 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 08:46:17.186726: step 40510, loss = 1.08 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 08:46:29.375016: step 40520, loss = 1.06 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 08:46:41.872562: step 40530, loss = 1.03 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 08:46:54.222741: step 40540, loss = 1.20 (23.3 examples/sec; 1.288 sec/batch)\n",
      "2019-06-17 08:47:06.595097: step 40550, loss = 1.05 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 08:47:18.930777: step 40560, loss = 0.93 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 08:47:31.156670: step 40570, loss = 1.01 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 08:47:43.613063: step 40580, loss = 1.23 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 08:47:55.901409: step 40590, loss = 1.04 (22.3 examples/sec; 1.343 sec/batch)\n",
      "2019-06-17 08:48:08.204372: step 40600, loss = 1.02 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 08:48:24.716379: step 40610, loss = 1.25 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 08:48:36.973137: step 40620, loss = 1.02 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-06-17 08:48:49.101461: step 40630, loss = 1.22 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 08:49:01.547732: step 40640, loss = 1.19 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 08:49:13.979013: step 40650, loss = 0.90 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 08:49:26.065198: step 40660, loss = 1.03 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 08:49:38.426991: step 40670, loss = 1.08 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 08:49:50.718207: step 40680, loss = 1.08 (23.7 examples/sec; 1.266 sec/batch)\n",
      "2019-06-17 08:50:02.977346: step 40690, loss = 1.11 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 08:50:15.425693: step 40700, loss = 1.18 (23.3 examples/sec; 1.287 sec/batch)\n",
      "2019-06-17 08:50:32.136877: step 40710, loss = 1.03 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 08:50:44.325818: step 40720, loss = 1.29 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 08:50:56.725386: step 40730, loss = 1.14 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 08:51:09.002867: step 40740, loss = 1.04 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 08:51:21.558948: step 40750, loss = 0.95 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 08:51:33.809672: step 40760, loss = 0.94 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 08:51:46.158665: step 40770, loss = 1.16 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 08:51:58.430171: step 40780, loss = 1.16 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 08:52:10.717474: step 40790, loss = 1.05 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 08:52:23.057545: step 40800, loss = 1.26 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-17 08:52:40.082651: step 40810, loss = 1.07 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 08:52:52.423523: step 40820, loss = 1.13 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 08:53:04.839745: step 40830, loss = 0.95 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 08:53:17.177499: step 40840, loss = 1.11 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-06-17 08:53:29.425039: step 40850, loss = 1.03 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 08:53:41.627071: step 40860, loss = 1.12 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 08:53:53.954162: step 40870, loss = 1.15 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 08:54:06.082391: step 40880, loss = 1.17 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 08:54:18.259924: step 40890, loss = 1.12 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 08:54:30.743891: step 40900, loss = 1.06 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 08:54:47.695687: step 40910, loss = 0.98 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 08:55:00.125839: step 40920, loss = 1.04 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 08:55:12.665146: step 40930, loss = 1.04 (22.4 examples/sec; 1.341 sec/batch)\n",
      "2019-06-17 08:55:24.952164: step 40940, loss = 1.22 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 08:55:37.178874: step 40950, loss = 1.03 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-17 08:55:49.577625: step 40960, loss = 1.08 (24.8 examples/sec; 1.208 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 08:56:01.957344: step 40970, loss = 1.01 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 08:56:14.178499: step 40980, loss = 1.14 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 08:56:26.580879: step 40990, loss = 1.26 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-06-17 08:56:38.822756: step 41000, loss = 0.96 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 08:56:55.850873: step 41010, loss = 1.21 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 08:57:08.043592: step 41020, loss = 1.06 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 08:57:20.394888: step 41030, loss = 1.14 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 08:57:32.987257: step 41040, loss = 1.08 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 08:57:45.550704: step 41050, loss = 1.01 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 08:57:57.748249: step 41060, loss = 0.96 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 08:58:10.139920: step 41070, loss = 1.05 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 08:58:22.570553: step 41080, loss = 0.96 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 08:58:34.808631: step 41090, loss = 1.21 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 08:58:47.006349: step 41100, loss = 0.91 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 08:59:03.840004: step 41110, loss = 1.09 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 08:59:16.261457: step 41120, loss = 1.21 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 08:59:28.779817: step 41130, loss = 0.97 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 08:59:40.869051: step 41140, loss = 1.48 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 08:59:53.090535: step 41150, loss = 1.12 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 09:00:05.286982: step 41160, loss = 1.07 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 09:00:17.586149: step 41170, loss = 1.09 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 09:00:30.150617: step 41180, loss = 0.95 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 09:00:42.540416: step 41190, loss = 1.11 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 09:00:54.773562: step 41200, loss = 0.84 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 09:01:11.573764: step 41210, loss = 0.94 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 09:01:23.830276: step 41220, loss = 1.12 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 09:01:36.529919: step 41230, loss = 1.03 (22.6 examples/sec; 1.330 sec/batch)\n",
      "2019-06-17 09:01:49.226194: step 41240, loss = 0.96 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 09:02:01.450492: step 41250, loss = 0.92 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 09:02:13.923199: step 41260, loss = 1.05 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 09:02:26.298538: step 41270, loss = 1.01 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 09:02:38.790174: step 41280, loss = 1.21 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 09:02:50.950771: step 41290, loss = 1.14 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 09:03:03.237809: step 41300, loss = 1.22 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 09:03:20.252006: step 41310, loss = 0.98 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 09:03:32.736816: step 41320, loss = 1.12 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 09:03:45.281954: step 41330, loss = 1.16 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-06-17 09:03:57.533467: step 41340, loss = 1.00 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 09:04:10.306004: step 41350, loss = 1.24 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 09:04:22.652360: step 41360, loss = 1.01 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 09:04:35.249780: step 41370, loss = 1.08 (23.1 examples/sec; 1.299 sec/batch)\n",
      "2019-06-17 09:04:47.713555: step 41380, loss = 1.13 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 09:05:00.072918: step 41390, loss = 1.09 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 09:05:12.309406: step 41400, loss = 1.11 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 09:05:29.558143: step 41410, loss = 0.89 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 09:05:42.016131: step 41420, loss = 1.07 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 09:05:54.589058: step 41430, loss = 0.99 (22.5 examples/sec; 1.333 sec/batch)\n",
      "2019-06-17 09:06:06.859236: step 41440, loss = 0.90 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 09:06:19.321368: step 41450, loss = 1.13 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 09:06:31.642848: step 41460, loss = 1.16 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 09:06:44.064360: step 41470, loss = 1.01 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 09:06:56.621794: step 41480, loss = 1.25 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-06-17 09:07:09.083277: step 41490, loss = 0.98 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 09:07:21.315601: step 41500, loss = 1.03 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 09:07:38.305114: step 41510, loss = 1.31 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 09:07:50.704744: step 41520, loss = 1.21 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 09:08:02.807170: step 41530, loss = 1.16 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 09:08:15.153889: step 41540, loss = 1.30 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-17 09:08:27.659707: step 41550, loss = 1.20 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 09:08:39.961966: step 41560, loss = 0.92 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-06-17 09:08:52.315491: step 41570, loss = 1.13 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 09:09:04.584436: step 41580, loss = 0.99 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 09:09:16.875283: step 41590, loss = 1.09 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 09:09:29.213952: step 41600, loss = 1.07 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 09:09:46.377792: step 41610, loss = 1.01 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 09:09:58.542989: step 41620, loss = 1.02 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 09:10:10.697844: step 41630, loss = 1.19 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 09:10:23.091014: step 41640, loss = 1.12 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 09:10:35.531831: step 41650, loss = 0.90 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 09:10:47.851923: step 41660, loss = 1.02 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 09:11:00.248604: step 41670, loss = 1.15 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 09:11:12.575401: step 41680, loss = 0.92 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 09:11:25.052876: step 41690, loss = 1.07 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 09:11:37.393325: step 41700, loss = 0.97 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 09:11:54.360942: step 41710, loss = 1.25 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 09:12:06.753130: step 41720, loss = 1.05 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 09:12:19.159265: step 41730, loss = 1.01 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 09:12:31.813003: step 41740, loss = 1.01 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 09:12:44.022256: step 41750, loss = 1.42 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 09:12:56.428037: step 41760, loss = 0.99 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 09:13:08.630044: step 41770, loss = 1.14 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 09:13:20.994896: step 41780, loss = 1.15 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 09:13:33.391395: step 41790, loss = 1.08 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 09:13:45.953546: step 41800, loss = 1.12 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-17 09:14:02.910917: step 41810, loss = 1.11 (22.6 examples/sec; 1.328 sec/batch)\n",
      "2019-06-17 09:14:15.231920: step 41820, loss = 1.12 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 09:14:27.591673: step 41830, loss = 1.08 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 09:14:39.732951: step 41840, loss = 1.01 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 09:14:52.445039: step 41850, loss = 0.99 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 09:15:04.801523: step 41860, loss = 1.09 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 09:15:17.041070: step 41870, loss = 1.18 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 09:15:29.188496: step 41880, loss = 0.89 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 09:15:41.818508: step 41890, loss = 0.98 (24.7 examples/sec; 1.215 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 09:15:54.196299: step 41900, loss = 1.04 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 09:16:11.367961: step 41910, loss = 1.01 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 09:16:23.883917: step 41920, loss = 0.93 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 09:16:36.249169: step 41930, loss = 1.06 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 09:16:48.583766: step 41940, loss = 0.92 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 09:17:00.966529: step 41950, loss = 1.01 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 09:17:13.411935: step 41960, loss = 1.09 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-06-17 09:17:25.783937: step 41970, loss = 1.17 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-17 09:17:38.241209: step 41980, loss = 1.01 (23.3 examples/sec; 1.286 sec/batch)\n",
      "2019-06-17 09:17:50.349924: step 41990, loss = 0.99 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 09:18:02.754567: step 42000, loss = 1.11 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 09:18:19.895590: step 42010, loss = 1.06 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 09:18:32.136135: step 42020, loss = 1.22 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 09:18:44.535397: step 42030, loss = 1.06 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 09:18:56.865534: step 42040, loss = 1.02 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 09:19:09.068008: step 42050, loss = 1.06 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 09:19:21.270360: step 42060, loss = 1.09 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 09:19:33.488020: step 42070, loss = 0.93 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-06-17 09:19:45.855253: step 42080, loss = 1.03 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 09:19:58.054133: step 42090, loss = 1.14 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 09:20:10.400392: step 42100, loss = 1.08 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 09:20:27.520866: step 42110, loss = 0.96 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 09:20:39.797382: step 42120, loss = 1.01 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 09:20:52.166641: step 42130, loss = 1.17 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 09:21:04.377319: step 42140, loss = 1.05 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 09:21:16.476029: step 42150, loss = 1.06 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 09:21:28.961211: step 42160, loss = 1.29 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 09:21:41.437387: step 42170, loss = 1.16 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 09:21:53.669730: step 42180, loss = 0.98 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 09:22:05.981499: step 42190, loss = 1.05 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 09:22:18.457415: step 42200, loss = 0.97 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 09:22:35.383158: step 42210, loss = 0.91 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 09:22:47.728445: step 42220, loss = 0.88 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 09:23:00.004212: step 42230, loss = 0.96 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 09:23:12.337246: step 42240, loss = 0.93 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 09:23:24.648157: step 42250, loss = 1.09 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 09:23:36.866380: step 42260, loss = 0.92 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 09:23:49.331247: step 42270, loss = 1.07 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 09:24:01.578308: step 42280, loss = 1.30 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 09:24:14.114540: step 42290, loss = 1.18 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 09:24:26.627871: step 42300, loss = 1.13 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 09:24:43.767043: step 42310, loss = 1.03 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 09:24:55.925812: step 42320, loss = 1.12 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 09:25:08.184029: step 42330, loss = 0.90 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 09:25:20.467671: step 42340, loss = 1.01 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 09:25:32.847659: step 42350, loss = 1.20 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 09:25:45.287980: step 42360, loss = 0.97 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 09:25:57.390277: step 42370, loss = 1.11 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 09:26:09.602545: step 42380, loss = 1.05 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 09:26:21.702470: step 42390, loss = 1.07 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 09:26:34.040597: step 42400, loss = 1.08 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 09:26:50.830416: step 42410, loss = 1.02 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 09:27:03.290502: step 42420, loss = 0.79 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 09:27:15.516698: step 42430, loss = 1.12 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 09:27:27.669791: step 42440, loss = 1.09 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 09:27:40.090343: step 42450, loss = 1.09 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 09:27:52.430176: step 42460, loss = 0.96 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 09:28:04.751853: step 42470, loss = 1.50 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 09:28:17.129288: step 42480, loss = 1.43 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 09:28:29.731539: step 42490, loss = 1.14 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 09:28:42.225508: step 42500, loss = 1.03 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 09:28:58.942636: step 42510, loss = 1.11 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-06-17 09:29:11.464222: step 42520, loss = 1.14 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 09:29:23.848350: step 42530, loss = 1.02 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 09:29:36.065278: step 42540, loss = 1.13 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 09:29:48.405964: step 42550, loss = 0.99 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 09:30:00.701472: step 42560, loss = 0.95 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 09:30:12.839156: step 42570, loss = 1.18 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 09:30:25.187645: step 42580, loss = 1.12 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 09:30:37.404531: step 42590, loss = 1.18 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 09:30:49.688595: step 42600, loss = 0.92 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 09:31:06.472288: step 42610, loss = 1.02 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 09:31:18.864418: step 42620, loss = 1.10 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 09:31:31.172480: step 42630, loss = 0.92 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 09:31:43.645558: step 42640, loss = 1.09 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 09:31:56.068082: step 42650, loss = 0.98 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 09:32:08.372035: step 42660, loss = 1.05 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 09:32:20.644163: step 42670, loss = 1.09 (23.0 examples/sec; 1.307 sec/batch)\n",
      "2019-06-17 09:32:32.922684: step 42680, loss = 1.06 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 09:32:45.269564: step 42690, loss = 0.98 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 09:32:57.792510: step 42700, loss = 1.08 (22.5 examples/sec; 1.336 sec/batch)\n",
      "2019-06-17 09:33:14.461186: step 42710, loss = 1.07 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 09:33:26.978315: step 42720, loss = 1.17 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 09:33:39.275437: step 42730, loss = 1.08 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 09:33:51.661267: step 42740, loss = 1.01 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 09:34:03.818176: step 42750, loss = 1.08 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 09:34:16.043066: step 42760, loss = 0.99 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-17 09:34:28.408018: step 42770, loss = 1.13 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 09:34:40.705187: step 42780, loss = 0.99 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 09:34:53.009664: step 42790, loss = 0.94 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 09:35:05.305181: step 42800, loss = 0.98 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 09:35:22.472435: step 42810, loss = 1.16 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 09:35:34.984925: step 42820, loss = 1.21 (24.2 examples/sec; 1.237 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 09:35:47.483972: step 42830, loss = 1.38 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 09:35:59.590747: step 42840, loss = 0.91 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 09:36:12.245173: step 42850, loss = 1.02 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 09:36:24.502445: step 42860, loss = 0.98 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 09:36:36.659107: step 42870, loss = 0.93 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 09:36:48.884828: step 42880, loss = 1.09 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-06-17 09:37:01.468770: step 42890, loss = 0.85 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 09:37:13.833848: step 42900, loss = 0.97 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 09:37:30.627639: step 42910, loss = 1.15 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 09:37:42.804361: step 42920, loss = 1.37 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 09:37:55.276457: step 42930, loss = 1.01 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 09:38:07.440404: step 42940, loss = 1.24 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 09:38:19.775905: step 42950, loss = 0.99 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 09:38:32.333813: step 42960, loss = 1.08 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 09:38:44.522568: step 42970, loss = 0.89 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 09:38:57.206775: step 42980, loss = 0.96 (22.6 examples/sec; 1.328 sec/batch)\n",
      "2019-06-17 09:39:09.523676: step 42990, loss = 1.21 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 09:39:21.817658: step 43000, loss = 0.99 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 09:39:38.674415: step 43010, loss = 1.01 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 09:39:51.100300: step 43020, loss = 1.14 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 09:40:03.579487: step 43030, loss = 0.97 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-17 09:40:16.159886: step 43040, loss = 1.21 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 09:40:28.777635: step 43050, loss = 1.03 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 09:40:40.931112: step 43060, loss = 1.10 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 09:40:53.283481: step 43070, loss = 1.24 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 09:41:05.596664: step 43080, loss = 1.03 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 09:41:17.842327: step 43090, loss = 1.06 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 09:41:30.044496: step 43100, loss = 0.98 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 09:41:46.809920: step 43110, loss = 0.96 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 09:41:59.150545: step 43120, loss = 0.97 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 09:42:11.446887: step 43130, loss = 0.90 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 09:42:23.587706: step 43140, loss = 1.11 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 09:42:35.908289: step 43150, loss = 1.02 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 09:42:48.495924: step 43160, loss = 0.97 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 09:43:00.843705: step 43170, loss = 1.00 (22.9 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 09:43:13.278551: step 43180, loss = 1.03 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 09:43:25.985715: step 43190, loss = 0.90 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 09:43:38.226473: step 43200, loss = 1.03 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 09:43:55.457486: step 43210, loss = 0.98 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 09:44:07.824807: step 43220, loss = 1.09 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 09:44:20.080462: step 43230, loss = 0.82 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 09:44:32.523695: step 43240, loss = 0.98 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-06-17 09:44:45.056131: step 43250, loss = 0.94 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 09:44:57.370328: step 43260, loss = 1.06 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 09:45:10.107427: step 43270, loss = 1.31 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-17 09:45:22.387858: step 43280, loss = 0.94 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 09:45:34.888760: step 43290, loss = 1.12 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 09:45:47.094648: step 43300, loss = 1.14 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 09:46:03.826636: step 43310, loss = 0.95 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 09:46:16.208344: step 43320, loss = 1.11 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-06-17 09:46:28.624377: step 43330, loss = 1.15 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 09:46:41.042706: step 43340, loss = 1.12 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-06-17 09:46:53.366623: step 43350, loss = 1.25 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 09:47:05.727472: step 43360, loss = 0.94 (22.3 examples/sec; 1.344 sec/batch)\n",
      "2019-06-17 09:47:18.086443: step 43370, loss = 0.92 (22.5 examples/sec; 1.334 sec/batch)\n",
      "2019-06-17 09:47:30.423625: step 43380, loss = 1.13 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-06-17 09:47:42.687530: step 43390, loss = 0.94 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 09:47:55.239701: step 43400, loss = 1.10 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 09:48:12.306132: step 43410, loss = 1.05 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 09:48:24.753239: step 43420, loss = 1.06 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 09:48:37.083727: step 43430, loss = 0.99 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 09:48:49.737789: step 43440, loss = 1.02 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 09:49:01.913763: step 43450, loss = 1.16 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 09:49:14.200932: step 43460, loss = 1.13 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 09:49:26.482452: step 43470, loss = 0.94 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-06-17 09:49:38.915950: step 43480, loss = 1.45 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 09:49:51.207587: step 43490, loss = 1.07 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 09:50:03.359721: step 43500, loss = 1.04 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 09:50:19.958305: step 43510, loss = 0.98 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 09:50:32.430080: step 43520, loss = 1.27 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 09:50:44.799827: step 43530, loss = 1.08 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 09:50:57.124321: step 43540, loss = 0.95 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 09:51:09.501384: step 43550, loss = 0.91 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 09:51:21.704838: step 43560, loss = 1.07 (23.5 examples/sec; 1.278 sec/batch)\n",
      "2019-06-17 09:51:34.020384: step 43570, loss = 0.94 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-06-17 09:51:46.275145: step 43580, loss = 1.12 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 09:51:58.597053: step 43590, loss = 1.09 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 09:52:10.917767: step 43600, loss = 0.92 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 09:52:27.511961: step 43610, loss = 0.94 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 09:52:39.911631: step 43620, loss = 0.96 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 09:52:52.163513: step 43630, loss = 1.14 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 09:53:04.570353: step 43640, loss = 1.13 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 09:53:17.032961: step 43650, loss = 1.15 (22.9 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 09:53:29.335275: step 43660, loss = 1.03 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 09:53:41.716681: step 43670, loss = 1.21 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 09:53:53.911636: step 43680, loss = 0.93 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 09:54:06.227843: step 43690, loss = 0.98 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 09:54:18.632239: step 43700, loss = 0.92 (22.5 examples/sec; 1.331 sec/batch)\n",
      "2019-06-17 09:54:35.686933: step 43710, loss = 1.11 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 09:54:48.069535: step 43720, loss = 0.87 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-06-17 09:55:00.390382: step 43730, loss = 1.13 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 09:55:12.795985: step 43740, loss = 0.90 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 09:55:25.016619: step 43750, loss = 0.97 (23.9 examples/sec; 1.256 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 09:55:37.497418: step 43760, loss = 1.03 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 09:55:49.681814: step 43770, loss = 0.93 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 09:56:01.975120: step 43780, loss = 1.20 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 09:56:14.154106: step 43790, loss = 1.00 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 09:56:26.492688: step 43800, loss = 1.17 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-06-17 09:56:43.571032: step 43810, loss = 1.03 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 09:56:56.058237: step 43820, loss = 0.98 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-17 09:57:08.270232: step 43830, loss = 0.93 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 09:57:20.700357: step 43840, loss = 1.12 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 09:57:32.924620: step 43850, loss = 1.03 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 09:57:45.257589: step 43860, loss = 1.03 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 09:57:57.475441: step 43870, loss = 1.19 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 09:58:09.863832: step 43880, loss = 1.34 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-06-17 09:58:22.122302: step 43890, loss = 1.20 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 09:58:34.496368: step 43900, loss = 0.94 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 09:58:51.210425: step 43910, loss = 1.00 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 09:59:03.568120: step 43920, loss = 1.05 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 09:59:15.835306: step 43930, loss = 1.03 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 09:59:28.224445: step 43940, loss = 0.88 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 09:59:40.608144: step 43950, loss = 1.28 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 09:59:53.007261: step 43960, loss = 1.09 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 10:00:05.395054: step 43970, loss = 1.08 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 10:00:17.767325: step 43980, loss = 1.12 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 10:00:30.115866: step 43990, loss = 0.99 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 10:00:42.582310: step 44000, loss = 0.92 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 10:00:59.700838: step 44010, loss = 0.90 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 10:01:11.872447: step 44020, loss = 1.22 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 10:01:24.196312: step 44030, loss = 1.02 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 10:01:36.353735: step 44040, loss = 1.16 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 10:01:48.632720: step 44050, loss = 1.19 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 10:02:00.763681: step 44060, loss = 1.05 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 10:02:13.480023: step 44070, loss = 1.13 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 10:02:25.762483: step 44080, loss = 1.05 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 10:02:38.061488: step 44090, loss = 1.13 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 10:02:50.316470: step 44100, loss = 0.95 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 10:03:07.628089: step 44110, loss = 1.14 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 10:03:19.925312: step 44120, loss = 1.25 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 10:03:32.245347: step 44130, loss = 1.06 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 10:03:44.825918: step 44140, loss = 0.95 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 10:03:57.157454: step 44150, loss = 1.05 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 10:04:09.705586: step 44160, loss = 1.20 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 10:04:22.280014: step 44170, loss = 1.12 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 10:04:34.526962: step 44180, loss = 0.96 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 10:04:47.076037: step 44190, loss = 1.12 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 10:04:59.424257: step 44200, loss = 1.06 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 10:05:15.976887: step 44210, loss = 1.26 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 10:05:28.561206: step 44220, loss = 1.21 (23.9 examples/sec; 1.258 sec/batch)\n",
      "2019-06-17 10:05:41.255332: step 44230, loss = 1.03 (23.0 examples/sec; 1.307 sec/batch)\n",
      "2019-06-17 10:05:53.723501: step 44240, loss = 0.95 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 10:06:06.106749: step 44250, loss = 1.02 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 10:06:18.599205: step 44260, loss = 1.04 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-06-17 10:06:31.028368: step 44270, loss = 1.12 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 10:06:43.348718: step 44280, loss = 1.20 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-17 10:06:55.865579: step 44290, loss = 1.02 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 10:07:08.077979: step 44300, loss = 0.93 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 10:07:25.037960: step 44310, loss = 1.04 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 10:07:37.292818: step 44320, loss = 0.99 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 10:07:49.773078: step 44330, loss = 0.99 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 10:08:01.999604: step 44340, loss = 1.07 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 10:08:14.267695: step 44350, loss = 1.03 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 10:08:26.623550: step 44360, loss = 0.96 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 10:08:38.942230: step 44370, loss = 0.97 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 10:08:51.391383: step 44380, loss = 1.04 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 10:09:03.757560: step 44390, loss = 1.08 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 10:09:16.104173: step 44400, loss = 1.19 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 10:09:32.886219: step 44410, loss = 1.06 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 10:09:45.214119: step 44420, loss = 1.18 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 10:09:57.631449: step 44430, loss = 0.97 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 10:10:10.108543: step 44440, loss = 1.01 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-06-17 10:10:22.458804: step 44450, loss = 1.06 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 10:10:34.937001: step 44460, loss = 1.03 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 10:10:47.066586: step 44470, loss = 1.07 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 10:10:59.343040: step 44480, loss = 0.96 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 10:11:11.576132: step 44490, loss = 1.16 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 10:11:23.927987: step 44500, loss = 0.88 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 10:11:40.611253: step 44510, loss = 1.19 (23.7 examples/sec; 1.266 sec/batch)\n",
      "2019-06-17 10:11:52.958188: step 44520, loss = 1.00 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 10:12:05.198715: step 44530, loss = 1.22 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 10:12:17.653162: step 44540, loss = 1.11 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 10:12:29.826838: step 44550, loss = 0.99 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 10:12:41.984927: step 44560, loss = 0.96 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 10:12:54.115616: step 44570, loss = 1.06 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 10:13:06.364019: step 44580, loss = 1.25 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 10:13:18.983544: step 44590, loss = 0.93 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 10:13:31.265838: step 44600, loss = 1.18 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 10:13:48.107870: step 44610, loss = 1.02 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 10:14:00.467486: step 44620, loss = 1.13 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 10:14:12.721470: step 44630, loss = 1.08 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 10:14:24.869233: step 44640, loss = 1.03 (23.9 examples/sec; 1.258 sec/batch)\n",
      "2019-06-17 10:14:37.334911: step 44650, loss = 1.06 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 10:14:49.650914: step 44660, loss = 1.21 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 10:15:01.868050: step 44670, loss = 0.88 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 10:15:14.034597: step 44680, loss = 1.08 (24.5 examples/sec; 1.226 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:15:26.284459: step 44690, loss = 0.95 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 10:15:38.660141: step 44700, loss = 1.09 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 10:15:55.455624: step 44710, loss = 0.95 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 10:16:07.973968: step 44720, loss = 1.06 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 10:16:20.196389: step 44730, loss = 1.05 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 10:16:32.820920: step 44740, loss = 0.97 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 10:16:45.214764: step 44750, loss = 1.08 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-06-17 10:16:57.621163: step 44760, loss = 0.98 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 10:17:10.077385: step 44770, loss = 1.13 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 10:17:22.551292: step 44780, loss = 1.23 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 10:17:34.828037: step 44790, loss = 1.19 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 10:17:47.364938: step 44800, loss = 1.21 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 10:18:04.347738: step 44810, loss = 1.05 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 10:18:16.739134: step 44820, loss = 1.43 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 10:18:29.211443: step 44830, loss = 1.09 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 10:18:41.500835: step 44840, loss = 1.10 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 10:18:53.794238: step 44850, loss = 1.18 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-17 10:19:06.171946: step 44860, loss = 1.19 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 10:19:18.622898: step 44870, loss = 1.12 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 10:19:30.919622: step 44880, loss = 1.01 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 10:19:43.202444: step 44890, loss = 1.25 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 10:19:55.573591: step 44900, loss = 1.27 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 10:20:12.151422: step 44910, loss = 1.07 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 10:20:24.371563: step 44920, loss = 1.05 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-17 10:20:36.570793: step 44930, loss = 1.24 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 10:20:48.883459: step 44940, loss = 1.25 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 10:21:01.149953: step 44950, loss = 0.91 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 10:21:13.464863: step 44960, loss = 1.04 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 10:21:25.863199: step 44970, loss = 1.14 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 10:21:38.117426: step 44980, loss = 0.99 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 10:21:50.369347: step 44990, loss = 1.19 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 10:22:02.742323: step 45000, loss = 1.10 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 10:22:23.090414: step 45010, loss = 1.07 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 10:22:35.495240: step 45020, loss = 1.16 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 10:22:47.758631: step 45030, loss = 0.97 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 10:23:00.121658: step 45040, loss = 0.98 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 10:23:12.461636: step 45050, loss = 0.95 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 10:23:24.760990: step 45060, loss = 1.23 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 10:23:37.264908: step 45070, loss = 0.88 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-06-17 10:23:49.720061: step 45080, loss = 1.18 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 10:24:02.302420: step 45090, loss = 0.97 (22.4 examples/sec; 1.338 sec/batch)\n",
      "2019-06-17 10:24:14.521360: step 45100, loss = 1.10 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 10:24:31.662322: step 45110, loss = 1.22 (23.3 examples/sec; 1.285 sec/batch)\n",
      "2019-06-17 10:24:43.991933: step 45120, loss = 1.00 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 10:24:56.262940: step 45130, loss = 1.03 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 10:25:08.394379: step 45140, loss = 1.25 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 10:25:20.605473: step 45150, loss = 0.94 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 10:25:32.724751: step 45160, loss = 1.03 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 10:25:45.005566: step 45170, loss = 1.22 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 10:25:57.351023: step 45180, loss = 1.25 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 10:26:09.755641: step 45190, loss = 1.12 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 10:26:22.064632: step 45200, loss = 1.16 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 10:26:39.188268: step 45210, loss = 0.95 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 10:26:51.291281: step 45220, loss = 1.01 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 10:27:03.666852: step 45230, loss = 1.13 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 10:27:16.032692: step 45240, loss = 1.09 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 10:27:28.573054: step 45250, loss = 0.92 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 10:27:41.039613: step 45260, loss = 0.91 (22.5 examples/sec; 1.333 sec/batch)\n",
      "2019-06-17 10:27:53.642075: step 45270, loss = 1.02 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 10:28:06.029449: step 45280, loss = 0.93 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 10:28:18.292463: step 45290, loss = 1.22 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 10:28:30.723165: step 45300, loss = 0.91 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 10:28:47.712216: step 45310, loss = 1.01 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 10:29:00.132996: step 45320, loss = 1.24 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 10:29:12.419400: step 45330, loss = 1.28 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 10:29:24.903823: step 45340, loss = 1.05 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 10:29:37.346598: step 45350, loss = 1.11 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 10:29:49.494260: step 45360, loss = 1.01 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 10:30:01.782161: step 45370, loss = 1.05 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 10:30:14.120733: step 45380, loss = 0.93 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 10:30:26.535212: step 45390, loss = 0.99 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 10:30:38.788906: step 45400, loss = 0.92 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 10:30:55.382886: step 45410, loss = 0.98 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 10:31:07.630169: step 45420, loss = 0.95 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 10:31:20.169952: step 45430, loss = 1.02 (23.5 examples/sec; 1.277 sec/batch)\n",
      "2019-06-17 10:31:32.491695: step 45440, loss = 1.22 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 10:31:44.846208: step 45450, loss = 1.06 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 10:31:57.306345: step 45460, loss = 1.00 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 10:32:09.958582: step 45470, loss = 1.01 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 10:32:22.270918: step 45480, loss = 1.11 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 10:32:34.607664: step 45490, loss = 1.02 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 10:32:46.946633: step 45500, loss = 1.18 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 10:33:04.019000: step 45510, loss = 1.12 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 10:33:16.562145: step 45520, loss = 0.97 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 10:33:29.007554: step 45530, loss = 0.96 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 10:33:41.269605: step 45540, loss = 0.90 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 10:33:53.471843: step 45550, loss = 1.12 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 10:34:05.726807: step 45560, loss = 1.07 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 10:34:18.003857: step 45570, loss = 1.06 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 10:34:30.211759: step 45580, loss = 1.04 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 10:34:42.432901: step 45590, loss = 0.87 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 10:34:54.718822: step 45600, loss = 1.06 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 10:35:11.625670: step 45610, loss = 1.10 (24.4 examples/sec; 1.230 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:35:23.917457: step 45620, loss = 0.97 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 10:35:36.406057: step 45630, loss = 1.08 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 10:35:48.666110: step 45640, loss = 0.92 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 10:36:01.280024: step 45650, loss = 1.13 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 10:36:13.594729: step 45660, loss = 0.94 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 10:36:25.803732: step 45670, loss = 0.91 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 10:36:37.996414: step 45680, loss = 0.99 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 10:36:50.177066: step 45690, loss = 1.14 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 10:37:02.624435: step 45700, loss = 1.09 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 10:37:19.409565: step 45710, loss = 1.13 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 10:37:31.617474: step 45720, loss = 1.11 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 10:37:43.777492: step 45730, loss = 1.04 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 10:37:56.264941: step 45740, loss = 1.09 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 10:38:08.725328: step 45750, loss = 1.19 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 10:38:20.854876: step 45760, loss = 0.96 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 10:38:33.174024: step 45770, loss = 0.96 (22.5 examples/sec; 1.333 sec/batch)\n",
      "2019-06-17 10:38:45.480411: step 45780, loss = 1.01 (23.8 examples/sec; 1.263 sec/batch)\n",
      "2019-06-17 10:38:57.654957: step 45790, loss = 1.23 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 10:39:10.207850: step 45800, loss = 1.04 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 10:39:26.812466: step 45810, loss = 1.22 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 10:39:39.141376: step 45820, loss = 0.91 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-17 10:39:51.534805: step 45830, loss = 0.94 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 10:40:03.903101: step 45840, loss = 1.16 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-06-17 10:40:16.473618: step 45850, loss = 1.11 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 10:40:29.139818: step 45860, loss = 0.98 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 10:40:41.342590: step 45870, loss = 1.20 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 10:40:53.635749: step 45880, loss = 1.00 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 10:41:05.927567: step 45890, loss = 1.01 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 10:41:18.199012: step 45900, loss = 0.91 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 10:41:34.926690: step 45910, loss = 1.06 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 10:41:47.145566: step 45920, loss = 0.99 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 10:41:59.388595: step 45930, loss = 1.05 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 10:42:11.749055: step 45940, loss = 1.19 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 10:42:24.109306: step 45950, loss = 0.97 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 10:42:36.483326: step 45960, loss = 1.00 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 10:42:48.847268: step 45970, loss = 0.97 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 10:43:01.254544: step 45980, loss = 0.86 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 10:43:13.614820: step 45990, loss = 0.98 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 10:43:25.967056: step 46000, loss = 1.07 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 10:43:42.629283: step 46010, loss = 0.87 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 10:43:54.806640: step 46020, loss = 1.12 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 10:44:07.181155: step 46030, loss = 0.97 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 10:44:19.628763: step 46040, loss = 1.25 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 10:44:31.933471: step 46050, loss = 1.09 (23.1 examples/sec; 1.299 sec/batch)\n",
      "2019-06-17 10:44:44.203386: step 46060, loss = 1.24 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 10:44:56.488101: step 46070, loss = 1.02 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 10:45:08.815662: step 46080, loss = 1.01 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 10:45:21.064174: step 46090, loss = 1.03 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-17 10:45:33.436120: step 46100, loss = 1.05 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 10:45:50.077316: step 46110, loss = 1.17 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 10:46:02.309555: step 46120, loss = 1.00 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 10:46:14.680100: step 46130, loss = 1.00 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 10:46:26.950416: step 46140, loss = 1.04 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 10:46:39.061619: step 46150, loss = 0.91 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 10:46:51.352172: step 46160, loss = 1.10 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 10:47:03.577519: step 46170, loss = 1.00 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 10:47:15.869656: step 46180, loss = 1.18 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 10:47:28.212125: step 46190, loss = 1.07 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 10:47:40.670349: step 46200, loss = 1.07 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 10:47:57.221408: step 46210, loss = 1.16 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 10:48:09.566994: step 46220, loss = 1.17 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 10:48:21.876630: step 46230, loss = 1.07 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 10:48:34.207685: step 46240, loss = 0.99 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 10:48:46.602730: step 46250, loss = 1.10 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-06-17 10:48:58.865724: step 46260, loss = 1.22 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 10:49:11.043556: step 46270, loss = 0.91 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 10:49:23.511106: step 46280, loss = 1.12 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 10:49:35.825103: step 46290, loss = 1.15 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 10:49:48.299947: step 46300, loss = 1.32 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-06-17 10:50:04.994000: step 46310, loss = 1.13 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 10:50:17.592959: step 46320, loss = 1.06 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 10:50:29.886362: step 46330, loss = 1.12 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 10:50:42.157575: step 46340, loss = 0.93 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 10:50:54.707800: step 46350, loss = 0.89 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 10:51:07.390673: step 46360, loss = 1.07 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 10:51:19.814223: step 46370, loss = 1.13 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-06-17 10:51:32.206249: step 46380, loss = 1.17 (22.8 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 10:51:44.594087: step 46390, loss = 1.14 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 10:51:56.959628: step 46400, loss = 1.01 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 10:52:13.870662: step 46410, loss = 0.97 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 10:52:26.111309: step 46420, loss = 0.96 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 10:52:38.599283: step 46430, loss = 0.93 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 10:52:51.025590: step 46440, loss = 0.93 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 10:53:03.217204: step 46450, loss = 0.94 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 10:53:15.742654: step 46460, loss = 0.98 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 10:53:27.950536: step 46470, loss = 1.13 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 10:53:40.197838: step 46480, loss = 1.14 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 10:53:52.669312: step 46490, loss = 1.02 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 10:54:04.920298: step 46500, loss = 1.00 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 10:54:21.528758: step 46510, loss = 0.94 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 10:54:33.705850: step 46520, loss = 1.00 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 10:54:46.142078: step 46530, loss = 1.05 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 10:54:58.309593: step 46540, loss = 0.96 (24.6 examples/sec; 1.219 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:55:10.685190: step 46550, loss = 1.11 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 10:55:22.882097: step 46560, loss = 0.95 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 10:55:35.137791: step 46570, loss = 0.95 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 10:55:47.457744: step 46580, loss = 1.14 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 10:55:59.892791: step 46590, loss = 1.09 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 10:56:12.149231: step 46600, loss = 0.84 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 10:56:29.031217: step 46610, loss = 1.00 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 10:56:41.343730: step 46620, loss = 0.95 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 10:56:53.569347: step 46630, loss = 1.04 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 10:57:05.945976: step 46640, loss = 1.05 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 10:57:18.314716: step 46650, loss = 1.14 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 10:57:30.763384: step 46660, loss = 1.01 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 10:57:42.886968: step 46670, loss = 0.96 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 10:57:55.327389: step 46680, loss = 0.90 (23.3 examples/sec; 1.288 sec/batch)\n",
      "2019-06-17 10:58:07.781528: step 46690, loss = 1.06 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 10:58:20.050956: step 46700, loss = 1.24 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 10:58:37.048129: step 46710, loss = 1.13 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 10:58:49.325761: step 46720, loss = 1.10 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 10:59:01.811905: step 46730, loss = 1.16 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 10:59:14.196070: step 46740, loss = 1.08 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 10:59:26.631842: step 46750, loss = 1.17 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 10:59:38.974509: step 46760, loss = 1.05 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 10:59:51.116142: step 46770, loss = 0.96 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 11:00:03.619467: step 46780, loss = 1.28 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-17 11:00:15.889380: step 46790, loss = 1.06 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 11:00:28.446698: step 46800, loss = 1.00 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 11:00:45.171082: step 46810, loss = 0.96 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 11:00:57.540146: step 46820, loss = 1.07 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 11:01:10.039108: step 46830, loss = 0.93 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 11:01:22.342407: step 46840, loss = 1.07 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 11:01:34.667332: step 46850, loss = 0.99 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 11:01:46.861432: step 46860, loss = 1.24 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 11:01:59.064245: step 46870, loss = 1.09 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 11:02:11.493156: step 46880, loss = 1.05 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 11:02:24.066492: step 46890, loss = 1.15 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 11:02:36.420733: step 46900, loss = 1.02 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 11:02:53.191992: step 46910, loss = 0.98 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 11:03:05.499799: step 46920, loss = 0.95 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 11:03:17.787745: step 46930, loss = 0.95 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 11:03:30.085214: step 46940, loss = 1.04 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 11:03:42.509854: step 46950, loss = 1.10 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 11:03:54.977467: step 46960, loss = 1.20 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 11:04:07.198630: step 46970, loss = 0.92 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 11:04:19.584313: step 46980, loss = 1.11 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 11:04:31.899602: step 46990, loss = 1.19 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 11:04:44.309857: step 47000, loss = 1.08 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 11:05:01.128936: step 47010, loss = 1.00 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 11:05:13.418793: step 47020, loss = 0.98 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 11:05:25.575968: step 47030, loss = 1.15 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 11:05:38.098705: step 47040, loss = 1.00 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 11:05:50.865116: step 47050, loss = 1.00 (23.1 examples/sec; 1.300 sec/batch)\n",
      "2019-06-17 11:06:03.006024: step 47060, loss = 1.16 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 11:06:15.336380: step 47070, loss = 1.26 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 11:06:27.667878: step 47080, loss = 1.10 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 11:06:40.222061: step 47090, loss = 1.13 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 11:06:52.740080: step 47100, loss = 0.96 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 11:07:09.482562: step 47110, loss = 1.18 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 11:07:21.763674: step 47120, loss = 0.91 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 11:07:33.979924: step 47130, loss = 1.07 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 11:07:46.604480: step 47140, loss = 1.12 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 11:07:58.909486: step 47150, loss = 1.12 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 11:08:11.145190: step 47160, loss = 1.06 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 11:08:23.351377: step 47170, loss = 1.02 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 11:08:35.580906: step 47180, loss = 1.04 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 11:08:48.130962: step 47190, loss = 1.13 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 11:09:00.721840: step 47200, loss = 1.05 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 11:09:17.383126: step 47210, loss = 1.10 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 11:09:29.783880: step 47220, loss = 1.13 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 11:09:42.043439: step 47230, loss = 1.03 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 11:09:54.466888: step 47240, loss = 1.13 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 11:10:06.858802: step 47250, loss = 0.94 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 11:10:19.101252: step 47260, loss = 0.89 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 11:10:31.726760: step 47270, loss = 1.18 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 11:10:44.010981: step 47280, loss = 1.10 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 11:10:56.227563: step 47290, loss = 0.94 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 11:11:08.499648: step 47300, loss = 1.04 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 11:11:25.178359: step 47310, loss = 1.00 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 11:11:37.377258: step 47320, loss = 1.30 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 11:11:49.691628: step 47330, loss = 1.08 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 11:12:02.084269: step 47340, loss = 0.97 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-17 11:12:14.328731: step 47350, loss = 0.99 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 11:12:26.593095: step 47360, loss = 1.09 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 11:12:38.986687: step 47370, loss = 1.13 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 11:12:51.525803: step 47380, loss = 1.13 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 11:13:04.340449: step 47390, loss = 0.89 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 11:13:16.593986: step 47400, loss = 1.05 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 11:13:33.485531: step 47410, loss = 1.30 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 11:13:45.843356: step 47420, loss = 0.92 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 11:13:58.248667: step 47430, loss = 0.94 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 11:14:10.549245: step 47440, loss = 0.98 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 11:14:22.940238: step 47450, loss = 1.11 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 11:14:35.222911: step 47460, loss = 1.24 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 11:14:47.817598: step 47470, loss = 0.99 (23.0 examples/sec; 1.303 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:15:00.149372: step 47480, loss = 0.92 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 11:15:12.515548: step 47490, loss = 0.93 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 11:15:24.661624: step 47500, loss = 0.92 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 11:15:41.422563: step 47510, loss = 1.13 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 11:15:53.613673: step 47520, loss = 1.26 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 11:16:05.980860: step 47530, loss = 1.17 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-17 11:16:18.334243: step 47540, loss = 1.02 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 11:16:30.674777: step 47550, loss = 1.18 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 11:16:43.066666: step 47560, loss = 1.13 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 11:16:55.631776: step 47570, loss = 1.01 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 11:17:08.143238: step 47580, loss = 1.14 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 11:17:20.498567: step 47590, loss = 1.03 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 11:17:32.773780: step 47600, loss = 1.08 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 11:17:49.429657: step 47610, loss = 0.96 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 11:18:01.688542: step 47620, loss = 1.00 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 11:18:14.056032: step 47630, loss = 1.01 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 11:18:26.474200: step 47640, loss = 1.07 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 11:18:38.806708: step 47650, loss = 0.94 (23.8 examples/sec; 1.263 sec/batch)\n",
      "2019-06-17 11:18:51.482720: step 47660, loss = 1.03 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 11:19:03.619258: step 47670, loss = 1.24 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 11:19:15.986846: step 47680, loss = 1.08 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 11:19:28.556536: step 47690, loss = 1.04 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-17 11:19:40.918670: step 47700, loss = 1.15 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 11:19:57.705029: step 47710, loss = 0.84 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 11:20:10.165274: step 47720, loss = 1.17 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 11:20:22.368807: step 47730, loss = 1.20 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 11:20:34.511422: step 47740, loss = 1.20 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 11:20:46.711384: step 47750, loss = 1.21 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 11:20:59.118724: step 47760, loss = 1.16 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 11:21:11.559426: step 47770, loss = 0.97 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 11:21:23.889599: step 47780, loss = 1.20 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 11:21:36.357355: step 47790, loss = 1.04 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 11:21:48.625062: step 47800, loss = 1.00 (23.1 examples/sec; 1.299 sec/batch)\n",
      "2019-06-17 11:22:05.554463: step 47810, loss = 0.90 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 11:22:17.977161: step 47820, loss = 1.07 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 11:22:30.681947: step 47830, loss = 1.14 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 11:22:43.015027: step 47840, loss = 1.09 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 11:22:55.143733: step 47850, loss = 1.02 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 11:23:07.352286: step 47860, loss = 0.96 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 11:23:19.706217: step 47870, loss = 0.89 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 11:23:32.135807: step 47880, loss = 1.06 (22.6 examples/sec; 1.328 sec/batch)\n",
      "2019-06-17 11:23:44.510336: step 47890, loss = 0.99 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 11:23:56.680362: step 47900, loss = 1.03 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 11:24:13.657217: step 47910, loss = 1.12 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 11:24:26.091655: step 47920, loss = 1.11 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 11:24:38.459044: step 47930, loss = 1.09 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 11:24:50.799902: step 47940, loss = 0.99 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 11:25:03.076872: step 47950, loss = 0.87 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 11:25:15.209491: step 47960, loss = 1.15 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 11:25:27.382619: step 47970, loss = 1.05 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 11:25:39.603934: step 47980, loss = 0.93 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 11:25:52.013363: step 47990, loss = 1.09 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-06-17 11:26:04.470758: step 48000, loss = 1.20 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 11:26:20.971711: step 48010, loss = 1.14 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 11:26:33.495088: step 48020, loss = 1.00 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 11:26:45.730226: step 48030, loss = 1.06 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 11:26:58.084821: step 48040, loss = 1.02 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 11:27:10.497678: step 48050, loss = 1.26 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 11:27:22.820248: step 48060, loss = 0.98 (22.8 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 11:27:35.099692: step 48070, loss = 1.02 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 11:27:47.663648: step 48080, loss = 0.98 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 11:28:00.032692: step 48090, loss = 1.04 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 11:28:12.216296: step 48100, loss = 1.00 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 11:28:28.738037: step 48110, loss = 1.26 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 11:28:41.031271: step 48120, loss = 1.00 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 11:28:53.535409: step 48130, loss = 1.11 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 11:29:05.860750: step 48140, loss = 1.10 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 11:29:18.143793: step 48150, loss = 1.21 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 11:29:30.347075: step 48160, loss = 0.95 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 11:29:42.750082: step 48170, loss = 1.17 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 11:29:54.954032: step 48180, loss = 1.02 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 11:30:07.187542: step 48190, loss = 1.18 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 11:30:19.432229: step 48200, loss = 1.11 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 11:30:36.305108: step 48210, loss = 0.99 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 11:30:48.724525: step 48220, loss = 0.92 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 11:31:01.083778: step 48230, loss = 1.16 (23.1 examples/sec; 1.297 sec/batch)\n",
      "2019-06-17 11:31:13.564044: step 48240, loss = 0.95 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-06-17 11:31:25.866439: step 48250, loss = 1.04 (23.4 examples/sec; 1.281 sec/batch)\n",
      "2019-06-17 11:31:38.182349: step 48260, loss = 1.02 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 11:31:50.552950: step 48270, loss = 0.95 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 11:32:02.809224: step 48280, loss = 1.09 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 11:32:15.122262: step 48290, loss = 1.11 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 11:32:27.468297: step 48300, loss = 1.03 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 11:32:44.173216: step 48310, loss = 0.96 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-06-17 11:32:56.533299: step 48320, loss = 0.86 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 11:33:08.819928: step 48330, loss = 0.97 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 11:33:21.232030: step 48340, loss = 1.46 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 11:33:33.363263: step 48350, loss = 1.00 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 11:33:45.816375: step 48360, loss = 0.96 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 11:33:58.164264: step 48370, loss = 1.35 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 11:34:10.501965: step 48380, loss = 1.00 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 11:34:22.776981: step 48390, loss = 0.97 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 11:34:35.036434: step 48400, loss = 1.11 (24.8 examples/sec; 1.211 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:34:51.991726: step 48410, loss = 1.01 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 11:35:04.371436: step 48420, loss = 1.18 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 11:35:16.627610: step 48430, loss = 1.04 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 11:35:29.320290: step 48440, loss = 1.04 (23.1 examples/sec; 1.300 sec/batch)\n",
      "2019-06-17 11:35:41.499644: step 48450, loss = 0.91 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 11:35:53.778198: step 48460, loss = 1.09 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 11:36:06.027915: step 48470, loss = 1.12 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 11:36:18.275049: step 48480, loss = 1.09 (23.5 examples/sec; 1.278 sec/batch)\n",
      "2019-06-17 11:36:30.813547: step 48490, loss = 1.15 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 11:36:43.140329: step 48500, loss = 1.06 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 11:37:00.460728: step 48510, loss = 1.02 (22.4 examples/sec; 1.337 sec/batch)\n",
      "2019-06-17 11:37:12.742480: step 48520, loss = 1.29 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 11:37:24.988838: step 48530, loss = 1.05 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 11:37:37.183872: step 48540, loss = 1.36 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 11:37:49.647814: step 48550, loss = 0.99 (22.6 examples/sec; 1.330 sec/batch)\n",
      "2019-06-17 11:38:01.834927: step 48560, loss = 0.96 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 11:38:14.212443: step 48570, loss = 1.19 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 11:38:26.448151: step 48580, loss = 0.91 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 11:38:38.930479: step 48590, loss = 1.13 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 11:38:51.430249: step 48600, loss = 1.10 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 11:39:08.261800: step 48610, loss = 1.02 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 11:39:20.633246: step 48620, loss = 1.04 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 11:39:33.029494: step 48630, loss = 0.84 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 11:39:45.234822: step 48640, loss = 1.06 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-06-17 11:39:57.586248: step 48650, loss = 0.90 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 11:40:10.178481: step 48660, loss = 1.09 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 11:40:22.489252: step 48670, loss = 1.20 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 11:40:34.879071: step 48680, loss = 1.39 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 11:40:47.125652: step 48690, loss = 1.00 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 11:40:59.377678: step 48700, loss = 0.93 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 11:41:16.677924: step 48710, loss = 0.98 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 11:41:29.082750: step 48720, loss = 1.24 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 11:41:41.199279: step 48730, loss = 0.91 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 11:41:53.653012: step 48740, loss = 1.00 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 11:42:05.784423: step 48750, loss = 0.96 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 11:42:18.028776: step 48760, loss = 0.94 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 11:42:30.305633: step 48770, loss = 1.16 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 11:42:42.531633: step 48780, loss = 1.08 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 11:42:54.893785: step 48790, loss = 0.90 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 11:43:07.390789: step 48800, loss = 1.18 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 11:43:24.125975: step 48810, loss = 1.03 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 11:43:36.494233: step 48820, loss = 0.90 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 11:43:48.730243: step 48830, loss = 1.31 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 11:44:01.026641: step 48840, loss = 1.06 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-06-17 11:44:13.233270: step 48850, loss = 1.09 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 11:44:25.577991: step 48860, loss = 1.11 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 11:44:38.044304: step 48870, loss = 1.04 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-17 11:44:50.485116: step 48880, loss = 0.93 (22.9 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 11:45:02.682039: step 48890, loss = 1.03 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 11:45:14.921369: step 48900, loss = 0.91 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 11:45:31.839670: step 48910, loss = 0.90 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 11:45:44.155279: step 48920, loss = 1.09 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 11:45:56.698861: step 48930, loss = 0.93 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 11:46:09.072280: step 48940, loss = 1.01 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 11:46:21.320878: step 48950, loss = 0.88 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 11:46:33.486969: step 48960, loss = 1.10 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 11:46:46.024798: step 48970, loss = 1.07 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 11:46:58.296730: step 48980, loss = 0.93 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 11:47:10.405754: step 48990, loss = 0.95 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 11:47:22.763642: step 49000, loss = 1.08 (22.8 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 11:47:39.200796: step 49010, loss = 0.84 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 11:47:51.420915: step 49020, loss = 1.07 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 11:48:03.707673: step 49030, loss = 1.18 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 11:48:16.188677: step 49040, loss = 1.18 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 11:48:28.551078: step 49050, loss = 1.01 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 11:48:40.745562: step 49060, loss = 1.12 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 11:48:52.890621: step 49070, loss = 0.95 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 11:49:05.465134: step 49080, loss = 0.98 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-06-17 11:49:17.630345: step 49090, loss = 1.07 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 11:49:30.163448: step 49100, loss = 0.99 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 11:49:47.073886: step 49110, loss = 1.19 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 11:49:59.283321: step 49120, loss = 1.22 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 11:50:11.396419: step 49130, loss = 1.19 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 11:50:23.849114: step 49140, loss = 1.09 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 11:50:36.156679: step 49150, loss = 0.95 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 11:50:48.548505: step 49160, loss = 1.22 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-06-17 11:51:00.740878: step 49170, loss = 1.02 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 11:51:12.986068: step 49180, loss = 0.99 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 11:51:25.343619: step 49190, loss = 1.04 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 11:51:37.725879: step 49200, loss = 1.13 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 11:51:54.933790: step 49210, loss = 1.14 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 11:52:07.221258: step 49220, loss = 0.98 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 11:52:19.610299: step 49230, loss = 0.98 (22.5 examples/sec; 1.336 sec/batch)\n",
      "2019-06-17 11:52:31.865235: step 49240, loss = 0.91 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 11:52:44.217703: step 49250, loss = 1.19 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 11:52:56.566409: step 49260, loss = 0.96 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 11:53:09.208166: step 49270, loss = 1.14 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 11:53:21.485243: step 49280, loss = 1.02 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 11:53:33.843123: step 49290, loss = 1.26 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 11:53:46.168701: step 49300, loss = 1.21 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 11:54:03.219431: step 49310, loss = 1.14 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 11:54:15.732481: step 49320, loss = 1.11 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-06-17 11:54:28.189847: step 49330, loss = 1.14 (24.0 examples/sec; 1.251 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:54:40.867577: step 49340, loss = 0.88 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 11:54:53.317957: step 49350, loss = 1.14 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 11:55:05.631977: step 49360, loss = 1.20 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 11:55:18.159260: step 49370, loss = 0.96 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 11:55:30.309192: step 49380, loss = 1.05 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 11:55:42.756334: step 49390, loss = 0.88 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 11:55:54.926927: step 49400, loss = 0.90 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 11:56:11.463677: step 49410, loss = 1.14 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 11:56:23.873059: step 49420, loss = 1.02 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 11:56:36.368932: step 49430, loss = 0.92 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-06-17 11:56:48.950808: step 49440, loss = 1.11 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 11:57:01.226406: step 49450, loss = 1.05 (23.1 examples/sec; 1.300 sec/batch)\n",
      "2019-06-17 11:57:13.632600: step 49460, loss = 1.06 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 11:57:25.844060: step 49470, loss = 0.87 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 11:57:38.530649: step 49480, loss = 0.95 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 11:57:50.870146: step 49490, loss = 1.02 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 11:58:03.146942: step 49500, loss = 0.96 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 11:58:20.059967: step 49510, loss = 0.91 (23.9 examples/sec; 1.258 sec/batch)\n",
      "2019-06-17 11:58:32.159408: step 49520, loss = 1.15 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 11:58:44.508354: step 49530, loss = 1.01 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-17 11:58:56.709079: step 49540, loss = 0.86 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 11:59:08.768691: step 49550, loss = 1.21 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 11:59:20.934389: step 49560, loss = 1.16 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 11:59:33.094681: step 49570, loss = 1.17 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 11:59:45.320246: step 49580, loss = 0.93 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 11:59:57.843286: step 49590, loss = 1.04 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 12:00:10.214343: step 49600, loss = 1.01 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 12:00:26.966788: step 49610, loss = 1.20 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 12:00:39.322108: step 49620, loss = 0.97 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 12:00:51.563212: step 49630, loss = 1.01 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 12:01:03.907403: step 49640, loss = 0.98 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 12:01:16.018010: step 49650, loss = 1.31 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 12:01:28.234363: step 49660, loss = 1.05 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 12:01:40.562641: step 49670, loss = 1.12 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 12:01:52.721428: step 49680, loss = 0.97 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 12:02:04.844269: step 49690, loss = 1.14 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 12:02:17.029585: step 49700, loss = 1.10 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 12:02:34.020796: step 49710, loss = 1.07 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 12:02:46.413788: step 49720, loss = 1.23 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 12:02:58.996042: step 49730, loss = 1.10 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 12:03:11.202539: step 49740, loss = 0.97 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 12:03:23.613639: step 49750, loss = 1.13 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 12:03:35.958634: step 49760, loss = 0.96 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 12:03:48.392944: step 49770, loss = 1.09 (23.2 examples/sec; 1.293 sec/batch)\n",
      "2019-06-17 12:04:00.711537: step 49780, loss = 1.18 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 12:04:12.963786: step 49790, loss = 0.98 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-06-17 12:04:25.432828: step 49800, loss = 0.89 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 12:04:42.367343: step 49810, loss = 1.05 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 12:04:54.940920: step 49820, loss = 0.96 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-17 12:05:07.353670: step 49830, loss = 1.07 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 12:05:19.641746: step 49840, loss = 0.97 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 12:05:31.812750: step 49850, loss = 1.06 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 12:05:44.598093: step 49860, loss = 0.98 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-06-17 12:05:56.986138: step 49870, loss = 1.05 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 12:06:09.224020: step 49880, loss = 0.94 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-06-17 12:06:21.356805: step 49890, loss = 1.13 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 12:06:33.746028: step 49900, loss = 1.15 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 12:06:50.911898: step 49910, loss = 0.96 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 12:07:03.238226: step 49920, loss = 1.27 (23.5 examples/sec; 1.278 sec/batch)\n",
      "2019-06-17 12:07:15.668651: step 49930, loss = 1.16 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 12:07:28.027543: step 49940, loss = 1.26 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 12:07:40.400506: step 49950, loss = 1.11 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 12:07:52.493034: step 49960, loss = 0.96 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 12:08:04.896940: step 49970, loss = 1.02 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 12:08:17.163176: step 49980, loss = 0.99 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-17 12:08:29.442303: step 49990, loss = 1.02 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 12:08:41.935885: step 50000, loss = 0.94 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 12:09:02.677060: step 50010, loss = 0.81 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 12:09:15.140679: step 50020, loss = 1.35 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 12:09:27.472522: step 50030, loss = 1.02 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 12:09:39.871144: step 50040, loss = 0.93 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 12:09:52.252479: step 50050, loss = 1.10 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 12:10:04.509050: step 50060, loss = 1.12 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 12:10:16.804139: step 50070, loss = 0.90 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 12:10:29.050836: step 50080, loss = 1.05 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 12:10:41.178241: step 50090, loss = 1.04 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 12:10:53.427624: step 50100, loss = 1.12 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-17 12:11:10.195611: step 50110, loss = 1.15 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 12:11:22.564535: step 50120, loss = 0.95 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 12:11:34.782258: step 50130, loss = 1.04 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 12:11:47.254430: step 50140, loss = 0.97 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 12:11:59.695930: step 50150, loss = 0.96 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 12:12:11.935596: step 50160, loss = 1.18 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 12:12:24.183683: step 50170, loss = 1.00 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 12:12:36.373967: step 50180, loss = 1.10 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 12:12:48.631593: step 50190, loss = 0.91 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 12:13:01.188109: step 50200, loss = 1.01 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 12:13:18.181780: step 50210, loss = 0.96 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 12:13:30.319847: step 50220, loss = 0.93 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 12:13:42.602893: step 50230, loss = 1.35 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 12:13:55.058528: step 50240, loss = 1.05 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 12:14:07.402966: step 50250, loss = 1.05 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 12:14:19.596136: step 50260, loss = 0.91 (24.8 examples/sec; 1.209 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 12:14:31.748862: step 50270, loss = 0.94 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 12:14:43.902988: step 50280, loss = 1.03 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 12:14:56.280568: step 50290, loss = 1.14 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 12:15:08.610133: step 50300, loss = 0.91 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 12:15:25.433215: step 50310, loss = 1.11 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 12:15:37.623357: step 50320, loss = 1.10 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 12:15:49.820807: step 50330, loss = 1.08 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 12:16:02.109546: step 50340, loss = 1.05 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 12:16:14.362619: step 50350, loss = 1.08 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 12:16:26.949520: step 50360, loss = 0.95 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 12:16:39.430312: step 50370, loss = 1.03 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 12:16:51.717485: step 50380, loss = 1.12 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 12:17:04.016486: step 50390, loss = 1.35 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 12:17:16.339773: step 50400, loss = 1.05 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 12:17:33.320912: step 50410, loss = 1.10 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 12:17:45.643616: step 50420, loss = 1.32 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 12:17:58.039339: step 50430, loss = 0.89 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 12:18:10.278005: step 50440, loss = 0.94 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 12:18:22.446104: step 50450, loss = 1.03 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 12:18:34.663600: step 50460, loss = 0.97 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-06-17 12:18:46.894501: step 50470, loss = 1.00 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 12:18:59.168929: step 50480, loss = 0.91 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 12:19:11.596011: step 50490, loss = 0.99 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 12:19:23.971098: step 50500, loss = 1.11 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 12:19:40.810001: step 50510, loss = 0.95 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 12:19:53.265649: step 50520, loss = 0.92 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 12:20:05.611531: step 50530, loss = 0.94 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 12:20:18.204728: step 50540, loss = 0.89 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 12:20:30.605120: step 50550, loss = 0.98 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 12:20:42.982778: step 50560, loss = 0.88 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 12:20:55.513334: step 50570, loss = 1.27 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 12:21:07.683580: step 50580, loss = 0.92 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 12:21:20.049343: step 50590, loss = 1.13 (22.9 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 12:21:32.328834: step 50600, loss = 1.07 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 12:21:49.375602: step 50610, loss = 1.05 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-17 12:22:01.539439: step 50620, loss = 0.93 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 12:22:14.163369: step 50630, loss = 1.21 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 12:22:26.675516: step 50640, loss = 1.18 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 12:22:38.948805: step 50650, loss = 0.91 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 12:22:51.249072: step 50660, loss = 1.08 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 12:23:03.685718: step 50670, loss = 0.95 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 12:23:16.047810: step 50680, loss = 1.26 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 12:23:28.268290: step 50690, loss = 0.99 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 12:23:40.472311: step 50700, loss = 1.03 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 12:23:57.209002: step 50710, loss = 0.90 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 12:24:09.562615: step 50720, loss = 0.85 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 12:24:22.062841: step 50730, loss = 1.09 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 12:24:34.516989: step 50740, loss = 1.13 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 12:24:46.894716: step 50750, loss = 0.85 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 12:24:59.226793: step 50760, loss = 1.15 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 12:25:11.792477: step 50770, loss = 0.98 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 12:25:24.243192: step 50780, loss = 1.19 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 12:25:36.569828: step 50790, loss = 1.09 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 12:25:49.032535: step 50800, loss = 1.25 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-06-17 12:26:06.542995: step 50810, loss = 1.03 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 12:26:18.990070: step 50820, loss = 0.99 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 12:26:31.382308: step 50830, loss = 0.92 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 12:26:43.715493: step 50840, loss = 0.90 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 12:26:55.830093: step 50850, loss = 0.95 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 12:27:08.119606: step 50860, loss = 1.01 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 12:27:20.406028: step 50870, loss = 1.04 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 12:27:33.073998: step 50880, loss = 1.05 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 12:27:45.463221: step 50890, loss = 1.20 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 12:27:57.875468: step 50900, loss = 1.29 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 12:28:14.761307: step 50910, loss = 1.02 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 12:28:27.009096: step 50920, loss = 1.12 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 12:28:39.481708: step 50930, loss = 1.06 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 12:28:51.756987: step 50940, loss = 1.06 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 12:29:04.223854: step 50950, loss = 1.03 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 12:29:16.713102: step 50960, loss = 1.08 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 12:29:29.069419: step 50970, loss = 1.07 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 12:29:41.563150: step 50980, loss = 0.88 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 12:29:53.847518: step 50990, loss = 1.16 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-06-17 12:30:06.314481: step 51000, loss = 1.06 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 12:30:23.283985: step 51010, loss = 0.89 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 12:30:35.487400: step 51020, loss = 1.16 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 12:30:47.800012: step 51030, loss = 0.94 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 12:31:00.190002: step 51040, loss = 1.32 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 12:31:12.519687: step 51050, loss = 0.89 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 12:31:24.910453: step 51060, loss = 0.97 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 12:31:37.488031: step 51070, loss = 1.15 (22.6 examples/sec; 1.328 sec/batch)\n",
      "2019-06-17 12:31:49.746839: step 51080, loss = 1.23 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 12:32:02.159515: step 51090, loss = 0.85 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 12:32:14.556016: step 51100, loss = 1.06 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 12:32:31.260622: step 51110, loss = 1.05 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 12:32:43.555099: step 51120, loss = 1.31 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 12:32:56.079821: step 51130, loss = 1.10 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 12:33:08.547677: step 51140, loss = 1.05 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 12:33:21.066676: step 51150, loss = 1.27 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 12:33:33.273328: step 51160, loss = 1.05 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 12:33:45.621373: step 51170, loss = 1.14 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 12:33:58.208650: step 51180, loss = 1.10 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 12:34:10.420974: step 51190, loss = 1.04 (24.8 examples/sec; 1.212 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 12:34:22.823294: step 51200, loss = 1.03 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 12:34:39.966188: step 51210, loss = 1.07 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 12:34:52.559376: step 51220, loss = 0.90 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 12:35:04.822035: step 51230, loss = 0.95 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 12:35:17.354528: step 51240, loss = 1.22 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 12:35:29.775185: step 51250, loss = 1.04 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 12:35:42.074573: step 51260, loss = 1.05 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 12:35:54.306319: step 51270, loss = 0.87 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 12:36:06.829178: step 51280, loss = 1.31 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 12:36:19.139915: step 51290, loss = 0.99 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 12:36:31.353815: step 51300, loss = 0.95 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 12:36:48.133710: step 51310, loss = 1.11 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 12:37:00.494802: step 51320, loss = 1.01 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 12:37:12.829376: step 51330, loss = 0.88 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 12:37:25.084273: step 51340, loss = 1.18 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 12:37:37.536019: step 51350, loss = 1.14 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 12:37:49.880018: step 51360, loss = 1.10 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 12:38:02.269341: step 51370, loss = 1.30 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 12:38:14.707001: step 51380, loss = 1.08 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 12:38:26.975224: step 51390, loss = 1.12 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 12:38:39.444549: step 51400, loss = 0.99 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 12:38:56.407922: step 51410, loss = 1.03 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 12:39:08.856774: step 51420, loss = 1.33 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 12:39:21.225107: step 51430, loss = 1.02 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 12:39:33.513810: step 51440, loss = 1.02 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 12:39:45.688019: step 51450, loss = 1.17 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 12:39:57.923069: step 51460, loss = 1.13 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 12:40:10.350540: step 51470, loss = 0.97 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 12:40:22.738017: step 51480, loss = 1.13 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 12:40:34.962132: step 51490, loss = 1.14 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 12:40:47.255297: step 51500, loss = 1.00 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 12:41:04.579779: step 51510, loss = 1.09 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 12:41:17.107051: step 51520, loss = 0.93 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 12:41:29.328722: step 51530, loss = 0.91 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 12:41:41.819227: step 51540, loss = 1.09 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-17 12:41:54.165592: step 51550, loss = 0.94 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 12:42:06.554837: step 51560, loss = 0.95 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 12:42:18.694487: step 51570, loss = 0.98 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 12:42:30.888015: step 51580, loss = 0.86 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 12:42:43.204845: step 51590, loss = 1.22 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 12:42:55.592968: step 51600, loss = 0.93 (22.5 examples/sec; 1.331 sec/batch)\n",
      "2019-06-17 12:43:12.558354: step 51610, loss = 0.86 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 12:43:24.698426: step 51620, loss = 0.99 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 12:43:37.091567: step 51630, loss = 1.15 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 12:43:49.405456: step 51640, loss = 1.14 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 12:44:01.775781: step 51650, loss = 1.21 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 12:44:14.053648: step 51660, loss = 1.05 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 12:44:26.302667: step 51670, loss = 1.32 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 12:44:38.555792: step 51680, loss = 1.10 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 12:44:51.016204: step 51690, loss = 1.25 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 12:45:03.526106: step 51700, loss = 1.23 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 12:45:20.059213: step 51710, loss = 0.98 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 12:45:32.512848: step 51720, loss = 1.08 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 12:45:44.882746: step 51730, loss = 1.09 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 12:45:57.160104: step 51740, loss = 0.96 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 12:46:09.538312: step 51750, loss = 1.10 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 12:46:21.747178: step 51760, loss = 1.06 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 12:46:34.132032: step 51770, loss = 1.02 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 12:46:46.416427: step 51780, loss = 0.95 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 12:46:58.763664: step 51790, loss = 1.03 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-17 12:47:10.933142: step 51800, loss = 1.22 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 12:47:27.683571: step 51810, loss = 1.18 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 12:47:40.123436: step 51820, loss = 1.21 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 12:47:52.395903: step 51830, loss = 0.98 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 12:48:04.752459: step 51840, loss = 1.17 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 12:48:17.260552: step 51850, loss = 1.01 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 12:48:29.431039: step 51860, loss = 1.09 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 12:48:41.905903: step 51870, loss = 0.98 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 12:48:54.299638: step 51880, loss = 1.04 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 12:49:06.479174: step 51890, loss = 1.16 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 12:49:18.660625: step 51900, loss = 1.09 (23.4 examples/sec; 1.280 sec/batch)\n",
      "2019-06-17 12:49:35.928979: step 51910, loss = 1.05 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 12:49:48.170757: step 51920, loss = 1.26 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 12:50:00.477592: step 51930, loss = 1.21 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 12:50:13.053163: step 51940, loss = 0.97 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 12:50:25.295265: step 51950, loss = 0.88 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 12:50:37.577912: step 51960, loss = 1.07 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 12:50:49.927500: step 51970, loss = 1.05 (22.9 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 12:51:02.565541: step 51980, loss = 1.19 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-06-17 12:51:14.828764: step 51990, loss = 0.91 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 12:51:27.027244: step 52000, loss = 0.90 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 12:51:44.214756: step 52010, loss = 1.00 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 12:51:56.656158: step 52020, loss = 0.99 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 12:52:09.080695: step 52030, loss = 1.05 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-06-17 12:52:21.484544: step 52040, loss = 1.08 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 12:52:33.885137: step 52050, loss = 0.90 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 12:52:46.142334: step 52060, loss = 1.08 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 12:52:58.600394: step 52070, loss = 1.04 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 12:53:11.271385: step 52080, loss = 1.19 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 12:53:23.462630: step 52090, loss = 1.09 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 12:53:35.950718: step 52100, loss = 0.98 (22.8 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 12:53:53.107926: step 52110, loss = 0.98 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 12:54:05.439279: step 52120, loss = 1.17 (22.7 examples/sec; 1.322 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 12:54:17.593292: step 52130, loss = 0.86 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 12:54:29.851261: step 52140, loss = 0.96 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 12:54:42.249572: step 52150, loss = 1.12 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 12:54:54.734372: step 52160, loss = 0.99 (22.6 examples/sec; 1.330 sec/batch)\n",
      "2019-06-17 12:55:06.991108: step 52170, loss = 0.93 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 12:55:19.495258: step 52180, loss = 1.05 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 12:55:31.618731: step 52190, loss = 1.09 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 12:55:43.812161: step 52200, loss = 1.19 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 12:56:00.591286: step 52210, loss = 1.21 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 12:56:13.172080: step 52220, loss = 1.05 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 12:56:25.569921: step 52230, loss = 0.94 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 12:56:37.807302: step 52240, loss = 1.04 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 12:56:50.138033: step 52250, loss = 1.05 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 12:57:02.473120: step 52260, loss = 1.14 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 12:57:14.755708: step 52270, loss = 1.11 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 12:57:27.122481: step 52280, loss = 1.04 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-17 12:57:39.692701: step 52290, loss = 0.92 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 12:57:52.110735: step 52300, loss = 1.03 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-06-17 12:58:09.124665: step 52310, loss = 0.91 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 12:58:21.323641: step 52320, loss = 1.25 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 12:58:33.636685: step 52330, loss = 0.93 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 12:58:45.922291: step 52340, loss = 1.17 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 12:58:58.204275: step 52350, loss = 1.09 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 12:59:10.493739: step 52360, loss = 1.08 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 12:59:22.932445: step 52370, loss = 1.01 (22.4 examples/sec; 1.341 sec/batch)\n",
      "2019-06-17 12:59:35.204523: step 52380, loss = 0.94 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 12:59:47.375509: step 52390, loss = 1.28 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 12:59:59.865310: step 52400, loss = 0.88 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 13:00:16.787090: step 52410, loss = 1.04 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 13:00:29.360667: step 52420, loss = 1.05 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 13:00:41.892504: step 52430, loss = 1.13 (22.6 examples/sec; 1.328 sec/batch)\n",
      "2019-06-17 13:00:54.211944: step 52440, loss = 1.01 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 13:01:06.566662: step 52450, loss = 1.12 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 13:01:19.002736: step 52460, loss = 1.08 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 13:01:31.531883: step 52470, loss = 0.87 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-06-17 13:01:44.004334: step 52480, loss = 0.94 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 13:01:56.420922: step 52490, loss = 1.13 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 13:02:08.843091: step 52500, loss = 0.92 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 13:02:25.542768: step 52510, loss = 1.32 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 13:02:37.971793: step 52520, loss = 1.07 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 13:02:50.271755: step 52530, loss = 1.14 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 13:03:02.473488: step 52540, loss = 1.03 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 13:03:14.738884: step 52550, loss = 1.04 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 13:03:27.125903: step 52560, loss = 1.09 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 13:03:39.536341: step 52570, loss = 1.11 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 13:03:51.778519: step 52580, loss = 0.89 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 13:04:04.127364: step 52590, loss = 0.89 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 13:04:16.408438: step 52600, loss = 1.04 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 13:04:33.308553: step 52610, loss = 0.95 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 13:04:45.544298: step 52620, loss = 1.10 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 13:04:57.952213: step 52630, loss = 0.90 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 13:05:10.333894: step 52640, loss = 0.89 (22.5 examples/sec; 1.333 sec/batch)\n",
      "2019-06-17 13:05:22.598066: step 52650, loss = 1.21 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-06-17 13:05:35.072517: step 52660, loss = 1.07 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 13:05:47.734728: step 52670, loss = 0.94 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 13:06:00.081552: step 52680, loss = 1.07 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 13:06:12.727572: step 52690, loss = 1.29 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-17 13:06:25.354820: step 52700, loss = 1.07 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 13:06:42.069795: step 52710, loss = 1.10 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 13:06:54.357119: step 52720, loss = 0.96 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 13:07:06.481832: step 52730, loss = 1.33 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 13:07:18.907770: step 52740, loss = 1.14 (22.9 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 13:07:31.549357: step 52750, loss = 1.35 (23.2 examples/sec; 1.292 sec/batch)\n",
      "2019-06-17 13:07:43.800277: step 52760, loss = 0.95 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 13:07:56.185249: step 52770, loss = 1.05 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 13:08:08.664690: step 52780, loss = 1.00 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 13:08:20.986202: step 52790, loss = 1.08 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 13:08:33.461790: step 52800, loss = 1.04 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 13:08:50.434034: step 52810, loss = 1.02 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 13:09:02.752022: step 52820, loss = 0.96 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 13:09:15.014220: step 52830, loss = 1.04 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-17 13:09:27.268079: step 52840, loss = 1.01 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 13:09:39.525323: step 52850, loss = 0.95 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 13:09:51.934056: step 52860, loss = 1.06 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-06-17 13:10:04.359142: step 52870, loss = 1.06 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 13:10:16.781956: step 52880, loss = 1.08 (23.5 examples/sec; 1.277 sec/batch)\n",
      "2019-06-17 13:10:29.254990: step 52890, loss = 1.32 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 13:10:41.482915: step 52900, loss = 0.94 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 13:10:58.295631: step 52910, loss = 1.14 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 13:11:10.594539: step 52920, loss = 1.04 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 13:11:22.822015: step 52930, loss = 1.12 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 13:11:35.085621: step 52940, loss = 1.03 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 13:11:47.476465: step 52950, loss = 0.95 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 13:11:59.868337: step 52960, loss = 1.11 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 13:12:12.238128: step 52970, loss = 0.86 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 13:12:24.465631: step 52980, loss = 1.02 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 13:12:37.036633: step 52990, loss = 0.88 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 13:12:49.312261: step 53000, loss = 1.01 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 13:13:06.509278: step 53010, loss = 0.91 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 13:13:18.727861: step 53020, loss = 1.00 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 13:13:31.005430: step 53030, loss = 1.00 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 13:13:43.473492: step 53040, loss = 1.03 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 13:13:56.014753: step 53050, loss = 1.13 (24.5 examples/sec; 1.225 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 13:14:08.249435: step 53060, loss = 1.09 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 13:14:20.441606: step 53070, loss = 1.00 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-06-17 13:14:32.879147: step 53080, loss = 1.04 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-06-17 13:14:45.139011: step 53090, loss = 0.92 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 13:14:57.396873: step 53100, loss = 1.00 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 13:15:14.980911: step 53110, loss = 1.21 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 13:15:27.246385: step 53120, loss = 0.94 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 13:15:39.605203: step 53130, loss = 0.89 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 13:15:51.874489: step 53140, loss = 0.97 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 13:16:04.050550: step 53150, loss = 0.98 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 13:16:16.380674: step 53160, loss = 1.05 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 13:16:28.706264: step 53170, loss = 0.98 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 13:16:40.866370: step 53180, loss = 0.96 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 13:16:53.163893: step 53190, loss = 1.11 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 13:17:05.613129: step 53200, loss = 1.10 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 13:17:22.588633: step 53210, loss = 1.15 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 13:17:35.146501: step 53220, loss = 0.99 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 13:17:47.497842: step 53230, loss = 1.01 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 13:17:59.854191: step 53240, loss = 1.04 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 13:18:12.273959: step 53250, loss = 1.12 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 13:18:24.750462: step 53260, loss = 0.92 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 13:18:36.944749: step 53270, loss = 1.16 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 13:18:49.320061: step 53280, loss = 1.05 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 13:19:01.614790: step 53290, loss = 0.91 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 13:19:14.011214: step 53300, loss = 0.92 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-06-17 13:19:31.032022: step 53310, loss = 0.92 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 13:19:43.667990: step 53320, loss = 1.01 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 13:19:56.317070: step 53330, loss = 1.00 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 13:20:08.718397: step 53340, loss = 1.05 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-17 13:20:20.950809: step 53350, loss = 1.08 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 13:20:33.422420: step 53360, loss = 1.20 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-06-17 13:20:45.655132: step 53370, loss = 1.13 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 13:20:58.151107: step 53380, loss = 1.03 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 13:21:10.628499: step 53390, loss = 0.98 (22.5 examples/sec; 1.333 sec/batch)\n",
      "2019-06-17 13:21:22.885927: step 53400, loss = 1.15 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 13:21:39.517103: step 53410, loss = 0.99 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 13:21:51.891530: step 53420, loss = 1.15 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 13:22:04.102877: step 53430, loss = 0.89 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 13:22:16.399350: step 53440, loss = 0.94 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 13:22:28.860912: step 53450, loss = 1.15 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 13:22:41.263473: step 53460, loss = 1.08 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 13:22:53.536385: step 53470, loss = 1.03 (22.3 examples/sec; 1.344 sec/batch)\n",
      "2019-06-17 13:23:05.738547: step 53480, loss = 1.07 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 13:23:18.093219: step 53490, loss = 0.98 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 13:23:30.462151: step 53500, loss = 1.06 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 13:23:47.475185: step 53510, loss = 1.07 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 13:23:59.755434: step 53520, loss = 1.02 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 13:24:12.171141: step 53530, loss = 1.06 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 13:24:24.718844: step 53540, loss = 0.99 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 13:24:37.318480: step 53550, loss = 0.97 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 13:24:49.610723: step 53560, loss = 1.09 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 13:25:01.954602: step 53570, loss = 1.04 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 13:25:14.651049: step 53580, loss = 0.98 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 13:25:26.921241: step 53590, loss = 0.90 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 13:25:39.192448: step 53600, loss = 1.05 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 13:25:56.179199: step 53610, loss = 0.96 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 13:26:08.722340: step 53620, loss = 1.07 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 13:26:20.859540: step 53630, loss = 0.96 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 13:26:33.247014: step 53640, loss = 1.07 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 13:26:45.635286: step 53650, loss = 1.11 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 13:26:58.096683: step 53660, loss = 1.25 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 13:27:10.288897: step 53670, loss = 1.01 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 13:27:22.501949: step 53680, loss = 1.07 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 13:27:34.813779: step 53690, loss = 1.33 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 13:27:47.061192: step 53700, loss = 1.03 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 13:28:04.221151: step 53710, loss = 0.97 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 13:28:16.639637: step 53720, loss = 1.26 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 13:28:28.989578: step 53730, loss = 0.94 (22.4 examples/sec; 1.337 sec/batch)\n",
      "2019-06-17 13:28:41.091082: step 53740, loss = 1.13 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 13:28:53.212672: step 53750, loss = 0.91 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 13:29:05.551579: step 53760, loss = 1.10 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 13:29:18.019634: step 53770, loss = 0.90 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 13:29:30.255715: step 53780, loss = 1.17 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 13:29:42.374341: step 53790, loss = 1.07 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 13:29:54.834408: step 53800, loss = 1.22 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 13:30:11.684424: step 53810, loss = 1.03 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 13:30:23.883500: step 53820, loss = 1.04 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 13:30:36.342258: step 53830, loss = 1.02 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 13:30:48.557728: step 53840, loss = 1.09 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 13:31:00.807837: step 53850, loss = 1.04 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 13:31:13.410784: step 53860, loss = 1.04 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 13:31:25.515762: step 53870, loss = 0.94 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 13:31:37.827690: step 53880, loss = 0.89 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 13:31:49.971716: step 53890, loss = 1.12 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 13:32:02.121101: step 53900, loss = 0.92 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 13:32:19.102785: step 53910, loss = 0.92 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 13:32:31.346163: step 53920, loss = 0.95 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 13:32:43.807477: step 53930, loss = 0.98 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 13:32:56.278837: step 53940, loss = 0.96 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 13:33:08.582106: step 53950, loss = 1.01 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 13:33:20.852522: step 53960, loss = 0.99 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 13:33:33.205321: step 53970, loss = 1.00 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 13:33:45.366103: step 53980, loss = 1.05 (24.5 examples/sec; 1.224 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 13:33:57.888206: step 53990, loss = 1.32 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 13:34:10.238458: step 54000, loss = 0.96 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 13:34:27.513189: step 54010, loss = 1.10 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 13:34:39.917101: step 54020, loss = 1.03 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-06-17 13:34:52.577111: step 54030, loss = 0.99 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 13:35:04.878352: step 54040, loss = 1.12 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 13:35:17.042403: step 54050, loss = 1.03 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 13:35:29.610492: step 54060, loss = 1.06 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-17 13:35:42.091721: step 54070, loss = 1.12 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 13:35:54.251853: step 54080, loss = 0.94 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 13:36:06.883018: step 54090, loss = 1.12 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 13:36:19.222992: step 54100, loss = 1.11 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 13:36:36.360366: step 54110, loss = 0.97 (22.5 examples/sec; 1.333 sec/batch)\n",
      "2019-06-17 13:36:48.842390: step 54120, loss = 1.19 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 13:37:00.997520: step 54130, loss = 1.00 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 13:37:13.232381: step 54140, loss = 1.26 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 13:37:25.717624: step 54150, loss = 1.18 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 13:37:37.981348: step 54160, loss = 0.97 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 13:37:50.276225: step 54170, loss = 1.07 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 13:38:02.570839: step 54180, loss = 0.98 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 13:38:14.827197: step 54190, loss = 0.98 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 13:38:27.117433: step 54200, loss = 1.14 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 13:38:43.851543: step 54210, loss = 0.92 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 13:38:56.376019: step 54220, loss = 0.97 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 13:39:08.907329: step 54230, loss = 1.10 (23.5 examples/sec; 1.278 sec/batch)\n",
      "2019-06-17 13:39:21.318247: step 54240, loss = 1.11 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 13:39:33.509530: step 54250, loss = 1.11 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 13:39:46.055714: step 54260, loss = 1.13 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 13:39:58.319743: step 54270, loss = 0.97 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 13:40:10.627279: step 54280, loss = 1.09 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 13:40:23.037391: step 54290, loss = 1.10 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 13:40:35.301681: step 54300, loss = 1.08 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 13:40:52.010441: step 54310, loss = 1.04 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 13:41:04.261158: step 54320, loss = 1.20 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 13:41:16.576213: step 54330, loss = 0.90 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 13:41:29.204354: step 54340, loss = 1.19 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-06-17 13:41:41.563485: step 54350, loss = 0.92 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 13:41:53.946757: step 54360, loss = 1.10 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 13:42:06.281224: step 54370, loss = 0.99 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 13:42:18.507795: step 54380, loss = 1.16 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 13:42:30.863679: step 54390, loss = 1.11 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 13:42:43.313447: step 54400, loss = 1.19 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 13:43:00.234793: step 54410, loss = 1.06 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 13:43:12.615992: step 54420, loss = 1.14 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 13:43:25.017259: step 54430, loss = 0.96 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 13:43:37.492746: step 54440, loss = 1.41 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 13:43:49.897682: step 54450, loss = 1.07 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 13:44:02.090502: step 54460, loss = 0.96 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 13:44:14.530312: step 54470, loss = 1.36 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 13:44:26.768880: step 54480, loss = 1.23 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 13:44:39.046776: step 54490, loss = 0.99 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 13:44:51.304534: step 54500, loss = 1.40 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 13:45:08.142516: step 54510, loss = 0.98 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 13:45:20.421572: step 54520, loss = 0.99 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 13:45:32.611515: step 54530, loss = 1.17 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 13:45:44.872699: step 54540, loss = 0.95 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 13:45:57.123261: step 54550, loss = 1.09 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 13:46:09.556469: step 54560, loss = 1.33 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 13:46:21.788886: step 54570, loss = 0.88 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 13:46:34.146609: step 54580, loss = 1.16 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 13:46:46.485706: step 54590, loss = 1.16 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 13:46:58.869189: step 54600, loss = 1.01 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 13:47:15.716708: step 54610, loss = 1.00 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 13:47:28.177692: step 54620, loss = 1.02 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 13:47:40.562157: step 54630, loss = 1.03 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 13:47:52.864321: step 54640, loss = 1.00 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 13:48:05.119978: step 54650, loss = 1.14 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 13:48:17.567384: step 54660, loss = 1.06 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 13:48:29.798034: step 54670, loss = 1.23 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 13:48:42.156653: step 54680, loss = 1.04 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 13:48:54.520786: step 54690, loss = 0.98 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 13:49:06.864561: step 54700, loss = 1.02 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-17 13:49:23.978412: step 54710, loss = 1.09 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 13:49:36.399133: step 54720, loss = 1.30 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 13:49:48.945959: step 54730, loss = 1.10 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 13:50:01.280144: step 54740, loss = 1.03 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 13:50:13.535437: step 54750, loss = 1.14 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 13:50:25.864482: step 54760, loss = 1.22 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 13:50:38.158921: step 54770, loss = 1.01 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 13:50:50.554875: step 54780, loss = 0.97 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 13:51:02.929513: step 54790, loss = 1.16 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 13:51:15.191714: step 54800, loss = 0.94 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 13:51:32.631282: step 54810, loss = 0.92 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 13:51:44.914520: step 54820, loss = 1.14 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 13:51:57.141339: step 54830, loss = 1.02 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 13:52:09.416558: step 54840, loss = 1.01 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 13:52:21.783637: step 54850, loss = 0.97 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 13:52:33.982873: step 54860, loss = 1.21 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 13:52:46.367419: step 54870, loss = 0.88 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 13:52:58.815208: step 54880, loss = 0.87 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 13:53:11.182240: step 54890, loss = 1.13 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 13:53:23.732502: step 54900, loss = 1.13 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 13:53:40.725099: step 54910, loss = 1.02 (23.8 examples/sec; 1.263 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 13:53:53.031641: step 54920, loss = 0.87 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-06-17 13:54:05.478406: step 54930, loss = 0.91 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 13:54:17.906459: step 54940, loss = 0.79 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 13:54:30.280238: step 54950, loss = 0.97 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 13:54:42.727556: step 54960, loss = 1.03 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 13:54:55.155648: step 54970, loss = 1.10 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 13:55:07.680446: step 54980, loss = 1.12 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 13:55:20.135460: step 54990, loss = 1.00 (22.5 examples/sec; 1.335 sec/batch)\n",
      "2019-06-17 13:55:32.429883: step 55000, loss = 1.14 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 13:55:52.665968: step 55010, loss = 0.93 (23.2 examples/sec; 1.294 sec/batch)\n",
      "2019-06-17 13:56:04.897508: step 55020, loss = 0.85 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 13:56:17.272068: step 55030, loss = 1.13 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 13:56:29.748686: step 55040, loss = 1.47 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 13:56:42.091917: step 55050, loss = 1.29 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 13:56:54.455438: step 55060, loss = 0.96 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 13:57:06.762234: step 55070, loss = 0.91 (23.6 examples/sec; 1.271 sec/batch)\n",
      "2019-06-17 13:57:19.264854: step 55080, loss = 1.03 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 13:57:31.704622: step 55090, loss = 0.93 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 13:57:43.984972: step 55100, loss = 0.99 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 13:58:01.470851: step 55110, loss = 1.06 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 13:58:13.937010: step 55120, loss = 1.05 (22.6 examples/sec; 1.328 sec/batch)\n",
      "2019-06-17 13:58:26.302219: step 55130, loss = 1.04 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 13:58:38.590123: step 55140, loss = 1.04 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 13:58:50.896613: step 55150, loss = 1.04 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-06-17 13:59:03.173247: step 55160, loss = 0.97 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 13:59:15.664140: step 55170, loss = 1.09 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 13:59:27.981356: step 55180, loss = 0.86 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-06-17 13:59:40.418233: step 55190, loss = 1.03 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 13:59:52.837690: step 55200, loss = 1.03 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 14:00:09.873349: step 55210, loss = 0.84 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 14:00:22.210616: step 55220, loss = 1.07 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 14:00:34.570154: step 55230, loss = 0.88 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 14:00:47.032356: step 55240, loss = 1.07 (22.9 examples/sec; 1.307 sec/batch)\n",
      "2019-06-17 14:00:59.381792: step 55250, loss = 1.13 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 14:01:11.845658: step 55260, loss = 1.11 (23.1 examples/sec; 1.298 sec/batch)\n",
      "2019-06-17 14:01:24.096959: step 55270, loss = 0.86 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 14:01:36.353405: step 55280, loss = 1.04 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 14:01:49.066731: step 55290, loss = 1.25 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 14:02:01.528546: step 55300, loss = 1.17 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 14:02:18.450263: step 55310, loss = 0.96 (23.0 examples/sec; 1.307 sec/batch)\n",
      "2019-06-17 14:02:30.916952: step 55320, loss = 0.99 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-17 14:02:43.632228: step 55330, loss = 1.05 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 14:02:56.066971: step 55340, loss = 1.11 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 14:03:08.417026: step 55350, loss = 0.96 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 14:03:20.778101: step 55360, loss = 0.89 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 14:03:33.034891: step 55370, loss = 0.95 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 14:03:45.631923: step 55380, loss = 1.30 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 14:03:58.112415: step 55390, loss = 0.98 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 14:04:10.390133: step 55400, loss = 0.98 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 14:04:27.264380: step 55410, loss = 1.03 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 14:04:39.676019: step 55420, loss = 1.00 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 14:04:52.301960: step 55430, loss = 0.97 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 14:05:04.460367: step 55440, loss = 1.00 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 14:05:16.888533: step 55450, loss = 1.03 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 14:05:29.320244: step 55460, loss = 1.03 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 14:05:41.611023: step 55470, loss = 1.11 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 14:05:53.930711: step 55480, loss = 1.03 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 14:06:06.300441: step 55490, loss = 0.98 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 14:06:18.956654: step 55500, loss = 1.24 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 14:06:36.232866: step 55510, loss = 1.02 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 14:06:48.565884: step 55520, loss = 1.06 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 14:07:00.794817: step 55530, loss = 1.04 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 14:07:12.968579: step 55540, loss = 1.24 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 14:07:25.281950: step 55550, loss = 0.93 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-06-17 14:07:37.546707: step 55560, loss = 1.10 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 14:07:49.919245: step 55570, loss = 1.15 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 14:08:02.312806: step 55580, loss = 1.06 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 14:08:14.658710: step 55590, loss = 1.20 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 14:08:26.981299: step 55600, loss = 1.06 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 14:08:43.990368: step 55610, loss = 0.94 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 14:08:56.515282: step 55620, loss = 0.96 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 14:09:09.214690: step 55630, loss = 0.98 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-17 14:09:21.406366: step 55640, loss = 0.92 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 14:09:33.769535: step 55650, loss = 1.34 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 14:09:46.379883: step 55660, loss = 1.08 (22.5 examples/sec; 1.333 sec/batch)\n",
      "2019-06-17 14:09:58.815291: step 55670, loss = 1.01 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 14:10:11.210881: step 55680, loss = 0.94 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 14:10:23.660823: step 55690, loss = 1.16 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 14:10:36.028592: step 55700, loss = 1.05 (22.4 examples/sec; 1.342 sec/batch)\n",
      "2019-06-17 14:10:53.015573: step 55710, loss = 1.05 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 14:11:05.282437: step 55720, loss = 1.09 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 14:11:17.688368: step 55730, loss = 1.36 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 14:11:30.051605: step 55740, loss = 1.23 (22.5 examples/sec; 1.331 sec/batch)\n",
      "2019-06-17 14:11:42.352145: step 55750, loss = 0.83 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 14:11:54.699699: step 55760, loss = 1.02 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 14:12:07.035862: step 55770, loss = 1.05 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 14:12:19.402691: step 55780, loss = 0.96 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 14:12:31.794198: step 55790, loss = 1.04 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-06-17 14:12:44.189580: step 55800, loss = 0.96 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 14:13:01.246861: step 55810, loss = 1.16 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-06-17 14:13:13.694498: step 55820, loss = 1.09 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 14:13:26.009171: step 55830, loss = 0.93 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 14:13:38.283141: step 55840, loss = 1.01 (24.6 examples/sec; 1.220 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 14:13:50.458710: step 55850, loss = 1.01 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 14:14:02.727950: step 55860, loss = 1.07 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 14:14:15.126015: step 55870, loss = 1.02 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-17 14:14:27.391897: step 55880, loss = 1.09 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-06-17 14:14:40.113001: step 55890, loss = 1.01 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 14:14:52.543103: step 55900, loss = 1.03 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 14:15:09.725123: step 55910, loss = 1.23 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 14:15:21.992093: step 55920, loss = 1.06 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 14:15:34.188497: step 55930, loss = 1.14 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 14:15:46.470188: step 55940, loss = 1.08 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 14:15:58.714628: step 55950, loss = 0.90 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 14:16:11.179484: step 55960, loss = 0.98 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-17 14:16:23.426992: step 55970, loss = 1.18 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 14:16:35.926373: step 55980, loss = 1.08 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 14:16:48.515182: step 55990, loss = 1.20 (22.5 examples/sec; 1.332 sec/batch)\n",
      "2019-06-17 14:17:00.725573: step 56000, loss = 1.26 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 14:17:17.757375: step 56010, loss = 1.08 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 14:17:30.332939: step 56020, loss = 0.92 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 14:17:42.533027: step 56030, loss = 0.97 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 14:17:54.766207: step 56040, loss = 1.03 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 14:18:07.408872: step 56050, loss = 1.15 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 14:18:19.727812: step 56060, loss = 0.98 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 14:18:32.001298: step 56070, loss = 1.13 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 14:18:44.277474: step 56080, loss = 0.98 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 14:18:56.402841: step 56090, loss = 1.08 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 14:19:08.549363: step 56100, loss = 1.11 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 14:19:25.218342: step 56110, loss = 0.81 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 14:19:37.479998: step 56120, loss = 0.98 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 14:19:49.744325: step 56130, loss = 0.97 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 14:20:02.347456: step 56140, loss = 1.02 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-06-17 14:20:14.527803: step 56150, loss = 1.06 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 14:20:26.665288: step 56160, loss = 0.84 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 14:20:38.981566: step 56170, loss = 0.90 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 14:20:51.469796: step 56180, loss = 1.13 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-06-17 14:21:03.789140: step 56190, loss = 1.14 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 14:21:16.110735: step 56200, loss = 1.39 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 14:21:32.935921: step 56210, loss = 1.18 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 14:21:45.071268: step 56220, loss = 1.09 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 14:21:57.322468: step 56230, loss = 1.01 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 14:22:09.613628: step 56240, loss = 1.10 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 14:22:21.782743: step 56250, loss = 0.95 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-17 14:22:34.070649: step 56260, loss = 1.05 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 14:22:46.615722: step 56270, loss = 1.16 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 14:22:58.818408: step 56280, loss = 1.00 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 14:23:11.006757: step 56290, loss = 0.99 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 14:23:23.286811: step 56300, loss = 0.95 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 14:23:40.055987: step 56310, loss = 1.21 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 14:23:52.408332: step 56320, loss = 1.25 (22.9 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 14:24:05.017831: step 56330, loss = 1.04 (22.6 examples/sec; 1.329 sec/batch)\n",
      "2019-06-17 14:24:17.226008: step 56340, loss = 1.10 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 14:24:29.289428: step 56350, loss = 1.03 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-06-17 14:24:41.712893: step 56360, loss = 1.21 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 14:24:54.089603: step 56370, loss = 1.14 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-06-17 14:25:06.328732: step 56380, loss = 1.09 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 14:25:18.548147: step 56390, loss = 1.04 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 14:25:30.844206: step 56400, loss = 1.02 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 14:25:47.480318: step 56410, loss = 0.99 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 14:26:00.159706: step 56420, loss = 1.15 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 14:26:12.596977: step 56430, loss = 1.02 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 14:26:24.877459: step 56440, loss = 1.04 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 14:26:37.034070: step 56450, loss = 0.86 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 14:26:49.282863: step 56460, loss = 1.03 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 14:27:01.500108: step 56470, loss = 1.11 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 14:27:13.804224: step 56480, loss = 0.93 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 14:27:26.225442: step 56490, loss = 1.29 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 14:27:38.680602: step 56500, loss = 1.08 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 14:27:55.384436: step 56510, loss = 1.09 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 14:28:07.823125: step 56520, loss = 1.03 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 14:28:20.044755: step 56530, loss = 0.97 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 14:28:32.383565: step 56540, loss = 1.02 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 14:28:44.552689: step 56550, loss = 1.28 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 14:28:57.236869: step 56560, loss = 1.10 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 14:29:09.459197: step 56570, loss = 1.14 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 14:29:21.866475: step 56580, loss = 1.04 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 14:29:33.977522: step 56590, loss = 1.08 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 14:29:46.148460: step 56600, loss = 0.99 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-06-17 14:30:03.568016: step 56610, loss = 0.97 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 14:30:15.876746: step 56620, loss = 1.14 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 14:30:28.043461: step 56630, loss = 1.13 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 14:30:40.684832: step 56640, loss = 1.08 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 14:30:52.985800: step 56650, loss = 1.14 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 14:31:05.189157: step 56660, loss = 1.29 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 14:31:17.422095: step 56670, loss = 0.94 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 14:31:29.806190: step 56680, loss = 1.13 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 14:31:42.076104: step 56690, loss = 1.20 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-17 14:31:54.436170: step 56700, loss = 1.00 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 14:32:11.353633: step 56710, loss = 0.91 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 14:32:23.732956: step 56720, loss = 0.98 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 14:32:35.954116: step 56730, loss = 0.99 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 14:32:48.399752: step 56740, loss = 0.96 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 14:33:00.906235: step 56750, loss = 0.92 (22.4 examples/sec; 1.337 sec/batch)\n",
      "2019-06-17 14:33:13.165606: step 56760, loss = 0.96 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 14:33:25.557362: step 56770, loss = 1.02 (24.9 examples/sec; 1.206 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 14:33:37.800376: step 56780, loss = 1.01 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 14:33:50.199022: step 56790, loss = 0.92 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-17 14:34:02.649448: step 56800, loss = 1.17 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 14:34:19.794997: step 56810, loss = 1.11 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 14:34:32.044939: step 56820, loss = 1.19 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 14:34:44.480338: step 56830, loss = 1.22 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 14:34:56.783999: step 56840, loss = 0.87 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 14:35:08.883005: step 56850, loss = 0.94 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 14:35:21.310735: step 56860, loss = 1.08 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 14:35:33.637253: step 56870, loss = 1.19 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 14:35:45.766166: step 56880, loss = 0.97 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 14:35:58.408869: step 56890, loss = 1.21 (22.6 examples/sec; 1.328 sec/batch)\n",
      "2019-06-17 14:36:10.880054: step 56900, loss = 1.25 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 14:36:27.509721: step 56910, loss = 1.05 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 14:36:39.784394: step 56920, loss = 1.04 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 14:36:52.074257: step 56930, loss = 0.99 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 14:37:04.379160: step 56940, loss = 0.97 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 14:37:16.831874: step 56950, loss = 1.11 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 14:37:29.414013: step 56960, loss = 1.14 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 14:37:41.644857: step 56970, loss = 1.05 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 14:37:53.786038: step 56980, loss = 1.12 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 14:38:05.969106: step 56990, loss = 1.29 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 14:38:18.379000: step 57000, loss = 1.01 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-06-17 14:38:35.576214: step 57010, loss = 1.03 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 14:38:47.893987: step 57020, loss = 0.93 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 14:39:00.070406: step 57030, loss = 1.07 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 14:39:12.318517: step 57040, loss = 1.09 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 14:39:24.611447: step 57050, loss = 1.00 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 14:39:36.994375: step 57060, loss = 1.03 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 14:39:49.347522: step 57070, loss = 1.01 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 14:40:01.903418: step 57080, loss = 1.15 (22.6 examples/sec; 1.328 sec/batch)\n",
      "2019-06-17 14:40:14.201287: step 57090, loss = 1.16 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 14:40:26.329805: step 57100, loss = 1.16 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 14:40:42.992827: step 57110, loss = 1.03 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 14:40:55.267632: step 57120, loss = 0.93 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 14:41:07.558487: step 57130, loss = 0.98 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 14:41:19.960970: step 57140, loss = 1.03 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 14:41:32.088830: step 57150, loss = 0.93 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 14:41:44.277676: step 57160, loss = 1.06 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 14:41:56.735159: step 57170, loss = 0.89 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 14:42:08.921223: step 57180, loss = 1.03 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 14:42:21.148560: step 57190, loss = 0.99 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 14:42:33.476383: step 57200, loss = 0.89 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 14:42:50.956311: step 57210, loss = 1.19 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 14:43:03.321397: step 57220, loss = 1.18 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 14:43:15.756083: step 57230, loss = 0.93 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 14:43:27.825772: step 57240, loss = 0.91 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-17 14:43:40.236995: step 57250, loss = 0.91 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 14:43:52.577347: step 57260, loss = 1.15 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 14:44:04.797673: step 57270, loss = 1.02 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 14:44:17.321971: step 57280, loss = 1.11 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 14:44:29.604761: step 57290, loss = 1.02 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 14:44:41.915414: step 57300, loss = 0.98 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 14:44:58.866839: step 57310, loss = 1.19 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 14:45:11.331409: step 57320, loss = 0.84 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 14:45:23.544737: step 57330, loss = 1.12 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 14:45:35.787318: step 57340, loss = 1.06 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 14:45:48.144565: step 57350, loss = 1.18 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-06-17 14:46:00.467334: step 57360, loss = 0.93 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 14:46:12.598977: step 57370, loss = 1.17 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 14:46:25.069647: step 57380, loss = 1.08 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-17 14:46:37.258351: step 57390, loss = 1.00 (23.6 examples/sec; 1.269 sec/batch)\n",
      "2019-06-17 14:46:49.617140: step 57400, loss = 0.98 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-06-17 14:47:06.403822: step 57410, loss = 0.99 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 14:47:18.705155: step 57420, loss = 1.02 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 14:47:30.954320: step 57430, loss = 0.93 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 14:47:43.083040: step 57440, loss = 1.06 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 14:47:55.524714: step 57450, loss = 1.35 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 14:48:07.973117: step 57460, loss = 0.97 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 14:48:20.370699: step 57470, loss = 1.11 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 14:48:32.882940: step 57480, loss = 1.09 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 14:48:45.112654: step 57490, loss = 0.91 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 14:48:57.435720: step 57500, loss = 0.92 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 14:49:14.416521: step 57510, loss = 0.98 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 14:49:26.848333: step 57520, loss = 1.01 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 14:49:39.372979: step 57530, loss = 1.12 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 14:49:51.641952: step 57540, loss = 1.11 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 14:50:04.056403: step 57550, loss = 1.03 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 14:50:16.394205: step 57560, loss = 1.09 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 14:50:28.586917: step 57570, loss = 1.07 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 14:50:40.774574: step 57580, loss = 1.09 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 14:50:53.043747: step 57590, loss = 0.91 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 14:51:05.295486: step 57600, loss = 1.01 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 14:51:22.788795: step 57610, loss = 1.02 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-17 14:51:35.038377: step 57620, loss = 1.10 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 14:51:47.495716: step 57630, loss = 0.91 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 14:51:59.692822: step 57640, loss = 0.97 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 14:52:11.907065: step 57650, loss = 1.06 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 14:52:24.289242: step 57660, loss = 1.17 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 14:52:36.629232: step 57670, loss = 1.11 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 14:52:48.953146: step 57680, loss = 1.07 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 14:53:01.221189: step 57690, loss = 1.11 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 14:53:13.419609: step 57700, loss = 1.06 (24.8 examples/sec; 1.210 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 14:53:30.238946: step 57710, loss = 1.20 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 14:53:42.312128: step 57720, loss = 1.05 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 14:53:54.885309: step 57730, loss = 1.00 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 14:54:07.193816: step 57740, loss = 0.97 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 14:54:19.473252: step 57750, loss = 1.18 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 14:54:31.743397: step 57760, loss = 1.03 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 14:54:44.056618: step 57770, loss = 1.26 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 14:54:56.554329: step 57780, loss = 1.07 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 14:55:08.957049: step 57790, loss = 1.15 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 14:55:21.201194: step 57800, loss = 1.02 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 14:55:38.239597: step 57810, loss = 1.04 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 14:55:50.351761: step 57820, loss = 1.14 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 14:56:02.633545: step 57830, loss = 1.12 (23.8 examples/sec; 1.263 sec/batch)\n",
      "2019-06-17 14:56:14.741356: step 57840, loss = 1.03 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-06-17 14:56:27.301223: step 57850, loss = 1.07 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 14:56:39.830898: step 57860, loss = 1.26 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 14:56:52.072316: step 57870, loss = 1.30 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-06-17 14:57:04.415690: step 57880, loss = 1.03 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 14:57:16.747083: step 57890, loss = 1.04 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 14:57:29.086687: step 57900, loss = 1.26 (23.4 examples/sec; 1.280 sec/batch)\n",
      "2019-06-17 14:57:46.106251: step 57910, loss = 1.16 (23.6 examples/sec; 1.273 sec/batch)\n",
      "2019-06-17 14:57:58.324096: step 57920, loss = 1.43 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 14:58:10.667757: step 57930, loss = 1.45 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 14:58:23.182104: step 57940, loss = 1.19 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 14:58:35.577728: step 57950, loss = 1.06 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-17 14:58:48.023668: step 57960, loss = 1.11 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 14:59:00.161820: step 57970, loss = 1.07 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 14:59:12.443675: step 57980, loss = 1.24 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 14:59:24.905346: step 57990, loss = 0.84 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 14:59:37.144711: step 58000, loss = 1.12 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 14:59:54.175315: step 58010, loss = 1.13 (23.3 examples/sec; 1.290 sec/batch)\n",
      "2019-06-17 15:00:06.609824: step 58020, loss = 1.12 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 15:00:18.822028: step 58030, loss = 1.11 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 15:00:31.042845: step 58040, loss = 1.35 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 15:00:43.584063: step 58050, loss = 1.11 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 15:00:55.873871: step 58060, loss = 1.12 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 15:01:08.033451: step 58070, loss = 1.22 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 15:01:20.268206: step 58080, loss = 1.21 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 15:01:32.611344: step 58090, loss = 1.05 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 15:01:45.066945: step 58100, loss = 1.02 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 15:02:02.341992: step 58110, loss = 1.01 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-06-17 15:02:14.761265: step 58120, loss = 1.00 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 15:02:26.984567: step 58130, loss = 0.99 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 15:02:39.352086: step 58140, loss = 0.99 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 15:02:51.602930: step 58150, loss = 0.95 (22.8 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 15:03:04.136865: step 58160, loss = 0.95 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 15:03:16.525136: step 58170, loss = 1.25 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 15:03:28.666041: step 58180, loss = 1.10 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 15:03:40.946349: step 58190, loss = 1.12 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 15:03:53.315180: step 58200, loss = 1.17 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 15:04:10.550376: step 58210, loss = 0.93 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 15:04:23.141095: step 58220, loss = 1.08 (23.3 examples/sec; 1.287 sec/batch)\n",
      "2019-06-17 15:04:35.659394: step 58230, loss = 0.93 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 15:04:47.846730: step 58240, loss = 1.08 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 15:05:00.173415: step 58250, loss = 1.02 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 15:05:12.407887: step 58260, loss = 1.04 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 15:05:24.612667: step 58270, loss = 1.28 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 15:05:36.902803: step 58280, loss = 0.98 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 15:05:49.387217: step 58290, loss = 1.01 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 15:06:01.951366: step 58300, loss = 1.17 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 15:06:18.913372: step 58310, loss = 1.04 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 15:06:31.169157: step 58320, loss = 1.05 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 15:06:43.321308: step 58330, loss = 1.21 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 15:06:55.558040: step 58340, loss = 1.09 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 15:07:07.824829: step 58350, loss = 0.96 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 15:07:20.503982: step 58360, loss = 0.92 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 15:07:32.834537: step 58370, loss = 1.18 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 15:07:45.109581: step 58380, loss = 0.98 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 15:07:57.239718: step 58390, loss = 1.10 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 15:08:09.805697: step 58400, loss = 1.10 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 15:08:26.985446: step 58410, loss = 1.14 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 15:08:39.223544: step 58420, loss = 0.99 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-17 15:08:51.681744: step 58430, loss = 0.88 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 15:09:03.872916: step 58440, loss = 0.93 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 15:09:16.190390: step 58450, loss = 1.14 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 15:09:28.725512: step 58460, loss = 0.93 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 15:09:40.871838: step 58470, loss = 1.23 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 15:09:53.246276: step 58480, loss = 1.26 (22.5 examples/sec; 1.332 sec/batch)\n",
      "2019-06-17 15:10:05.520313: step 58490, loss = 1.00 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 15:10:18.118032: step 58500, loss = 1.08 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 15:10:35.294222: step 58510, loss = 0.91 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 15:10:47.624030: step 58520, loss = 0.99 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 15:10:59.816750: step 58530, loss = 1.05 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 15:11:12.051513: step 58540, loss = 1.06 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 15:11:24.537042: step 58550, loss = 1.08 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 15:11:36.841958: step 58560, loss = 0.99 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 15:11:49.174996: step 58570, loss = 1.07 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 15:12:01.388705: step 58580, loss = 1.15 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 15:12:13.642656: step 58590, loss = 1.12 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 15:12:26.076408: step 58600, loss = 0.99 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 15:12:43.341238: step 58610, loss = 1.09 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 15:12:55.587581: step 58620, loss = 1.04 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 15:13:07.799441: step 58630, loss = 1.10 (24.8 examples/sec; 1.208 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 15:13:20.107975: step 58640, loss = 1.00 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 15:13:32.665708: step 58650, loss = 1.19 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 15:13:45.193727: step 58660, loss = 1.13 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-17 15:13:57.357276: step 58670, loss = 1.18 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 15:14:09.611961: step 58680, loss = 1.06 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-06-17 15:14:21.759411: step 58690, loss = 1.08 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 15:14:34.225096: step 58700, loss = 1.01 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 15:14:51.053942: step 58710, loss = 1.14 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 15:15:03.525065: step 58720, loss = 0.96 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 15:15:15.846071: step 58730, loss = 0.95 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 15:15:28.270711: step 58740, loss = 0.99 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 15:15:40.563614: step 58750, loss = 1.01 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 15:15:52.667910: step 58760, loss = 0.95 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 15:16:04.909997: step 58770, loss = 0.92 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 15:16:17.178577: step 58780, loss = 0.97 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 15:16:29.457589: step 58790, loss = 1.13 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 15:16:41.725382: step 58800, loss = 0.95 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 15:16:58.598905: step 58810, loss = 0.81 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 15:17:10.886743: step 58820, loss = 1.03 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 15:17:23.262565: step 58830, loss = 1.09 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 15:17:35.472555: step 58840, loss = 0.95 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 15:17:47.822076: step 58850, loss = 0.94 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 15:18:00.069531: step 58860, loss = 1.01 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 15:18:12.481403: step 58870, loss = 1.07 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 15:18:24.855368: step 58880, loss = 1.27 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 15:18:37.250323: step 58890, loss = 1.20 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 15:18:50.187064: step 58900, loss = 1.04 (22.5 examples/sec; 1.335 sec/batch)\n",
      "2019-06-17 15:19:07.098033: step 58910, loss = 1.00 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 15:19:19.327905: step 58920, loss = 1.15 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 15:19:31.608041: step 58930, loss = 1.20 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 15:19:43.701858: step 58940, loss = 1.05 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 15:19:56.105072: step 58950, loss = 0.91 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 15:20:08.460888: step 58960, loss = 1.23 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 15:20:20.742835: step 58970, loss = 1.33 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 15:20:32.984216: step 58980, loss = 0.90 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 15:20:45.257699: step 58990, loss = 0.96 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 15:20:57.750927: step 59000, loss = 1.11 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 15:21:14.830216: step 59010, loss = 0.86 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 15:21:27.098748: step 59020, loss = 1.17 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 15:21:39.717936: step 59030, loss = 1.01 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 15:21:52.159492: step 59040, loss = 0.94 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 15:22:04.326602: step 59050, loss = 0.99 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 15:22:16.667462: step 59060, loss = 1.13 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 15:22:28.855617: step 59070, loss = 0.92 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 15:22:41.099698: step 59080, loss = 0.94 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 15:22:53.442509: step 59090, loss = 1.00 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 15:23:05.826698: step 59100, loss = 1.05 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 15:23:22.859235: step 59110, loss = 0.95 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 15:23:35.262294: step 59120, loss = 1.03 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 15:23:47.691375: step 59130, loss = 0.98 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-06-17 15:24:00.120152: step 59140, loss = 1.31 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 15:24:12.591709: step 59150, loss = 0.98 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 15:24:24.877330: step 59160, loss = 1.04 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 15:24:37.380522: step 59170, loss = 1.25 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 15:24:49.627090: step 59180, loss = 1.01 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 15:25:01.951635: step 59190, loss = 1.08 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 15:25:14.643451: step 59200, loss = 1.13 (22.6 examples/sec; 1.326 sec/batch)\n",
      "2019-06-17 15:25:31.452113: step 59210, loss = 1.01 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 15:25:43.731417: step 59220, loss = 1.01 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 15:25:55.912149: step 59230, loss = 1.06 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 15:26:08.220397: step 59240, loss = 1.05 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 15:26:20.772490: step 59250, loss = 0.90 (22.6 examples/sec; 1.327 sec/batch)\n",
      "2019-06-17 15:26:33.162466: step 59260, loss = 1.03 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 15:26:45.341694: step 59270, loss = 0.99 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 15:26:57.673355: step 59280, loss = 1.13 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 15:27:09.807536: step 59290, loss = 1.24 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 15:27:22.197978: step 59300, loss = 0.93 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 15:27:39.278578: step 59310, loss = 1.01 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 15:27:51.406496: step 59320, loss = 0.89 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 15:28:03.650510: step 59330, loss = 1.03 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 15:28:15.853855: step 59340, loss = 1.11 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 15:28:28.211627: step 59350, loss = 1.29 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 15:28:40.565627: step 59360, loss = 1.10 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 15:28:52.881054: step 59370, loss = 1.06 (22.5 examples/sec; 1.331 sec/batch)\n",
      "2019-06-17 15:29:05.372750: step 59380, loss = 1.06 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 15:29:17.773269: step 59390, loss = 0.91 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 15:29:30.029500: step 59400, loss = 0.99 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 15:29:46.975750: step 59410, loss = 0.94 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 15:29:59.362579: step 59420, loss = 1.24 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 15:30:11.941638: step 59430, loss = 1.10 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 15:30:24.205735: step 59440, loss = 1.17 (23.7 examples/sec; 1.268 sec/batch)\n",
      "2019-06-17 15:30:36.515098: step 59450, loss = 1.03 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 15:30:48.715255: step 59460, loss = 1.02 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 15:31:01.058524: step 59470, loss = 1.10 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 15:31:13.508077: step 59480, loss = 1.03 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 15:31:25.817027: step 59490, loss = 1.23 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 15:31:38.266931: step 59500, loss = 0.89 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 15:31:55.301046: step 59510, loss = 0.92 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 15:32:07.636898: step 59520, loss = 1.00 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 15:32:19.831347: step 59530, loss = 1.21 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 15:32:32.227396: step 59540, loss = 1.05 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-06-17 15:32:44.440228: step 59550, loss = 1.03 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 15:32:56.805033: step 59560, loss = 0.91 (24.8 examples/sec; 1.208 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 15:33:09.034232: step 59570, loss = 1.36 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 15:33:21.246071: step 59580, loss = 1.10 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 15:33:33.561282: step 59590, loss = 0.97 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-06-17 15:33:45.915611: step 59600, loss = 1.06 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 15:34:02.737651: step 59610, loss = 0.87 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 15:34:14.979843: step 59620, loss = 1.02 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 15:34:27.234805: step 59630, loss = 1.19 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 15:34:39.486792: step 59640, loss = 1.01 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 15:34:51.677620: step 59650, loss = 1.10 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 15:35:03.992454: step 59660, loss = 1.13 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 15:35:16.448101: step 59670, loss = 1.07 (23.1 examples/sec; 1.299 sec/batch)\n",
      "2019-06-17 15:35:28.556570: step 59680, loss = 1.06 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 15:35:40.795280: step 59690, loss = 1.04 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 15:35:53.139851: step 59700, loss = 1.18 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 15:36:10.031303: step 59710, loss = 0.92 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 15:36:22.381939: step 59720, loss = 0.92 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-17 15:36:34.512668: step 59730, loss = 1.04 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 15:36:47.003608: step 59740, loss = 1.14 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 15:36:59.416885: step 59750, loss = 0.94 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 15:37:11.783020: step 59760, loss = 1.08 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 15:37:24.142601: step 59770, loss = 1.07 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 15:37:36.398226: step 59780, loss = 1.06 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 15:37:48.689588: step 59790, loss = 0.96 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-17 15:38:01.009525: step 59800, loss = 1.08 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-06-17 15:38:18.076570: step 59810, loss = 0.94 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 15:38:30.501299: step 59820, loss = 0.98 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 15:38:42.713325: step 59830, loss = 0.90 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 15:38:54.839709: step 59840, loss = 1.07 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 15:39:06.927368: step 59850, loss = 0.93 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 15:39:19.057028: step 59860, loss = 0.97 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 15:39:31.277431: step 59870, loss = 1.09 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 15:39:43.749370: step 59880, loss = 1.08 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 15:39:56.254155: step 59890, loss = 0.97 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 15:40:08.465201: step 59900, loss = 1.28 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 15:40:25.498956: step 59910, loss = 1.06 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 15:40:37.835749: step 59920, loss = 1.14 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 15:40:50.025076: step 59930, loss = 0.87 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 15:41:02.158976: step 59940, loss = 1.21 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 15:41:14.468277: step 59950, loss = 1.05 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-06-17 15:41:26.798154: step 59960, loss = 0.95 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 15:41:38.993861: step 59970, loss = 0.99 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 15:41:51.612170: step 59980, loss = 0.94 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 15:42:03.912167: step 59990, loss = 1.11 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 15:42:16.106771: step 60000, loss = 1.13 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 15:42:36.785114: step 60010, loss = 1.02 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 15:42:49.012667: step 60020, loss = 0.98 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 15:43:01.230220: step 60030, loss = 1.13 (23.3 examples/sec; 1.289 sec/batch)\n",
      "2019-06-17 15:43:13.717614: step 60040, loss = 0.83 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 15:43:26.301716: step 60050, loss = 1.00 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 15:43:38.479205: step 60060, loss = 1.20 (22.9 examples/sec; 1.307 sec/batch)\n",
      "2019-06-17 15:43:50.773483: step 60070, loss = 1.14 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 15:44:03.251835: step 60080, loss = 0.90 (22.5 examples/sec; 1.332 sec/batch)\n",
      "2019-06-17 15:44:15.605673: step 60090, loss = 1.16 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 15:44:28.045094: step 60100, loss = 1.16 (23.1 examples/sec; 1.297 sec/batch)\n",
      "2019-06-17 15:44:45.129213: step 60110, loss = 0.99 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 15:44:57.555558: step 60120, loss = 1.21 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 15:45:10.002682: step 60130, loss = 0.92 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 15:45:22.455448: step 60140, loss = 0.97 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 15:45:34.938394: step 60150, loss = 1.12 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-06-17 15:45:47.329889: step 60160, loss = 0.85 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 15:45:59.794858: step 60170, loss = 1.00 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 15:46:12.028246: step 60180, loss = 0.97 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 15:46:24.403040: step 60190, loss = 1.15 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-06-17 15:46:36.711648: step 60200, loss = 0.89 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-06-17 15:46:53.667603: step 60210, loss = 0.90 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 15:47:05.824798: step 60220, loss = 1.11 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-06-17 15:47:18.369708: step 60230, loss = 1.00 (23.4 examples/sec; 1.282 sec/batch)\n",
      "2019-06-17 15:47:30.604037: step 60240, loss = 1.17 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 15:47:43.041583: step 60250, loss = 0.96 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 15:47:55.531644: step 60260, loss = 0.91 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 15:48:07.866287: step 60270, loss = 1.05 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-06-17 15:48:20.019676: step 60280, loss = 1.09 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 15:48:32.234481: step 60290, loss = 1.09 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 15:48:44.449037: step 60300, loss = 1.09 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 15:49:01.454069: step 60310, loss = 0.99 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 15:49:13.966813: step 60320, loss = 0.91 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 15:49:26.030606: step 60330, loss = 0.96 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 15:49:38.354857: step 60340, loss = 1.11 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 15:49:50.495139: step 60350, loss = 0.85 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 15:50:02.749763: step 60360, loss = 1.03 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 15:50:14.891509: step 60370, loss = 0.95 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 15:50:27.511863: step 60380, loss = 0.94 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 15:50:39.810209: step 60390, loss = 0.93 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 15:50:52.154188: step 60400, loss = 0.95 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 15:51:09.194135: step 60410, loss = 0.93 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-06-17 15:51:21.686321: step 60420, loss = 1.02 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 15:51:33.964560: step 60430, loss = 0.91 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 15:51:46.282416: step 60440, loss = 0.96 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 15:51:58.448243: step 60450, loss = 0.88 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 15:52:10.529327: step 60460, loss = 1.02 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 15:52:22.749050: step 60470, loss = 1.00 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 15:52:35.005870: step 60480, loss = 1.13 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 15:52:47.438077: step 60490, loss = 1.15 (24.8 examples/sec; 1.209 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 15:52:59.841924: step 60500, loss = 0.87 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-06-17 15:53:17.005931: step 60510, loss = 0.85 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 15:53:29.335508: step 60520, loss = 1.13 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 15:53:41.431129: step 60530, loss = 1.00 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 15:53:53.698745: step 60540, loss = 0.92 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 15:54:06.091610: step 60550, loss = 1.07 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 15:54:18.582955: step 60560, loss = 0.92 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 15:54:31.006621: step 60570, loss = 1.07 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 15:54:43.364059: step 60580, loss = 1.08 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 15:54:55.806367: step 60590, loss = 0.89 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 15:55:08.155046: step 60600, loss = 1.10 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 15:55:25.306548: step 60610, loss = 1.06 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 15:55:37.674531: step 60620, loss = 0.86 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 15:55:50.317246: step 60630, loss = 0.88 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 15:56:02.675155: step 60640, loss = 1.04 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-06-17 15:56:15.122295: step 60650, loss = 0.95 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 15:56:27.387968: step 60660, loss = 1.15 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 15:56:39.606153: step 60670, loss = 1.10 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 15:56:52.089964: step 60680, loss = 0.88 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 15:57:04.251085: step 60690, loss = 1.17 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 15:57:16.741693: step 60700, loss = 0.96 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 15:57:33.957759: step 60710, loss = 1.04 (22.6 examples/sec; 1.328 sec/batch)\n",
      "2019-06-17 15:57:46.271070: step 60720, loss = 0.94 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-06-17 15:57:58.729609: step 60730, loss = 1.06 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-17 15:58:11.081621: step 60740, loss = 1.04 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 15:58:23.345755: step 60750, loss = 0.88 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 15:58:35.648059: step 60760, loss = 0.91 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 15:58:47.983949: step 60770, loss = 1.00 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 15:59:00.203341: step 60780, loss = 1.06 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 15:59:12.582569: step 60790, loss = 1.04 (22.5 examples/sec; 1.332 sec/batch)\n",
      "2019-06-17 15:59:24.738164: step 60800, loss = 0.94 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 15:59:41.966096: step 60810, loss = 1.24 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 15:59:54.420403: step 60820, loss = 1.08 (22.9 examples/sec; 1.307 sec/batch)\n",
      "2019-06-17 16:00:06.799198: step 60830, loss = 0.96 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 16:00:19.616054: step 60840, loss = 1.29 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 16:00:32.174812: step 60850, loss = 1.08 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 16:00:44.620104: step 60860, loss = 1.11 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 16:00:57.048703: step 60870, loss = 1.16 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 16:01:09.461155: step 60880, loss = 1.00 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 16:01:21.774095: step 60890, loss = 1.14 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 16:01:34.180004: step 60900, loss = 0.91 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 16:01:51.500068: step 60910, loss = 1.31 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 16:02:03.616358: step 60920, loss = 0.90 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 16:02:15.977999: step 60930, loss = 1.23 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-06-17 16:02:28.389544: step 60940, loss = 0.94 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 16:02:40.627973: step 60950, loss = 1.07 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 16:02:52.849038: step 60960, loss = 0.90 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 16:03:05.072329: step 60970, loss = 1.17 (24.1 examples/sec; 1.242 sec/batch)\n",
      "2019-06-17 16:03:17.309738: step 60980, loss = 1.33 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 16:03:29.598837: step 60990, loss = 1.09 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 16:03:41.871152: step 61000, loss = 1.04 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 16:03:58.756633: step 61010, loss = 1.01 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 16:04:11.247279: step 61020, loss = 0.97 (22.6 examples/sec; 1.328 sec/batch)\n",
      "2019-06-17 16:04:23.521469: step 61030, loss = 1.00 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 16:04:35.892998: step 61040, loss = 0.87 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 16:04:48.389789: step 61050, loss = 1.04 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 16:05:00.642425: step 61060, loss = 0.98 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 16:05:12.868093: step 61070, loss = 0.81 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 16:05:25.166366: step 61080, loss = 1.12 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 16:05:37.245106: step 61090, loss = 0.85 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 16:05:49.731737: step 61100, loss = 0.98 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 16:06:06.668540: step 61110, loss = 1.15 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 16:06:18.921395: step 61120, loss = 1.03 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 16:06:31.133655: step 61130, loss = 0.95 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 16:06:43.533439: step 61140, loss = 0.98 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-06-17 16:06:55.952813: step 61150, loss = 1.03 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 16:07:08.418229: step 61160, loss = 1.14 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-06-17 16:07:20.507656: step 61170, loss = 1.13 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 16:07:32.829136: step 61180, loss = 1.39 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 16:07:45.090432: step 61190, loss = 1.09 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-06-17 16:07:57.327447: step 61200, loss = 1.05 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-06-17 16:08:14.260666: step 61210, loss = 1.02 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-17 16:08:26.447539: step 61220, loss = 0.96 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 16:08:38.795996: step 61230, loss = 0.90 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 16:08:50.938327: step 61240, loss = 0.93 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 16:09:03.380989: step 61250, loss = 1.24 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 16:09:15.639177: step 61260, loss = 1.16 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 16:09:27.833674: step 61270, loss = 0.91 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 16:09:40.215344: step 61280, loss = 1.25 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 16:09:52.313531: step 61290, loss = 1.13 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-17 16:10:04.666247: step 61300, loss = 0.94 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 16:10:21.451462: step 61310, loss = 1.10 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 16:10:33.570267: step 61320, loss = 1.24 (23.1 examples/sec; 1.300 sec/batch)\n",
      "2019-06-17 16:10:46.115293: step 61330, loss = 0.90 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 16:10:58.501286: step 61340, loss = 1.09 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 16:11:10.866005: step 61350, loss = 0.98 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 16:11:23.162104: step 61360, loss = 1.12 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-17 16:11:35.433727: step 61370, loss = 0.92 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-17 16:11:47.995398: step 61380, loss = 1.18 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 16:12:00.150700: step 61390, loss = 0.88 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 16:12:12.620794: step 61400, loss = 1.07 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-06-17 16:12:29.103484: step 61410, loss = 1.05 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-06-17 16:12:41.234947: step 61420, loss = 1.17 (25.4 examples/sec; 1.183 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 16:12:53.469021: step 61430, loss = 1.06 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 16:13:05.657387: step 61440, loss = 0.87 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 16:13:17.912621: step 61450, loss = 1.17 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 16:13:30.156340: step 61460, loss = 1.11 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 16:13:42.501687: step 61470, loss = 0.93 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 16:13:54.670611: step 61480, loss = 0.99 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 16:14:06.979080: step 61490, loss = 1.04 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-17 16:14:19.180979: step 61500, loss = 0.85 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 16:14:36.201061: step 61510, loss = 1.18 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 16:14:48.291914: step 61520, loss = 1.11 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 16:15:00.376439: step 61530, loss = 1.22 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 16:15:12.787218: step 61540, loss = 0.99 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 16:15:24.834995: step 61550, loss = 1.00 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 16:15:37.181505: step 61560, loss = 1.02 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 16:15:49.437377: step 61570, loss = 1.05 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-06-17 16:16:01.609863: step 61580, loss = 0.92 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 16:16:13.684523: step 61590, loss = 1.03 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 16:16:25.715068: step 61600, loss = 1.05 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 16:16:42.380602: step 61610, loss = 0.84 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-17 16:16:54.672209: step 61620, loss = 1.10 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 16:17:06.861892: step 61630, loss = 1.07 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 16:17:19.069516: step 61640, loss = 0.98 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 16:17:31.309531: step 61650, loss = 0.98 (22.9 examples/sec; 1.307 sec/batch)\n",
      "2019-06-17 16:17:43.394692: step 61660, loss = 0.98 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 16:17:55.391867: step 61670, loss = 1.08 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 16:18:07.684403: step 61680, loss = 0.98 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 16:18:19.928570: step 61690, loss = 0.98 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 16:18:32.179493: step 61700, loss = 0.93 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 16:18:49.089939: step 61710, loss = 1.19 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 16:19:01.188915: step 61720, loss = 1.07 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 16:19:13.420720: step 61730, loss = 1.35 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 16:19:25.707971: step 61740, loss = 0.94 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 16:19:37.895078: step 61750, loss = 1.21 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 16:19:50.081205: step 61760, loss = 0.87 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-06-17 16:20:02.319500: step 61770, loss = 1.15 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 16:20:14.614129: step 61780, loss = 0.90 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 16:20:26.743942: step 61790, loss = 0.91 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 16:20:39.134608: step 61800, loss = 0.95 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 16:20:56.130031: step 61810, loss = 1.10 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 16:21:08.386546: step 61820, loss = 1.13 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 16:21:20.550968: step 61830, loss = 1.01 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-17 16:21:32.581983: step 61840, loss = 1.09 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 16:21:44.904891: step 61850, loss = 1.00 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 16:21:57.334430: step 61860, loss = 1.18 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 16:22:09.540357: step 61870, loss = 1.09 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 16:22:21.758390: step 61880, loss = 1.08 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 16:22:34.165958: step 61890, loss = 0.98 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 16:22:46.379766: step 61900, loss = 1.10 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 16:23:03.069562: step 61910, loss = 1.02 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-06-17 16:23:15.234419: step 61920, loss = 1.16 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 16:23:27.364395: step 61930, loss = 0.97 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 16:23:39.481029: step 61940, loss = 1.04 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 16:23:51.764859: step 61950, loss = 1.01 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 16:24:04.124730: step 61960, loss = 1.08 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 16:24:16.144558: step 61970, loss = 1.07 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 16:24:28.204240: step 61980, loss = 0.95 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 16:24:40.344001: step 61990, loss = 1.07 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-06-17 16:24:52.620070: step 62000, loss = 0.89 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 16:25:09.499635: step 62010, loss = 1.08 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 16:25:21.595856: step 62020, loss = 0.98 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-06-17 16:25:33.812232: step 62030, loss = 1.07 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 16:25:45.902550: step 62040, loss = 1.32 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 16:25:58.173289: step 62050, loss = 1.00 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 16:26:10.668912: step 62060, loss = 1.23 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 16:26:22.828886: step 62070, loss = 0.96 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 16:26:34.965795: step 62080, loss = 1.18 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 16:26:47.391623: step 62090, loss = 1.04 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-06-17 16:26:59.559773: step 62100, loss = 1.09 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 16:27:16.514008: step 62110, loss = 1.16 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 16:27:28.540209: step 62120, loss = 0.95 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 16:27:40.811189: step 62130, loss = 1.01 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 16:27:53.096736: step 62140, loss = 1.10 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 16:28:05.300502: step 62150, loss = 1.11 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 16:28:17.908161: step 62160, loss = 1.04 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-06-17 16:28:30.015076: step 62170, loss = 0.94 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 16:28:42.104650: step 62180, loss = 1.07 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 16:28:54.407931: step 62190, loss = 1.00 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 16:29:06.734885: step 62200, loss = 1.26 (23.3 examples/sec; 1.286 sec/batch)\n",
      "2019-06-17 16:29:23.361555: step 62210, loss = 1.04 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 16:29:35.459204: step 62220, loss = 1.01 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 16:29:47.583840: step 62230, loss = 0.96 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-17 16:29:59.890733: step 62240, loss = 1.14 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 16:30:12.278399: step 62250, loss = 0.91 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 16:30:24.389604: step 62260, loss = 1.00 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-06-17 16:30:36.550645: step 62270, loss = 0.93 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 16:30:48.952223: step 62280, loss = 1.03 (22.9 examples/sec; 1.307 sec/batch)\n",
      "2019-06-17 16:31:01.169595: step 62290, loss = 1.01 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 16:31:13.337373: step 62300, loss = 1.00 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 16:31:30.108402: step 62310, loss = 1.02 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 16:31:42.285163: step 62320, loss = 1.02 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 16:31:54.441691: step 62330, loss = 0.97 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-06-17 16:32:06.710734: step 62340, loss = 1.04 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-06-17 16:32:18.837410: step 62350, loss = 0.93 (25.1 examples/sec; 1.197 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 16:32:31.171330: step 62360, loss = 0.98 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 16:32:43.223174: step 62370, loss = 0.97 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 16:32:55.568801: step 62380, loss = 0.94 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 16:33:07.726940: step 62390, loss = 1.01 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 16:33:19.915003: step 62400, loss = 0.96 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 16:33:36.447769: step 62410, loss = 1.08 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 16:33:48.651343: step 62420, loss = 0.98 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 16:34:00.898496: step 62430, loss = 0.98 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 16:34:13.125621: step 62440, loss = 0.98 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 16:34:25.300525: step 62450, loss = 1.03 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 16:34:37.896583: step 62460, loss = 1.08 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 16:34:50.469336: step 62470, loss = 0.94 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 16:35:02.756426: step 62480, loss = 0.93 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 16:35:14.909392: step 62490, loss = 1.16 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 16:35:27.127843: step 62500, loss = 1.10 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 16:35:43.744471: step 62510, loss = 1.17 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 16:35:56.068207: step 62520, loss = 1.18 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 16:36:08.708644: step 62530, loss = 1.07 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 16:36:20.958427: step 62540, loss = 1.13 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 16:36:33.438701: step 62550, loss = 1.02 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-06-17 16:36:45.720658: step 62560, loss = 1.11 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 16:36:58.007987: step 62570, loss = 1.21 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 16:37:10.165856: step 62580, loss = 0.92 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 16:37:22.237591: step 62590, loss = 1.04 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 16:37:34.458349: step 62600, loss = 1.14 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 16:37:51.252755: step 62610, loss = 0.95 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-17 16:38:03.707741: step 62620, loss = 0.97 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-17 16:38:15.760422: step 62630, loss = 1.28 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 16:38:28.102743: step 62640, loss = 1.33 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 16:38:40.302537: step 62650, loss = 0.88 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 16:38:52.346965: step 62660, loss = 0.84 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 16:39:04.625983: step 62670, loss = 0.99 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-06-17 16:39:16.790091: step 62680, loss = 1.27 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 16:39:29.203479: step 62690, loss = 1.25 (22.8 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 16:39:41.423606: step 62700, loss = 1.02 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-06-17 16:39:58.386483: step 62710, loss = 1.16 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 16:40:10.525216: step 62720, loss = 1.02 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-06-17 16:40:22.586077: step 62730, loss = 1.18 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 16:40:34.721066: step 62740, loss = 1.01 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-06-17 16:40:46.879051: step 62750, loss = 0.98 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-06-17 16:40:59.074955: step 62760, loss = 1.11 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-06-17 16:41:11.123172: step 62770, loss = 1.08 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 16:41:23.162994: step 62780, loss = 1.13 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 16:41:35.479005: step 62790, loss = 1.13 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-17 16:41:47.557598: step 62800, loss = 1.22 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-06-17 16:42:04.437955: step 62810, loss = 0.94 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 16:42:16.790728: step 62820, loss = 1.09 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 16:42:29.115805: step 62830, loss = 1.28 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-17 16:42:41.376598: step 62840, loss = 0.98 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 16:42:53.410054: step 62850, loss = 0.96 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 16:43:05.723985: step 62860, loss = 1.13 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 16:43:18.069166: step 62870, loss = 0.98 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 16:43:30.268127: step 62880, loss = 1.04 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 16:43:42.443910: step 62890, loss = 0.84 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 16:43:54.590714: step 62900, loss = 1.16 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 16:44:11.613148: step 62910, loss = 1.34 (22.6 examples/sec; 1.325 sec/batch)\n",
      "2019-06-17 16:44:23.885695: step 62920, loss = 1.13 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 16:44:36.021876: step 62930, loss = 0.97 (23.7 examples/sec; 1.268 sec/batch)\n",
      "2019-06-17 16:44:48.131591: step 62940, loss = 1.03 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 16:45:00.655092: step 62950, loss = 1.13 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 16:45:12.950270: step 62960, loss = 1.12 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 16:45:25.111263: step 62970, loss = 1.06 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 16:45:37.515130: step 62980, loss = 1.18 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 16:45:49.787620: step 62990, loss = 1.17 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 16:46:02.029128: step 63000, loss = 1.09 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-06-17 16:46:18.730946: step 63010, loss = 1.19 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 16:46:31.181230: step 63020, loss = 1.04 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 16:46:43.339176: step 63030, loss = 1.32 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 16:46:55.641177: step 63040, loss = 0.96 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 16:47:07.906430: step 63050, loss = 1.04 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 16:47:20.059401: step 63060, loss = 1.04 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 16:47:32.316718: step 63070, loss = 0.96 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 16:47:44.584611: step 63080, loss = 1.06 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 16:47:56.868745: step 63090, loss = 1.00 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-06-17 16:48:08.990918: step 63100, loss = 1.01 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 16:48:25.863388: step 63110, loss = 1.07 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 16:48:37.973121: step 63120, loss = 1.02 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 16:48:50.362667: step 63130, loss = 1.06 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 16:49:02.478780: step 63140, loss = 1.00 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 16:49:14.553866: step 63150, loss = 1.33 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 16:49:26.684894: step 63160, loss = 1.09 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 16:49:38.871193: step 63170, loss = 0.86 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 16:49:51.297109: step 63180, loss = 1.02 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 16:50:03.448290: step 63190, loss = 0.95 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 16:50:15.706965: step 63200, loss = 0.99 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 16:50:32.492585: step 63210, loss = 1.11 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 16:50:44.519854: step 63220, loss = 0.98 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-17 16:50:56.676831: step 63230, loss = 0.91 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 16:51:08.965161: step 63240, loss = 0.93 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 16:51:21.129341: step 63250, loss = 1.06 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 16:51:33.377094: step 63260, loss = 1.13 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 16:51:45.687192: step 63270, loss = 1.04 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-17 16:51:57.772647: step 63280, loss = 1.11 (25.0 examples/sec; 1.202 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 16:52:09.885204: step 63290, loss = 1.16 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 16:52:22.363175: step 63300, loss = 1.17 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 16:52:38.976588: step 63310, loss = 1.08 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 16:52:51.360108: step 63320, loss = 1.08 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 16:53:03.487634: step 63330, loss = 1.02 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 16:53:15.717050: step 63340, loss = 1.12 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 16:53:27.930394: step 63350, loss = 1.00 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 16:53:40.272176: step 63360, loss = 0.87 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 16:53:52.416494: step 63370, loss = 1.05 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 16:54:04.748673: step 63380, loss = 0.92 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 16:54:17.178704: step 63390, loss = 1.01 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-17 16:54:29.600812: step 63400, loss = 1.08 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 16:54:46.682919: step 63410, loss = 1.24 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-06-17 16:54:59.015516: step 63420, loss = 1.01 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 16:55:11.281017: step 63430, loss = 1.07 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 16:55:23.523925: step 63440, loss = 1.12 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-17 16:55:35.799957: step 63450, loss = 0.91 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 16:55:48.062512: step 63460, loss = 1.08 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 16:56:00.137685: step 63470, loss = 0.93 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 16:56:12.510818: step 63480, loss = 1.00 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-17 16:56:24.783292: step 63490, loss = 1.17 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 16:56:37.040299: step 63500, loss = 1.06 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 16:56:54.247553: step 63510, loss = 0.97 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-06-17 16:57:06.724060: step 63520, loss = 0.99 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 16:57:18.938925: step 63530, loss = 1.11 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 16:57:31.254958: step 63540, loss = 1.35 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 16:57:43.415133: step 63550, loss = 0.91 (23.1 examples/sec; 1.299 sec/batch)\n",
      "2019-06-17 16:57:55.516233: step 63560, loss = 1.21 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 16:58:07.609976: step 63570, loss = 0.96 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 16:58:19.850999: step 63580, loss = 1.09 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 16:58:32.112241: step 63590, loss = 1.27 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 16:58:44.452298: step 63600, loss = 0.93 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 16:59:01.313799: step 63610, loss = 0.95 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 16:59:13.570667: step 63620, loss = 0.99 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 16:59:26.251522: step 63630, loss = 1.03 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 16:59:38.426173: step 63640, loss = 1.27 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-06-17 16:59:50.550522: step 63650, loss = 1.10 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-06-17 17:00:02.686782: step 63660, loss = 0.82 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 17:00:14.857684: step 63670, loss = 0.98 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 17:00:26.992950: step 63680, loss = 1.07 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 17:00:39.336025: step 63690, loss = 0.88 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-17 17:00:51.805109: step 63700, loss = 0.99 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-06-17 17:01:08.663114: step 63710, loss = 1.27 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 17:01:20.898397: step 63720, loss = 1.02 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 17:01:33.147976: step 63730, loss = 1.20 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-17 17:01:45.522046: step 63740, loss = 1.01 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 17:01:57.803188: step 63750, loss = 1.22 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 17:02:10.267159: step 63760, loss = 0.94 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 17:02:22.465117: step 63770, loss = 1.20 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 17:02:34.823337: step 63780, loss = 1.05 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 17:02:47.088139: step 63790, loss = 1.07 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 17:02:59.328360: step 63800, loss = 1.41 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-17 17:03:16.239140: step 63810, loss = 1.02 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 17:03:28.329503: step 63820, loss = 0.91 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 17:03:40.558818: step 63830, loss = 1.10 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 17:03:52.957529: step 63840, loss = 1.07 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 17:04:05.214106: step 63850, loss = 0.98 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-17 17:04:17.562386: step 63860, loss = 0.95 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 17:04:29.793319: step 63870, loss = 1.05 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 17:04:42.079473: step 63880, loss = 0.92 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 17:04:54.325533: step 63890, loss = 1.19 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 17:05:06.471053: step 63900, loss = 1.11 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-06-17 17:05:23.178053: step 63910, loss = 1.05 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 17:05:35.369801: step 63920, loss = 0.88 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-06-17 17:05:47.653096: step 63930, loss = 0.93 (23.1 examples/sec; 1.296 sec/batch)\n",
      "2019-06-17 17:05:59.810615: step 63940, loss = 0.88 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 17:06:11.982885: step 63950, loss = 0.98 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 17:06:24.152589: step 63960, loss = 1.03 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 17:06:36.484802: step 63970, loss = 1.03 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-06-17 17:06:48.619869: step 63980, loss = 0.93 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 17:07:00.774990: step 63990, loss = 0.87 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 17:07:13.090391: step 64000, loss = 1.01 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 17:07:29.681302: step 64010, loss = 1.17 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-17 17:07:42.087801: step 64020, loss = 0.90 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-06-17 17:07:54.229586: step 64030, loss = 0.93 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 17:08:06.628460: step 64040, loss = 0.98 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 17:08:18.899704: step 64050, loss = 1.10 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 17:08:31.152867: step 64060, loss = 1.03 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 17:08:43.690692: step 64070, loss = 1.11 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-06-17 17:08:55.854507: step 64080, loss = 1.26 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 17:09:07.939968: step 64090, loss = 1.12 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 17:09:20.163317: step 64100, loss = 1.10 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 17:09:36.810655: step 64110, loss = 0.93 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 17:09:49.458539: step 64120, loss = 0.98 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 17:10:01.722066: step 64130, loss = 1.05 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-06-17 17:10:14.086434: step 64140, loss = 1.37 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 17:10:26.325604: step 64150, loss = 1.01 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 17:10:38.784680: step 64160, loss = 0.95 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 17:10:51.039836: step 64170, loss = 0.95 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 17:11:03.441017: step 64180, loss = 1.25 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 17:11:15.814992: step 64190, loss = 1.04 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 17:11:27.968134: step 64200, loss = 0.97 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-06-17 17:11:44.767340: step 64210, loss = 0.87 (25.1 examples/sec; 1.195 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 17:11:56.912902: step 64220, loss = 1.01 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 17:12:09.086003: step 64230, loss = 1.20 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 17:12:21.439110: step 64240, loss = 0.96 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 17:12:33.764826: step 64250, loss = 1.04 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-06-17 17:12:46.150230: step 64260, loss = 1.02 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-06-17 17:12:58.494609: step 64270, loss = 0.88 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-06-17 17:13:10.870697: step 64280, loss = 0.94 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-06-17 17:13:23.141645: step 64290, loss = 1.01 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-17 17:13:35.203662: step 64300, loss = 0.98 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 17:13:52.230931: step 64310, loss = 1.05 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 17:14:04.550539: step 64320, loss = 1.12 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 17:14:16.827933: step 64330, loss = 0.94 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 17:14:29.087161: step 64340, loss = 1.05 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 17:14:41.255347: step 64350, loss = 0.99 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-17 17:14:53.450266: step 64360, loss = 1.03 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 17:15:05.611406: step 64370, loss = 0.93 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 17:15:17.883793: step 64380, loss = 0.90 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 17:15:30.167035: step 64390, loss = 1.05 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 17:15:42.369353: step 64400, loss = 1.15 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 17:15:59.059013: step 64410, loss = 0.97 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-06-17 17:16:11.314545: step 64420, loss = 1.05 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 17:16:23.339682: step 64430, loss = 1.34 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-06-17 17:16:35.463640: step 64440, loss = 1.00 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 17:16:47.772630: step 64450, loss = 1.02 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 17:16:59.994717: step 64460, loss = 0.99 (23.9 examples/sec; 1.258 sec/batch)\n",
      "2019-06-17 17:17:12.205548: step 64470, loss = 1.01 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 17:17:24.315322: step 64480, loss = 0.99 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 17:17:36.605701: step 64490, loss = 1.02 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 17:17:48.850370: step 64500, loss = 0.90 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-06-17 17:18:05.277324: step 64510, loss = 1.00 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 17:18:17.678140: step 64520, loss = 0.90 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-17 17:18:29.885696: step 64530, loss = 1.26 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 17:18:42.027225: step 64540, loss = 0.85 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 17:18:54.697708: step 64550, loss = 1.13 (22.9 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 17:19:06.889315: step 64560, loss = 1.37 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-06-17 17:19:19.238831: step 64570, loss = 0.97 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 17:19:31.351584: step 64580, loss = 0.99 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-06-17 17:19:43.620324: step 64590, loss = 1.01 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 17:19:55.946095: step 64600, loss = 1.28 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 17:20:12.773639: step 64610, loss = 1.02 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 17:20:24.812533: step 64620, loss = 0.98 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 17:20:36.892651: step 64630, loss = 0.99 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 17:20:48.968004: step 64640, loss = 0.91 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 17:21:01.225653: step 64650, loss = 1.16 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 17:21:13.404488: step 64660, loss = 1.22 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 17:21:25.672000: step 64670, loss = 0.90 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 17:21:37.771306: step 64680, loss = 1.00 (24.0 examples/sec; 1.247 sec/batch)\n",
      "2019-06-17 17:21:50.041259: step 64690, loss = 1.00 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-06-17 17:22:02.185683: step 64700, loss = 1.11 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-17 17:22:18.971445: step 64710, loss = 0.93 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 17:22:31.155406: step 64720, loss = 0.94 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-06-17 17:22:43.409045: step 64730, loss = 1.12 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 17:22:55.607587: step 64740, loss = 1.29 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-06-17 17:23:07.806416: step 64750, loss = 1.00 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-06-17 17:23:19.990539: step 64760, loss = 1.21 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-06-17 17:23:32.171924: step 64770, loss = 0.98 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 17:23:44.194606: step 64780, loss = 1.12 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 17:23:56.226239: step 64790, loss = 1.25 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 17:24:08.479632: step 64800, loss = 1.21 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 17:24:25.804585: step 64810, loss = 1.04 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 17:24:38.256796: step 64820, loss = 0.98 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 17:24:50.342348: step 64830, loss = 0.97 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 17:25:02.590669: step 64840, loss = 1.03 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-06-17 17:25:14.855319: step 64850, loss = 0.92 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-06-17 17:25:27.183646: step 64860, loss = 1.23 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-06-17 17:25:39.331781: step 64870, loss = 1.00 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 17:25:51.892802: step 64880, loss = 0.98 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 17:26:04.377351: step 64890, loss = 0.94 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-06-17 17:26:16.708048: step 64900, loss = 1.03 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-06-17 17:26:33.638030: step 64910, loss = 1.12 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-06-17 17:26:45.802325: step 64920, loss = 1.08 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 17:26:58.054211: step 64930, loss = 1.07 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 17:27:10.348813: step 64940, loss = 1.08 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 17:27:22.832681: step 64950, loss = 0.89 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 17:27:35.011918: step 64960, loss = 1.08 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 17:27:47.169238: step 64970, loss = 0.96 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 17:27:59.808348: step 64980, loss = 1.09 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 17:28:12.196320: step 64990, loss = 1.03 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 17:28:24.341812: step 65000, loss = 0.90 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 17:28:45.011413: step 65010, loss = 1.10 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 17:28:57.078328: step 65020, loss = 0.94 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 17:29:09.267823: step 65030, loss = 0.88 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 17:29:21.423962: step 65040, loss = 0.97 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 17:29:33.780139: step 65050, loss = 1.04 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 17:29:46.055310: step 65060, loss = 1.02 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 17:29:58.243380: step 65070, loss = 1.01 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-06-17 17:30:10.543806: step 65080, loss = 1.08 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 17:30:22.700698: step 65090, loss = 1.17 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 17:30:34.977117: step 65100, loss = 0.98 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 17:30:51.673482: step 65110, loss = 0.91 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 17:31:03.698597: step 65120, loss = 1.10 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 17:31:15.749948: step 65130, loss = 1.09 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 17:31:28.041872: step 65140, loss = 1.22 (24.8 examples/sec; 1.208 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 17:31:40.155234: step 65150, loss = 0.90 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 17:31:52.517276: step 65160, loss = 1.13 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 17:32:04.900451: step 65170, loss = 1.08 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 17:32:17.423516: step 65180, loss = 1.13 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 17:32:29.717520: step 65190, loss = 1.08 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 17:32:41.865610: step 65200, loss = 1.09 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 17:32:58.330454: step 65210, loss = 0.92 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 17:33:10.493731: step 65220, loss = 1.01 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 17:33:22.774757: step 65230, loss = 1.24 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 17:33:34.835442: step 65240, loss = 0.89 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 17:33:47.114256: step 65250, loss = 0.88 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 17:33:59.544332: step 65260, loss = 1.08 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-06-17 17:34:11.647268: step 65270, loss = 1.03 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 17:34:23.855592: step 65280, loss = 1.21 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-06-17 17:34:36.360260: step 65290, loss = 0.93 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-06-17 17:34:48.553720: step 65300, loss = 1.04 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 17:35:05.366619: step 65310, loss = 1.00 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-06-17 17:35:17.543940: step 65320, loss = 1.11 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-06-17 17:35:29.878197: step 65330, loss = 1.00 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 17:35:42.402948: step 65340, loss = 1.04 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 17:35:54.674133: step 65350, loss = 0.99 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 17:36:07.043868: step 65360, loss = 1.06 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-06-17 17:36:19.179054: step 65370, loss = 0.95 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 17:36:31.395765: step 65380, loss = 1.11 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-06-17 17:36:43.681251: step 65390, loss = 0.87 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 17:36:55.815187: step 65400, loss = 0.94 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 17:37:12.944457: step 65410, loss = 1.20 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-06-17 17:37:25.242287: step 65420, loss = 1.16 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 17:37:37.547464: step 65430, loss = 1.08 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 17:37:49.630944: step 65440, loss = 0.87 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-06-17 17:38:02.019016: step 65450, loss = 1.09 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 17:38:14.315459: step 65460, loss = 1.03 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 17:38:26.749354: step 65470, loss = 0.85 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-06-17 17:38:38.971471: step 65480, loss = 0.94 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-06-17 17:38:51.344389: step 65490, loss = 1.25 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-06-17 17:39:03.716043: step 65500, loss = 1.01 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 17:39:20.433844: step 65510, loss = 0.95 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 17:39:32.952337: step 65520, loss = 1.01 (23.2 examples/sec; 1.294 sec/batch)\n",
      "2019-06-17 17:39:45.116699: step 65530, loss = 1.29 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-06-17 17:39:57.287589: step 65540, loss = 1.04 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-06-17 17:40:09.365998: step 65550, loss = 1.02 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 17:40:21.736785: step 65560, loss = 1.03 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 17:40:34.075506: step 65570, loss = 1.10 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-06-17 17:40:46.198978: step 65580, loss = 0.96 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-06-17 17:40:58.543106: step 65590, loss = 1.08 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 17:41:10.716660: step 65600, loss = 1.13 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 17:41:27.641157: step 65610, loss = 0.93 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-06-17 17:41:39.806916: step 65620, loss = 1.01 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-06-17 17:41:52.289482: step 65630, loss = 1.19 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-06-17 17:42:04.612086: step 65640, loss = 0.87 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 17:42:16.772341: step 65650, loss = 0.87 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-06-17 17:42:29.003501: step 65660, loss = 1.19 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 17:42:41.092829: step 65670, loss = 1.17 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-06-17 17:42:53.263913: step 65680, loss = 0.99 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-06-17 17:43:05.623554: step 65690, loss = 0.95 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 17:43:17.866691: step 65700, loss = 1.02 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-06-17 17:43:34.919362: step 65710, loss = 1.15 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 17:43:47.157838: step 65720, loss = 1.27 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 17:43:59.562416: step 65730, loss = 1.01 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 17:44:11.970549: step 65740, loss = 1.02 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-06-17 17:44:24.198169: step 65750, loss = 1.04 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 17:44:36.348935: step 65760, loss = 1.07 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 17:44:48.411372: step 65770, loss = 0.98 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 17:45:00.831495: step 65780, loss = 0.99 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 17:45:13.044924: step 65790, loss = 1.02 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 17:45:25.232762: step 65800, loss = 1.17 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-06-17 17:45:42.073741: step 65810, loss = 1.07 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 17:45:54.380516: step 65820, loss = 1.06 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 17:46:06.676595: step 65830, loss = 0.91 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 17:46:18.853922: step 65840, loss = 1.14 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-06-17 17:46:30.988592: step 65850, loss = 1.03 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 17:46:43.253984: step 65860, loss = 1.07 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-06-17 17:46:55.487594: step 65870, loss = 1.01 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-06-17 17:47:07.693911: step 65880, loss = 1.20 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 17:47:19.937107: step 65890, loss = 0.93 (23.3 examples/sec; 1.286 sec/batch)\n",
      "2019-06-17 17:47:32.116509: step 65900, loss = 1.23 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-06-17 17:47:49.001429: step 65910, loss = 1.09 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-06-17 17:48:01.269066: step 65920, loss = 0.77 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 17:48:13.515099: step 65930, loss = 1.04 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-06-17 17:48:25.837277: step 65940, loss = 1.08 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-06-17 17:48:38.288643: step 65950, loss = 1.10 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-06-17 17:48:50.375913: step 65960, loss = 1.08 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 17:49:02.708097: step 65970, loss = 1.02 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 17:49:14.930090: step 65980, loss = 1.02 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 17:49:27.079915: step 65990, loss = 1.03 (23.1 examples/sec; 1.300 sec/batch)\n",
      "2019-06-17 17:49:39.327230: step 66000, loss = 0.98 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-06-17 17:49:56.200330: step 66010, loss = 1.13 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-06-17 17:50:08.524076: step 66020, loss = 1.00 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-06-17 17:50:20.823837: step 66030, loss = 0.91 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-06-17 17:50:32.842529: step 66040, loss = 0.99 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-06-17 17:50:45.072579: step 66050, loss = 0.93 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-06-17 17:50:57.661576: step 66060, loss = 0.98 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-06-17 17:51:10.052704: step 66070, loss = 0.95 (22.7 examples/sec; 1.319 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-17 17:51:22.419414: step 66080, loss = 1.05 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-06-17 17:51:34.591992: step 66090, loss = 0.95 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-06-17 17:51:46.875846: step 66100, loss = 1.15 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-06-17 17:52:03.969983: step 66110, loss = 1.10 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-06-17 17:52:16.173245: step 66120, loss = 0.99 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-06-17 17:52:28.370272: step 66130, loss = 1.05 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-06-17 17:52:40.709190: step 66140, loss = 1.00 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-06-17 17:52:53.018612: step 66150, loss = 1.15 (22.8 examples/sec; 1.319 sec/batch)\n",
      "2019-06-17 17:53:05.260874: step 66160, loss = 1.00 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-06-17 17:53:17.625182: step 66170, loss = 0.95 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-06-17 17:53:30.111993: step 66180, loss = 1.23 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-06-17 17:53:42.382567: step 66190, loss = 1.13 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-06-17 17:53:54.541792: step 66200, loss = 0.95 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-06-17 17:54:11.374911: step 66210, loss = 0.96 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-06-17 17:54:23.598661: step 66220, loss = 1.21 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-06-17 17:54:35.889906: step 66230, loss = 1.07 (25.0 examples/sec; 1.202 sec/batch)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "func = join(deeppath_code,'01_training/xClasses/bazel-bin/inception/imagenet_train' )\n",
    "\n",
    "# The first line in the checkpoint file contains the path to the latest checkpoint file set.\n",
    "with open(join(intermediate_checkpoints,'checkpoint')) as f: \n",
    "    checkpoint_path = f.readline().split('\"')[1]\n",
    "# Maintain a total count of training batches \n",
    "completed = completed_batches(intermediate_checkpoints, pretrained_checkpoints)\n",
    "print('Completed {} batches'.format(completed))\n",
    "\n",
    "# The --train_dir directory is immediately deleted, so save intermediate checkpoints by moving into \n",
    "# pretrained_checkpoints and renaming according to current batches\n",
    "os.rename(intermediate_checkpoints, join(pretrained_checkpoints,'_' + completed))\n",
    "\n",
    "# Change checkpoint_path to point to pretrained_checkpoints\n",
    "checkpoint_path = checkpoint_path.replace('intermediate_checkpoints','pretrained_checkpoints'  + '/_' + completed)\n",
    "os.makedirs(intermediate_checkpoints)      \n",
    "          \n",
    "!python $func \\\n",
    "    --num_gpus=$num_gpus --batch_size=$batch_size --train_dir=$intermediate_checkpoints --data_dir=$trainValid_records \\\n",
    "    --ClassNumber=$class_number --mode=$training_mode --pretrained_model_checkpoint_path=$checkpoint_path \\\n",
    "    --fine_tune=False --initial_learning_rate=$initial_learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "Validation is ideally performed while training is in progress to help decide when sufficient training has been performed. On Jupyter we can't run training and validation concurrently, and so have to stop training in order to validate, then do more training if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bcliffor/jupyter_DeepPATH/eval_all.sh: line 19: \r\n",
      "export CHECKPOINT_PATH=/mnt/disks/deeppath-data/intermediate_checkpoints\r\n",
      "CHECKPOINT_PATH=/mnt/disks/deeppath-data/intermediate_checkpoints\r\n",
      "export OUTPUT_DIR=/mnt/disks/deeppath-data/evaluations\r\n",
      "OUTPUT_DIR=/mnt/disks/deeppath-data/evaluations\r\n",
      "export DATA_DIR=/mnt/disks/deeppath-data/Data/images/TFRecord_TrainValid\r\n",
      "DATA_DIR=/mnt/disks/deeppath-data/Data/images/TFRecord_TrainValid\r\n",
      "export LABEL_FILE=/mnt/disks/deeppath-data/Data/images/sorted/data_labels\r\n",
      "LABEL_FILE=/mnt/disks/deeppath-data/Data/images/sorted/data_labels\r\n",
      "export NC_IMAGENET_EVAL=/home/bcliffor/DeepPATH/DeepPATH_code/s02_testing/xClasses/nc_imagenet_eval.py\r\n",
      "NC_IMAGENET_EVAL=/home/bcliffor/DeepPATH/DeepPATH_code/s02_testing/xClasses/nc_imagenet_eval.py\r\n",
      "export BOOTSTRAP=/home/bcliffor/DeepPATH/DeepPATH_code/s03_postprocessing/v0h_ROC_MultiOutput_BootStrap.py\r\n",
      "BOOTSTRAP=/home/bcliffor/DeepPATH/DeepPATH_code/s03_postprocessing/v0h_ROC_MultiOutput_BootStrap.py\r\n",
      "#export NbClasses=3\r\n",
      "#NbClasses=3\r\n",
      ": No such file or directory\r\n",
      "+ export CHECKPOINT_PATH=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints\r\n",
      "+ CHECKPOINT_PATH=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints\r\n",
      "+ export OUTPUT_DIR=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000\r\n",
      "+ OUTPUT_DIR=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000\r\n",
      "+ export DATA_DIR=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/TFRecord_TrainValid\r\n",
      "+ DATA_DIR=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/TFRecord_TrainValid\r\n",
      "+ export LABEL_FILE=/home/bcliffor/jupyter_DeepPATH/tumor_labels.txt\r\n",
      "+ LABEL_FILE=/home/bcliffor/jupyter_DeepPATH/tumor_labels.txt\r\n",
      "+ export NC_IMAGENET_EVAL=/home/bcliffor/DeepPATH/DeepPATH_code/02_testing/xClasses/nc_imagenet_eval.py\r\n",
      "+ NC_IMAGENET_EVAL=/home/bcliffor/DeepPATH/DeepPATH_code/02_testing/xClasses/nc_imagenet_eval.py\r\n",
      "+ export BOOTSTRAP=/home/bcliffor/DeepPATH/DeepPATH_code/03_postprocessing/0h_ROC_MultiOutput_BootStrap.py\r\n",
      "+ BOOTSTRAP=/home/bcliffor/DeepPATH/DeepPATH_code/03_postprocessing/0h_ROC_MultiOutput_BootStrap.py\r\n",
      "+ export LOG_DIR=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/logs\r\n",
      "+ LOG_DIR=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/logs\r\n",
      "+ export MODE=0_softmax\r\n",
      "+ MODE=0_softmax\r\n",
      "+ declare -i count=-1\r\n",
      "+ export NbClasses=3\r\n",
      "+ NbClasses=3\r\n",
      "+ mkdir -p /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/tmp_checkpoints\r\n",
      "+ export CUR_CHECKPOINT=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/tmp_checkpoints\r\n",
      "+ CUR_CHECKPOINT=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/tmp_checkpoints\r\n",
      "+ declare -i step=5000\r\n",
      "+ ((  count == -1  ))\r\n",
      "+ echo 'Determining max checkpoint'\r\n",
      "Determining max checkpoint\r\n",
      "+ (( count = 0 ))\r\n",
      "+ (( max_count = 0 ))\r\n",
      "+ true\r\n",
      "+ '[' -f /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-0.meta ']'\r\n",
      "+ echo /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-0.meta ' exists'\r\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-0.meta  exists\r\n",
      "+ (( count = 0 ))\r\n",
      "+ (( max_count = max_count + step ))\r\n",
      "+ true\r\n",
      "+ '[' -f /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-5000.meta ']'\r\n",
      "+ echo /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-5000.meta ' exists'\r\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-5000.meta  exists\r\n",
      "+ (( count = 5000 ))\r\n",
      "+ (( max_count = max_count + step ))\r\n",
      "+ true\r\n",
      "+ '[' -f /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-10000.meta ']'\r\n",
      "+ echo /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-10000.meta ' exists'\r\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-10000.meta  exists\r\n",
      "+ (( count = 10000 ))\r\n",
      "+ (( max_count = max_count + step ))\r\n",
      "+ true\r\n",
      "+ '[' -f /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-15000.meta ']'\r\n",
      "+ echo /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-15000.meta ' exists'\r\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-15000.meta  exists\r\n",
      "+ (( count = 15000 ))\r\n",
      "+ (( max_count = max_count + step ))\r\n",
      "+ true\r\n",
      "+ '[' -f /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-20000.meta ']'\r\n",
      "+ echo /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-20000.meta ' exists'\r\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-20000.meta  exists\r\n",
      "+ (( count = 20000 ))\r\n",
      "+ (( max_count = max_count + step ))\r\n",
      "+ true\r\n",
      "+ '[' -f /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-25000.meta ']'\r\n",
      "+ echo /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-25000.meta ' exists'\r\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-25000.meta  exists\r\n",
      "+ (( count = 25000 ))\r\n",
      "+ (( max_count = max_count + step ))\r\n",
      "+ true\r\n",
      "+ '[' -f /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-30000.meta ']'\r\n",
      "+ echo /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-30000.meta ' exists'\r\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-30000.meta  exists\r\n",
      "+ (( count = 30000 ))\r\n",
      "+ (( max_count = max_count + step ))\r\n",
      "+ true\r\n",
      "+ '[' -f /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-35000.meta ']'\r\n",
      "+ echo /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-35000.meta ' exists'\r\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-35000.meta  exists\r\n",
      "+ (( count = 35000 ))\r\n",
      "+ (( max_count = max_count + step ))\r\n",
      "+ true\r\n",
      "+ '[' -f /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-40000.meta ']'\r\n",
      "+ echo /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-40000.meta ' exists'\r\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-40000.meta  exists\r\n",
      "+ (( count = 40000 ))\r\n",
      "+ (( max_count = max_count + step ))\r\n",
      "+ true\r\n",
      "+ '[' -f /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-45000.meta ']'\r\n",
      "+ echo /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-45000.meta ' exists'\r\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-45000.meta  exists\r\n",
      "+ (( count = 45000 ))\r\n",
      "+ (( max_count = max_count + step ))\r\n",
      "+ true\r\n",
      "+ '[' -f /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-50000.meta ']'\r\n",
      "+ echo /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-50000.meta ' exists'\r\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-50000.meta  exists\r\n",
      "+ (( count = 50000 ))\r\n",
      "+ (( max_count = max_count + step ))\r\n",
      "+ true\r\n",
      "+ '[' -f /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-55000.meta ']'\r\n",
      "+ echo /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-55000.meta ' exists'\r\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-55000.meta  exists\r\n",
      "+ (( count = 55000 ))\r\n",
      "+ (( max_count = max_count + step ))\r\n",
      "+ true\r\n",
      "+ '[' -f /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-60000.meta ']'\r\n",
      "+ echo /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-60000.meta ' exists'\r\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-60000.meta  exists\r\n",
      "+ (( count = 60000 ))\r\n",
      "+ (( max_count = max_count + step ))\r\n",
      "+ true\r\n",
      "+ '[' -f /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-65000.meta ']'\r\n",
      "+ echo /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-65000.meta ' exists'\r\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-65000.meta  exists\r\n",
      "+ (( count = 65000 ))\r\n",
      "+ (( max_count = max_count + step ))\r\n",
      "+ true\r\n",
      "+ '[' -f /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-70000.meta ']'\r\n",
      "+ break\r\n",
      "+ echo 'count is ' 65000\r\n",
      "count is  65000\r\n",
      "+ true\r\n",
      "+ echo 65000\r\n",
      "65000\r\n",
      "+ '[' -f /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-65000.meta ']'\r\n",
      "+ echo /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-65000.meta ' exists'\r\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-65000.meta  exists\r\n",
      "+ export TEST_OUTPUT=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/test_65000k\r\n",
      "+ TEST_OUTPUT=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/test_65000k\r\n",
      "+ '[' '!' -d /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/test_65000k ']'\r\n",
      "+ mkdir -p /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/test_65000k\r\n",
      "+ ln -s /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-65000.data-00000-of-00001 /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-65000.index /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-65000.meta /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/tmp_checkpoints/.\r\n",
      "+ touch /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/tmp_checkpoints/checkpoint\r\n",
      "+ echo 'model_checkpoint_path: \"/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/tmp_checkpoints/model.ckpt-65000\"'\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ echo 'all_model_checkpoint_paths: \"/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/tmp_checkpoints/model.ckpt-65000\"'\n",
      "+ python /home/bcliffor/DeepPATH/DeepPATH_code/02_testing/xClasses/nc_imagenet_eval.py --checkpoint_dir=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/tmp_checkpoints --eval_dir=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000 --data_dir=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/TFRecord_TrainValid --batch_size 30 --run_once --ImageSet_basename=valid_ --ClassNumber 3 --mode=0_softmax --TVmode=test\n",
      "+ mv /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/out_All_Stats.txt /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/out_filename_Stats.txt /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/test_65000k/.\n",
      "+ export OUTFILENAME=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/test_65000k/out_filename_Stats.txt\n",
      "+ OUTFILENAME=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/test_65000k/out_filename_Stats.txt\n",
      "+ python /home/bcliffor/DeepPATH/DeepPATH_code/03_postprocessing/0h_ROC_MultiOutput_BootStrap.py --file_stats=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/test_65000k/out_filename_Stats.txt --output_dir=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/test_65000k --labels_names=/home/bcliffor/jupyter_DeepPATH/tumor_labels.txt\n",
      "++ expr 65000 + 5000\n",
      "+ count=70000\n",
      "+ true\n",
      "+ echo 70000\n",
      "70000\n",
      "+ '[' -f /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-70000.meta ']'\n",
      "+ echo /mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-70000.meta ' does not exist'\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/intermediate_checkpoints/model.ckpt-70000.meta  does not exist\n",
      "+ break\n",
      "+ for FILE in $OUTPUT_DIR/test_*/out2_roc_data_AvPb_*\n",
      "+ arrFILE=(${FILE//\\// })\n",
      "+ PARTS=out2_roc_data_AvPb_c1auc_0.9987_CIs_0.9969_0.9999_t0.236266.txt\n",
      "+ arrPARTS=(${PARTS//_/ })\n",
      "++ cat /home/bcliffor/jupyter_DeepPATH/tumor_labels.txt\n",
      "+ LABELS='Solid_Tissue_Normal\n",
      "TCGA-LUAD\n",
      "TCGA-LUSC'\n",
      "+ arrLABELS=(${LABELS// / })\n",
      "++ echo c1auc\n",
      "++ sed -e s/auc//\n",
      "++ sed -e 's/^c//'\n",
      "+ LABEL=1\n",
      "+ '[' 1 '!=' macro ']'\n",
      "+ '[' 1 '!=' micro ']'\n",
      "+ LABEL=Solid_Tissue_Normal\n",
      "+ TARGET=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/valid_out2_AvPb_AUCs_Solid_Tissue_Normal.txt\n",
      "+ echo -n '/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/ '\n",
      "+ echo -n 'test_65000k auc '\n",
      "+ sed -e s/test_//\n",
      "+ sed -e s/k//\n",
      "+ '[' Solid_Tissue_Normal = macro ']'\n",
      "+ '[' Solid_Tissue_Normal = micro ']'\n",
      "+ echo -n '0.9987 CIs 0.9969 0.9999 t0.236266.txt'\n",
      "+ sed -e 's/\\.txt//'\n",
      "+ for FILE in $OUTPUT_DIR/test_*/out2_roc_data_AvPb_*\n",
      "+ arrFILE=(${FILE//\\// })\n",
      "+ PARTS=out2_roc_data_AvPb_c2auc_0.9472_CIs_0.9239_0.9685_t0.449129.txt\n",
      "+ arrPARTS=(${PARTS//_/ })\n",
      "++ cat /home/bcliffor/jupyter_DeepPATH/tumor_labels.txt\n",
      "+ LABELS='Solid_Tissue_Normal\n",
      "TCGA-LUAD\n",
      "TCGA-LUSC'\n",
      "+ arrLABELS=(${LABELS// / })\n",
      "++ echo c2auc\n",
      "++ sed -e s/auc//\n",
      "++ sed -e 's/^c//'\n",
      "+ LABEL=2\n",
      "+ '[' 2 '!=' macro ']'\n",
      "+ '[' 2 '!=' micro ']'\n",
      "+ LABEL=TCGA-LUAD\n",
      "+ TARGET=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/valid_out2_AvPb_AUCs_TCGA-LUAD.txt\n",
      "+ echo -n '/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/ '\n",
      "+ echo -n 'test_65000k auc '\n",
      "+ sed -e s/test_//\n",
      "+ sed -e s/k//\n",
      "+ '[' TCGA-LUAD = macro ']'\n",
      "+ '[' TCGA-LUAD = micro ']'\n",
      "+ echo -n '0.9472 CIs 0.9239 0.9685 t0.449129.txt'\n",
      "+ sed -e 's/\\.txt//'\n",
      "+ for FILE in $OUTPUT_DIR/test_*/out2_roc_data_AvPb_*\n",
      "+ arrFILE=(${FILE//\\// })\n",
      "+ PARTS=out2_roc_data_AvPb_c3auc_0.9378_CIs_0.9102_0.9629_t0.476067.txt\n",
      "+ arrPARTS=(${PARTS//_/ })\n",
      "++ cat /home/bcliffor/jupyter_DeepPATH/tumor_labels.txt\n",
      "+ LABELS='Solid_Tissue_Normal\n",
      "TCGA-LUAD\n",
      "TCGA-LUSC'\n",
      "+ arrLABELS=(${LABELS// / })\n",
      "++ echo c3auc\n",
      "++ sed -e s/auc//\n",
      "++ sed -e 's/^c//'\n",
      "+ LABEL=3\n",
      "+ '[' 3 '!=' macro ']'\n",
      "+ '[' 3 '!=' micro ']'\n",
      "+ LABEL=TCGA-LUSC\n",
      "+ TARGET=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/valid_out2_AvPb_AUCs_TCGA-LUSC.txt\n",
      "+ echo -n '/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/ '\n",
      "+ echo -n 'test_65000k auc '\n",
      "+ sed -e s/test_//\n",
      "+ sed -e s/k//\n",
      "+ '[' TCGA-LUSC = macro ']'\n",
      "+ '[' TCGA-LUSC = micro ']'\n",
      "+ echo -n '0.9378 CIs 0.9102 0.9629 t0.476067.txt'\n",
      "+ sed -e 's/\\.txt//'\n",
      "+ for FILE in $OUTPUT_DIR/test_*/out2_roc_data_AvPb_*\n",
      "+ arrFILE=(${FILE//\\// })\n",
      "+ PARTS=out2_roc_data_AvPb_macro_auc_0.9630_CIs_0.9452_0.9770.txt\n",
      "+ arrPARTS=(${PARTS//_/ })\n",
      "++ cat /home/bcliffor/jupyter_DeepPATH/tumor_labels.txt\n",
      "+ LABELS='Solid_Tissue_Normal\n",
      "TCGA-LUAD\n",
      "TCGA-LUSC'\n",
      "+ arrLABELS=(${LABELS// / })\n",
      "++ echo macro\n",
      "++ sed -e s/auc//\n",
      "++ sed -e 's/^c//'\n",
      "+ LABEL=macro\n",
      "+ '[' macro '!=' macro ']'\n",
      "+ TARGET=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/valid_out2_AvPb_AUCs_macro.txt\n",
      "+ echo -n '/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/ '\n",
      "+ echo -n 'test_65000k auc '\n",
      "+ sed -e s/test_//\n",
      "+ sed -e s/k//\n",
      "+ '[' macro = macro ']'\n",
      "+ echo -n '0.9630 CIs 0.9452 0.9770.txt'\n",
      "+ sed -e 's/\\.txt//'\n",
      "+ for FILE in $OUTPUT_DIR/test_*/out2_roc_data_AvPb_*\n",
      "+ arrFILE=(${FILE//\\// })\n",
      "+ PARTS=out2_roc_data_AvPb_micro_auc_0.9653_CIs_0.9550_0.9747.txt\n",
      "+ arrPARTS=(${PARTS//_/ })\n",
      "++ cat /home/bcliffor/jupyter_DeepPATH/tumor_labels.txt\n",
      "+ LABELS='Solid_Tissue_Normal\n",
      "TCGA-LUAD\n",
      "TCGA-LUSC'\n",
      "+ arrLABELS=(${LABELS// / })\n",
      "++ echo micro\n",
      "++ sed -e s/auc//\n",
      "++ sed -e 's/^c//'\n",
      "+ LABEL=micro\n",
      "+ '[' micro '!=' macro ']'\n",
      "+ '[' micro '!=' micro ']'\n",
      "+ TARGET=/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/valid_out2_AvPb_AUCs_micro.txt\n",
      "+ echo -n '/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So3Tu_Sort/Cl3FtTrue_Train/eval_results/_205000/ '\n",
      "+ echo -n 'test_65000k auc '\n",
      "+ sed -e s/test_//\n",
      "+ sed -e s/k//\n",
      "+ '[' micro = macro ']'\n",
      "+ '[' micro = micro ']'\n",
      "+ echo -n '0.9653 CIs 0.9550 0.9747.txt'\n",
      "+ sed -e 's/\\.txt//'\n"
     ]
    }
   ],
   "source": [
    "# Training outputs a checkpoint every 5000 batches. The training_checkpoint parameter to eval_all.sh controls which\n",
    "# checkpoints are validated:\n",
    "#      -1: Validate the maximal checkpoint\n",
    "#      n*5000, n>=0: Validate all checkpoints from n*5000\n",
    "# \n",
    "training_count = -1\n",
    "\n",
    "# Save results to a batch specific directory\n",
    "# Derive completed batches\n",
    "with open(join(intermediate_checkpoints,'checkpoint')) as f: \n",
    "    checkpoint_path = f.readline().split('\"')[1]\n",
    "completed = completed_batches(intermediate_checkpoints, pretrained_checkpoints)#completed = str(batches + int(checkpoint_path.split('-')[-1]))\n",
    "eval_results_batch = join(eval_results,'_' + completed)\n",
    "try:\n",
    "    shutil.rmtree(eval_results_batch)\n",
    "except:\n",
    "    pass\n",
    "os.mkdir(eval_results_batch)\n",
    "\n",
    "\n",
    "eval = join(os.environ[\"HOME\"],'jupyter_DeepPATH','eval_all.sh')\n",
    "!$eval \\\n",
    "    $intermediate_checkpoints $eval_results_batch $trainValid_records $data_labels_path \\\n",
    "    $deeppath_code/02_testing/xClasses/nc_imagenet_eval.py \\\n",
    "    $deeppath_code/03_postprocessing/0h_ROC_MultiOutput_BootStrap.py $training_logs $training_mode $training_count $class_number \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save results to a batch specific directory\n",
    "# Derive completed batches\n",
    "with open(join(intermediate_checkpoints,'checkpoint')) as f: \n",
    "    checkpoint_path = f.readline().split('\"')[1]\n",
    "completed = completed_batches(intermediate_checkpoints, pretrained_checkpoints)\n",
    "#completed = str(batches + int(checkpoint_path.split('-')[-1]))\n",
    "test_results_batch = join(test_results,'_' + completed)\n",
    "try:\n",
    "    shutil.rmtree(test_results_batch)\n",
    "except:\n",
    "    pass\n",
    "os.mkdir(test_results_batch)\n",
    "\n",
    "func = join(deeppath_code,'02_testing/xClasses/nc_imagenet_eval.py' )\n",
    "root = func.rsplit('/',1)[1].split('.')[0]\n",
    "\n",
    "#Redirect output because Jupyter has a limit on test output\n",
    "out_log_file = join(training_logs,root+'.test.out.log')\n",
    "err_log_file = join(training_logs,root+'.test.err.log')\n",
    "\n",
    "!python $func --checkpoint_dir=$intermediate_checkpoints --eval_dir=$test_results_batch \\\n",
    "    --data_dir=$test_records  --batch_size=$batch_size --ImageSet_basename='test_' --run_once --ClassNumber=$class_number \\\n",
    "    --mode=$training_mode --TVmode='test' > $out_log_file 2> $err_log_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze results\n",
    "The following generates heatmaps for specified slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "func = join(deeppath_code,'03_postprocessing/0f_HeatMap_nClasses.py' )\n",
    "root = func.rsplit('/',1)[1].split('.')[0]\n",
    "\n",
    "# Rendered tile size is reduced by the resample_factor\n",
    "resample_factor = 10\n",
    "\n",
    "# The Cmap parameter should be 'CancerType' for tumor classification, or one the mutations,\n",
    "# e.g. 'EGFR', 'STK11', etc. for mutation classification\n",
    "cmap = 'EGFR'\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(heatmaps)\n",
    "except:\n",
    "    pass\n",
    "os.mkdir(heatmaps)\n",
    "\n",
    "try:\n",
    "    os.mkdir(output)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tile_stats = join(test_results, 'out_filename_Stats.txt')\n",
    "!python $func  --image_file=$sorted_tiles --tiles_overlap=$overlap --output_dir=$heatmaps \\\n",
    "    --tiles_stats=$tile_stats --resample_factor=$resample_factor  --filter_tile '' \\\n",
    "    --Cmap=$cmap --tiles_size $tile_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"/home/bcliffor/tmp/j2.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(join(tiling, \n",
    "  'px299/heatmaps/heatmap_CancerType_test_TCGA-NC-A5HR-01A-02-TS2.1B2A21A9-E685-461D-A3FF-42A0D9D7FC23_TCGA-LUSC.jpg'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate ROC curve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:\n",
      "339604\n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "339604\n",
      "[[0.0326477  0.37976137 0.587591  ]\n",
      " [0.1068468  0.64383716 0.249316  ]\n",
      " [0.13589796 0.65300399 0.21109807]\n",
      " ...\n",
      " [0.03229432 0.93459162 0.03311405]\n",
      " [0.0208959  0.96064372 0.01846034]\n",
      " [0.03245703 0.94596064 0.02158233]]\n",
      "339604\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "[0. 0. 0. ... 0. 0. 0.] [0.0326477  0.1068468  0.13589796 ... 0.03229432 0.0208959  0.03245703] [0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.0326477  0.1068468  0.13589796 ... 0.03229432 0.0208959  0.03245703]\n",
      "0.9813095029729171\n",
      "0.9823827515944198\n",
      "Confidence interval for the score: [0.981 - 0.982]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "0.9070283600544368\n",
      "0.9100956549659147\n",
      "Confidence interval for the score: [0.907 - 0.91]\n",
      "[1. 1. 1. ... 1. 1. 1.] [0.37976137 0.64383716 0.65300399 ... 0.93459162 0.96064372 0.94596064] [0. 1. 1. ... 1. 1. 1.]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "[0.37976137 0.64383716 0.65300399 ... 0.93459162 0.96064372 0.94596064]\n",
      "0.8749974113572004\n",
      "0.877326737991325\n",
      "Confidence interval for the score: [0.875 - 0.877]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "[0. 1. 1. ... 1. 1. 1.]\n",
      "0.786454677211057\n",
      "0.789224865566501\n",
      "Confidence interval for the score: [0.786 - 0.789]\n",
      "[0. 0. 0. ... 0. 0. 0.] [0.587591   0.249316   0.21109807 ... 0.03311405 0.01846034 0.02158233] [1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.587591   0.249316   0.21109807 ... 0.03311405 0.01846034 0.02158233]\n",
      "0.8807422236045552\n",
      "0.8829692788694183\n",
      "Confidence interval for the score: [0.881 - 0.883]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "0.797416116437252\n",
      "0.8001433750237064\n",
      "Confidence interval for the score: [0.797 - 0.8]\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0.0326477  0.37976137 0.587591   ... 0.03245703 0.94596064 0.02158233]\n",
      "0.9211393094941268\n",
      "0.9222091642423917\n",
      "Confidence interval for the score: [0.921 - 0.922]\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "[0. 0. 1. ... 0. 1. 0.]\n",
      "0.8305912716757606\n",
      "0.8321565984570952\n",
      "Confidence interval for the score: [0.831 - 0.832]\n",
      "y_ref_PerTile.ravel(), y_score_PcS_PerTile.ravel()\n",
      "[0. 1. 0. ... 0. 1. 0.] [0. 0. 1. ... 0. 1. 0.]\n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "[[0.0326477  0.37976137 0.587591  ]\n",
      " [0.1068468  0.64383716 0.249316  ]\n",
      " [0.13589796 0.65300399 0.21109807]\n",
      " ...\n",
      " [0.03229432 0.93459162 0.03311405]\n",
      " [0.0208959  0.96064372 0.01846034]\n",
      " [0.03245703 0.94596064 0.02158233]]\n",
      "0.9124915802767992\n",
      "0.9140509266360648\n",
      "Confidence interval for the score: [0.912 - 0.914]\n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "0.8306459293983477\n",
      "0.8328122264692599\n",
      "Confidence interval for the score: [0.831 - 0.833]\n",
      "******* FP / TP for average probability\n",
      "{0: array([0.        , 0.        , 0.        , ..., 0.99923111, 0.99923111,\n",
      "       1.        ]), 1: array([0.00000000e+00, 5.12820513e-06, 5.12820513e-06, ...,\n",
      "       9.99979487e-01, 9.99979487e-01, 1.00000000e+00]), 2: array([0.        , 0.        , 0.        , ..., 0.99999003, 0.99999003,\n",
      "       1.        ]), 'macro': array([0.00000000e+00, 3.52700096e-06, 4.98303277e-06, ...,\n",
      "       9.99979487e-01, 9.99990034e-01, 1.00000000e+00]), 'micro': array([0.00000000e+00, 0.00000000e+00, 1.47230304e-06, ...,\n",
      "       9.99991166e-01, 9.99991166e-01, 1.00000000e+00])}\n",
      "{0: array([0.00000000e+00, 1.78326230e-05, 1.60493607e-04, ...,\n",
      "       9.99982167e-01, 1.00000000e+00, 1.00000000e+00]), 1: array([0.00000000e+00, 0.00000000e+00, 1.38308760e-05, ...,\n",
      "       9.99986169e-01, 1.00000000e+00, 1.00000000e+00]), 2: array([0.00000000e+00, 7.19823211e-06, 2.15946963e-05, ...,\n",
      "       9.99992802e-01, 1.00000000e+00, 1.00000000e+00]), 'macro': array([6.06961011e-05, 2.27133916e-04, 2.29533326e-04, ...,\n",
      "       9.99997601e-01, 1.00000000e+00, 1.00000000e+00]), 'micro': array([0.00000000e+00, 2.94460607e-06, 2.94460607e-06, ...,\n",
      "       9.99997055e-01, 1.00000000e+00, 1.00000000e+00])}\n",
      "******* FP / TP for percent selected\n",
      "{0: array([0.        , 0.02296078, 1.        ]), 1: array([0.        , 0.15462051, 1.        ]), 2: array([0.        , 0.19787125, 1.        ]), 'macro': array([0.        , 0.02296078, 0.15462051, 0.19787125, 1.        ]), 'micro': array([0.        , 0.11243978, 1.        ])}\n",
      "{0: array([0.       , 0.8402197, 1.       ]), 1: array([0.        , 0.73027717, 1.        ]), 2: array([0.        , 0.79551982, 1.        ]), 'macro': array([0.       , 0.3469918, 0.7378876, 0.8028067, 1.       ]), 'micro': array([0.        , 0.77512043, 1.        ])}\n",
      "n_classes\n",
      "3\n",
      "[1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "[0.91445028 0.0262476  0.05166126 0.02618167 0.70560961 0.03034344\n",
      " 0.03177299 0.49380027 0.02994601 0.05465468 0.03584457 0.80162356\n",
      " 0.08252511 0.86584079 0.0569804  0.02315094 0.0249296  0.03889579\n",
      " 0.03140031 0.02531793 0.06995785 0.75681237 0.05813095 0.02668228\n",
      " 0.03873479 0.03354801 0.71980474 0.05799005 0.0273576  0.80517876\n",
      " 0.06972693 0.03468034 0.03729197 0.04307577 0.02576836 0.80193551\n",
      " 0.03911436 0.1161481  0.04818234 0.03768662 0.8767029  0.03593808\n",
      " 0.02726033 0.02714807 0.81799852 0.03990153 0.0452359  0.0656359\n",
      " 0.03229409 0.02925313 0.02439583 0.03260097 0.82478713 0.07411166\n",
      " 0.84964795 0.79979781 0.03962313 0.04540945 0.02918069 0.02596875\n",
      " 0.76972793 0.02335639 0.89867346 0.17479524 0.85992619 0.03497694\n",
      " 0.09182787 0.05728224 0.09527239 0.03023213 0.03041606 0.04996097\n",
      " 0.02924233 0.04110754 0.90477906 0.89098558 0.05124745 0.06108195\n",
      " 0.60114554 0.02899822 0.25158358 0.02492372 0.15451362 0.07828535\n",
      " 0.02540989 0.04520133 0.15211521 0.86708715 0.04356077 0.03116613\n",
      " 0.02386249 0.04595065 0.0283117  0.02837899 0.04795036 0.03230515\n",
      " 0.03197383 0.03445342 0.70185539 0.04489825 0.94010068 0.03142962\n",
      " 0.05714528 0.03075438 0.03012269 0.05660126 0.05801468 0.04988986\n",
      " 0.86849604 0.16968533 0.82194655 0.06962655 0.09671297 0.03054811\n",
      " 0.04236912 0.02814824 0.77211843 0.22651054 0.0251612  0.03285424\n",
      " 0.02462442 0.02742358 0.73975573 0.02753451 0.08025476 0.85306641\n",
      " 0.43075584 0.05763527 0.76231598 0.08495854 0.21491527 0.0459938\n",
      " 0.02882199 0.04361522 0.05195256 0.3027376  0.4974879  0.48206519\n",
      " 0.04576326 0.04031706 0.03214053 0.02601457 0.79694333 0.03252883\n",
      " 0.08291878 0.54147447 0.74255613 0.84097194 0.02603131 0.13874136\n",
      " 0.83677685 0.03956719 0.40222136 0.03091806 0.46439573 0.76413496\n",
      " 0.74434461 0.49161866 0.57775429 0.04591663 0.04517663 0.08398841\n",
      " 0.15589739 0.03685856 0.03915855 0.03267077 0.79890261 0.39912727\n",
      " 0.04649407 0.81522039 0.02987068 0.03297595 0.09102228 0.0386282\n",
      " 0.07967682 0.13932094 0.0459764  0.5769421  0.03910379 0.02780899\n",
      " 0.03355583 0.02909204 0.86509026 0.03316095 0.02915376 0.0356029\n",
      " 0.09318169 0.04724894 0.15998368 0.06637093 0.02561683 0.91675064\n",
      " 0.06436597 0.74442879 0.04905239 0.02856826 0.06032985 0.0287189\n",
      " 0.07853759 0.02682361 0.84296559 0.04830858 0.0232741  0.04910497\n",
      " 0.03709877 0.03230381 0.18681716 0.88935423 0.03284425 0.79254289\n",
      " 0.62091095 0.03685417 0.03622314 0.84110183 0.18232283 0.09806442\n",
      " 0.40718899 0.06465693 0.04396077 0.11355797 0.94483532 0.75082913\n",
      " 0.02552869 0.02761223 0.03480826 0.03320546 0.51145925 0.08593741\n",
      " 0.63225121 0.77859423 0.02678997 0.22010179 0.06699434 0.08507886\n",
      " 0.04236672 0.02499829 0.09695996 0.93628149 0.02841997 0.62183267\n",
      " 0.04268431 0.04153094 0.92225689 0.02582473 0.84873365 0.04398258\n",
      " 0.06278113 0.48311983 0.14701993 0.89214009 0.74504787 0.04480109\n",
      " 0.10270733 0.8004177  0.03030624 0.04252528 0.82594586 0.02791524\n",
      " 0.07229533 0.03784113 0.07173557 0.18270746 0.85677854 0.02808505\n",
      " 0.02289974 0.88785603 0.02960292 0.02728406 0.02564244 0.67513135\n",
      " 0.30170938 0.02995089 0.04688904 0.0342048  0.02703589 0.02567904\n",
      " 0.94649523 0.05522002 0.02998997 0.09443744 0.02543318 0.02788754\n",
      " 0.03176587 0.87502302 0.10164851 0.6605302  0.07298672 0.56846394\n",
      " 0.04005459 0.02937327 0.02686215 0.05000685 0.03227471 0.03329871\n",
      " 0.85916049 0.06400444 0.93658808 0.03165479 0.16801963 0.02766751\n",
      " 0.83205843 0.02279164 0.86028819 0.03478557 0.04508986 0.02742652\n",
      " 0.89147836 0.02856213 0.02405348 0.11486118 0.9141155  0.64556616\n",
      " 0.03428219 0.02810057 0.06817099 0.83103621 0.56961582 0.03322972\n",
      " 0.90228896 0.0364014  0.05076919 0.85335161 0.02692934 0.03045681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999999\n",
      "1.0\n",
      "Confidence interval for the score: [1.000 - 1.0]\n",
      "[1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "[1.00000000e+00 0.00000000e+00 9.25925926e-03 0.00000000e+00\n",
      " 8.10773481e-01 1.31061599e-03 0.00000000e+00 5.41025641e-01\n",
      " 1.98609732e-03 3.16154971e-02 3.95647873e-03 8.95364238e-01\n",
      " 5.06756757e-02 9.44649446e-01 1.06635071e-02 0.00000000e+00\n",
      " 0.00000000e+00 4.98960499e-03 0.00000000e+00 2.18340611e-03\n",
      " 3.78787879e-02 8.84097035e-01 3.01910043e-02 0.00000000e+00\n",
      " 8.19672131e-03 7.28155340e-03 8.52097130e-01 3.32749562e-02\n",
      " 1.13895216e-03 9.06976744e-01 3.34928230e-02 4.86618005e-03\n",
      " 1.74927114e-02 8.36820084e-03 0.00000000e+00 8.88809182e-01\n",
      " 2.45098039e-03 7.69230769e-02 1.82926829e-02 5.76759966e-03\n",
      " 9.68000000e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 9.01408451e-01 8.26019618e-03 1.09890110e-02 2.81622912e-02\n",
      " 5.55941626e-03 1.52671756e-03 0.00000000e+00 0.00000000e+00\n",
      " 9.15074310e-01 3.21839080e-02 9.67283951e-01 9.34426230e-01\n",
      " 8.34151129e-03 8.11359026e-03 2.00000000e-03 0.00000000e+00\n",
      " 9.07801418e-01 0.00000000e+00 9.78609626e-01 1.83035714e-01\n",
      " 9.63503650e-01 3.80228137e-03 6.43274854e-02 2.14652357e-02\n",
      " 8.35762877e-02 0.00000000e+00 3.90625000e-03 1.63934426e-02\n",
      " 0.00000000e+00 1.42226582e-02 9.77664975e-01 9.66269841e-01\n",
      " 1.51006711e-02 3.18664643e-02 6.96969697e-01 1.69635284e-03\n",
      " 2.27544910e-01 0.00000000e+00 1.23119557e-01 3.69003690e-02\n",
      " 0.00000000e+00 1.29533679e-02 1.24933546e-01 9.85714286e-01\n",
      " 1.60069219e-02 4.21496312e-03 0.00000000e+00 1.26582278e-02\n",
      " 1.86567164e-03 1.11172874e-03 1.43084261e-02 3.81679389e-03\n",
      " 0.00000000e+00 4.24929178e-03 8.26869806e-01 1.87188020e-02\n",
      " 1.00000000e+00 2.52206810e-03 2.22222222e-02 1.85988841e-03\n",
      " 1.53846154e-03 1.91176471e-02 1.53482881e-02 0.00000000e+00\n",
      " 9.81617647e-01 1.55875300e-01 9.06525573e-01 2.07373272e-02\n",
      " 7.69230769e-02 2.94117647e-03 1.30500759e-02 0.00000000e+00\n",
      " 9.21933086e-01 2.20279720e-01 0.00000000e+00 6.55737705e-03\n",
      " 0.00000000e+00 4.09836066e-03 8.60428232e-01 0.00000000e+00\n",
      " 3.59322034e-02 9.47963801e-01 4.74803150e-01 3.96853772e-02\n",
      " 8.97674419e-01 5.16304348e-02 2.16450216e-01 1.23203285e-02\n",
      " 0.00000000e+00 1.71796707e-02 2.03045685e-02 3.10077519e-01\n",
      " 5.49407115e-01 5.62231760e-01 1.78571429e-02 1.09689214e-02\n",
      " 8.84955752e-03 0.00000000e+00 9.00481541e-01 4.11985019e-03\n",
      " 5.38827258e-02 5.86505190e-01 8.60421836e-01 9.51704545e-01\n",
      " 4.20875421e-04 1.23348018e-01 9.71052632e-01 7.53138075e-03\n",
      " 4.40298507e-01 1.81159420e-03 4.80996622e-01 8.80368098e-01\n",
      " 8.96103896e-01 6.13636364e-01 6.95652174e-01 1.70519136e-02\n",
      " 5.41516245e-03 3.61445783e-02 1.40482786e-01 6.49350649e-03\n",
      " 9.72590628e-03 5.08474576e-03 8.75675676e-01 3.95161290e-01\n",
      " 1.77321512e-02 9.30080117e-01 0.00000000e+00 1.97889182e-03\n",
      " 2.81690141e-02 1.47814910e-02 3.58649789e-02 1.20381175e-01\n",
      " 0.00000000e+00 6.87224670e-01 5.18518519e-03 0.00000000e+00\n",
      " 7.52823087e-03 0.00000000e+00 9.65736041e-01 4.35729847e-03\n",
      " 8.41042893e-04 0.00000000e+00 7.14514579e-02 9.07029478e-03\n",
      " 1.13043478e-01 3.37078652e-02 0.00000000e+00 9.87421384e-01\n",
      " 5.76923077e-02 8.72340426e-01 1.38888889e-02 0.00000000e+00\n",
      " 2.85714286e-02 0.00000000e+00 6.35930048e-02 0.00000000e+00\n",
      " 9.26613616e-01 2.46710526e-02 0.00000000e+00 1.76358437e-02\n",
      " 2.84090909e-03 1.54679041e-03 2.06140351e-01 9.74683544e-01\n",
      " 3.78787879e-03 8.99503722e-01 6.92406692e-01 1.03896104e-02\n",
      " 4.72334683e-03 9.60431655e-01 1.24528302e-01 5.30035336e-02\n",
      " 4.57109283e-01 3.55029586e-02 8.54700855e-03 5.88235294e-02\n",
      " 1.00000000e+00 8.09230769e-01 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 6.66666667e-01 4.51127820e-02\n",
      " 7.71929825e-01 9.29460581e-01 3.20718409e-04 2.09976798e-01\n",
      " 3.23886640e-02 6.55586334e-02 1.43369176e-02 0.00000000e+00\n",
      " 7.27603456e-02 9.94290375e-01 1.76056338e-03 7.40492170e-01\n",
      " 3.87596899e-03 9.30232558e-03 9.86394558e-01 0.00000000e+00\n",
      " 9.73684211e-01 9.61538462e-03 1.93548387e-02 5.50387597e-01\n",
      " 1.15000000e-01 9.70088375e-01 8.71657754e-01 1.91082803e-02\n",
      " 7.31995277e-02 9.32467532e-01 1.88679245e-03 1.50722855e-02\n",
      " 9.21618205e-01 1.23380629e-03 5.02890173e-02 1.13636364e-02\n",
      " 3.04709141e-02 1.64447018e-01 9.69339623e-01 1.27713921e-03\n",
      " 0.00000000e+00 9.72776770e-01 2.14362272e-03 1.21506683e-03\n",
      " 0.00000000e+00 8.05555556e-01 2.94117647e-01 9.41176471e-04\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 1.00000000e+00 2.02788340e-02 0.00000000e+00 6.23556582e-02\n",
      " 0.00000000e+00 1.96078431e-03 9.38086304e-04 9.85294118e-01\n",
      " 6.41304348e-02 7.18253968e-01 4.37453920e-02 6.94684796e-01\n",
      " 1.72413793e-02 0.00000000e+00 0.00000000e+00 1.85606061e-02\n",
      " 0.00000000e+00 2.52525253e-03 9.63488844e-01 2.83464567e-02\n",
      " 9.90407674e-01 0.00000000e+00 1.56118143e-01 0.00000000e+00\n",
      " 9.39393939e-01 0.00000000e+00 9.84984985e-01 5.58659218e-03\n",
      " 1.07604017e-02 0.00000000e+00 9.88800000e-01 0.00000000e+00\n",
      " 0.00000000e+00 8.27300931e-02 9.80295567e-01 7.13286713e-01\n",
      " 7.50469043e-03 0.00000000e+00 3.89721627e-02 9.54063604e-01\n",
      " 6.70533643e-01 2.09205021e-03 9.94565217e-01 1.19250426e-02\n",
      " 2.30891720e-02 9.56834532e-01 0.00000000e+00 3.06748466e-03]\n",
      "0.9999999999999999\n",
      "1.0\n",
      "Confidence interval for the score: [1.000 - 1.0]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0.\n",
      " 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0.\n",
      " 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0.04886252 0.10568217 0.05741269 0.49101902 0.19464723 0.58402177\n",
      " 0.40660365 0.38303278 0.18790069 0.18647267 0.77071379 0.03418564\n",
      " 0.66751187 0.05038391 0.82773451 0.10700707 0.02544218 0.49225044\n",
      " 0.61860577 0.83920374 0.47160618 0.09283896 0.10537682 0.11474439\n",
      " 0.31795556 0.29516608 0.1642911  0.68726527 0.14113261 0.11931771\n",
      " 0.70360539 0.18952728 0.18508431 0.18983529 0.17313489 0.06386019\n",
      " 0.83486602 0.15063577 0.25789632 0.74116443 0.07097742 0.49604911\n",
      " 0.09749898 0.08986229 0.05573988 0.14029298 0.78050218 0.39209758\n",
      " 0.24451973 0.7403208  0.05179185 0.14035351 0.08508662 0.57562707\n",
      " 0.09206674 0.10059418 0.04930693 0.73540469 0.30298424 0.2432788\n",
      " 0.1070894  0.63957118 0.0626523  0.72945824 0.05320522 0.4103775\n",
      " 0.16295291 0.46660501 0.69077099 0.85087538 0.69589436 0.32916028\n",
      " 0.35253446 0.06088325 0.04144157 0.06480391 0.53656396 0.38729758\n",
      " 0.17916129 0.20312612 0.51709348 0.5087932  0.61649768 0.63396284\n",
      " 0.43281843 0.55495497 0.63321879 0.06940963 0.7525395  0.66871997\n",
      " 0.74255241 0.7761024  0.5314711  0.2007702  0.17716501 0.2762995\n",
      " 0.26181066 0.72141118 0.12980924 0.16675603 0.03066437 0.09830208\n",
      " 0.56791773 0.52641733 0.21543843 0.77270379 0.54477257 0.53838651\n",
      " 0.06773384 0.28819876 0.10778613 0.52990091 0.40215679 0.03891654\n",
      " 0.7311255  0.24800791 0.11965832 0.16421376 0.74296183 0.11844576\n",
      " 0.12116917 0.59263659 0.14853404 0.25302339 0.54151413 0.07041996\n",
      " 0.33518984 0.42530106 0.14981584 0.70151988 0.14636121 0.81384078\n",
      " 0.29110734 0.46142992 0.18443599 0.53334537 0.33847649 0.14677779\n",
      " 0.26590097 0.58771656 0.90950719 0.32454545 0.07140167 0.04966747\n",
      " 0.78486031 0.40808919 0.09779414 0.06795336 0.29834637 0.04289669\n",
      " 0.07637733 0.53078181 0.3987908  0.6504137  0.04539192 0.0627146\n",
      " 0.16560639 0.33207792 0.24800316 0.10604549 0.19898941 0.55649818\n",
      " 0.68514087 0.07086399 0.28129738 0.76959106 0.08110195 0.06634391\n",
      " 0.86626258 0.07689723 0.04195344 0.09088696 0.76581317 0.63018342\n",
      " 0.77347203 0.69661201 0.64478755 0.17493689 0.06895454 0.86492005\n",
      " 0.53946561 0.62754909 0.06075611 0.16970704 0.53603651 0.90830438\n",
      " 0.71771225 0.19557519 0.42889773 0.3463142  0.13649428 0.04256332\n",
      " 0.82096692 0.13266872 0.8529126  0.73258312 0.78194233 0.24109181\n",
      " 0.77651714 0.25441925 0.04384567 0.32804297 0.31194414 0.40902292\n",
      " 0.88036909 0.20370213 0.4681796  0.04455198 0.72650892 0.12987376\n",
      " 0.32249997 0.47770535 0.71062302 0.06393794 0.43383735 0.52519564\n",
      " 0.36273124 0.69024744 0.59598758 0.23548911 0.03133411 0.15380452\n",
      " 0.89460587 0.76551315 0.30205585 0.04688907 0.3293878  0.52033346\n",
      " 0.2541392  0.11643653 0.21681732 0.43891709 0.6920857  0.84853426\n",
      " 0.67604918 0.67301858 0.86192928 0.03073825 0.93226402 0.19201889\n",
      " 0.82818827 0.53503555 0.03620477 0.44848593 0.07147484 0.33370721\n",
      " 0.66834611 0.14527203 0.71781838 0.07246439 0.14585995 0.86333852\n",
      " 0.76424013 0.09255905 0.28591667 0.848396   0.03489325 0.2843725\n",
      " 0.67034944 0.28148777 0.72948506 0.59984559 0.07092364 0.50074557\n",
      " 0.89851209 0.05805122 0.75186325 0.15658848 0.28713388 0.18134928\n",
      " 0.51203759 0.27109077 0.40259087 0.04755881 0.07938923 0.34993362\n",
      " 0.02811474 0.83121133 0.5025974  0.74449152 0.2165295  0.47633825\n",
      " 0.2503223  0.06490404 0.30288714 0.08307778 0.74715589 0.29237975\n",
      " 0.79257402 0.20647469 0.33700059 0.06722871 0.35076337 0.50742856\n",
      " 0.06883296 0.83980373 0.03893233 0.16211785 0.74131443 0.59489532\n",
      " 0.09097067 0.86742378 0.07338341 0.31716767 0.22947082 0.07237438\n",
      " 0.0568829  0.71884356 0.13474316 0.53023877 0.05087362 0.20441231\n",
      " 0.74908661 0.17099184 0.25966855 0.0936958  0.18840309 0.14456373\n",
      " 0.05643742 0.15049076 0.11433402 0.06040156 0.61694557 0.09875246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9703212148353214\n",
      "0.9917162276975361\n",
      "Confidence interval for the score: [0.970 - 0.992]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0.\n",
      " 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0.\n",
      " 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0.00000000e+00 3.69290573e-02 1.85185185e-02 4.81781377e-01\n",
      " 1.47790055e-01 6.67103539e-01 3.45153664e-01 4.12820513e-01\n",
      " 1.19165839e-01 1.39437135e-01 8.61523244e-01 1.32450331e-03\n",
      " 7.26351351e-01 1.66051661e-02 9.07582938e-01 4.68150422e-02\n",
      " 6.58761528e-04 5.13929314e-01 6.74378749e-01 9.43231441e-01\n",
      " 5.26515152e-01 1.88679245e-02 4.06654344e-02 6.41102457e-02\n",
      " 2.49414520e-01 2.30582524e-01 1.13318617e-01 7.40805604e-01\n",
      " 8.31435080e-02 7.20930233e-02 7.70334928e-01 1.11922141e-01\n",
      " 1.04956268e-01 1.12040911e-01 1.12338858e-01 7.89096126e-03\n",
      " 8.94607843e-01 8.20512821e-02 2.18292683e-01 8.02714165e-01\n",
      " 2.40000000e-02 5.20888889e-01 4.21729807e-02 4.09126672e-02\n",
      " 2.81690141e-02 4.64636035e-02 8.59340659e-01 3.64677804e-01\n",
      " 1.61917999e-01 8.35114504e-01 1.94174757e-02 3.92609700e-02\n",
      " 4.03397028e-02 5.93103448e-01 2.71604938e-02 2.45901639e-02\n",
      " 1.91364082e-02 8.66125761e-01 2.26000000e-01 2.13429257e-01\n",
      " 4.96453901e-02 7.23893805e-01 1.87165775e-02 7.63392857e-01\n",
      " 0.00000000e+00 4.06021992e-01 9.69785575e-02 4.76901540e-01\n",
      " 7.49271137e-01 9.49432892e-01 7.69531250e-01 2.88056206e-01\n",
      " 3.32914573e-01 1.91270231e-02 4.06091371e-03 1.98412698e-02\n",
      " 5.80536913e-01 3.68740516e-01 1.41414141e-01 1.46734521e-01\n",
      " 5.68862275e-01 5.33407572e-01 7.01108472e-01 7.19557196e-01\n",
      " 4.13309982e-01 5.99417098e-01 7.39500266e-01 1.42857143e-02\n",
      " 8.17001947e-01 7.65015806e-01 8.26542491e-01 8.84493671e-01\n",
      " 5.41044776e-01 1.40633685e-01 1.04928458e-01 2.08015267e-01\n",
      " 2.17054264e-01 7.80453258e-01 6.78670360e-02 1.26039933e-01\n",
      " 0.00000000e+00 4.72887768e-02 6.31372549e-01 5.45567266e-01\n",
      " 1.83076923e-01 8.49019608e-01 6.12750885e-01 5.36585366e-01\n",
      " 7.35294118e-03 2.13429257e-01 7.05467372e-02 5.66820276e-01\n",
      " 4.48717949e-01 0.00000000e+00 8.30652504e-01 1.52802893e-01\n",
      " 3.34572491e-02 8.39160839e-02 7.96324655e-01 6.06557377e-02\n",
      " 5.93379138e-02 6.72131148e-01 8.56463125e-02 1.93589744e-01\n",
      " 6.21694915e-01 2.71493213e-02 3.20472441e-01 4.12942438e-01\n",
      " 8.37209302e-02 8.15217391e-01 7.79220779e-02 9.41478439e-01\n",
      " 2.54814815e-01 4.49534717e-01 1.16751269e-01 5.94315245e-01\n",
      " 3.22134387e-01 5.36480687e-02 2.04761905e-01 6.50822669e-01\n",
      " 9.79351032e-01 2.73037543e-01 1.44462279e-02 8.98876404e-03\n",
      " 8.63708399e-01 4.10034602e-01 3.41191067e-02 1.70454545e-02\n",
      " 2.36111111e-01 5.28634361e-03 7.89473684e-03 5.53974895e-01\n",
      " 4.45273632e-01 7.30978261e-01 7.17905405e-03 6.13496933e-03\n",
      " 7.79220779e-02 2.95454545e-01 1.73913043e-01 4.81242895e-02\n",
      " 1.19133574e-01 6.02409639e-01 7.36842105e-01 5.56586271e-03\n",
      " 2.49336870e-01 8.49152542e-01 5.13513514e-02 8.06451613e-03\n",
      " 9.31871209e-01 2.18499636e-02 0.00000000e+00 2.70448549e-02\n",
      " 9.01408451e-01 7.05655527e-01 8.66033755e-01 7.79463244e-01\n",
      " 6.90909091e-01 9.69162996e-02 2.85185185e-02 9.22330097e-01\n",
      " 5.63362610e-01 7.20000000e-01 1.64974619e-02 6.82643428e-02\n",
      " 5.99543434e-01 9.85915493e-01 8.11598847e-01 1.13378685e-01\n",
      " 4.43478261e-01 2.80898876e-01 5.81039755e-02 1.25786164e-02\n",
      " 9.42307692e-01 7.44680851e-02 9.31818182e-01 8.25852783e-01\n",
      " 9.20634921e-01 1.88524590e-01 8.39427663e-01 1.83874140e-01\n",
      " 1.76834660e-03 2.67269737e-01 2.96296296e-01 4.08484271e-01\n",
      " 9.74431818e-01 1.26836814e-01 5.00000000e-01 6.32911392e-03\n",
      " 8.03030303e-01 8.56079404e-02 2.96010296e-01 4.82251082e-01\n",
      " 7.75641026e-01 3.59712230e-03 4.83018868e-01 5.54770318e-01\n",
      " 3.60752056e-01 7.98816568e-01 6.35327635e-01 1.07843137e-01\n",
      " 0.00000000e+00 1.29230769e-01 9.66216216e-01 8.27450980e-01\n",
      " 2.82258065e-01 4.32900433e-03 3.33333333e-01 5.31328321e-01\n",
      " 1.57894737e-01 2.90456432e-02 1.35663887e-01 4.75638051e-01\n",
      " 8.13765182e-01 8.98430286e-01 7.63440860e-01 7.36777368e-01\n",
      " 9.23146885e-01 2.03915171e-03 9.98239437e-01 1.36465324e-01\n",
      " 9.27648579e-01 5.73023256e-01 4.53514739e-03 4.27118644e-01\n",
      " 8.77192982e-03 3.41346154e-01 7.16129032e-01 6.20155039e-02\n",
      " 8.18000000e-01 2.71923861e-02 9.62566845e-02 9.29228592e-01\n",
      " 8.47697757e-01 2.33766234e-02 2.47169811e-01 9.14487850e-01\n",
      " 1.26422250e-03 1.94941394e-01 7.33526012e-01 2.64462810e-01\n",
      " 8.39335180e-01 6.60104227e-01 1.17924528e-02 5.16815666e-01\n",
      " 9.82022472e-01 1.63339383e-02 8.45659164e-01 7.41190765e-02\n",
      " 1.73469388e-01 1.16666667e-01 6.47058824e-01 2.07058824e-01\n",
      " 5.00000000e-01 0.00000000e+00 2.70700637e-02 3.23432343e-01\n",
      " 0.00000000e+00 9.48035488e-01 5.35053554e-01 8.24480370e-01\n",
      " 1.49273448e-01 4.98039216e-01 1.97936210e-01 7.35294118e-03\n",
      " 2.71739130e-01 3.25396825e-02 8.48857213e-01 2.75648949e-01\n",
      " 8.87931034e-01 1.27873563e-01 3.12460864e-01 2.31060606e-02\n",
      " 3.52601156e-01 4.87373737e-01 1.62271805e-02 9.18110236e-01\n",
      " 9.59232614e-03 9.97566910e-02 7.72151899e-01 5.85514834e-01\n",
      " 3.03030303e-02 9.60244648e-01 6.00600601e-03 2.92364991e-01\n",
      " 1.57819225e-01 3.96912900e-02 6.40000000e-03 8.21752266e-01\n",
      " 7.35294118e-02 5.73940021e-01 1.47783251e-02 1.81818182e-01\n",
      " 8.32551595e-01 9.26829268e-02 1.98286938e-01 3.18021201e-02\n",
      " 1.14849188e-01 5.54393305e-02 5.43478261e-03 9.02896082e-02\n",
      " 3.42356688e-02 3.59712230e-03 7.28222997e-01 4.29447853e-02]\n",
      "0.9669312169312169\n",
      "0.9906870778208561\n",
      "Confidence interval for the score: [0.967 - 0.991]\n",
      "[0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1.\n",
      " 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0.\n",
      " 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "[0.0366872  0.86807023 0.89092605 0.48279931 0.09974316 0.38563479\n",
      " 0.56162336 0.12316695 0.7821533  0.75887265 0.19344164 0.1641908\n",
      " 0.24996302 0.0837753  0.11528509 0.86984199 0.94962822 0.46885377\n",
      " 0.34999392 0.13547833 0.45843597 0.15034867 0.83649223 0.85857333\n",
      " 0.64330965 0.67128591 0.11590417 0.25474467 0.83150978 0.07550353\n",
      " 0.22666768 0.77579238 0.77762371 0.76708895 0.80109675 0.1342043\n",
      " 0.12601962 0.73321612 0.69392134 0.22114895 0.05231968 0.46801281\n",
      " 0.87524069 0.88298963 0.12626159 0.81980548 0.17426192 0.54226652\n",
      " 0.72318617 0.23042607 0.92381232 0.82704551 0.09012625 0.35026127\n",
      " 0.05828531 0.09960801 0.91106994 0.21918586 0.66783507 0.73075244\n",
      " 0.12318268 0.33707243 0.03867424 0.09574652 0.08686859 0.55464555\n",
      " 0.74521922 0.47611274 0.21395662 0.11889249 0.27368958 0.62087875\n",
      " 0.61822322 0.89800922 0.05377937 0.04421051 0.41218859 0.55162048\n",
      " 0.21969318 0.76787566 0.23132295 0.46628308 0.22898871 0.28775182\n",
      " 0.54177168 0.3998437  0.21466599 0.06350321 0.20389972 0.3001139\n",
      " 0.2335851  0.17794695 0.4402172  0.77085081 0.77488463 0.69139535\n",
      " 0.70621551 0.24413539 0.16833537 0.78834572 0.02923495 0.8702683\n",
      " 0.37493699 0.44282829 0.75443888 0.17069495 0.39721275 0.41172363\n",
      " 0.06377012 0.54211591 0.07026732 0.40047253 0.50113023 0.93053535\n",
      " 0.22650538 0.72384384 0.10822325 0.6092757  0.23187697 0.8487\n",
      " 0.85420641 0.37993983 0.11171023 0.7194421  0.3782311  0.07651364\n",
      " 0.23405432 0.51706367 0.08786818 0.21352158 0.63872351 0.14016541\n",
      " 0.68007067 0.49495486 0.76361145 0.16391702 0.16403561 0.37115702\n",
      " 0.68833577 0.37196638 0.05835227 0.64943998 0.131655   0.9178037\n",
      " 0.1322209  0.05043634 0.15964973 0.0910747  0.67562232 0.81836195\n",
      " 0.08684581 0.429651   0.19898783 0.31866825 0.49021235 0.17315045\n",
      " 0.090049   0.17630341 0.17424255 0.84803787 0.75583396 0.35951342\n",
      " 0.15896174 0.89227745 0.67954408 0.19773817 0.11999544 0.53452882\n",
      " 0.08724335 0.10788238 0.92817588 0.87613709 0.14316455 0.33118837\n",
      " 0.14685115 0.16406705 0.30923605 0.24812101 0.89194167 0.10727095\n",
      " 0.42697856 0.34335887 0.07415364 0.79713201 0.43480973 0.05609272\n",
      " 0.18910606 0.75717588 0.41111859 0.58731487 0.83788888 0.04068605\n",
      " 0.11466709 0.12290249 0.09803501 0.23884862 0.15772783 0.73018929\n",
      " 0.14494527 0.71875714 0.11318874 0.62364845 0.66478175 0.54187211\n",
      " 0.08253214 0.76399405 0.34500324 0.06609379 0.24064683 0.07758335\n",
      " 0.05658908 0.48544049 0.25315384 0.09496022 0.38383982 0.37673993\n",
      " 0.23007977 0.24509563 0.36005165 0.65095291 0.02383057 0.09536635\n",
      " 0.07986544 0.20687462 0.66313589 0.91990547 0.15915296 0.39372913\n",
      " 0.11360959 0.10496923 0.75639271 0.34098112 0.24091996 0.06638689\n",
      " 0.2815841  0.30198313 0.04111076 0.03298026 0.03931601 0.18614844\n",
      " 0.12912742 0.42343351 0.04153834 0.52568934 0.07979151 0.62231021\n",
      " 0.26887276 0.37160813 0.13516169 0.03539552 0.10909219 0.09186039\n",
      " 0.13305255 0.10702325 0.68377709 0.10907872 0.13916089 0.68771226\n",
      " 0.25735523 0.6806711  0.19877937 0.21744695 0.07229782 0.47116938\n",
      " 0.07858817 0.05409275 0.21853383 0.81612746 0.68722369 0.14351936\n",
      " 0.18625301 0.69895834 0.55052008 0.91823639 0.89357488 0.62438734\n",
      " 0.02539003 0.11356864 0.46741263 0.16107104 0.75803732 0.49577421\n",
      " 0.71791183 0.06007293 0.59546435 0.25639202 0.17985739 0.1391563\n",
      " 0.16737139 0.76415204 0.63613727 0.88276444 0.61696192 0.45927273\n",
      " 0.07200654 0.09619183 0.02447959 0.80622736 0.09066595 0.37743717\n",
      " 0.0769709  0.10978458 0.0663284  0.64804675 0.72543933 0.9001991\n",
      " 0.05163874 0.25259432 0.84120336 0.35490006 0.03501088 0.15002152\n",
      " 0.21663119 0.8009076  0.67216046 0.07526799 0.24198109 0.82220655\n",
      " 0.04127362 0.81310784 0.8348968  0.08624683 0.3561251  0.87079073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9713611859838275\n",
      "0.9920317113618897\n",
      "Confidence interval for the score: [0.971 - 0.992]\n",
      "[0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1.\n",
      " 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0.\n",
      " 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "[0.         0.96307094 0.97222222 0.51821862 0.04143646 0.33158585\n",
      " 0.65484634 0.04615385 0.87884806 0.82894737 0.13452028 0.10331126\n",
      " 0.22297297 0.03874539 0.08175355 0.95318496 0.99934124 0.48108108\n",
      " 0.32562125 0.05458515 0.43560606 0.09703504 0.92914356 0.93588975\n",
      " 0.74238876 0.76213592 0.03458425 0.22591944 0.91571754 0.02093023\n",
      " 0.19617225 0.88321168 0.87755102 0.87959089 0.88766114 0.10329986\n",
      " 0.10294118 0.84102564 0.76341463 0.19151824 0.008      0.47911111\n",
      " 0.95782702 0.95908733 0.07042254 0.9452762  0.12967033 0.6071599\n",
      " 0.83252259 0.16335878 0.98058252 0.96073903 0.04458599 0.37471264\n",
      " 0.00555556 0.04098361 0.97252208 0.12576065 0.772      0.78657074\n",
      " 0.04255319 0.27610619 0.0026738  0.05357143 0.03649635 0.59017573\n",
      " 0.83869396 0.50163322 0.16715258 0.05056711 0.2265625  0.69555035\n",
      " 0.66708543 0.96665032 0.01827411 0.01388889 0.40436242 0.59939302\n",
      " 0.16161616 0.85156913 0.20359281 0.46659243 0.17577197 0.24354244\n",
      " 0.58669002 0.38762953 0.13556619 0.         0.16699113 0.23076923\n",
      " 0.17345751 0.1028481  0.45708955 0.85825459 0.88076312 0.78816794\n",
      " 0.78294574 0.21529745 0.10526316 0.85524126 0.         0.95018916\n",
      " 0.34640523 0.45257285 0.81538462 0.13186275 0.37190083 0.46341463\n",
      " 0.01102941 0.63069544 0.02292769 0.4124424  0.47435897 0.99705882\n",
      " 0.15629742 0.84719711 0.04460967 0.6958042  0.20367534 0.93278689\n",
      " 0.94066209 0.32377049 0.05392546 0.80641026 0.34237288 0.02488688\n",
      " 0.20472441 0.54737218 0.01860465 0.13315217 0.70562771 0.04620123\n",
      " 0.74518519 0.53328561 0.86294416 0.09560724 0.1284585  0.38412017\n",
      " 0.77738095 0.33820841 0.01179941 0.72696246 0.08507223 0.98689139\n",
      " 0.08240887 0.00346021 0.10545906 0.03125    0.76346801 0.87136564\n",
      " 0.02105263 0.43849372 0.11442786 0.26721014 0.51182432 0.11349693\n",
      " 0.02597403 0.09090909 0.13043478 0.9348238  0.87545126 0.36144578\n",
      " 0.12267511 0.98794063 0.74093722 0.14576271 0.07297297 0.59677419\n",
      " 0.05039664 0.04806992 1.         0.97097625 0.07042254 0.27956298\n",
      " 0.09810127 0.10015558 0.30909091 0.21585903 0.9662963  0.0776699\n",
      " 0.42910916 0.28       0.0177665  0.92737836 0.39961552 0.01408451\n",
      " 0.1169497  0.87755102 0.44347826 0.68539326 0.94189602 0.\n",
      " 0.         0.05319149 0.05429293 0.17414722 0.05079365 0.81147541\n",
      " 0.09697933 0.81612586 0.07161804 0.70805921 0.7037037  0.57387989\n",
      " 0.02272727 0.8716164  0.29385965 0.01898734 0.19318182 0.01488834\n",
      " 0.01158301 0.50735931 0.21963563 0.03597122 0.39245283 0.39222615\n",
      " 0.18213866 0.16568047 0.35612536 0.83333333 0.         0.06153846\n",
      " 0.03378378 0.17254902 0.71774194 0.995671   0.         0.4235589\n",
      " 0.07017544 0.04149378 0.86401539 0.31438515 0.15384615 0.03601108\n",
      " 0.22222222 0.26322263 0.00409277 0.00367047 0.         0.12304251\n",
      " 0.06847545 0.41767442 0.00907029 0.57288136 0.01754386 0.64903846\n",
      " 0.26451613 0.3875969  0.067      0.00271924 0.03208556 0.05166313\n",
      " 0.07910272 0.04415584 0.7509434  0.07043986 0.07711757 0.8038248\n",
      " 0.21618497 0.72417355 0.13019391 0.17544876 0.01886792 0.48190719\n",
      " 0.01797753 0.01088929 0.15219721 0.92466586 0.82653061 0.07777778\n",
      " 0.05882353 0.792      0.5        1.         0.97292994 0.67656766\n",
      " 0.         0.03168568 0.46494645 0.11316397 0.85072655 0.5\n",
      " 0.8011257  0.00735294 0.66413043 0.24920635 0.10739739 0.02966625\n",
      " 0.09482759 0.87212644 0.68753914 0.95833333 0.64739884 0.51010101\n",
      " 0.02028398 0.05354331 0.         0.90024331 0.07172996 0.41448517\n",
      " 0.03030303 0.03975535 0.00900901 0.70204842 0.83142037 0.96030871\n",
      " 0.0048     0.17824773 0.92647059 0.34332989 0.00492611 0.1048951\n",
      " 0.15994371 0.90731707 0.7627409  0.01413428 0.21461717 0.94246862\n",
      " 0.         0.89778535 0.94267516 0.03956835 0.271777   0.95398773]\n",
      "0.9675245098039216\n",
      "0.9903957965459322\n",
      "Confidence interval for the score: [0.968 - 0.99]\n",
      "[1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1.\n",
      " 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      "[0.91445028 0.04886252 0.0366872  0.0262476  0.10568217 0.86807023\n",
      " 0.05166126 0.05741269 0.89092605 0.02618167 0.49101902 0.48279931\n",
      " 0.70560961 0.19464723 0.09974316 0.03034344 0.58402177 0.38563479\n",
      " 0.03177299 0.40660365 0.56162336 0.49380027 0.38303278 0.12316695\n",
      " 0.02994601 0.18790069 0.7821533  0.05465468 0.18647267 0.75887265\n",
      " 0.03584457 0.77071379 0.19344164 0.80162356 0.03418564 0.1641908\n",
      " 0.08252511 0.66751187 0.24996302 0.86584079 0.05038391 0.0837753\n",
      " 0.0569804  0.82773451 0.11528509 0.02315094 0.10700707 0.86984199\n",
      " 0.0249296  0.02544218 0.94962822 0.03889579 0.49225044 0.46885377\n",
      " 0.03140031 0.61860577 0.34999392 0.02531793 0.83920374 0.13547833\n",
      " 0.06995785 0.47160618 0.45843597 0.75681237 0.09283896 0.15034867\n",
      " 0.05813095 0.10537682 0.83649223 0.02668228 0.11474439 0.85857333\n",
      " 0.03873479 0.31795556 0.64330965 0.03354801 0.29516608 0.67128591\n",
      " 0.71980474 0.1642911  0.11590417 0.05799005 0.68726527 0.25474467\n",
      " 0.0273576  0.14113261 0.83150978 0.80517876 0.11931771 0.07550353\n",
      " 0.06972693 0.70360539 0.22666768 0.03468034 0.18952728 0.77579238\n",
      " 0.03729197 0.18508431 0.77762371 0.04307577 0.18983529 0.76708895\n",
      " 0.02576836 0.17313489 0.80109675 0.80193551 0.06386019 0.1342043\n",
      " 0.03911436 0.83486602 0.12601962 0.1161481  0.15063577 0.73321612\n",
      " 0.04818234 0.25789632 0.69392134 0.03768662 0.74116443 0.22114895\n",
      " 0.8767029  0.07097742 0.05231968 0.03593808 0.49604911 0.46801281\n",
      " 0.02726033 0.09749898 0.87524069 0.02714807 0.08986229 0.88298963\n",
      " 0.81799852 0.05573988 0.12626159 0.03990153 0.14029298 0.81980548\n",
      " 0.0452359  0.78050218 0.17426192 0.0656359  0.39209758 0.54226652\n",
      " 0.03229409 0.24451973 0.72318617 0.02925313 0.7403208  0.23042607\n",
      " 0.02439583 0.05179185 0.92381232 0.03260097 0.14035351 0.82704551\n",
      " 0.82478713 0.08508662 0.09012625 0.07411166 0.57562707 0.35026127\n",
      " 0.84964795 0.09206674 0.05828531 0.79979781 0.10059418 0.09960801\n",
      " 0.03962313 0.04930693 0.91106994 0.04540945 0.73540469 0.21918586\n",
      " 0.02918069 0.30298424 0.66783507 0.02596875 0.2432788  0.73075244\n",
      " 0.76972793 0.1070894  0.12318268 0.02335639 0.63957118 0.33707243\n",
      " 0.89867346 0.0626523  0.03867424 0.17479524 0.72945824 0.09574652\n",
      " 0.85992619 0.05320522 0.08686859 0.03497694 0.4103775  0.55464555\n",
      " 0.09182787 0.16295291 0.74521922 0.05728224 0.46660501 0.47611274\n",
      " 0.09527239 0.69077099 0.21395662 0.03023213 0.85087538 0.11889249\n",
      " 0.03041606 0.69589436 0.27368958 0.04996097 0.32916028 0.62087875\n",
      " 0.02924233 0.35253446 0.61822322 0.04110754 0.06088325 0.89800922\n",
      " 0.90477906 0.04144157 0.05377937 0.89098558 0.06480391 0.04421051\n",
      " 0.05124745 0.53656396 0.41218859 0.06108195 0.38729758 0.55162048\n",
      " 0.60114554 0.17916129 0.21969318 0.02899822 0.20312612 0.76787566\n",
      " 0.25158358 0.51709348 0.23132295 0.02492372 0.5087932  0.46628308\n",
      " 0.15451362 0.61649768 0.22898871 0.07828535 0.63396284 0.28775182\n",
      " 0.02540989 0.43281843 0.54177168 0.04520133 0.55495497 0.3998437\n",
      " 0.15211521 0.63321879 0.21466599 0.86708715 0.06940963 0.06350321\n",
      " 0.04356077 0.7525395  0.20389972 0.03116613 0.66871997 0.3001139\n",
      " 0.02386249 0.74255241 0.2335851  0.04595065 0.7761024  0.17794695\n",
      " 0.0283117  0.5314711  0.4402172  0.02837899 0.2007702  0.77085081\n",
      " 0.04795036 0.17716501 0.77488463 0.03230515 0.2762995  0.69139535\n",
      " 0.03197383 0.26181066 0.70621551 0.03445342 0.72141118 0.24413539\n",
      " 0.70185539 0.12980924 0.16833537 0.04489825 0.16675603 0.78834572\n",
      " 0.94010068 0.03066437 0.02923495 0.03142962 0.09830208 0.8702683\n",
      " 0.05714528 0.56791773 0.37493699 0.03075438 0.52641733 0.44282829\n",
      " 0.03012269 0.21543843 0.75443888 0.05660126 0.77270379 0.17069495\n",
      " 0.05801468 0.54477257 0.39721275 0.04988986 0.53838651 0.41172363\n",
      " 0.86849604 0.06773384 0.06377012 0.16968533 0.28819876 0.54211591\n",
      " 0.82194655 0.10778613 0.07026732 0.06962655 0.52990091 0.40047253\n",
      " 0.09671297 0.40215679 0.50113023 0.03054811 0.03891654 0.93053535\n",
      " 0.04236912 0.7311255  0.22650538 0.02814824 0.24800791 0.72384384\n",
      " 0.77211843 0.11965832 0.10822325 0.22651054 0.16421376 0.6092757\n",
      " 0.0251612  0.74296183 0.23187697 0.03285424 0.11844576 0.8487\n",
      " 0.02462442 0.12116917 0.85420641 0.02742358 0.59263659 0.37993983\n",
      " 0.73975573 0.14853404 0.11171023 0.02753451 0.25302339 0.7194421\n",
      " 0.08025476 0.54151413 0.3782311  0.85306641 0.07041996 0.07651364\n",
      " 0.43075584 0.33518984 0.23405432 0.05763527 0.42530106 0.51706367\n",
      " 0.76231598 0.14981584 0.08786818 0.08495854 0.70151988 0.21352158\n",
      " 0.21491527 0.14636121 0.63872351 0.0459938  0.81384078 0.14016541\n",
      " 0.02882199 0.29110734 0.68007067 0.04361522 0.46142992 0.49495486\n",
      " 0.05195256 0.18443599 0.76361145 0.3027376  0.53334537 0.16391702\n",
      " 0.4974879  0.33847649 0.16403561 0.48206519 0.14677779 0.37115702\n",
      " 0.04576326 0.26590097 0.68833577 0.04031706 0.58771656 0.37196638\n",
      " 0.03214053 0.90950719 0.05835227 0.02601457 0.32454545 0.64943998\n",
      " 0.79694333 0.07140167 0.131655   0.03252883 0.04966747 0.9178037\n",
      " 0.08291878 0.78486031 0.1322209  0.54147447 0.40808919 0.05043634\n",
      " 0.74255613 0.09779414 0.15964973 0.84097194 0.06795336 0.0910747\n",
      " 0.02603131 0.29834637 0.67562232 0.13874136 0.04289669 0.81836195\n",
      " 0.83677685 0.07637733 0.08684581 0.03956719 0.53078181 0.429651\n",
      " 0.40222136 0.3987908  0.19898783 0.03091806 0.6504137  0.31866825\n",
      " 0.46439573 0.04539192 0.49021235 0.76413496 0.0627146  0.17315045\n",
      " 0.74434461 0.16560639 0.090049   0.49161866 0.33207792 0.17630341\n",
      " 0.57775429 0.24800316 0.17424255 0.04591663 0.10604549 0.84803787\n",
      " 0.04517663 0.19898941 0.75583396 0.08398841 0.55649818 0.35951342\n",
      " 0.15589739 0.68514087 0.15896174 0.03685856 0.07086399 0.89227745\n",
      " 0.03915855 0.28129738 0.67954408 0.03267077 0.76959106 0.19773817\n",
      " 0.79890261 0.08110195 0.11999544 0.39912727 0.06634391 0.53452882\n",
      " 0.04649407 0.86626258 0.08724335 0.81522039 0.07689723 0.10788238\n",
      " 0.02987068 0.04195344 0.92817588 0.03297595 0.09088696 0.87613709\n",
      " 0.09102228 0.76581317 0.14316455 0.0386282  0.63018342 0.33118837\n",
      " 0.07967682 0.77347203 0.14685115 0.13932094 0.69661201 0.16406705\n",
      " 0.0459764  0.64478755 0.30923605 0.5769421  0.17493689 0.24812101\n",
      " 0.03910379 0.06895454 0.89194167 0.02780899 0.86492005 0.10727095\n",
      " 0.03355583 0.53946561 0.42697856 0.02909204 0.62754909 0.34335887\n",
      " 0.86509026 0.06075611 0.07415364 0.03316095 0.16970704 0.79713201\n",
      " 0.02915376 0.53603651 0.43480973 0.0356029  0.90830438 0.05609272\n",
      " 0.09318169 0.71771225 0.18910606 0.04724894 0.19557519 0.75717588\n",
      " 0.15998368 0.42889773 0.41111859 0.06637093 0.3463142  0.58731487\n",
      " 0.02561683 0.13649428 0.83788888 0.91675064 0.04256332 0.04068605\n",
      " 0.06436597 0.82096692 0.11466709 0.74442879 0.13266872 0.12290249\n",
      " 0.04905239 0.8529126  0.09803501 0.02856826 0.73258312 0.23884862\n",
      " 0.06032985 0.78194233 0.15772783 0.0287189  0.24109181 0.73018929\n",
      " 0.07853759 0.77651714 0.14494527 0.02682361 0.25441925 0.71875714\n",
      " 0.84296559 0.04384567 0.11318874 0.04830858 0.32804297 0.62364845\n",
      " 0.0232741  0.31194414 0.66478175 0.04910497 0.40902292 0.54187211\n",
      " 0.03709877 0.88036909 0.08253214 0.03230381 0.20370213 0.76399405\n",
      " 0.18681716 0.4681796  0.34500324 0.88935423 0.04455198 0.06609379\n",
      " 0.03284425 0.72650892 0.24064683 0.79254289 0.12987376 0.07758335\n",
      " 0.62091095 0.32249997 0.05658908 0.03685417 0.47770535 0.48544049\n",
      " 0.03622314 0.71062302 0.25315384 0.84110183 0.06393794 0.09496022\n",
      " 0.18232283 0.43383735 0.38383982 0.09806442 0.52519564 0.37673993\n",
      " 0.40718899 0.36273124 0.23007977 0.06465693 0.69024744 0.24509563\n",
      " 0.04396077 0.59598758 0.36005165 0.11355797 0.23548911 0.65095291\n",
      " 0.94483532 0.03133411 0.02383057 0.75082913 0.15380452 0.09536635\n",
      " 0.02552869 0.89460587 0.07986544 0.02761223 0.76551315 0.20687462\n",
      " 0.03480826 0.30205585 0.66313589 0.03320546 0.04688907 0.91990547\n",
      " 0.51145925 0.3293878  0.15915296 0.08593741 0.52033346 0.39372913\n",
      " 0.63225121 0.2541392  0.11360959 0.77859423 0.11643653 0.10496923\n",
      " 0.02678997 0.21681732 0.75639271 0.22010179 0.43891709 0.34098112\n",
      " 0.06699434 0.6920857  0.24091996 0.08507886 0.84853426 0.06638689\n",
      " 0.04236672 0.67604918 0.2815841  0.02499829 0.67301858 0.30198313\n",
      " 0.09695996 0.86192928 0.04111076 0.93628149 0.03073825 0.03298026\n",
      " 0.02841997 0.93226402 0.03931601 0.62183267 0.19201889 0.18614844\n",
      " 0.04268431 0.82818827 0.12912742 0.04153094 0.53503555 0.42343351\n",
      " 0.92225689 0.03620477 0.04153834 0.02582473 0.44848593 0.52568934\n",
      " 0.84873365 0.07147484 0.07979151 0.04398258 0.33370721 0.62231021\n",
      " 0.06278113 0.66834611 0.26887276 0.48311983 0.14527203 0.37160813\n",
      " 0.14701993 0.71781838 0.13516169 0.89214009 0.07246439 0.03539552\n",
      " 0.74504787 0.14585995 0.10909219 0.04480109 0.86333852 0.09186039\n",
      " 0.10270733 0.76424013 0.13305255 0.8004177  0.09255905 0.10702325\n",
      " 0.03030624 0.28591667 0.68377709 0.04252528 0.848396   0.10907872\n",
      " 0.82594586 0.03489325 0.13916089 0.02791524 0.2843725  0.68771226\n",
      " 0.07229533 0.67034944 0.25735523 0.03784113 0.28148777 0.6806711\n",
      " 0.07173557 0.72948506 0.19877937 0.18270746 0.59984559 0.21744695\n",
      " 0.85677854 0.07092364 0.07229782 0.02808505 0.50074557 0.47116938\n",
      " 0.02289974 0.89851209 0.07858817 0.88785603 0.05805122 0.05409275\n",
      " 0.02960292 0.75186325 0.21853383 0.02728406 0.15658848 0.81612746\n",
      " 0.02564244 0.28713388 0.68722369 0.67513135 0.18134928 0.14351936\n",
      " 0.30170938 0.51203759 0.18625301 0.02995089 0.27109077 0.69895834\n",
      " 0.04688904 0.40259087 0.55052008 0.0342048  0.04755881 0.91823639\n",
      " 0.02703589 0.07938923 0.89357488 0.02567904 0.34993362 0.62438734\n",
      " 0.94649523 0.02811474 0.02539003 0.05522002 0.83121133 0.11356864\n",
      " 0.02998997 0.5025974  0.46741263 0.09443744 0.74449152 0.16107104\n",
      " 0.02543318 0.2165295  0.75803732 0.02788754 0.47633825 0.49577421\n",
      " 0.03176587 0.2503223  0.71791183 0.87502302 0.06490404 0.06007293\n",
      " 0.10164851 0.30288714 0.59546435 0.6605302  0.08307778 0.25639202\n",
      " 0.07298672 0.74715589 0.17985739 0.56846394 0.29237975 0.1391563\n",
      " 0.04005459 0.79257402 0.16737139 0.02937327 0.20647469 0.76415204\n",
      " 0.02686215 0.33700059 0.63613727 0.05000685 0.06722871 0.88276444\n",
      " 0.03227471 0.35076337 0.61696192 0.03329871 0.50742856 0.45927273\n",
      " 0.85916049 0.06883296 0.07200654 0.06400444 0.83980373 0.09619183\n",
      " 0.93658808 0.03893233 0.02447959 0.03165479 0.16211785 0.80622736\n",
      " 0.16801963 0.74131443 0.09066595 0.02766751 0.59489532 0.37743717\n",
      " 0.83205843 0.09097067 0.0769709  0.02279164 0.86742378 0.10978458\n",
      " 0.86028819 0.07338341 0.0663284  0.03478557 0.31716767 0.64804675\n",
      " 0.04508986 0.22947082 0.72543933 0.02742652 0.07237438 0.9001991\n",
      " 0.89147836 0.0568829  0.05163874 0.02856213 0.71884356 0.25259432\n",
      " 0.02405348 0.13474316 0.84120336 0.11486118 0.53023877 0.35490006\n",
      " 0.9141155  0.05087362 0.03501088 0.64556616 0.20441231 0.15002152\n",
      " 0.03428219 0.74908661 0.21663119 0.02810057 0.17099184 0.8009076\n",
      " 0.06817099 0.25966855 0.67216046 0.83103621 0.0936958  0.07526799\n",
      " 0.56961582 0.18840309 0.24198109 0.03322972 0.14456373 0.82220655\n",
      " 0.90228896 0.05643742 0.04127362 0.0364014  0.15049076 0.81310784\n",
      " 0.05076919 0.11433402 0.8348968  0.85335161 0.06040156 0.08624683\n",
      " 0.02692934 0.61694557 0.3561251  0.03045681 0.09875246 0.87079073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9854669501623871\r\n",
      "0.9937025614508117\r\n",
      "Confidence interval for the score: [0.985 - 0.994]\r\n",
      "972 972\r\n",
      "[1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0.\r\n",
      " 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.\r\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1.\r\n",
      " 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1.\r\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0.\r\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.\r\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\r\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0.\r\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\r\n",
      " 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1.\r\n",
      " 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0.\r\n",
      " 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1.\r\n",
      " 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.\r\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0.\r\n",
      " 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.\r\n",
      " 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.\r\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0.\r\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1.\r\n",
      " 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0.\r\n",
      " 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1.\r\n",
      " 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1.\r\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.\r\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1.\r\n",
      " 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0.\r\n",
      " 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.\r\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0.\r\n",
      " 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.\r\n",
      " 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.\r\n",
      " 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1.\r\n",
      " 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0.\r\n",
      " 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.\r\n",
      " 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0.\r\n",
      " 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1.\r\n",
      " 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1.\r\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0.\r\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\r\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0.\r\n",
      " 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1.\r\n",
      " 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0.\r\n",
      " 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1.\r\n",
      " 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1.]\r\n",
      "[1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\r\n",
      " 3.69290573e-02 9.63070943e-01 9.25925926e-03 1.85185185e-02\r\n",
      " 9.72222222e-01 0.00000000e+00 4.81781377e-01 5.18218623e-01\r\n",
      " 8.10773481e-01 1.47790055e-01 4.14364641e-02 1.31061599e-03\r\n",
      " 6.67103539e-01 3.31585845e-01 0.00000000e+00 3.45153664e-01\r\n",
      " 6.54846336e-01 5.41025641e-01 4.12820513e-01 4.61538462e-02\r\n",
      " 1.98609732e-03 1.19165839e-01 8.78848064e-01 3.16154971e-02\r\n",
      " 1.39437135e-01 8.28947368e-01 3.95647873e-03 8.61523244e-01\r\n",
      " 1.34520277e-01 8.95364238e-01 1.32450331e-03 1.03311258e-01\r\n",
      " 5.06756757e-02 7.26351351e-01 2.22972973e-01 9.44649446e-01\r\n",
      " 1.66051661e-02 3.87453875e-02 1.06635071e-02 9.07582938e-01\r\n",
      " 8.17535545e-02 0.00000000e+00 4.68150422e-02 9.53184958e-01\r\n",
      " 0.00000000e+00 6.58761528e-04 9.99341238e-01 4.98960499e-03\r\n",
      " 5.13929314e-01 4.81081081e-01 0.00000000e+00 6.74378749e-01\r\n",
      " 3.25621251e-01 2.18340611e-03 9.43231441e-01 5.45851528e-02\r\n",
      " 3.78787879e-02 5.26515152e-01 4.35606061e-01 8.84097035e-01\r\n",
      " 1.88679245e-02 9.70350404e-02 3.01910043e-02 4.06654344e-02\r\n",
      " 9.29143561e-01 0.00000000e+00 6.41102457e-02 9.35889754e-01\r\n",
      " 8.19672131e-03 2.49414520e-01 7.42388759e-01 7.28155340e-03\r\n",
      " 2.30582524e-01 7.62135922e-01 8.52097130e-01 1.13318617e-01\r\n",
      " 3.45842531e-02 3.32749562e-02 7.40805604e-01 2.25919440e-01\r\n",
      " 1.13895216e-03 8.31435080e-02 9.15717540e-01 9.06976744e-01\r\n",
      " 7.20930233e-02 2.09302326e-02 3.34928230e-02 7.70334928e-01\r\n",
      " 1.96172249e-01 4.86618005e-03 1.11922141e-01 8.83211679e-01\r\n",
      " 1.74927114e-02 1.04956268e-01 8.77551020e-01 8.36820084e-03\r\n",
      " 1.12040911e-01 8.79590888e-01 0.00000000e+00 1.12338858e-01\r\n",
      " 8.87661142e-01 8.88809182e-01 7.89096126e-03 1.03299857e-01\r\n",
      " 2.45098039e-03 8.94607843e-01 1.02941176e-01 7.69230769e-02\r\n",
      " 8.20512821e-02 8.41025641e-01 1.82926829e-02 2.18292683e-01\r\n",
      " 7.63414634e-01 5.76759966e-03 8.02714165e-01 1.91518236e-01\r\n",
      " 9.68000000e-01 2.40000000e-02 8.00000000e-03 0.00000000e+00\r\n",
      " 5.20888889e-01 4.79111111e-01 0.00000000e+00 4.21729807e-02\r\n",
      " 9.57827019e-01 0.00000000e+00 4.09126672e-02 9.59087333e-01\r\n",
      " 9.01408451e-01 2.81690141e-02 7.04225352e-02 8.26019618e-03\r\n",
      " 4.64636035e-02 9.45276200e-01 1.09890110e-02 8.59340659e-01\r\n",
      " 1.29670330e-01 2.81622912e-02 3.64677804e-01 6.07159905e-01\r\n",
      " 5.55941626e-03 1.61917999e-01 8.32522585e-01 1.52671756e-03\r\n",
      " 8.35114504e-01 1.63358779e-01 0.00000000e+00 1.94174757e-02\r\n",
      " 9.80582524e-01 0.00000000e+00 3.92609700e-02 9.60739030e-01\r\n",
      " 9.15074310e-01 4.03397028e-02 4.45859873e-02 3.21839080e-02\r\n",
      " 5.93103448e-01 3.74712644e-01 9.67283951e-01 2.71604938e-02\r\n",
      " 5.55555556e-03 9.34426230e-01 2.45901639e-02 4.09836066e-02\r\n",
      " 8.34151129e-03 1.91364082e-02 9.72522080e-01 8.11359026e-03\r\n",
      " 8.66125761e-01 1.25760649e-01 2.00000000e-03 2.26000000e-01\r\n",
      " 7.72000000e-01 0.00000000e+00 2.13429257e-01 7.86570743e-01\r\n",
      " 9.07801418e-01 4.96453901e-02 4.25531915e-02 0.00000000e+00\r\n",
      " 7.23893805e-01 2.76106195e-01 9.78609626e-01 1.87165775e-02\r\n",
      " 2.67379679e-03 1.83035714e-01 7.63392857e-01 5.35714286e-02\r\n",
      " 9.63503650e-01 0.00000000e+00 3.64963504e-02 3.80228137e-03\r\n",
      " 4.06021992e-01 5.90175727e-01 6.43274854e-02 9.69785575e-02\r\n",
      " 8.38693957e-01 2.14652357e-02 4.76901540e-01 5.01633224e-01\r\n",
      " 8.35762877e-02 7.49271137e-01 1.67152575e-01 0.00000000e+00\r\n",
      " 9.49432892e-01 5.05671078e-02 3.90625000e-03 7.69531250e-01\r\n",
      " 2.26562500e-01 1.63934426e-02 2.88056206e-01 6.95550351e-01\r\n",
      " 0.00000000e+00 3.32914573e-01 6.67085427e-01 1.42226582e-02\r\n",
      " 1.91270231e-02 9.66650319e-01 9.77664975e-01 4.06091371e-03\r\n",
      " 1.82741117e-02 9.66269841e-01 1.98412698e-02 1.38888889e-02\r\n",
      " 1.51006711e-02 5.80536913e-01 4.04362416e-01 3.18664643e-02\r\n",
      " 3.68740516e-01 5.99393020e-01 6.96969697e-01 1.41414141e-01\r\n",
      " 1.61616162e-01 1.69635284e-03 1.46734521e-01 8.51569126e-01\r\n",
      " 2.27544910e-01 5.68862275e-01 2.03592814e-01 0.00000000e+00\r\n",
      " 5.33407572e-01 4.66592428e-01 1.23119557e-01 7.01108472e-01\r\n",
      " 1.75771971e-01 3.69003690e-02 7.19557196e-01 2.43542435e-01\r\n",
      " 0.00000000e+00 4.13309982e-01 5.86690018e-01 1.29533679e-02\r\n",
      " 5.99417098e-01 3.87629534e-01 1.24933546e-01 7.39500266e-01\r\n",
      " 1.35566188e-01 9.85714286e-01 1.42857143e-02 0.00000000e+00\r\n",
      " 1.60069219e-02 8.17001947e-01 1.66991131e-01 4.21496312e-03\r\n",
      " 7.65015806e-01 2.30769231e-01 0.00000000e+00 8.26542491e-01\r\n",
      " 1.73457509e-01 1.26582278e-02 8.84493671e-01 1.02848101e-01\r\n",
      " 1.86567164e-03 5.41044776e-01 4.57089552e-01 1.11172874e-03\r\n",
      " 1.40633685e-01 8.58254586e-01 1.43084261e-02 1.04928458e-01\r\n",
      " 8.80763116e-01 3.81679389e-03 2.08015267e-01 7.88167939e-01\r\n",
      " 0.00000000e+00 2.17054264e-01 7.82945736e-01 4.24929178e-03\r\n",
      " 7.80453258e-01 2.15297450e-01 8.26869806e-01 6.78670360e-02\r\n",
      " 1.05263158e-01 1.87188020e-02 1.26039933e-01 8.55241265e-01\r\n",
      " 1.00000000e+00 0.00000000e+00 0.00000000e+00 2.52206810e-03\r\n",
      " 4.72887768e-02 9.50189155e-01 2.22222222e-02 6.31372549e-01\r\n",
      " 3.46405229e-01 1.85988841e-03 5.45567266e-01 4.52572846e-01\r\n",
      " 1.53846154e-03 1.83076923e-01 8.15384615e-01 1.91176471e-02\r\n",
      " 8.49019608e-01 1.31862745e-01 1.53482881e-02 6.12750885e-01\r\n",
      " 3.71900826e-01 0.00000000e+00 5.36585366e-01 4.63414634e-01\r\n",
      " 9.81617647e-01 7.35294118e-03 1.10294118e-02 1.55875300e-01\r\n",
      " 2.13429257e-01 6.30695444e-01 9.06525573e-01 7.05467372e-02\r\n",
      " 2.29276896e-02 2.07373272e-02 5.66820276e-01 4.12442396e-01\r\n",
      " 7.69230769e-02 4.48717949e-01 4.74358974e-01 2.94117647e-03\r\n",
      " 0.00000000e+00 9.97058824e-01 1.30500759e-02 8.30652504e-01\r\n",
      " 1.56297420e-01 0.00000000e+00 1.52802893e-01 8.47197107e-01\r\n",
      " 9.21933086e-01 3.34572491e-02 4.46096654e-02 2.20279720e-01\r\n",
      " 8.39160839e-02 6.95804196e-01 0.00000000e+00 7.96324655e-01\r\n",
      " 2.03675345e-01 6.55737705e-03 6.06557377e-02 9.32786885e-01\r\n",
      " 0.00000000e+00 5.93379138e-02 9.40662086e-01 4.09836066e-03\r\n",
      " 6.72131148e-01 3.23770492e-01 8.60428232e-01 8.56463125e-02\r\n",
      " 5.39254560e-02 0.00000000e+00 1.93589744e-01 8.06410256e-01\r\n",
      " 3.59322034e-02 6.21694915e-01 3.42372881e-01 9.47963801e-01\r\n",
      " 2.71493213e-02 2.48868778e-02 4.74803150e-01 3.20472441e-01\r\n",
      " 2.04724409e-01 3.96853772e-02 4.12942438e-01 5.47372184e-01\r\n",
      " 8.97674419e-01 8.37209302e-02 1.86046512e-02 5.16304348e-02\r\n",
      " 8.15217391e-01 1.33152174e-01 2.16450216e-01 7.79220779e-02\r\n",
      " 7.05627706e-01 1.23203285e-02 9.41478439e-01 4.62012320e-02\r\n",
      " 0.00000000e+00 2.54814815e-01 7.45185185e-01 1.71796707e-02\r\n",
      " 4.49534717e-01 5.33285612e-01 2.03045685e-02 1.16751269e-01\r\n",
      " 8.62944162e-01 3.10077519e-01 5.94315245e-01 9.56072351e-02\r\n",
      " 5.49407115e-01 3.22134387e-01 1.28458498e-01 5.62231760e-01\r\n",
      " 5.36480687e-02 3.84120172e-01 1.78571429e-02 2.04761905e-01\r\n",
      " 7.77380952e-01 1.09689214e-02 6.50822669e-01 3.38208410e-01\r\n",
      " 8.84955752e-03 9.79351032e-01 1.17994100e-02 0.00000000e+00\r\n",
      " 2.73037543e-01 7.26962457e-01 9.00481541e-01 1.44462279e-02\r\n",
      " 8.50722311e-02 4.11985019e-03 8.98876404e-03 9.86891386e-01\r\n",
      " 5.38827258e-02 8.63708399e-01 8.24088748e-02 5.86505190e-01\r\n",
      " 4.10034602e-01 3.46020761e-03 8.60421836e-01 3.41191067e-02\r\n",
      " 1.05459057e-01 9.51704545e-01 1.70454545e-02 3.12500000e-02\r\n",
      " 4.20875421e-04 2.36111111e-01 7.63468013e-01 1.23348018e-01\r\n",
      " 5.28634361e-03 8.71365639e-01 9.71052632e-01 7.89473684e-03\r\n",
      " 2.10526316e-02 7.53138075e-03 5.53974895e-01 4.38493724e-01\r\n",
      " 4.40298507e-01 4.45273632e-01 1.14427861e-01 1.81159420e-03\r\n",
      " 7.30978261e-01 2.67210145e-01 4.80996622e-01 7.17905405e-03\r\n",
      " 5.11824324e-01 8.80368098e-01 6.13496933e-03 1.13496933e-01\r\n",
      " 8.96103896e-01 7.79220779e-02 2.59740260e-02 6.13636364e-01\r\n",
      " 2.95454545e-01 9.09090909e-02 6.95652174e-01 1.73913043e-01\r\n",
      " 1.30434783e-01 1.70519136e-02 4.81242895e-02 9.34823797e-01\r\n",
      " 5.41516245e-03 1.19133574e-01 8.75451264e-01 3.61445783e-02\r\n",
      " 6.02409639e-01 3.61445783e-01 1.40482786e-01 7.36842105e-01\r\n",
      " 1.22675109e-01 6.49350649e-03 5.56586271e-03 9.87940631e-01\r\n",
      " 9.72590628e-03 2.49336870e-01 7.40937224e-01 5.08474576e-03\r\n",
      " 8.49152542e-01 1.45762712e-01 8.75675676e-01 5.13513514e-02\r\n",
      " 7.29729730e-02 3.95161290e-01 8.06451613e-03 5.96774194e-01\r\n",
      " 1.77321512e-02 9.31871209e-01 5.03966402e-02 9.30080117e-01\r\n",
      " 2.18499636e-02 4.80699199e-02 0.00000000e+00 0.00000000e+00\r\n",
      " 1.00000000e+00 1.97889182e-03 2.70448549e-02 9.70976253e-01\r\n",
      " 2.81690141e-02 9.01408451e-01 7.04225352e-02 1.47814910e-02\r\n",
      " 7.05655527e-01 2.79562982e-01 3.58649789e-02 8.66033755e-01\r\n",
      " 9.81012658e-02 1.20381175e-01 7.79463244e-01 1.00155581e-01\r\n",
      " 0.00000000e+00 6.90909091e-01 3.09090909e-01 6.87224670e-01\r\n",
      " 9.69162996e-02 2.15859031e-01 5.18518519e-03 2.85185185e-02\r\n",
      " 9.66296296e-01 0.00000000e+00 9.22330097e-01 7.76699029e-02\r\n",
      " 7.52823087e-03 5.63362610e-01 4.29109159e-01 0.00000000e+00\r\n",
      " 7.20000000e-01 2.80000000e-01 9.65736041e-01 1.64974619e-02\r\n",
      " 1.77664975e-02 4.35729847e-03 6.82643428e-02 9.27378359e-01\r\n",
      " 8.41042893e-04 5.99543434e-01 3.99615523e-01 0.00000000e+00\r\n",
      " 9.85915493e-01 1.40845070e-02 7.14514579e-02 8.11598847e-01\r\n",
      " 1.16949696e-01 9.07029478e-03 1.13378685e-01 8.77551020e-01\r\n",
      " 1.13043478e-01 4.43478261e-01 4.43478261e-01 3.37078652e-02\r\n",
      " 2.80898876e-01 6.85393258e-01 0.00000000e+00 5.81039755e-02\r\n",
      " 9.41896024e-01 9.87421384e-01 1.25786164e-02 0.00000000e+00\r\n",
      " 5.76923077e-02 9.42307692e-01 0.00000000e+00 8.72340426e-01\r\n",
      " 7.44680851e-02 5.31914894e-02 1.38888889e-02 9.31818182e-01\r\n",
      " 5.42929293e-02 0.00000000e+00 8.25852783e-01 1.74147217e-01\r\n",
      " 2.85714286e-02 9.20634921e-01 5.07936508e-02 0.00000000e+00\r\n",
      " 1.88524590e-01 8.11475410e-01 6.35930048e-02 8.39427663e-01\r\n",
      " 9.69793323e-02 0.00000000e+00 1.83874140e-01 8.16125860e-01\r\n",
      " 9.26613616e-01 1.76834660e-03 7.16180371e-02 2.46710526e-02\r\n",
      " 2.67269737e-01 7.08059211e-01 0.00000000e+00 2.96296296e-01\r\n",
      " 7.03703704e-01 1.76358437e-02 4.08484271e-01 5.73879886e-01\r\n",
      " 2.84090909e-03 9.74431818e-01 2.27272727e-02 1.54679041e-03\r\n",
      " 1.26836814e-01 8.71616396e-01 2.06140351e-01 5.00000000e-01\r\n",
      " 2.93859649e-01 9.74683544e-01 6.32911392e-03 1.89873418e-02\r\n",
      " 3.78787879e-03 8.03030303e-01 1.93181818e-01 8.99503722e-01\r\n",
      " 8.56079404e-02 1.48883375e-02 6.92406692e-01 2.96010296e-01\r\n",
      " 1.15830116e-02 1.03896104e-02 4.82251082e-01 5.07359307e-01\r\n",
      " 4.72334683e-03 7.75641026e-01 2.19635628e-01 9.60431655e-01\r\n",
      " 3.59712230e-03 3.59712230e-02 1.24528302e-01 4.83018868e-01\r\n",
      " 3.92452830e-01 5.30035336e-02 5.54770318e-01 3.92226148e-01\r\n",
      " 4.57109283e-01 3.60752056e-01 1.82138660e-01 3.55029586e-02\r\n",
      " 7.98816568e-01 1.65680473e-01 8.54700855e-03 6.35327635e-01\r\n",
      " 3.56125356e-01 5.88235294e-02 1.07843137e-01 8.33333333e-01\r\n",
      " 1.00000000e+00 0.00000000e+00 0.00000000e+00 8.09230769e-01\r\n",
      " 1.29230769e-01 6.15384615e-02 0.00000000e+00 9.66216216e-01\r\n",
      " 3.37837838e-02 0.00000000e+00 8.27450980e-01 1.72549020e-01\r\n",
      " 0.00000000e+00 2.82258065e-01 7.17741935e-01 0.00000000e+00\r\n",
      " 4.32900433e-03 9.95670996e-01 6.66666667e-01 3.33333333e-01\r\n",
      " 0.00000000e+00 4.51127820e-02 5.31328321e-01 4.23558897e-01\r\n",
      " 7.71929825e-01 1.57894737e-01 7.01754386e-02 9.29460581e-01\r\n",
      " 2.90456432e-02 4.14937759e-02 3.20718409e-04 1.35663887e-01\r\n",
      " 8.64015394e-01 2.09976798e-01 4.75638051e-01 3.14385151e-01\r\n",
      " 3.23886640e-02 8.13765182e-01 1.53846154e-01 6.55586334e-02\r\n",
      " 8.98430286e-01 3.60110803e-02 1.43369176e-02 7.63440860e-01\r\n",
      " 2.22222222e-01 0.00000000e+00 7.36777368e-01 2.63222632e-01\r\n",
      " 7.27603456e-02 9.23146885e-01 4.09276944e-03 9.94290375e-01\r\n",
      " 2.03915171e-03 3.67047308e-03 1.76056338e-03 9.98239437e-01\r\n",
      " 0.00000000e+00 7.40492170e-01 1.36465324e-01 1.23042506e-01\r\n",
      " 3.87596899e-03 9.27648579e-01 6.84754522e-02 9.30232558e-03\r\n",
      " 5.73023256e-01 4.17674419e-01 9.86394558e-01 4.53514739e-03\r\n",
      " 9.07029478e-03 0.00000000e+00 4.27118644e-01 5.72881356e-01\r\n",
      " 9.73684211e-01 8.77192982e-03 1.75438596e-02 9.61538462e-03\r\n",
      " 3.41346154e-01 6.49038462e-01 1.93548387e-02 7.16129032e-01\r\n",
      " 2.64516129e-01 5.50387597e-01 6.20155039e-02 3.87596899e-01\r\n",
      " 1.15000000e-01 8.18000000e-01 6.70000000e-02 9.70088375e-01\r\n",
      " 2.71923861e-02 2.71923861e-03 8.71657754e-01 9.62566845e-02\r\n",
      " 3.20855615e-02 1.91082803e-02 9.29228592e-01 5.16631281e-02\r\n",
      " 7.31995277e-02 8.47697757e-01 7.91027155e-02 9.32467532e-01\r\n",
      " 2.33766234e-02 4.41558442e-02 1.88679245e-03 2.47169811e-01\r\n",
      " 7.50943396e-01 1.50722855e-02 9.14487850e-01 7.04398647e-02\r\n",
      " 9.21618205e-01 1.26422250e-03 7.71175727e-02 1.23380629e-03\r\n",
      " 1.94941394e-01 8.03824800e-01 5.02890173e-02 7.33526012e-01\r\n",
      " 2.16184971e-01 1.13636364e-02 2.64462810e-01 7.24173554e-01\r\n",
      " 3.04709141e-02 8.39335180e-01 1.30193906e-01 1.64447018e-01\r\n",
      " 6.60104227e-01 1.75448755e-01 9.69339623e-01 1.17924528e-02\r\n",
      " 1.88679245e-02 1.27713921e-03 5.16815666e-01 4.81907195e-01\r\n",
      " 0.00000000e+00 9.82022472e-01 1.79775281e-02 9.72776770e-01\r\n",
      " 1.63339383e-02 1.08892922e-02 2.14362272e-03 8.45659164e-01\r\n",
      " 1.52197213e-01 1.21506683e-03 7.41190765e-02 9.24665857e-01\r\n",
      " 0.00000000e+00 1.73469388e-01 8.26530612e-01 8.05555556e-01\r\n",
      " 1.16666667e-01 7.77777778e-02 2.94117647e-01 6.47058824e-01\r\n",
      " 5.88235294e-02 9.41176471e-04 2.07058824e-01 7.92000000e-01\r\n",
      " 0.00000000e+00 5.00000000e-01 5.00000000e-01 0.00000000e+00\r\n",
      " 0.00000000e+00 1.00000000e+00 0.00000000e+00 2.70700637e-02\r\n",
      " 9.72929936e-01 0.00000000e+00 3.23432343e-01 6.76567657e-01\r\n",
      " 1.00000000e+00 0.00000000e+00 0.00000000e+00 2.02788340e-02\r\n",
      " 9.48035488e-01 3.16856781e-02 0.00000000e+00 5.35053554e-01\r\n",
      " 4.64946446e-01 6.23556582e-02 8.24480370e-01 1.13163972e-01\r\n",
      " 0.00000000e+00 1.49273448e-01 8.50726552e-01 1.96078431e-03\r\n",
      " 4.98039216e-01 5.00000000e-01 9.38086304e-04 1.97936210e-01\r\n",
      " 8.01125704e-01 9.85294118e-01 7.35294118e-03 7.35294118e-03\r\n",
      " 6.41304348e-02 2.71739130e-01 6.64130435e-01 7.18253968e-01\r\n",
      " 3.25396825e-02 2.49206349e-01 4.37453920e-02 8.48857213e-01\r\n",
      " 1.07397395e-01 6.94684796e-01 2.75648949e-01 2.96662546e-02\r\n",
      " 1.72413793e-02 8.87931034e-01 9.48275862e-02 0.00000000e+00\r\n",
      " 1.27873563e-01 8.72126437e-01 0.00000000e+00 3.12460864e-01\r\n",
      " 6.87539136e-01 1.85606061e-02 2.31060606e-02 9.58333333e-01\r\n",
      " 0.00000000e+00 3.52601156e-01 6.47398844e-01 2.52525253e-03\r\n",
      " 4.87373737e-01 5.10101010e-01 9.63488844e-01 1.62271805e-02\r\n",
      " 2.02839757e-02 2.83464567e-02 9.18110236e-01 5.35433071e-02\r\n",
      " 9.90407674e-01 9.59232614e-03 0.00000000e+00 0.00000000e+00\r\n",
      " 9.97566910e-02 9.00243309e-01 1.56118143e-01 7.72151899e-01\r\n",
      " 7.17299578e-02 0.00000000e+00 5.85514834e-01 4.14485166e-01\r\n",
      " 9.39393939e-01 3.03030303e-02 3.03030303e-02 0.00000000e+00\r\n",
      " 9.60244648e-01 3.97553517e-02 9.84984985e-01 6.00600601e-03\r\n",
      " 9.00900901e-03 5.58659218e-03 2.92364991e-01 7.02048417e-01\r\n",
      " 1.07604017e-02 1.57819225e-01 8.31420373e-01 0.00000000e+00\r\n",
      " 3.96912900e-02 9.60308710e-01 9.88800000e-01 6.40000000e-03\r\n",
      " 4.80000000e-03 0.00000000e+00 8.21752266e-01 1.78247734e-01\r\n",
      " 0.00000000e+00 7.35294118e-02 9.26470588e-01 8.27300931e-02\r\n",
      " 5.73940021e-01 3.43329886e-01 9.80295567e-01 1.47783251e-02\r\n",
      " 4.92610837e-03 7.13286713e-01 1.81818182e-01 1.04895105e-01\r\n",
      " 7.50469043e-03 8.32551595e-01 1.59943715e-01 0.00000000e+00\r\n",
      " 9.26829268e-02 9.07317073e-01 3.89721627e-02 1.98286938e-01\r\n",
      " 7.62740899e-01 9.54063604e-01 3.18021201e-02 1.41342756e-02\r\n",
      " 6.70533643e-01 1.14849188e-01 2.14617169e-01 2.09205021e-03\r\n",
      " 5.54393305e-02 9.42468619e-01 9.94565217e-01 5.43478261e-03\r\n",
      " 0.00000000e+00 1.19250426e-02 9.02896082e-02 8.97785349e-01\r\n",
      " 2.30891720e-02 3.42356688e-02 9.42675159e-01 9.56834532e-01\r\n",
      " 3.59712230e-03 3.95683453e-02 0.00000000e+00 7.28222997e-01\r\n",
      " 2.71777003e-01 3.06748466e-03 4.29447853e-02 9.53987730e-01]\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9837234707361171\r\n",
      "0.9931511375436609\r\n",
      "Confidence interval for the score: [0.984 - 0.993]\r\n",
      "[[1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]]\r\n",
      "[[0.91445028 0.04886252 0.0366872 ]\r\n",
      " [0.0262476  0.10568217 0.86807023]\r\n",
      " [0.05166126 0.05741269 0.89092605]\r\n",
      " [0.02618167 0.49101902 0.48279931]\r\n",
      " [0.70560961 0.19464723 0.09974316]\r\n",
      " [0.03034344 0.58402177 0.38563479]\r\n",
      " [0.03177299 0.40660365 0.56162336]\r\n",
      " [0.49380027 0.38303278 0.12316695]\r\n",
      " [0.02994601 0.18790069 0.7821533 ]\r\n",
      " [0.05465468 0.18647267 0.75887265]\r\n",
      " [0.03584457 0.77071379 0.19344164]\r\n",
      " [0.80162356 0.03418564 0.1641908 ]\r\n",
      " [0.08252511 0.66751187 0.24996302]\r\n",
      " [0.86584079 0.05038391 0.0837753 ]\r\n",
      " [0.0569804  0.82773451 0.11528509]\r\n",
      " [0.02315094 0.10700707 0.86984199]\r\n",
      " [0.0249296  0.02544218 0.94962822]\r\n",
      " [0.03889579 0.49225044 0.46885377]\r\n",
      " [0.03140031 0.61860577 0.34999392]\r\n",
      " [0.02531793 0.83920374 0.13547833]\r\n",
      " [0.06995785 0.47160618 0.45843597]\r\n",
      " [0.75681237 0.09283896 0.15034867]\r\n",
      " [0.05813095 0.10537682 0.83649223]\r\n",
      " [0.02668228 0.11474439 0.85857333]\r\n",
      " [0.03873479 0.31795556 0.64330965]\r\n",
      " [0.03354801 0.29516608 0.67128591]\r\n",
      " [0.71980474 0.1642911  0.11590417]\r\n",
      " [0.05799005 0.68726527 0.25474467]\r\n",
      " [0.0273576  0.14113261 0.83150978]\r\n",
      " [0.80517876 0.11931771 0.07550353]\r\n",
      " [0.06972693 0.70360539 0.22666768]\r\n",
      " [0.03468034 0.18952728 0.77579238]\r\n",
      " [0.03729197 0.18508431 0.77762371]\r\n",
      " [0.04307577 0.18983529 0.76708895]\r\n",
      " [0.02576836 0.17313489 0.80109675]\r\n",
      " [0.80193551 0.06386019 0.1342043 ]\r\n",
      " [0.03911436 0.83486602 0.12601962]\r\n",
      " [0.1161481  0.15063577 0.73321612]\r\n",
      " [0.04818234 0.25789632 0.69392134]\r\n",
      " [0.03768662 0.74116443 0.22114895]\r\n",
      " [0.8767029  0.07097742 0.05231968]\r\n",
      " [0.03593808 0.49604911 0.46801281]\r\n",
      " [0.02726033 0.09749898 0.87524069]\r\n",
      " [0.02714807 0.08986229 0.88298963]\r\n",
      " [0.81799852 0.05573988 0.12626159]\r\n",
      " [0.03990153 0.14029298 0.81980548]\r\n",
      " [0.0452359  0.78050218 0.17426192]\r\n",
      " [0.0656359  0.39209758 0.54226652]\r\n",
      " [0.03229409 0.24451973 0.72318617]\r\n",
      " [0.02925313 0.7403208  0.23042607]\r\n",
      " [0.02439583 0.05179185 0.92381232]\r\n",
      " [0.03260097 0.14035351 0.82704551]\r\n",
      " [0.82478713 0.08508662 0.09012625]\r\n",
      " [0.07411166 0.57562707 0.35026127]\r\n",
      " [0.84964795 0.09206674 0.05828531]\r\n",
      " [0.79979781 0.10059418 0.09960801]\r\n",
      " [0.03962313 0.04930693 0.91106994]\r\n",
      " [0.04540945 0.73540469 0.21918586]\r\n",
      " [0.02918069 0.30298424 0.66783507]\r\n",
      " [0.02596875 0.2432788  0.73075244]\r\n",
      " [0.76972793 0.1070894  0.12318268]\r\n",
      " [0.02335639 0.63957118 0.33707243]\r\n",
      " [0.89867346 0.0626523  0.03867424]\r\n",
      " [0.17479524 0.72945824 0.09574652]\r\n",
      " [0.85992619 0.05320522 0.08686859]\r\n",
      " [0.03497694 0.4103775  0.55464555]\r\n",
      " [0.09182787 0.16295291 0.74521922]\r\n",
      " [0.05728224 0.46660501 0.47611274]\r\n",
      " [0.09527239 0.69077099 0.21395662]\r\n",
      " [0.03023213 0.85087538 0.11889249]\r\n",
      " [0.03041606 0.69589436 0.27368958]\r\n",
      " [0.04996097 0.32916028 0.62087875]\r\n",
      " [0.02924233 0.35253446 0.61822322]\r\n",
      " [0.04110754 0.06088325 0.89800922]\r\n",
      " [0.90477906 0.04144157 0.05377937]\r\n",
      " [0.89098558 0.06480391 0.04421051]\r\n",
      " [0.05124745 0.53656396 0.41218859]\r\n",
      " [0.06108195 0.38729758 0.55162048]\r\n",
      " [0.60114554 0.17916129 0.21969318]\r\n",
      " [0.02899822 0.20312612 0.76787566]\r\n",
      " [0.25158358 0.51709348 0.23132295]\r\n",
      " [0.02492372 0.5087932  0.46628308]\r\n",
      " [0.15451362 0.61649768 0.22898871]\r\n",
      " [0.07828535 0.63396284 0.28775182]\r\n",
      " [0.02540989 0.43281843 0.54177168]\r\n",
      " [0.04520133 0.55495497 0.3998437 ]\r\n",
      " [0.15211521 0.63321879 0.21466599]\r\n",
      " [0.86708715 0.06940963 0.06350321]\r\n",
      " [0.04356077 0.7525395  0.20389972]\r\n",
      " [0.03116613 0.66871997 0.3001139 ]\r\n",
      " [0.02386249 0.74255241 0.2335851 ]\r\n",
      " [0.04595065 0.7761024  0.17794695]\r\n",
      " [0.0283117  0.5314711  0.4402172 ]\r\n",
      " [0.02837899 0.2007702  0.77085081]\r\n",
      " [0.04795036 0.17716501 0.77488463]\r\n",
      " [0.03230515 0.2762995  0.69139535]\r\n",
      " [0.03197383 0.26181066 0.70621551]\r\n",
      " [0.03445342 0.72141118 0.24413539]\r\n",
      " [0.70185539 0.12980924 0.16833537]\r\n",
      " [0.04489825 0.16675603 0.78834572]\r\n",
      " [0.94010068 0.03066437 0.02923495]\r\n",
      " [0.03142962 0.09830208 0.8702683 ]\r\n",
      " [0.05714528 0.56791773 0.37493699]\r\n",
      " [0.03075438 0.52641733 0.44282829]\r\n",
      " [0.03012269 0.21543843 0.75443888]\r\n",
      " [0.05660126 0.77270379 0.17069495]\r\n",
      " [0.05801468 0.54477257 0.39721275]\r\n",
      " [0.04988986 0.53838651 0.41172363]\r\n",
      " [0.86849604 0.06773384 0.06377012]\r\n",
      " [0.16968533 0.28819876 0.54211591]\r\n",
      " [0.82194655 0.10778613 0.07026732]\r\n",
      " [0.06962655 0.52990091 0.40047253]\r\n",
      " [0.09671297 0.40215679 0.50113023]\r\n",
      " [0.03054811 0.03891654 0.93053535]\r\n",
      " [0.04236912 0.7311255  0.22650538]\r\n",
      " [0.02814824 0.24800791 0.72384384]\r\n",
      " [0.77211843 0.11965832 0.10822325]\r\n",
      " [0.22651054 0.16421376 0.6092757 ]\r\n",
      " [0.0251612  0.74296183 0.23187697]\r\n",
      " [0.03285424 0.11844576 0.8487    ]\r\n",
      " [0.02462442 0.12116917 0.85420641]\r\n",
      " [0.02742358 0.59263659 0.37993983]\r\n",
      " [0.73975573 0.14853404 0.11171023]\r\n",
      " [0.02753451 0.25302339 0.7194421 ]\r\n",
      " [0.08025476 0.54151413 0.3782311 ]\r\n",
      " [0.85306641 0.07041996 0.07651364]\r\n",
      " [0.43075584 0.33518984 0.23405432]\r\n",
      " [0.05763527 0.42530106 0.51706367]\r\n",
      " [0.76231598 0.14981584 0.08786818]\r\n",
      " [0.08495854 0.70151988 0.21352158]\r\n",
      " [0.21491527 0.14636121 0.63872351]\r\n",
      " [0.0459938  0.81384078 0.14016541]\r\n",
      " [0.02882199 0.29110734 0.68007067]\r\n",
      " [0.04361522 0.46142992 0.49495486]\r\n",
      " [0.05195256 0.18443599 0.76361145]\r\n",
      " [0.3027376  0.53334537 0.16391702]\r\n",
      " [0.4974879  0.33847649 0.16403561]\r\n",
      " [0.48206519 0.14677779 0.37115702]\r\n",
      " [0.04576326 0.26590097 0.68833577]\r\n",
      " [0.04031706 0.58771656 0.37196638]\r\n",
      " [0.03214053 0.90950719 0.05835227]\r\n",
      " [0.02601457 0.32454545 0.64943998]\r\n",
      " [0.79694333 0.07140167 0.131655  ]\r\n",
      " [0.03252883 0.04966747 0.9178037 ]\r\n",
      " [0.08291878 0.78486031 0.1322209 ]\r\n",
      " [0.54147447 0.40808919 0.05043634]\r\n",
      " [0.74255613 0.09779414 0.15964973]\r\n",
      " [0.84097194 0.06795336 0.0910747 ]\r\n",
      " [0.02603131 0.29834637 0.67562232]\r\n",
      " [0.13874136 0.04289669 0.81836195]\r\n",
      " [0.83677685 0.07637733 0.08684581]\r\n",
      " [0.03956719 0.53078181 0.429651  ]\r\n",
      " [0.40222136 0.3987908  0.19898783]\r\n",
      " [0.03091806 0.6504137  0.31866825]\r\n",
      " [0.46439573 0.04539192 0.49021235]\r\n",
      " [0.76413496 0.0627146  0.17315045]\r\n",
      " [0.74434461 0.16560639 0.090049  ]\r\n",
      " [0.49161866 0.33207792 0.17630341]\r\n",
      " [0.57775429 0.24800316 0.17424255]\r\n",
      " [0.04591663 0.10604549 0.84803787]\r\n",
      " [0.04517663 0.19898941 0.75583396]\r\n",
      " [0.08398841 0.55649818 0.35951342]\r\n",
      " [0.15589739 0.68514087 0.15896174]\r\n",
      " [0.03685856 0.07086399 0.89227745]\r\n",
      " [0.03915855 0.28129738 0.67954408]\r\n",
      " [0.03267077 0.76959106 0.19773817]\r\n",
      " [0.79890261 0.08110195 0.11999544]\r\n",
      " [0.39912727 0.06634391 0.53452882]\r\n",
      " [0.04649407 0.86626258 0.08724335]\r\n",
      " [0.81522039 0.07689723 0.10788238]\r\n",
      " [0.02987068 0.04195344 0.92817588]\r\n",
      " [0.03297595 0.09088696 0.87613709]\r\n",
      " [0.09102228 0.76581317 0.14316455]\r\n",
      " [0.0386282  0.63018342 0.33118837]\r\n",
      " [0.07967682 0.77347203 0.14685115]\r\n",
      " [0.13932094 0.69661201 0.16406705]\r\n",
      " [0.0459764  0.64478755 0.30923605]\r\n",
      " [0.5769421  0.17493689 0.24812101]\r\n",
      " [0.03910379 0.06895454 0.89194167]\r\n",
      " [0.02780899 0.86492005 0.10727095]\r\n",
      " [0.03355583 0.53946561 0.42697856]\r\n",
      " [0.02909204 0.62754909 0.34335887]\r\n",
      " [0.86509026 0.06075611 0.07415364]\r\n",
      " [0.03316095 0.16970704 0.79713201]\r\n",
      " [0.02915376 0.53603651 0.43480973]\r\n",
      " [0.0356029  0.90830438 0.05609272]\r\n",
      " [0.09318169 0.71771225 0.18910606]\r\n",
      " [0.04724894 0.19557519 0.75717588]\r\n",
      " [0.15998368 0.42889773 0.41111859]\r\n",
      " [0.06637093 0.3463142  0.58731487]\r\n",
      " [0.02561683 0.13649428 0.83788888]\r\n",
      " [0.91675064 0.04256332 0.04068605]\r\n",
      " [0.06436597 0.82096692 0.11466709]\r\n",
      " [0.74442879 0.13266872 0.12290249]\r\n",
      " [0.04905239 0.8529126  0.09803501]\r\n",
      " [0.02856826 0.73258312 0.23884862]\r\n",
      " [0.06032985 0.78194233 0.15772783]\r\n",
      " [0.0287189  0.24109181 0.73018929]\r\n",
      " [0.07853759 0.77651714 0.14494527]\r\n",
      " [0.02682361 0.25441925 0.71875714]\r\n",
      " [0.84296559 0.04384567 0.11318874]\r\n",
      " [0.04830858 0.32804297 0.62364845]\r\n",
      " [0.0232741  0.31194414 0.66478175]\r\n",
      " [0.04910497 0.40902292 0.54187211]\r\n",
      " [0.03709877 0.88036909 0.08253214]\r\n",
      " [0.03230381 0.20370213 0.76399405]\r\n",
      " [0.18681716 0.4681796  0.34500324]\r\n",
      " [0.88935423 0.04455198 0.06609379]\r\n",
      " [0.03284425 0.72650892 0.24064683]\r\n",
      " [0.79254289 0.12987376 0.07758335]\r\n",
      " [0.62091095 0.32249997 0.05658908]\r\n",
      " [0.03685417 0.47770535 0.48544049]\r\n",
      " [0.03622314 0.71062302 0.25315384]\r\n",
      " [0.84110183 0.06393794 0.09496022]\r\n",
      " [0.18232283 0.43383735 0.38383982]\r\n",
      " [0.09806442 0.52519564 0.37673993]\r\n",
      " [0.40718899 0.36273124 0.23007977]\r\n",
      " [0.06465693 0.69024744 0.24509563]\r\n",
      " [0.04396077 0.59598758 0.36005165]\r\n",
      " [0.11355797 0.23548911 0.65095291]\r\n",
      " [0.94483532 0.03133411 0.02383057]\r\n",
      " [0.75082913 0.15380452 0.09536635]\r\n",
      " [0.02552869 0.89460587 0.07986544]\r\n",
      " [0.02761223 0.76551315 0.20687462]\r\n",
      " [0.03480826 0.30205585 0.66313589]\r\n",
      " [0.03320546 0.04688907 0.91990547]\r\n",
      " [0.51145925 0.3293878  0.15915296]\r\n",
      " [0.08593741 0.52033346 0.39372913]\r\n",
      " [0.63225121 0.2541392  0.11360959]\r\n",
      " [0.77859423 0.11643653 0.10496923]\r\n",
      " [0.02678997 0.21681732 0.75639271]\r\n",
      " [0.22010179 0.43891709 0.34098112]\r\n",
      " [0.06699434 0.6920857  0.24091996]\r\n",
      " [0.08507886 0.84853426 0.06638689]\r\n",
      " [0.04236672 0.67604918 0.2815841 ]\r\n",
      " [0.02499829 0.67301858 0.30198313]\r\n",
      " [0.09695996 0.86192928 0.04111076]\r\n",
      " [0.93628149 0.03073825 0.03298026]\r\n",
      " [0.02841997 0.93226402 0.03931601]\r\n",
      " [0.62183267 0.19201889 0.18614844]\r\n",
      " [0.04268431 0.82818827 0.12912742]\r\n",
      " [0.04153094 0.53503555 0.42343351]\r\n",
      " [0.92225689 0.03620477 0.04153834]\r\n",
      " [0.02582473 0.44848593 0.52568934]\r\n",
      " [0.84873365 0.07147484 0.07979151]\r\n",
      " [0.04398258 0.33370721 0.62231021]\r\n",
      " [0.06278113 0.66834611 0.26887276]\r\n",
      " [0.48311983 0.14527203 0.37160813]\r\n",
      " [0.14701993 0.71781838 0.13516169]\r\n",
      " [0.89214009 0.07246439 0.03539552]\r\n",
      " [0.74504787 0.14585995 0.10909219]\r\n",
      " [0.04480109 0.86333852 0.09186039]\r\n",
      " [0.10270733 0.76424013 0.13305255]\r\n",
      " [0.8004177  0.09255905 0.10702325]\r\n",
      " [0.03030624 0.28591667 0.68377709]\r\n",
      " [0.04252528 0.848396   0.10907872]\r\n",
      " [0.82594586 0.03489325 0.13916089]\r\n",
      " [0.02791524 0.2843725  0.68771226]\r\n",
      " [0.07229533 0.67034944 0.25735523]\r\n",
      " [0.03784113 0.28148777 0.6806711 ]\r\n",
      " [0.07173557 0.72948506 0.19877937]\r\n",
      " [0.18270746 0.59984559 0.21744695]\r\n",
      " [0.85677854 0.07092364 0.07229782]\r\n",
      " [0.02808505 0.50074557 0.47116938]\r\n",
      " [0.02289974 0.89851209 0.07858817]\r\n",
      " [0.88785603 0.05805122 0.05409275]\r\n",
      " [0.02960292 0.75186325 0.21853383]\r\n",
      " [0.02728406 0.15658848 0.81612746]\r\n",
      " [0.02564244 0.28713388 0.68722369]\r\n",
      " [0.67513135 0.18134928 0.14351936]\r\n",
      " [0.30170938 0.51203759 0.18625301]\r\n",
      " [0.02995089 0.27109077 0.69895834]\r\n",
      " [0.04688904 0.40259087 0.55052008]\r\n",
      " [0.0342048  0.04755881 0.91823639]\r\n",
      " [0.02703589 0.07938923 0.89357488]\r\n",
      " [0.02567904 0.34993362 0.62438734]\r\n",
      " [0.94649523 0.02811474 0.02539003]\r\n",
      " [0.05522002 0.83121133 0.11356864]\r\n",
      " [0.02998997 0.5025974  0.46741263]\r\n",
      " [0.09443744 0.74449152 0.16107104]\r\n",
      " [0.02543318 0.2165295  0.75803732]\r\n",
      " [0.02788754 0.47633825 0.49577421]\r\n",
      " [0.03176587 0.2503223  0.71791183]\r\n",
      " [0.87502302 0.06490404 0.06007293]\r\n",
      " [0.10164851 0.30288714 0.59546435]\r\n",
      " [0.6605302  0.08307778 0.25639202]\r\n",
      " [0.07298672 0.74715589 0.17985739]\r\n",
      " [0.56846394 0.29237975 0.1391563 ]\r\n",
      " [0.04005459 0.79257402 0.16737139]\r\n",
      " [0.02937327 0.20647469 0.76415204]\r\n",
      " [0.02686215 0.33700059 0.63613727]\r\n",
      " [0.05000685 0.06722871 0.88276444]\r\n",
      " [0.03227471 0.35076337 0.61696192]\r\n",
      " [0.03329871 0.50742856 0.45927273]\r\n",
      " [0.85916049 0.06883296 0.07200654]\r\n",
      " [0.06400444 0.83980373 0.09619183]\r\n",
      " [0.93658808 0.03893233 0.02447959]\r\n",
      " [0.03165479 0.16211785 0.80622736]\r\n",
      " [0.16801963 0.74131443 0.09066595]\r\n",
      " [0.02766751 0.59489532 0.37743717]\r\n",
      " [0.83205843 0.09097067 0.0769709 ]\r\n",
      " [0.02279164 0.86742378 0.10978458]\r\n",
      " [0.86028819 0.07338341 0.0663284 ]\r\n",
      " [0.03478557 0.31716767 0.64804675]\r\n",
      " [0.04508986 0.22947082 0.72543933]\r\n",
      " [0.02742652 0.07237438 0.9001991 ]\r\n",
      " [0.89147836 0.0568829  0.05163874]\r\n",
      " [0.02856213 0.71884356 0.25259432]\r\n",
      " [0.02405348 0.13474316 0.84120336]\r\n",
      " [0.11486118 0.53023877 0.35490006]\r\n",
      " [0.9141155  0.05087362 0.03501088]\r\n",
      " [0.64556616 0.20441231 0.15002152]\r\n",
      " [0.03428219 0.74908661 0.21663119]\r\n",
      " [0.02810057 0.17099184 0.8009076 ]\r\n",
      " [0.06817099 0.25966855 0.67216046]\r\n",
      " [0.83103621 0.0936958  0.07526799]\r\n",
      " [0.56961582 0.18840309 0.24198109]\r\n",
      " [0.03322972 0.14456373 0.82220655]\r\n",
      " [0.90228896 0.05643742 0.04127362]\r\n",
      " [0.0364014  0.15049076 0.81310784]\r\n",
      " [0.05076919 0.11433402 0.8348968 ]\r\n",
      " [0.85335161 0.06040156 0.08624683]\r\n",
      " [0.02692934 0.61694557 0.3561251 ]\r\n",
      " [0.03045681 0.09875246 0.87079073]]\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9811941159174046\r\n",
      "0.9947057928964758\r\n",
      "Confidence interval for the score: [0.981 - 0.995]\r\n",
      "[[1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 1. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 0. 1.]\r\n",
      " [0. 0. 1.]\r\n",
      " [1. 0. 0.]\r\n",
      " [0. 1. 0.]\r\n",
      " [0. 0. 1.]]\r\n",
      "[[1.00000000e+00 0.00000000e+00 0.00000000e+00]\r\n",
      " [0.00000000e+00 3.69290573e-02 9.63070943e-01]\r\n",
      " [9.25925926e-03 1.85185185e-02 9.72222222e-01]\r\n",
      " [0.00000000e+00 4.81781377e-01 5.18218623e-01]\r\n",
      " [8.10773481e-01 1.47790055e-01 4.14364641e-02]\r\n",
      " [1.31061599e-03 6.67103539e-01 3.31585845e-01]\r\n",
      " [0.00000000e+00 3.45153664e-01 6.54846336e-01]\r\n",
      " [5.41025641e-01 4.12820513e-01 4.61538462e-02]\r\n",
      " [1.98609732e-03 1.19165839e-01 8.78848064e-01]\r\n",
      " [3.16154971e-02 1.39437135e-01 8.28947368e-01]\r\n",
      " [3.95647873e-03 8.61523244e-01 1.34520277e-01]\r\n",
      " [8.95364238e-01 1.32450331e-03 1.03311258e-01]\r\n",
      " [5.06756757e-02 7.26351351e-01 2.22972973e-01]\r\n",
      " [9.44649446e-01 1.66051661e-02 3.87453875e-02]\r\n",
      " [1.06635071e-02 9.07582938e-01 8.17535545e-02]\r\n",
      " [0.00000000e+00 4.68150422e-02 9.53184958e-01]\r\n",
      " [0.00000000e+00 6.58761528e-04 9.99341238e-01]\r\n",
      " [4.98960499e-03 5.13929314e-01 4.81081081e-01]\r\n",
      " [0.00000000e+00 6.74378749e-01 3.25621251e-01]\r\n",
      " [2.18340611e-03 9.43231441e-01 5.45851528e-02]\r\n",
      " [3.78787879e-02 5.26515152e-01 4.35606061e-01]\r\n",
      " [8.84097035e-01 1.88679245e-02 9.70350404e-02]\r\n",
      " [3.01910043e-02 4.06654344e-02 9.29143561e-01]\r\n",
      " [0.00000000e+00 6.41102457e-02 9.35889754e-01]\r\n",
      " [8.19672131e-03 2.49414520e-01 7.42388759e-01]\r\n",
      " [7.28155340e-03 2.30582524e-01 7.62135922e-01]\r\n",
      " [8.52097130e-01 1.13318617e-01 3.45842531e-02]\r\n",
      " [3.32749562e-02 7.40805604e-01 2.25919440e-01]\r\n",
      " [1.13895216e-03 8.31435080e-02 9.15717540e-01]\r\n",
      " [9.06976744e-01 7.20930233e-02 2.09302326e-02]\r\n",
      " [3.34928230e-02 7.70334928e-01 1.96172249e-01]\r\n",
      " [4.86618005e-03 1.11922141e-01 8.83211679e-01]\r\n",
      " [1.74927114e-02 1.04956268e-01 8.77551020e-01]\r\n",
      " [8.36820084e-03 1.12040911e-01 8.79590888e-01]\r\n",
      " [0.00000000e+00 1.12338858e-01 8.87661142e-01]\r\n",
      " [8.88809182e-01 7.89096126e-03 1.03299857e-01]\r\n",
      " [2.45098039e-03 8.94607843e-01 1.02941176e-01]\r\n",
      " [7.69230769e-02 8.20512821e-02 8.41025641e-01]\r\n",
      " [1.82926829e-02 2.18292683e-01 7.63414634e-01]\r\n",
      " [5.76759966e-03 8.02714165e-01 1.91518236e-01]\r\n",
      " [9.68000000e-01 2.40000000e-02 8.00000000e-03]\r\n",
      " [0.00000000e+00 5.20888889e-01 4.79111111e-01]\r\n",
      " [0.00000000e+00 4.21729807e-02 9.57827019e-01]\r\n",
      " [0.00000000e+00 4.09126672e-02 9.59087333e-01]\r\n",
      " [9.01408451e-01 2.81690141e-02 7.04225352e-02]\r\n",
      " [8.26019618e-03 4.64636035e-02 9.45276200e-01]\r\n",
      " [1.09890110e-02 8.59340659e-01 1.29670330e-01]\r\n",
      " [2.81622912e-02 3.64677804e-01 6.07159905e-01]\r\n",
      " [5.55941626e-03 1.61917999e-01 8.32522585e-01]\r\n",
      " [1.52671756e-03 8.35114504e-01 1.63358779e-01]\r\n",
      " [0.00000000e+00 1.94174757e-02 9.80582524e-01]\r\n",
      " [0.00000000e+00 3.92609700e-02 9.60739030e-01]\r\n",
      " [9.15074310e-01 4.03397028e-02 4.45859873e-02]\r\n",
      " [3.21839080e-02 5.93103448e-01 3.74712644e-01]\r\n",
      " [9.67283951e-01 2.71604938e-02 5.55555556e-03]\r\n",
      " [9.34426230e-01 2.45901639e-02 4.09836066e-02]\r\n",
      " [8.34151129e-03 1.91364082e-02 9.72522080e-01]\r\n",
      " [8.11359026e-03 8.66125761e-01 1.25760649e-01]\r\n",
      " [2.00000000e-03 2.26000000e-01 7.72000000e-01]\r\n",
      " [0.00000000e+00 2.13429257e-01 7.86570743e-01]\r\n",
      " [9.07801418e-01 4.96453901e-02 4.25531915e-02]\r\n",
      " [0.00000000e+00 7.23893805e-01 2.76106195e-01]\r\n",
      " [9.78609626e-01 1.87165775e-02 2.67379679e-03]\r\n",
      " [1.83035714e-01 7.63392857e-01 5.35714286e-02]\r\n",
      " [9.63503650e-01 0.00000000e+00 3.64963504e-02]\r\n",
      " [3.80228137e-03 4.06021992e-01 5.90175727e-01]\r\n",
      " [6.43274854e-02 9.69785575e-02 8.38693957e-01]\r\n",
      " [2.14652357e-02 4.76901540e-01 5.01633224e-01]\r\n",
      " [8.35762877e-02 7.49271137e-01 1.67152575e-01]\r\n",
      " [0.00000000e+00 9.49432892e-01 5.05671078e-02]\r\n",
      " [3.90625000e-03 7.69531250e-01 2.26562500e-01]\r\n",
      " [1.63934426e-02 2.88056206e-01 6.95550351e-01]\r\n",
      " [0.00000000e+00 3.32914573e-01 6.67085427e-01]\r\n",
      " [1.42226582e-02 1.91270231e-02 9.66650319e-01]\r\n",
      " [9.77664975e-01 4.06091371e-03 1.82741117e-02]\r\n",
      " [9.66269841e-01 1.98412698e-02 1.38888889e-02]\r\n",
      " [1.51006711e-02 5.80536913e-01 4.04362416e-01]\r\n",
      " [3.18664643e-02 3.68740516e-01 5.99393020e-01]\r\n",
      " [6.96969697e-01 1.41414141e-01 1.61616162e-01]\r\n",
      " [1.69635284e-03 1.46734521e-01 8.51569126e-01]\r\n",
      " [2.27544910e-01 5.68862275e-01 2.03592814e-01]\r\n",
      " [0.00000000e+00 5.33407572e-01 4.66592428e-01]\r\n",
      " [1.23119557e-01 7.01108472e-01 1.75771971e-01]\r\n",
      " [3.69003690e-02 7.19557196e-01 2.43542435e-01]\r\n",
      " [0.00000000e+00 4.13309982e-01 5.86690018e-01]\r\n",
      " [1.29533679e-02 5.99417098e-01 3.87629534e-01]\r\n",
      " [1.24933546e-01 7.39500266e-01 1.35566188e-01]\r\n",
      " [9.85714286e-01 1.42857143e-02 0.00000000e+00]\r\n",
      " [1.60069219e-02 8.17001947e-01 1.66991131e-01]\r\n",
      " [4.21496312e-03 7.65015806e-01 2.30769231e-01]\r\n",
      " [0.00000000e+00 8.26542491e-01 1.73457509e-01]\r\n",
      " [1.26582278e-02 8.84493671e-01 1.02848101e-01]\r\n",
      " [1.86567164e-03 5.41044776e-01 4.57089552e-01]\r\n",
      " [1.11172874e-03 1.40633685e-01 8.58254586e-01]\r\n",
      " [1.43084261e-02 1.04928458e-01 8.80763116e-01]\r\n",
      " [3.81679389e-03 2.08015267e-01 7.88167939e-01]\r\n",
      " [0.00000000e+00 2.17054264e-01 7.82945736e-01]\r\n",
      " [4.24929178e-03 7.80453258e-01 2.15297450e-01]\r\n",
      " [8.26869806e-01 6.78670360e-02 1.05263158e-01]\r\n",
      " [1.87188020e-02 1.26039933e-01 8.55241265e-01]\r\n",
      " [1.00000000e+00 0.00000000e+00 0.00000000e+00]\r\n",
      " [2.52206810e-03 4.72887768e-02 9.50189155e-01]\r\n",
      " [2.22222222e-02 6.31372549e-01 3.46405229e-01]\r\n",
      " [1.85988841e-03 5.45567266e-01 4.52572846e-01]\r\n",
      " [1.53846154e-03 1.83076923e-01 8.15384615e-01]\r\n",
      " [1.91176471e-02 8.49019608e-01 1.31862745e-01]\r\n",
      " [1.53482881e-02 6.12750885e-01 3.71900826e-01]\r\n",
      " [0.00000000e+00 5.36585366e-01 4.63414634e-01]\r\n",
      " [9.81617647e-01 7.35294118e-03 1.10294118e-02]\r\n",
      " [1.55875300e-01 2.13429257e-01 6.30695444e-01]\r\n",
      " [9.06525573e-01 7.05467372e-02 2.29276896e-02]\r\n",
      " [2.07373272e-02 5.66820276e-01 4.12442396e-01]\r\n",
      " [7.69230769e-02 4.48717949e-01 4.74358974e-01]\r\n",
      " [2.94117647e-03 0.00000000e+00 9.97058824e-01]\r\n",
      " [1.30500759e-02 8.30652504e-01 1.56297420e-01]\r\n",
      " [0.00000000e+00 1.52802893e-01 8.47197107e-01]\r\n",
      " [9.21933086e-01 3.34572491e-02 4.46096654e-02]\r\n",
      " [2.20279720e-01 8.39160839e-02 6.95804196e-01]\r\n",
      " [0.00000000e+00 7.96324655e-01 2.03675345e-01]\r\n",
      " [6.55737705e-03 6.06557377e-02 9.32786885e-01]\r\n",
      " [0.00000000e+00 5.93379138e-02 9.40662086e-01]\r\n",
      " [4.09836066e-03 6.72131148e-01 3.23770492e-01]\r\n",
      " [8.60428232e-01 8.56463125e-02 5.39254560e-02]\r\n",
      " [0.00000000e+00 1.93589744e-01 8.06410256e-01]\r\n",
      " [3.59322034e-02 6.21694915e-01 3.42372881e-01]\r\n",
      " [9.47963801e-01 2.71493213e-02 2.48868778e-02]\r\n",
      " [4.74803150e-01 3.20472441e-01 2.04724409e-01]\r\n",
      " [3.96853772e-02 4.12942438e-01 5.47372184e-01]\r\n",
      " [8.97674419e-01 8.37209302e-02 1.86046512e-02]\r\n",
      " [5.16304348e-02 8.15217391e-01 1.33152174e-01]\r\n",
      " [2.16450216e-01 7.79220779e-02 7.05627706e-01]\r\n",
      " [1.23203285e-02 9.41478439e-01 4.62012320e-02]\r\n",
      " [0.00000000e+00 2.54814815e-01 7.45185185e-01]\r\n",
      " [1.71796707e-02 4.49534717e-01 5.33285612e-01]\r\n",
      " [2.03045685e-02 1.16751269e-01 8.62944162e-01]\r\n",
      " [3.10077519e-01 5.94315245e-01 9.56072351e-02]\r\n",
      " [5.49407115e-01 3.22134387e-01 1.28458498e-01]\r\n",
      " [5.62231760e-01 5.36480687e-02 3.84120172e-01]\r\n",
      " [1.78571429e-02 2.04761905e-01 7.77380952e-01]\r\n",
      " [1.09689214e-02 6.50822669e-01 3.38208410e-01]\r\n",
      " [8.84955752e-03 9.79351032e-01 1.17994100e-02]\r\n",
      " [0.00000000e+00 2.73037543e-01 7.26962457e-01]\r\n",
      " [9.00481541e-01 1.44462279e-02 8.50722311e-02]\r\n",
      " [4.11985019e-03 8.98876404e-03 9.86891386e-01]\r\n",
      " [5.38827258e-02 8.63708399e-01 8.24088748e-02]\r\n",
      " [5.86505190e-01 4.10034602e-01 3.46020761e-03]\r\n",
      " [8.60421836e-01 3.41191067e-02 1.05459057e-01]\r\n",
      " [9.51704545e-01 1.70454545e-02 3.12500000e-02]\r\n",
      " [4.20875421e-04 2.36111111e-01 7.63468013e-01]\r\n",
      " [1.23348018e-01 5.28634361e-03 8.71365639e-01]\r\n",
      " [9.71052632e-01 7.89473684e-03 2.10526316e-02]\r\n",
      " [7.53138075e-03 5.53974895e-01 4.38493724e-01]\r\n",
      " [4.40298507e-01 4.45273632e-01 1.14427861e-01]\r\n",
      " [1.81159420e-03 7.30978261e-01 2.67210145e-01]\r\n",
      " [4.80996622e-01 7.17905405e-03 5.11824324e-01]\r\n",
      " [8.80368098e-01 6.13496933e-03 1.13496933e-01]\r\n",
      " [8.96103896e-01 7.79220779e-02 2.59740260e-02]\r\n",
      " [6.13636364e-01 2.95454545e-01 9.09090909e-02]\r\n",
      " [6.95652174e-01 1.73913043e-01 1.30434783e-01]\r\n",
      " [1.70519136e-02 4.81242895e-02 9.34823797e-01]\r\n",
      " [5.41516245e-03 1.19133574e-01 8.75451264e-01]\r\n",
      " [3.61445783e-02 6.02409639e-01 3.61445783e-01]\r\n",
      " [1.40482786e-01 7.36842105e-01 1.22675109e-01]\r\n",
      " [6.49350649e-03 5.56586271e-03 9.87940631e-01]\r\n",
      " [9.72590628e-03 2.49336870e-01 7.40937224e-01]\r\n",
      " [5.08474576e-03 8.49152542e-01 1.45762712e-01]\r\n",
      " [8.75675676e-01 5.13513514e-02 7.29729730e-02]\r\n",
      " [3.95161290e-01 8.06451613e-03 5.96774194e-01]\r\n",
      " [1.77321512e-02 9.31871209e-01 5.03966402e-02]\r\n",
      " [9.30080117e-01 2.18499636e-02 4.80699199e-02]\r\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00]\r\n",
      " [1.97889182e-03 2.70448549e-02 9.70976253e-01]\r\n",
      " [2.81690141e-02 9.01408451e-01 7.04225352e-02]\r\n",
      " [1.47814910e-02 7.05655527e-01 2.79562982e-01]\r\n",
      " [3.58649789e-02 8.66033755e-01 9.81012658e-02]\r\n",
      " [1.20381175e-01 7.79463244e-01 1.00155581e-01]\r\n",
      " [0.00000000e+00 6.90909091e-01 3.09090909e-01]\r\n",
      " [6.87224670e-01 9.69162996e-02 2.15859031e-01]\r\n",
      " [5.18518519e-03 2.85185185e-02 9.66296296e-01]\r\n",
      " [0.00000000e+00 9.22330097e-01 7.76699029e-02]\r\n",
      " [7.52823087e-03 5.63362610e-01 4.29109159e-01]\r\n",
      " [0.00000000e+00 7.20000000e-01 2.80000000e-01]\r\n",
      " [9.65736041e-01 1.64974619e-02 1.77664975e-02]\r\n",
      " [4.35729847e-03 6.82643428e-02 9.27378359e-01]\r\n",
      " [8.41042893e-04 5.99543434e-01 3.99615523e-01]\r\n",
      " [0.00000000e+00 9.85915493e-01 1.40845070e-02]\r\n",
      " [7.14514579e-02 8.11598847e-01 1.16949696e-01]\r\n",
      " [9.07029478e-03 1.13378685e-01 8.77551020e-01]\r\n",
      " [1.13043478e-01 4.43478261e-01 4.43478261e-01]\r\n",
      " [3.37078652e-02 2.80898876e-01 6.85393258e-01]\r\n",
      " [0.00000000e+00 5.81039755e-02 9.41896024e-01]\r\n",
      " [9.87421384e-01 1.25786164e-02 0.00000000e+00]\r\n",
      " [5.76923077e-02 9.42307692e-01 0.00000000e+00]\r\n",
      " [8.72340426e-01 7.44680851e-02 5.31914894e-02]\r\n",
      " [1.38888889e-02 9.31818182e-01 5.42929293e-02]\r\n",
      " [0.00000000e+00 8.25852783e-01 1.74147217e-01]\r\n",
      " [2.85714286e-02 9.20634921e-01 5.07936508e-02]\r\n",
      " [0.00000000e+00 1.88524590e-01 8.11475410e-01]\r\n",
      " [6.35930048e-02 8.39427663e-01 9.69793323e-02]\r\n",
      " [0.00000000e+00 1.83874140e-01 8.16125860e-01]\r\n",
      " [9.26613616e-01 1.76834660e-03 7.16180371e-02]\r\n",
      " [2.46710526e-02 2.67269737e-01 7.08059211e-01]\r\n",
      " [0.00000000e+00 2.96296296e-01 7.03703704e-01]\r\n",
      " [1.76358437e-02 4.08484271e-01 5.73879886e-01]\r\n",
      " [2.84090909e-03 9.74431818e-01 2.27272727e-02]\r\n",
      " [1.54679041e-03 1.26836814e-01 8.71616396e-01]\r\n",
      " [2.06140351e-01 5.00000000e-01 2.93859649e-01]\r\n",
      " [9.74683544e-01 6.32911392e-03 1.89873418e-02]\r\n",
      " [3.78787879e-03 8.03030303e-01 1.93181818e-01]\r\n",
      " [8.99503722e-01 8.56079404e-02 1.48883375e-02]\r\n",
      " [6.92406692e-01 2.96010296e-01 1.15830116e-02]\r\n",
      " [1.03896104e-02 4.82251082e-01 5.07359307e-01]\r\n",
      " [4.72334683e-03 7.75641026e-01 2.19635628e-01]\r\n",
      " [9.60431655e-01 3.59712230e-03 3.59712230e-02]\r\n",
      " [1.24528302e-01 4.83018868e-01 3.92452830e-01]\r\n",
      " [5.30035336e-02 5.54770318e-01 3.92226148e-01]\r\n",
      " [4.57109283e-01 3.60752056e-01 1.82138660e-01]\r\n",
      " [3.55029586e-02 7.98816568e-01 1.65680473e-01]\r\n",
      " [8.54700855e-03 6.35327635e-01 3.56125356e-01]\r\n",
      " [5.88235294e-02 1.07843137e-01 8.33333333e-01]\r\n",
      " [1.00000000e+00 0.00000000e+00 0.00000000e+00]\r\n",
      " [8.09230769e-01 1.29230769e-01 6.15384615e-02]\r\n",
      " [0.00000000e+00 9.66216216e-01 3.37837838e-02]\r\n",
      " [0.00000000e+00 8.27450980e-01 1.72549020e-01]\r\n",
      " [0.00000000e+00 2.82258065e-01 7.17741935e-01]\r\n",
      " [0.00000000e+00 4.32900433e-03 9.95670996e-01]\r\n",
      " [6.66666667e-01 3.33333333e-01 0.00000000e+00]\r\n",
      " [4.51127820e-02 5.31328321e-01 4.23558897e-01]\r\n",
      " [7.71929825e-01 1.57894737e-01 7.01754386e-02]\r\n",
      " [9.29460581e-01 2.90456432e-02 4.14937759e-02]\r\n",
      " [3.20718409e-04 1.35663887e-01 8.64015394e-01]\r\n",
      " [2.09976798e-01 4.75638051e-01 3.14385151e-01]\r\n",
      " [3.23886640e-02 8.13765182e-01 1.53846154e-01]\r\n",
      " [6.55586334e-02 8.98430286e-01 3.60110803e-02]\r\n",
      " [1.43369176e-02 7.63440860e-01 2.22222222e-01]\r\n",
      " [0.00000000e+00 7.36777368e-01 2.63222632e-01]\r\n",
      " [7.27603456e-02 9.23146885e-01 4.09276944e-03]\r\n",
      " [9.94290375e-01 2.03915171e-03 3.67047308e-03]\r\n",
      " [1.76056338e-03 9.98239437e-01 0.00000000e+00]\r\n",
      " [7.40492170e-01 1.36465324e-01 1.23042506e-01]\r\n",
      " [3.87596899e-03 9.27648579e-01 6.84754522e-02]\r\n",
      " [9.30232558e-03 5.73023256e-01 4.17674419e-01]\r\n",
      " [9.86394558e-01 4.53514739e-03 9.07029478e-03]\r\n",
      " [0.00000000e+00 4.27118644e-01 5.72881356e-01]\r\n",
      " [9.73684211e-01 8.77192982e-03 1.75438596e-02]\r\n",
      " [9.61538462e-03 3.41346154e-01 6.49038462e-01]\r\n",
      " [1.93548387e-02 7.16129032e-01 2.64516129e-01]\r\n",
      " [5.50387597e-01 6.20155039e-02 3.87596899e-01]\r\n",
      " [1.15000000e-01 8.18000000e-01 6.70000000e-02]\r\n",
      " [9.70088375e-01 2.71923861e-02 2.71923861e-03]\r\n",
      " [8.71657754e-01 9.62566845e-02 3.20855615e-02]\r\n",
      " [1.91082803e-02 9.29228592e-01 5.16631281e-02]\r\n",
      " [7.31995277e-02 8.47697757e-01 7.91027155e-02]\r\n",
      " [9.32467532e-01 2.33766234e-02 4.41558442e-02]\r\n",
      " [1.88679245e-03 2.47169811e-01 7.50943396e-01]\r\n",
      " [1.50722855e-02 9.14487850e-01 7.04398647e-02]\r\n",
      " [9.21618205e-01 1.26422250e-03 7.71175727e-02]\r\n",
      " [1.23380629e-03 1.94941394e-01 8.03824800e-01]\r\n",
      " [5.02890173e-02 7.33526012e-01 2.16184971e-01]\r\n",
      " [1.13636364e-02 2.64462810e-01 7.24173554e-01]\r\n",
      " [3.04709141e-02 8.39335180e-01 1.30193906e-01]\r\n",
      " [1.64447018e-01 6.60104227e-01 1.75448755e-01]\r\n",
      " [9.69339623e-01 1.17924528e-02 1.88679245e-02]\r\n",
      " [1.27713921e-03 5.16815666e-01 4.81907195e-01]\r\n",
      " [0.00000000e+00 9.82022472e-01 1.79775281e-02]\r\n",
      " [9.72776770e-01 1.63339383e-02 1.08892922e-02]\r\n",
      " [2.14362272e-03 8.45659164e-01 1.52197213e-01]\r\n",
      " [1.21506683e-03 7.41190765e-02 9.24665857e-01]\r\n",
      " [0.00000000e+00 1.73469388e-01 8.26530612e-01]\r\n",
      " [8.05555556e-01 1.16666667e-01 7.77777778e-02]\r\n",
      " [2.94117647e-01 6.47058824e-01 5.88235294e-02]\r\n",
      " [9.41176471e-04 2.07058824e-01 7.92000000e-01]\r\n",
      " [0.00000000e+00 5.00000000e-01 5.00000000e-01]\r\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00]\r\n",
      " [0.00000000e+00 2.70700637e-02 9.72929936e-01]\r\n",
      " [0.00000000e+00 3.23432343e-01 6.76567657e-01]\r\n",
      " [1.00000000e+00 0.00000000e+00 0.00000000e+00]\r\n",
      " [2.02788340e-02 9.48035488e-01 3.16856781e-02]\r\n",
      " [0.00000000e+00 5.35053554e-01 4.64946446e-01]\r\n",
      " [6.23556582e-02 8.24480370e-01 1.13163972e-01]\r\n",
      " [0.00000000e+00 1.49273448e-01 8.50726552e-01]\r\n",
      " [1.96078431e-03 4.98039216e-01 5.00000000e-01]\r\n",
      " [9.38086304e-04 1.97936210e-01 8.01125704e-01]\r\n",
      " [9.85294118e-01 7.35294118e-03 7.35294118e-03]\r\n",
      " [6.41304348e-02 2.71739130e-01 6.64130435e-01]\r\n",
      " [7.18253968e-01 3.25396825e-02 2.49206349e-01]\r\n",
      " [4.37453920e-02 8.48857213e-01 1.07397395e-01]\r\n",
      " [6.94684796e-01 2.75648949e-01 2.96662546e-02]\r\n",
      " [1.72413793e-02 8.87931034e-01 9.48275862e-02]\r\n",
      " [0.00000000e+00 1.27873563e-01 8.72126437e-01]\r\n",
      " [0.00000000e+00 3.12460864e-01 6.87539136e-01]\r\n",
      " [1.85606061e-02 2.31060606e-02 9.58333333e-01]\r\n",
      " [0.00000000e+00 3.52601156e-01 6.47398844e-01]\r\n",
      " [2.52525253e-03 4.87373737e-01 5.10101010e-01]\r\n",
      " [9.63488844e-01 1.62271805e-02 2.02839757e-02]\r\n",
      " [2.83464567e-02 9.18110236e-01 5.35433071e-02]\r\n",
      " [9.90407674e-01 9.59232614e-03 0.00000000e+00]\r\n",
      " [0.00000000e+00 9.97566910e-02 9.00243309e-01]\r\n",
      " [1.56118143e-01 7.72151899e-01 7.17299578e-02]\r\n",
      " [0.00000000e+00 5.85514834e-01 4.14485166e-01]\r\n",
      " [9.39393939e-01 3.03030303e-02 3.03030303e-02]\r\n",
      " [0.00000000e+00 9.60244648e-01 3.97553517e-02]\r\n",
      " [9.84984985e-01 6.00600601e-03 9.00900901e-03]\r\n",
      " [5.58659218e-03 2.92364991e-01 7.02048417e-01]\r\n",
      " [1.07604017e-02 1.57819225e-01 8.31420373e-01]\r\n",
      " [0.00000000e+00 3.96912900e-02 9.60308710e-01]\r\n",
      " [9.88800000e-01 6.40000000e-03 4.80000000e-03]\r\n",
      " [0.00000000e+00 8.21752266e-01 1.78247734e-01]\r\n",
      " [0.00000000e+00 7.35294118e-02 9.26470588e-01]\r\n",
      " [8.27300931e-02 5.73940021e-01 3.43329886e-01]\r\n",
      " [9.80295567e-01 1.47783251e-02 4.92610837e-03]\r\n",
      " [7.13286713e-01 1.81818182e-01 1.04895105e-01]\r\n",
      " [7.50469043e-03 8.32551595e-01 1.59943715e-01]\r\n",
      " [0.00000000e+00 9.26829268e-02 9.07317073e-01]\r\n",
      " [3.89721627e-02 1.98286938e-01 7.62740899e-01]\r\n",
      " [9.54063604e-01 3.18021201e-02 1.41342756e-02]\r\n",
      " [6.70533643e-01 1.14849188e-01 2.14617169e-01]\r\n",
      " [2.09205021e-03 5.54393305e-02 9.42468619e-01]\r\n",
      " [9.94565217e-01 5.43478261e-03 0.00000000e+00]\r\n",
      " [1.19250426e-02 9.02896082e-02 8.97785349e-01]\r\n",
      " [2.30891720e-02 3.42356688e-02 9.42675159e-01]\r\n",
      " [9.56834532e-01 3.59712230e-03 3.95683453e-02]\r\n",
      " [0.00000000e+00 7.28222997e-01 2.71777003e-01]\r\n",
      " [3.06748466e-03 4.29447853e-02 9.53987730e-01]]\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9788331674848557\r\n",
      "0.9939124733281308\r\n",
      "Confidence interval for the score: [0.979 - 0.994]\r\n",
      "******* FP / TP for average probability\r\n",
      "{0: array([0., 0., 0., 1.]), 1: array([0.   , 0.   , 0.   , 0.005, 0.005, 0.01 , 0.01 , 0.02 , 0.02 ,\r\n",
      "       0.025, 0.025, 0.035, 0.035, 0.04 , 0.04 , 0.045, 0.045, 0.05 ,\r\n",
      "       0.05 , 0.055, 0.055, 0.06 , 0.06 , 0.065, 0.065, 0.09 , 0.09 ,\r\n",
      "       0.105, 0.105, 0.11 , 0.11 , 0.205, 0.205, 0.255, 0.255, 0.305,\r\n",
      "       0.305, 1.   ]), 2: array([0.        , 0.        , 0.        , 0.00473934, 0.00473934,\r\n",
      "       0.00947867, 0.00947867, 0.01421801, 0.01421801, 0.02369668,\r\n",
      "       0.02369668, 0.02843602, 0.02843602, 0.03317536, 0.03317536,\r\n",
      "       0.03791469, 0.03791469, 0.04739336, 0.04739336, 0.06161137,\r\n",
      "       0.06161137, 0.07109005, 0.07109005, 0.07582938, 0.07582938,\r\n",
      "       0.11374408, 0.11374408, 0.13270142, 0.13270142, 0.13744076,\r\n",
      "       0.13744076, 0.20379147, 0.20379147, 0.20853081, 0.20853081,\r\n",
      "       1.        ]), 'macro': array([0.        , 0.00473934, 0.005     , 0.00947867, 0.01      ,\r\n",
      "       0.01421801, 0.02      , 0.02369668, 0.025     , 0.02843602,\r\n",
      "       0.03317536, 0.035     , 0.03791469, 0.04      , 0.045     ,\r\n",
      "       0.04739336, 0.05      , 0.055     , 0.06      , 0.06161137,\r\n",
      "       0.065     , 0.07109005, 0.07582938, 0.09      , 0.105     ,\r\n",
      "       0.11      , 0.11374408, 0.13270142, 0.13744076, 0.20379147,\r\n",
      "       0.205     , 0.20853081, 0.255     , 0.305     , 1.        ]), 'micro': array([0.        , 0.        , 0.        , 0.00154321, 0.00154321,\r\n",
      "       0.00308642, 0.00308642, 0.00462963, 0.00462963, 0.00925926,\r\n",
      "       0.00925926, 0.01234568, 0.01234568, 0.01388889, 0.01388889,\r\n",
      "       0.01697531, 0.01697531, 0.01851852, 0.01851852, 0.02006173,\r\n",
      "       0.02006173, 0.02160494, 0.02160494, 0.02314815, 0.02314815,\r\n",
      "       0.02469136, 0.02469136, 0.02623457, 0.02623457, 0.02932099,\r\n",
      "       0.02932099, 0.0308642 , 0.0308642 , 0.03240741, 0.03240741,\r\n",
      "       0.03395062, 0.03395062, 0.03549383, 0.03549383, 0.03703704,\r\n",
      "       0.03703704, 0.04012346, 0.04012346, 0.04166667, 0.04166667,\r\n",
      "       0.04475309, 0.04475309, 0.05092593, 0.05092593, 0.05246914,\r\n",
      "       0.05246914, 0.05401235, 0.05401235, 0.05555556, 0.05555556,\r\n",
      "       0.0617284 , 0.0617284 , 0.0632716 , 0.0632716 , 0.06635802,\r\n",
      "       0.06635802, 0.06944444, 0.06944444, 0.07253086, 0.07253086,\r\n",
      "       0.07407407, 0.07407407, 0.07561728, 0.07561728, 0.09876543,\r\n",
      "       0.09876543, 0.10030864, 0.10030864, 0.10185185, 0.10185185,\r\n",
      "       0.10339506, 0.10339506, 0.14506173, 0.14506173, 0.16666667,\r\n",
      "       0.16666667, 0.2037037 , 0.2037037 , 1.        ])}\r\n",
      "{0: array([0.        , 0.01149425, 1.        , 1.        ]), 1: array([0.        , 0.00806452, 0.63709677, 0.63709677, 0.69354839,\r\n",
      "       0.69354839, 0.71774194, 0.71774194, 0.75806452, 0.75806452,\r\n",
      "       0.85483871, 0.85483871, 0.86290323, 0.86290323, 0.87096774,\r\n",
      "       0.87096774, 0.88709677, 0.88709677, 0.91129032, 0.91129032,\r\n",
      "       0.91935484, 0.91935484, 0.93548387, 0.93548387, 0.94354839,\r\n",
      "       0.94354839, 0.9516129 , 0.9516129 , 0.96774194, 0.96774194,\r\n",
      "       0.97580645, 0.97580645, 0.98387097, 0.98387097, 0.99193548,\r\n",
      "       0.99193548, 1.        , 1.        ]), 2: array([0.        , 0.00884956, 0.68141593, 0.68141593, 0.69911504,\r\n",
      "       0.69911504, 0.73451327, 0.73451327, 0.81415929, 0.81415929,\r\n",
      "       0.83185841, 0.83185841, 0.86725664, 0.86725664, 0.87610619,\r\n",
      "       0.87610619, 0.89380531, 0.89380531, 0.90265487, 0.90265487,\r\n",
      "       0.91150442, 0.91150442, 0.92920354, 0.92920354, 0.9380531 ,\r\n",
      "       0.9380531 , 0.94690265, 0.94690265, 0.96460177, 0.96460177,\r\n",
      "       0.97345133, 0.97345133, 0.99115044, 0.99115044, 1.        ,\r\n",
      "       1.        ]), 'macro': array([0.77283757, 0.77873727, 0.79755448, 0.80935389, 0.8174184 ,\r\n",
      "       0.84396708, 0.85740794, 0.86330764, 0.89556571, 0.90736512,\r\n",
      "       0.91031497, 0.91300314, 0.91890285, 0.92159102, 0.92696736,\r\n",
      "       0.92991721, 0.93798173, 0.9406699 , 0.94604625, 0.9489961 ,\r\n",
      "       0.95168427, 0.95758398, 0.96053383, 0.963222  , 0.96859834,\r\n",
      "       0.97128652, 0.97423637, 0.98013607, 0.98308593, 0.98898563,\r\n",
      "       0.9916738 , 0.99462366, 0.99731183, 1.        , 1.        ]), 'micro': array([0.        , 0.00308642, 0.64197531, 0.64197531, 0.65740741,\r\n",
      "       0.65740741, 0.68518519, 0.68518519, 0.74691358, 0.74691358,\r\n",
      "       0.77777778, 0.77777778, 0.80246914, 0.80246914, 0.8117284 ,\r\n",
      "       0.8117284 , 0.81481481, 0.81481481, 0.83641975, 0.83641975,\r\n",
      "       0.85802469, 0.85802469, 0.86728395, 0.86728395, 0.87962963,\r\n",
      "       0.87962963, 0.88271605, 0.88271605, 0.88888889, 0.88888889,\r\n",
      "       0.89197531, 0.89197531, 0.89814815, 0.89814815, 0.90432099,\r\n",
      "       0.90432099, 0.90740741, 0.90740741, 0.91049383, 0.91049383,\r\n",
      "       0.91975309, 0.91975309, 0.92592593, 0.92592593, 0.93209877,\r\n",
      "       0.93209877, 0.93518519, 0.93518519, 0.9382716 , 0.9382716 ,\r\n",
      "       0.94135802, 0.94135802, 0.94444444, 0.94444444, 0.95061728,\r\n",
      "       0.95061728, 0.9537037 , 0.9537037 , 0.95679012, 0.95679012,\r\n",
      "       0.95987654, 0.95987654, 0.96296296, 0.96296296, 0.96604938,\r\n",
      "       0.96604938, 0.9691358 , 0.9691358 , 0.97222222, 0.97222222,\r\n",
      "       0.97839506, 0.97839506, 0.98148148, 0.98148148, 0.98765432,\r\n",
      "       0.98765432, 0.99074074, 0.99074074, 0.99382716, 0.99382716,\r\n",
      "       0.99691358, 0.99691358, 1.        , 1.        ])}\r\n",
      "******* FP / TP for percent selected\r\n",
      "{0: array([0.        , 0.        , 0.        , 0.092827  , 0.10126582,\r\n",
      "       0.73417722, 1.        ]), 1: array([0.   , 0.   , 0.   , 0.005, 0.005, 0.01 , 0.01 , 0.015, 0.015,\r\n",
      "       0.02 , 0.02 , 0.025, 0.025, 0.03 , 0.03 , 0.04 , 0.05 , 0.05 ,\r\n",
      "       0.055, 0.055, 0.06 , 0.06 , 0.07 , 0.07 , 0.075, 0.075, 0.085,\r\n",
      "       0.085, 0.095, 0.095, 0.1  , 0.1  , 0.115, 0.115, 0.165, 0.165,\r\n",
      "       0.19 , 0.19 , 0.23 , 0.24 , 0.255, 0.255, 0.43 , 0.43 , 0.5  ,\r\n",
      "       0.51 , 0.86 , 0.87 , 0.925, 0.935, 0.96 , 1.   ]), 2: array([0.        , 0.        , 0.        , 0.        , 0.        ,\r\n",
      "       0.00473934, 0.00473934, 0.00947867, 0.00947867, 0.01421801,\r\n",
      "       0.01421801, 0.01895735, 0.01895735, 0.02369668, 0.02369668,\r\n",
      "       0.02843602, 0.02843602, 0.03317536, 0.03317536, 0.06635071,\r\n",
      "       0.06635071, 0.07109005, 0.07582938, 0.07582938, 0.08056872,\r\n",
      "       0.08056872, 0.08530806, 0.08530806, 0.12322275, 0.12322275,\r\n",
      "       0.15165877, 0.15165877, 0.16587678, 0.16587678, 0.1943128 ,\r\n",
      "       0.1943128 , 0.19905213, 0.19905213, 0.20379147, 0.20379147,\r\n",
      "       0.5971564 , 0.60663507, 0.9478673 , 1.        ]), 'macro': array([0.        , 0.00473934, 0.005     , 0.00947867, 0.01      ,\r\n",
      "       0.01421801, 0.015     , 0.01895735, 0.02      , 0.02369668,\r\n",
      "       0.025     , 0.02843602, 0.03      , 0.03317536, 0.04      ,\r\n",
      "       0.05      , 0.055     , 0.06      , 0.06635071, 0.07      ,\r\n",
      "       0.07109005, 0.075     , 0.07582938, 0.08056872, 0.085     ,\r\n",
      "       0.08530806, 0.092827  , 0.095     , 0.1       , 0.10126582,\r\n",
      "       0.115     , 0.12322275, 0.15165877, 0.165     , 0.16587678,\r\n",
      "       0.19      , 0.1943128 , 0.19905213, 0.20379147, 0.23      ,\r\n",
      "       0.24      , 0.255     , 0.43      , 0.5       , 0.51      ,\r\n",
      "       0.5971564 , 0.60663507, 0.73417722, 0.86      , 0.87      ,\r\n",
      "       0.925     , 0.935     , 0.9478673 , 0.96      , 1.        ]), 'micro': array([0.        , 0.        , 0.        , 0.        , 0.        ,\r\n",
      "       0.        , 0.        , 0.00154321, 0.00154321, 0.00308642,\r\n",
      "       0.00308642, 0.00462963, 0.00462963, 0.00617284, 0.00617284,\r\n",
      "       0.00771605, 0.00771605, 0.01080247, 0.01080247, 0.01234568,\r\n",
      "       0.01234568, 0.0154321 , 0.0154321 , 0.01697531, 0.01697531,\r\n",
      "       0.02160494, 0.02160494, 0.02314815, 0.02314815, 0.02469136,\r\n",
      "       0.02469136, 0.02623457, 0.02623457, 0.03395062, 0.03395062,\r\n",
      "       0.03549383, 0.04012346, 0.04012346, 0.04166667, 0.04166667,\r\n",
      "       0.04320988, 0.04320988, 0.04475309, 0.04475309, 0.0462963 ,\r\n",
      "       0.0462963 , 0.04783951, 0.04783951, 0.05092593, 0.05092593,\r\n",
      "       0.05401235, 0.05555556, 0.05555556, 0.06018519, 0.06018519,\r\n",
      "       0.0617284 , 0.0617284 , 0.06481481, 0.06481481, 0.06944444,\r\n",
      "       0.06944444, 0.0787037 , 0.0787037 , 0.08333333, 0.08333333,\r\n",
      "       0.08487654, 0.08487654, 0.08950617, 0.08950617, 0.10030864,\r\n",
      "       0.10030864, 0.10648148, 0.10648148, 0.11728395, 0.11728395,\r\n",
      "       0.12191358, 0.12191358, 0.1404321 , 0.1404321 , 0.17592593,\r\n",
      "       0.17901235, 0.19135802, 0.19135802, 0.31790123, 0.31790123,\r\n",
      "       0.37191358, 0.375     , 0.37962963, 0.38271605, 0.40123457,\r\n",
      "       0.40432099, 0.42746914, 0.43055556, 0.54938272, 0.55246914,\r\n",
      "       0.5617284 , 0.56481481, 0.61574074, 0.61882716, 0.67592593,\r\n",
      "       0.67901235, 0.71296296, 0.71604938, 0.74228395, 0.74691358,\r\n",
      "       0.80709877, 0.81018519, 0.87345679, 1.        ])}\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([0.        , 0.04597701, 1.        , 1.        , 1.        ,\r\n",
      "       1.        , 1.        ]), 1: array([0.        , 0.00806452, 0.65322581, 0.65322581, 0.66129032,\r\n",
      "       0.66129032, 0.70967742, 0.70967742, 0.72580645, 0.72580645,\r\n",
      "       0.76612903, 0.76612903, 0.83064516, 0.83064516, 0.86290323,\r\n",
      "       0.86290323, 0.86290323, 0.88709677, 0.88709677, 0.90322581,\r\n",
      "       0.90322581, 0.91129032, 0.91129032, 0.92741935, 0.92741935,\r\n",
      "       0.93548387, 0.93548387, 0.94354839, 0.94354839, 0.9516129 ,\r\n",
      "       0.9516129 , 0.95967742, 0.95967742, 0.96774194, 0.96774194,\r\n",
      "       0.97580645, 0.97580645, 0.98387097, 0.98387097, 0.98387097,\r\n",
      "       0.98387097, 0.99193548, 0.99193548, 1.        , 1.        ,\r\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\r\n",
      "       1.        , 1.        ]), 2: array([0.        , 0.01769912, 0.38938053, 0.40707965, 0.52212389,\r\n",
      "       0.52212389, 0.68141593, 0.68141593, 0.69911504, 0.69911504,\r\n",
      "       0.80530973, 0.80530973, 0.81415929, 0.81415929, 0.84070796,\r\n",
      "       0.84070796, 0.84955752, 0.84955752, 0.88495575, 0.88495575,\r\n",
      "       0.89380531, 0.89380531, 0.90265487, 0.92035398, 0.92035398,\r\n",
      "       0.92920354, 0.92920354, 0.9380531 , 0.9380531 , 0.94690265,\r\n",
      "       0.94690265, 0.95575221, 0.95575221, 0.96460177, 0.96460177,\r\n",
      "       0.98230088, 0.98230088, 0.99115044, 0.99115044, 1.        ,\r\n",
      "       1.        , 1.        , 1.        , 1.        ]), 'macro': array([0.72511657, 0.77821391, 0.78090208, 0.78680179, 0.80293082,\r\n",
      "       0.83832905, 0.8437054 , 0.84665525, 0.86009611, 0.86894567,\r\n",
      "       0.89045104, 0.89340089, 0.90415358, 0.91595299, 0.91595299,\r\n",
      "       0.92401751, 0.92939385, 0.93208202, 0.93503188, 0.94040822,\r\n",
      "       0.94040822, 0.94553002, 0.95194595, 0.9548958 , 0.95758398,\r\n",
      "       0.96053383, 0.96053383, 0.963222  , 0.96591017, 0.96591017,\r\n",
      "       0.96859834, 0.9715482 , 0.97449805, 0.97718622, 0.98013607,\r\n",
      "       0.98282425, 0.98872395, 0.9916738 , 0.99462366, 0.99462366,\r\n",
      "       0.99462366, 0.99731183, 1.        , 1.        , 1.        ,\r\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\r\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ]), 'micro': array([0.        , 0.01851852, 0.32716049, 0.33333333, 0.39197531,\r\n",
      "       0.39814815, 0.49691358, 0.49691358, 0.64506173, 0.64506173,\r\n",
      "       0.65432099, 0.65432099, 0.76234568, 0.76234568, 0.7654321 ,\r\n",
      "       0.7654321 , 0.77160494, 0.77160494, 0.79012346, 0.79012346,\r\n",
      "       0.80864198, 0.80864198, 0.81481481, 0.81481481, 0.84567901,\r\n",
      "       0.84567901, 0.87037037, 0.87037037, 0.88271605, 0.88271605,\r\n",
      "       0.88580247, 0.88580247, 0.89506173, 0.89506173, 0.89814815,\r\n",
      "       0.89814815, 0.90123457, 0.91049383, 0.91049383, 0.92283951,\r\n",
      "       0.92283951, 0.92592593, 0.92592593, 0.93209877, 0.93209877,\r\n",
      "       0.93518519, 0.93518519, 0.9382716 , 0.9382716 , 0.94135802,\r\n",
      "       0.94135802, 0.94444444, 0.94753086, 0.94753086, 0.95061728,\r\n",
      "       0.95061728, 0.9537037 , 0.9537037 , 0.95679012, 0.95679012,\r\n",
      "       0.95987654, 0.95987654, 0.96296296, 0.96296296, 0.96604938,\r\n",
      "       0.96604938, 0.9691358 , 0.9691358 , 0.97222222, 0.97222222,\r\n",
      "       0.98148148, 0.98148148, 0.9845679 , 0.9845679 , 0.98765432,\r\n",
      "       0.98765432, 0.99074074, 0.99074074, 0.99382716, 0.99382716,\r\n",
      "       0.99382716, 0.99382716, 0.99691358, 0.99691358, 1.        ,\r\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\r\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\r\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\r\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\r\n",
      "       1.        , 1.        , 1.        , 1.        ])}\r\n"
     ]
    }
   ],
   "source": [
    "# Save results to a batch specific directory\n",
    "# Derive completed batches\n",
    "with open(join(intermediate_checkpoints,'checkpoint')) as f: \n",
    "    checkpoint_path = f.readline().split('\"')[1]\n",
    "completed = completed_batches(intermediate_checkpoints, pretrained_checkpoints)\n",
    "#completed = str(batches + int(checkpoint_path.split('-')[-1]))\n",
    "roc_curves_batch = join(roc_curves, '_' + completed)\n",
    "print(\"Generating ROC curve data at {}\".format(roc_curves_batch))\n",
    "try:\n",
    "    shutil.rmtree(roc_curves_batch)\n",
    "except:\n",
    "    pass\n",
    "os.mkdir(roc_curves_batch)\n",
    "\n",
    "func = join(deeppath_code, '03_postprocessing/0h_ROC_MultiOutput_BootStrap.py')\n",
    "root = func.rsplit('/',1)[1].split('.')[0]\n",
    "\n",
    "file_stats = join(test_results, '_' + completed, 'out_filename_Stats.txt')\n",
    "!python $func  --file_stats=$file_stats --output_dir=$roc_curves_batch  --labels_names=$data_labels_path \\\n",
    "    --ref_stats=''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optionally save training results to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# Populate the tar file\n",
    "# Note we don't save validation results.\n",
    "loc = join(training,'data.tar')\n",
    "with tarfile.open(loc, \"w\") as tar:\n",
    "    for name in [intermediate_checkpoints, training_logs,  roc_curves,  test_results, heatmaps]:\n",
    "        tar.add(name)\n",
    "\n",
    "# Copy it to GCS\n",
    "gcs = join('gs://', deeppath_data_bucket,tiling_params, sorting_params, training_params, 'data.tar')\n",
    "!gsutil -m cp $loc $gcs \n",
    "\n",
    "# Delete the tar file\n",
    "!rm $loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "\n",
    "with open(data_labels_path) as h:\n",
    "    labels = [line.rstrip('\\n') for line in h]\n",
    "def draw_rocs(roc_base, roc_curves_batch):\n",
    "    plt.figure(figsize=[10,8])\n",
    "    colors = ['deeppink', 'navy'] + ['C'+str(x) for x in range(len(labels))]\n",
    "    lws = [4, 4] + [2] * len(labels)\n",
    "    curves = ['micro', 'macro'] + [x for x in labels]\n",
    "    fileIds = ['micro', 'macro'] + ['c'+str(x+1)+'auc' for x in range(len(labels))]\n",
    "    linestyles = [':', ':'] + ['-'] * len(labels)\n",
    "    for curve, fileId, color, lw, linestyle in zip(curves, fileIds, colors, lws, linestyles):\n",
    "        f = [x for x in os.listdir(roc_curves_batch) if x.find(roc_base + fileId)==0][0]\n",
    "        lines = np.loadtxt(join(roc_curves_batch,f), comments=\"#\", delimiter=\"\\t\", unpack=True)\n",
    "        plt.plot(lines[0], lines[1], color = color, lw=lw, linestyle=linestyle,\n",
    "             label = '{0} ROC curve (area = {1:0.3f})'.format(curve,float(f[f.find('auc_')+4:f.find('auc_')+10])))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function plot in module matplotlib.pyplot:\n",
      "\n",
      "plot(*args, **kwargs)\n",
      "    Plot y versus x as lines and/or markers.\n",
      "    \n",
      "    Call signatures::\n",
      "    \n",
      "        plot([x], y, [fmt], data=None, **kwargs)\n",
      "        plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n",
      "    \n",
      "    The coordinates of the points or line nodes are given by *x*, *y*.\n",
      "    \n",
      "    The optional parameter *fmt* is a convenient way for defining basic\n",
      "    formatting like color, marker and linestyle. It's a shortcut string\n",
      "    notation described in the *Notes* section below.\n",
      "    \n",
      "    >>> plot(x, y)        # plot x and y using default line style and color\n",
      "    >>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n",
      "    >>> plot(y)           # plot y using x as index array 0..N-1\n",
      "    >>> plot(y, 'r+')     # ditto, but with red plusses\n",
      "    \n",
      "    You can use `.Line2D` properties as keyword arguments for more\n",
      "    control on the  appearance. Line properties and *fmt* can be mixed.\n",
      "    The following two calls yield identical results:\n",
      "    \n",
      "    >>> plot(x, y, 'go--', linewidth=2, markersize=12)\n",
      "    >>> plot(x, y, color='green', marker='o', linestyle='dashed',\n",
      "            linewidth=2, markersize=12)\n",
      "    \n",
      "    When conflicting with *fmt*, keyword arguments take precedence.\n",
      "    \n",
      "    **Plotting labelled data**\n",
      "    \n",
      "    There's a convenient way for plotting objects with labelled data (i.e.\n",
      "    data that can be accessed by index ``obj['y']``). Instead of giving\n",
      "    the data in *x* and *y*, you can provide the object in the *data*\n",
      "    parameter and just give the labels for *x* and *y*::\n",
      "    \n",
      "    >>> plot('xlabel', 'ylabel', data=obj)\n",
      "    \n",
      "    All indexable objects are supported. This could e.g. be a `dict`, a\n",
      "    `pandas.DataFame` or a structured numpy array.\n",
      "    \n",
      "    \n",
      "    **Plotting multiple sets of data**\n",
      "    \n",
      "    There are various ways to plot multiple sets of data.\n",
      "    \n",
      "    - The most straight forward way is just to call `plot` multiple times.\n",
      "      Example:\n",
      "    \n",
      "      >>> plot(x1, y1, 'bo')\n",
      "      >>> plot(x2, y2, 'go')\n",
      "    \n",
      "    - Alternatively, if your data is already a 2d array, you can pass it\n",
      "      directly to *x*, *y*. A separate data set will be drawn for every\n",
      "      column.\n",
      "    \n",
      "      Example: an array ``a`` where the first column represents the *x*\n",
      "      values and the other columns are the *y* columns::\n",
      "    \n",
      "      >>> plot(a[0], a[1:])\n",
      "    \n",
      "    - The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n",
      "      groups::\n",
      "    \n",
      "      >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n",
      "    \n",
      "      In this case, any additional keyword argument applies to all\n",
      "      datasets. Also this syntax cannot be combined with the *data*\n",
      "      parameter.\n",
      "    \n",
      "    By default, each line is assigned a different style specified by a\n",
      "    'style cycle'. The *fmt* and line property parameters are only\n",
      "    necessary if you want explicit deviations from these defaults.\n",
      "    Alternatively, you can also change the style cycle using the\n",
      "    'axes.prop_cycle' rcParam.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    x, y : array-like or scalar\n",
      "        The horizontal / vertical coordinates of the data points.\n",
      "        *x* values are optional. If not given, they default to\n",
      "        ``[0, ..., N-1]``.\n",
      "    \n",
      "        Commonly, these parameters are arrays of length N. However,\n",
      "        scalars are supported as well (equivalent to an array with\n",
      "        constant value).\n",
      "    \n",
      "        The parameters can also be 2-dimensional. Then, the columns\n",
      "        represent separate data sets.\n",
      "    \n",
      "    fmt : str, optional\n",
      "        A format string, e.g. 'ro' for red circles. See the *Notes*\n",
      "        section for a full description of the format strings.\n",
      "    \n",
      "        Format strings are just an abbreviation for quickly setting\n",
      "        basic line properties. All of these and more can also be\n",
      "        controlled by keyword arguments.\n",
      "    \n",
      "    data : indexable object, optional\n",
      "        An object with labelled data. If given, provide the label names to\n",
      "        plot in *x* and *y*.\n",
      "    \n",
      "        .. note::\n",
      "            Technically there's a slight ambiguity in calls where the\n",
      "            second label is a valid *fmt*. `plot('n', 'o', data=obj)`\n",
      "            could be `plt(x, y)` or `plt(y, fmt)`. In such cases,\n",
      "            the former interpretation is chosen, but a warning is issued.\n",
      "            You may suppress the warning by adding an empty format string\n",
      "            `plot('n', 'o', '', data=obj)`.\n",
      "    \n",
      "    \n",
      "    Other Parameters\n",
      "    ----------------\n",
      "    scalex, scaley : bool, optional, default: True\n",
      "        These parameters determined if the view limits are adapted to\n",
      "        the data limits. The values are passed on to `autoscale_view`.\n",
      "    \n",
      "    **kwargs : `.Line2D` properties, optional\n",
      "        *kwargs* are used to specify properties like a line label (for\n",
      "        auto legends), linewidth, antialiasing, marker face color.\n",
      "        Example::\n",
      "    \n",
      "        >>> plot([1,2,3], [1,2,3], 'go-', label='line 1', linewidth=2)\n",
      "        >>> plot([1,2,3], [1,4,9], 'rs',  label='line 2')\n",
      "    \n",
      "        If you make multiple lines with one plot command, the kwargs\n",
      "        apply to all those lines.\n",
      "    \n",
      "        Here is a list of available `.Line2D` properties:\n",
      "    \n",
      "          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array \n",
      "      alpha: float (0.0 transparent through 1.0 opaque) \n",
      "      animated: bool \n",
      "      antialiased or aa: bool \n",
      "      clip_box: a `.Bbox` instance \n",
      "      clip_on: bool \n",
      "      clip_path: [(`~matplotlib.path.Path`, `.Transform`) | `.Patch` | None] \n",
      "      color or c: any matplotlib color \n",
      "      contains: a callable function \n",
      "      dash_capstyle: ['butt' | 'round' | 'projecting'] \n",
      "      dash_joinstyle: ['miter' | 'round' | 'bevel'] \n",
      "      dashes: sequence of on/off ink in points \n",
      "      drawstyle: ['default' | 'steps' | 'steps-pre' | 'steps-mid' | 'steps-post'] \n",
      "      figure: a `.Figure` instance \n",
      "      fillstyle: ['full' | 'left' | 'right' | 'bottom' | 'top' | 'none'] \n",
      "      gid: an id string \n",
      "      label: object \n",
      "      linestyle or ls: ['solid' | 'dashed', 'dashdot', 'dotted' | (offset, on-off-dash-seq) | ``'-'`` | ``'--'`` | ``'-.'`` | ``':'`` | ``'None'`` | ``' '`` | ``''``]\n",
      "      linewidth or lw: float value in points \n",
      "      marker: :mod:`A valid marker style <matplotlib.markers>`\n",
      "      markeredgecolor or mec: any matplotlib color \n",
      "      markeredgewidth or mew: float value in points \n",
      "      markerfacecolor or mfc: any matplotlib color \n",
      "      markerfacecoloralt or mfcalt: any matplotlib color \n",
      "      markersize or ms: float \n",
      "      markevery: [None | int | length-2 tuple of int | slice | list/array of int | float | length-2 tuple of float]\n",
      "      path_effects: `.AbstractPathEffect` \n",
      "      picker: float distance in points or callable pick function ``fn(artist, event)`` \n",
      "      pickradius: float distance in points\n",
      "      rasterized: bool or None \n",
      "      sketch_params: (scale: float, length: float, randomness: float) \n",
      "      snap: bool or None \n",
      "      solid_capstyle: ['butt' | 'round' |  'projecting'] \n",
      "      solid_joinstyle: ['miter' | 'round' | 'bevel'] \n",
      "      transform: a :class:`matplotlib.transforms.Transform` instance \n",
      "      url: a url string \n",
      "      visible: bool \n",
      "      xdata: 1D array \n",
      "      ydata: 1D array \n",
      "      zorder: float \n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    lines\n",
      "        A list of `.Line2D` objects representing the plotted data.\n",
      "    \n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    scatter : XY scatter plot with markers of variing size and/or color (\n",
      "        sometimes also called bubble chart).\n",
      "    \n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    **Format Strings**\n",
      "    \n",
      "    A format string consists of a part for color, marker and line::\n",
      "    \n",
      "        fmt = '[color][marker][line]'\n",
      "    \n",
      "    Each of them is optional. If not provided, the value from the style\n",
      "    cycle is used. Exception: If ``line`` is given, but no ``marker``,\n",
      "    the data will be a line without markers.\n",
      "    \n",
      "    **Colors**\n",
      "    \n",
      "    The following color abbreviations are supported:\n",
      "    \n",
      "    =============    ===============================\n",
      "    character        color\n",
      "    =============    ===============================\n",
      "    ``'b'``          blue\n",
      "    ``'g'``          green\n",
      "    ``'r'``          red\n",
      "    ``'c'``          cyan\n",
      "    ``'m'``          magenta\n",
      "    ``'y'``          yellow\n",
      "    ``'k'``          black\n",
      "    ``'w'``          white\n",
      "    =============    ===============================\n",
      "    \n",
      "    If the color is the only part of the format string, you can\n",
      "    additionally use any  `matplotlib.colors` spec, e.g. full names\n",
      "    (``'green'``) or hex strings (``'#008000'``).\n",
      "    \n",
      "    **Markers**\n",
      "    \n",
      "    =============    ===============================\n",
      "    character        description\n",
      "    =============    ===============================\n",
      "    ``'.'``          point marker\n",
      "    ``','``          pixel marker\n",
      "    ``'o'``          circle marker\n",
      "    ``'v'``          triangle_down marker\n",
      "    ``'^'``          triangle_up marker\n",
      "    ``'<'``          triangle_left marker\n",
      "    ``'>'``          triangle_right marker\n",
      "    ``'1'``          tri_down marker\n",
      "    ``'2'``          tri_up marker\n",
      "    ``'3'``          tri_left marker\n",
      "    ``'4'``          tri_right marker\n",
      "    ``'s'``          square marker\n",
      "    ``'p'``          pentagon marker\n",
      "    ``'*'``          star marker\n",
      "    ``'h'``          hexagon1 marker\n",
      "    ``'H'``          hexagon2 marker\n",
      "    ``'+'``          plus marker\n",
      "    ``'x'``          x marker\n",
      "    ``'D'``          diamond marker\n",
      "    ``'d'``          thin_diamond marker\n",
      "    ``'|'``          vline marker\n",
      "    ``'_'``          hline marker\n",
      "    =============    ===============================\n",
      "    \n",
      "    **Line Styles**\n",
      "    \n",
      "    =============    ===============================\n",
      "    character        description\n",
      "    =============    ===============================\n",
      "    ``'-'``          solid line style\n",
      "    ``'--'``         dashed line style\n",
      "    ``'-.'``         dash-dot line style\n",
      "    ``':'``          dotted line style\n",
      "    =============    ===============================\n",
      "    \n",
      "    Example format strings::\n",
      "    \n",
      "        'b'    # blue markers with default shape\n",
      "        'ro'   # red circles\n",
      "        'g-'   # green solid line\n",
      "        '--'   # dashed line with default color\n",
      "        'k^:'  # black triangle_up markers connected by a dotted line\n",
      "    \n",
      "    .. note::\n",
      "        In addition to the above described arguments, this function can take a\n",
      "        **data** keyword argument. If such a **data** argument is given, the\n",
      "        following arguments are replaced by **data[<arg>]**:\n",
      "    \n",
      "        * All arguments with the following names: 'x', 'y'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(plt.plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Draw curves based on average probability\n",
    "roc_base = 'out2_roc_data_AvPb_'\n",
    "draw_rocs(roc_base, roc_curves_batch)\n",
    "\n",
    "# Draw curves based on percent of correctly classified tiles\n",
    "roc_base = 'out2_roc_data_PcSel_'\n",
    "draw_rocs(roc_base, roc_curves_batch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

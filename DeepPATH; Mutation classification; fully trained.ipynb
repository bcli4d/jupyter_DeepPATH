{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook uses transfer learning on the pre-trained Inception v3 model to classify mutations in TCGA-LUAD images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We first define a few locations where results are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, shutil\n",
    "from os.path import join\n",
    "#deep_path = join(os.environ[\"HOME\"],'DeepPATH')\n",
    "deeppath_code = join(os.environ[\"HOME\"],'DeepPATH/DeepPATH_code')\n",
    "deeppath_data = '/mnt/disks/deeppath-data'\n",
    "data_base = join(deeppath_data,'Data')\n",
    "raw_images = join(data_base,'Raw')\n",
    "data_labels = join(data_base,'data_labels')\n",
    "#tilings = join(data_base,'tilings')\n",
    "inception_checkpoints = join(deeppath_data,'inception_checkpoints')\n",
    "\n",
    "for d in (deeppath_data, data_base, raw_images, inception_checkpoints):\n",
    "  try:\n",
    "    os.makedirs(d)\n",
    "  except:\n",
    "    print(\"{} exists\".format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various GCS buckets and relevant defines\n",
    "cgc_deeppath_bucket = 'cgc-deeppath' # Bucket containing users credentials\n",
    "deeppath_data_bucket = 'deeppath-data-whc' # Bucket where results are saved\n",
    "\n",
    "svs_images_bucket = 'imaging-west'# Bucket containing TCGA pathology images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify tiling parameters\n",
    "We place results into a tiling specific directory tree. The first five values below control the tiling. Change these to perform a different tiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size = 299\n",
    "overlap = 0\n",
    "background = 25\n",
    "jobs = 8\n",
    "magnification = 20\n",
    "tiling_params = \"Px{}Ol{}Bg{}Mg{}_Tile\".format(str(tile_size), str(overlap), str(background),str(magnification))\n",
    "tiling = join(data_base, tiling_params)\n",
    "tiling_logs = join(tiling,'logs')\n",
    "# Directory where tiles are stored              \n",
    "tiles = join(tiling,'tiles')\n",
    "images_metadata = 'images_metadata.json' # Name of images metadata file  (which will be created below)\n",
    "images_metadata_path = join(tiling, images_metadata) # Path to images metadata file\n",
    "mutations_metadata = 'mutations_metadata.txt' # Name of mutation metadata file  (which will be created below)\n",
    "mutations_metadata_path = join(tiling, mutations_metadata) # Path to metadata file\n",
    "hugo_symbols_path = join(tiling, 'hugo_symbols.txt')\n",
    "\n",
    "for d in (tiling, tiling_logs, tiles):\n",
    "  try:\n",
    "    os.makedirs(d)\n",
    "  except:\n",
    "    print(\"{} exists\".format(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify sorting parameter\n",
    "### Option 10 does not sort into separate groups (directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_option = 10\n",
    "#sortings = join(tiling, 'sortings')\n",
    "\n",
    "sorting_params = 'So{}_Sort'.format(sorting_option)\n",
    "sorting = join(tiling, sorting_params)\n",
    "sorting_logs = join(sorting, 'logs')\n",
    "sorted_tiles = join(sorting, 'sorted')\n",
    "# Directories where sharded tensor flow formatted records are stored\n",
    "trainValid_records = join(sorting, 'TFRecord_TrainValid')\n",
    "test_records = join(sorting, 'TFRecord_Test')\n",
    "for d in (sorting, sorting_logs, sorted_tiles, trainValid_records, test_records):\n",
    "  try:\n",
    "    os.makedirs(d)\n",
    "  except:\n",
    "    print(\"{} exists\".format(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify training, validation, test parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_number = !cat $hugo_symbols_path |wc -l # Number of classes\n",
    "class_number = int(class_number[0])\n",
    "fine_tune = False\n",
    "\n",
    "#trainings = join(sorting, \"trainings\")\n",
    "training_params = \"Cl{}Ft{}_Train\".format(str(class_number),str(fine_tune))\n",
    "training = join(sorting, training_params)\n",
    "training_logs = join(training, 'logs')\n",
    "training_mode = '1_sigmoid'\n",
    "\n",
    "# Directory where intermediate training checkpoints are stored\n",
    "intermediate_checkpoints = join(training, 'intermediate_checkpoints')\n",
    "# Directory where validation and test results are stored\n",
    "eval_results = join(training, 'eval_results')\n",
    "test_results = join(training, 'test_results')\n",
    "roc_curves = join(training, 'roc_curves')\n",
    "heatmaps = join(training,'Px'+str(tile_size))\n",
    "\n",
    "for d in (training, training_logs, intermediate_checkpoints,\n",
    "         eval_results, test_results, roc_curves):\n",
    "  try:\n",
    "    os.makedirs(d)\n",
    "  except:\n",
    "    print(\"{} exists\".format(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get/Update DeepPATH code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DeepPATH'...\n",
      "remote: Enumerating objects: 12, done.\u001b[K\n",
      "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
      "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
      "remote: Total 2152 (delta 0), reused 5 (delta 0), pack-reused 2140\u001b[K\n",
      "Receiving objects: 100% (2152/2152), 1.88 MiB | 0 bytes/s, done.\n",
      "Resolving deltas: 100% (1266/1266), done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists(deeppath_code):\n",
    "    os.chdir(os.environ[\"HOME\"])\n",
    "    #!git clone https://github.com/ncoudray/DeepPATH.git\n",
    "    !git clone -b whc1 https://github.com/bcli4d/DeepPATH.git\n",
    "else:\n",
    "    #!git pull https://github.com/ncoudray/DeepPATH.git\n",
    "    !git -C $deeppath_code pull https://github.com/bcli4d/DeepPATH.git\n",
    "        \n",
    "sys.path.insert(0,deeppath_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get metadata from BQ\n",
    "Rather than obtain metdata from the GDC, we perform an SQL query to obtain the names of LUAD images and mutation metadata. \n",
    "### We must first establish credentials. \n",
    "See https://cloud.google.com/bigquery/docs/quickstarts/quickstart-client-libraries#bigquery_simple_app_query-python for help. Put your credentials in some GCS bucket and modify the \"bucket\" and \"credentials\" variables below as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob GAC.json uploaded to /tmp/GAC.json.\n"
     ]
    }
   ],
   "source": [
    "from gcs_access import upload_from_GCS\n",
    "bucket = 'cgc-deeppath'                                                                                         \n",
    "credentials = 'GAC.json' \n",
    "upload_from_GCS(bucket, credentials, \"/tmp/GAC.json\")\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"/tmp/GAC.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now perform a BQ query to get metadata. \n",
    "For this purpose, we call query_bq() in which the needed SQL is hardcoded. query_bq() also formats the returned data in the format expected by the tiling and sorting phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_metadata_from_bq import query_mutation_metadata\n",
    "\n",
    "sql_query = \"\"\"\n",
    "    WITH\n",
    "      luads AS (\n",
    "      SELECT\n",
    "        t1.file_gdc_id AS file_gdc_id,\n",
    "        t1.svsFilename AS svsFilename,\n",
    "        t1.sample_barcode \n",
    "        AS sample_barcode,\n",
    "        t2.callerName AS callerName,\n",
    "        t2.Hugo_Symbol AS Hugo_Symbol\n",
    "      FROM\n",
    "        `isb-cgc.metadata.TCGA_slide_images` t1\n",
    "      INNER JOIN\n",
    "        `isb-cgc.TCGA_hg38_data_v0.Somatic_Mutation_DR10` t2\n",
    "      ON\n",
    "        t1.sample_barcode = t2.sample_barcode_tumor\n",
    "      WHERE\n",
    "        (t2.project_short_name='TCGA-LUAD')\n",
    "        AND NOT t1.slide_barcode LIKE '%TCGA-__-____-___-__-D%'),\n",
    "      mutations AS (\n",
    "      SELECT\n",
    "        file_gdc_id,\n",
    "        svsFilename,\n",
    "        sample_barcode,\n",
    "        Hugo_Symbol\n",
    "      FROM\n",
    "        luads\n",
    "      WHERE\n",
    "        callerName LIKE '%mutect%'\n",
    "        AND Hugo_Symbol IN ('EGFR',\n",
    "          'FAT1',\n",
    "          'FAT4',\n",
    "          'KEAP1',\n",
    "          'KRAS',\n",
    "          'LRP1B',\n",
    "          'NF1',\n",
    "          'SETBP1',\n",
    "          'STK11',\n",
    "          'TP53')),\n",
    "      wts AS (\n",
    "      SELECT\n",
    "        file_gdc_id,\n",
    "        svsFilename,\n",
    "        sample_barcode,\n",
    "        'WT' AS Hugo_Symbol\n",
    "      FROM\n",
    "        luads\n",
    "      WHERE\n",
    "        sample_barcode not in (\n",
    "          select sample_barcode\n",
    "          from mutations))\n",
    "    SELECT\n",
    "      file_gdc_id,\n",
    "      svsFilename,\n",
    "      sample_barcode,\n",
    "      Hugo_Symbol\n",
    "    FROM\n",
    "      mutations\n",
    "    UNION DISTINCT\n",
    "    SELECT\n",
    "      file_gdc_id,\n",
    "      svsFilename,\n",
    "      sample_barcode,\n",
    "      Hugo_Symbol\n",
    "    FROM\n",
    "      wts\n",
    "    ORDER BY\n",
    "      sample_barcode, Hugo_Symbol \n",
    "  \"\"\"\n",
    "\n",
    "query_mutation_metadata(images_metadata_path, mutations_metadata_path, hugo_symbols_path, sql_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tile the images listed in the manifest\n",
    "Images are uploaded, on at a time, from GCS and tiled according to parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally load previously saved results from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('gs://deeppath-data-whc/Px512Ol0Bg25Mg5_Tile/data.tar.gz', '/mnt/disks/deeppath-data/Data/Px512Ol0Bg25Mg5_Tile/data.tar.gz')\n",
      "Found\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "gcs = join('gs://', deeppath_data_bucket, tiling_params,'data.tar')\n",
    "loc = join(tiling,'data.tar')\n",
    "\n",
    "result = !gsutil ls $gcs\n",
    "if result[0] ==gcs:\n",
    "    !gsutil -m cp $gcs $loc \n",
    "\n",
    "    with tarfile.open(loc) as tar:\n",
    "        tar.extractall()\n",
    "\n",
    "    !rm $loc\n",
    "else:\n",
    "    print('{} not found'.format(loc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform tiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "tiler = join(deeppath_code, '00_preprocessing/0b_tileLoop_deepzoom4.py')\n",
    "out_log_file = join(tiling_logs,'tiler_out.log')\n",
    "err_log_file = join(tiling_logs,'tiler_err.log')\n",
    "!rm $out_log_file\n",
    "!rm $err_log_file\n",
    "!rm $raw_images/*\n",
    "skipped_file_name = \"\"\n",
    "skipped_file_id = \"\"\n",
    "with open(images_metadata_path) as fid:\n",
    "    jdata = json.loads(fid.read())\n",
    "    jdata.sort(key = lambda x: x['file_name'])\n",
    "    for file in jdata:\n",
    "        print(\"\")\n",
    "        print(\"Checking {}\".format(file))\n",
    "        tiled_dziname = join(tiles,file['file_name'].rsplit('.',1)[0]+'.dzi')\n",
    "        tiled_filesname = join(tiles,file['file_name'].rsplit('.',1)[0]+'_files')\n",
    "        \n",
    "        if os.path.exists(tiled_dziname):\n",
    "            # If the .dzi file exists, then we presume that tiling was completed\n",
    "            skipped_file_name = file['file_name']\n",
    "            skipped_file_id = file['file_id']\n",
    "            print(\"Skipping {}\".format(file['file_name']))\n",
    "        else:\n",
    "            print(\"Processing {}\".format(file['file_name']))\n",
    "            if os.path.exists(tiled_filesname):\n",
    "                # Apparently this file was only partially tiled, so delete it completely\n",
    "                print (\"removing {}\".format(tiled_filesname))\n",
    "                shutil.rmtree(tiled_filesname)\n",
    "            GCS_filename = 'gs://'+join(svs_images_bucket,file['file_id'],file['file_name'])\n",
    "            local_filename = join(raw_images,file['file_name'])\n",
    "            oldstderr = sys.stderr\n",
    "            sys.stderr = open(err_log_file, 'a')\n",
    "            oldstdout = sys.stdout\n",
    "            sys.stdout = open(out_log_file, 'a')\n",
    "            !gsutil -m cp $GCS_filename $local_filename\n",
    "            !python $tiler --output=$tiles --Mag=$magnification \\\n",
    "                --size=$tile_size --overlap=$overlap --Background=$background --jobs=$jobs \\\n",
    "                $raw_images/*svs\n",
    "            #print(\"gsutil -m cp {} {}\".format(GCS_filename, local_filename))\n",
    "            !rm $local_filename\n",
    "            sys.stderr = oldstderr\n",
    "            sys.stdout = oldstdout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally save tiling results to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/data.tar.gz [Content-Type=application/x-tar]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run\n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "/ [1/1 files][ 30.9 GiB/ 30.9 GiB] 100% Done  84.1 MiB/s ETA 00:00:00           \n",
      "Operation completed over 1 objects/30.9 GiB.                                     \n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "# Populate the tar file. Don't try to compress.\n",
    "loc = join(tiling,'data.tar')\n",
    "with tarfile.open(loc, \"w\") as tar:\n",
    "    for name in [tiles, tiling_logs]:\n",
    "        tar.add(name)\n",
    "\n",
    "# Copy it to GCS\n",
    "gcs = join('gs://', deeppath_data_bucket,tiling_params,'data.tar')\n",
    "!gsutil -m cp $loc $gcs \n",
    "\n",
    "# Delete the tar file\n",
    "!rm $loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort tiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally load previously saved sorting results from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://deeppath-data-whc/Px512Ol0Bg25Mg5_Tile/So3_Sort/data.tar.gz...\n",
      "\\ [1/1 files][  5.8 GiB/  5.8 GiB] 100% Done  73.8 MiB/s ETA 00:00:00           \n",
      "Operation completed over 1 objects/5.8 GiB.                                      \n",
      "-rw-r--r-- 1 bcliffor bcliffor 6178730916 Apr 22 19:58 /mnt/disks/deeppath-data/Data/Px512Ol0Bg25Mg5_Tile/So3_Sort/data.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "gcs = join('gs://', deeppath_data_bucket, tiling_params, sorting_params, 'data.tar.gz')\n",
    "loc = join(sorting,'data.tar.gz')\n",
    "\n",
    "result = !gsutil ls $gcs\n",
    "if result[0] == gcs:\n",
    "    !gsutil -m cp $gcs $loc \n",
    "    \n",
    "    !ls -l $loc\n",
    "    with tarfile.open(loc) as tar:\n",
    "        tar.extractall('/')\n",
    "\n",
    "    !rm $loc\n",
    "else:\n",
    "    print('{} not found'.format(gcs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sort routine outputs to the cwd. So change cwd as needed.\n",
    "# First, remove existing sort results\n",
    "try:\n",
    "    shutil.rmtree(sorted_tiles)\n",
    "except:\n",
    "    pass\n",
    "os.mkdir(sorted_tiles)\n",
    "cwd = os.getcwd()\n",
    "os.chdir(sorted_tiles)\n",
    "\n",
    "func = join(deeppath_code,'00_preprocessing/0d_SortTiles.py' )\n",
    "root = func.rsplit('/',1)[1].split('.')[0]\n",
    "\n",
    "out_log_file = join(sorting_logs,root+'.out.log')\n",
    "err_log_file = join(sorting_logs,root+'.err.log')\n",
    "oldstderr = sys.stderr\n",
    "sys.stderr = open(err_log_file, 'w')\n",
    "oldstdout = sys.stdout\n",
    "sys.stdout = open(out_log_file, 'w')\n",
    "\n",
    "%xmode Verbose\n",
    "\n",
    "try:\n",
    "    !python $func --SourceFolder=$tiles --JsonFile=$images_metadata_path --PercentValid=15 --PercentTest=15 \\\n",
    "        --PatientID=12 --Magnification=$magnification --MagDiffAllowed=0.0 --SortingOption=$sorting_option --nSplit=0\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    sys.stderr = oldstderr\n",
    "    sys.stdout = oldstdout\n",
    "    os.chdir(cwd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the JPEG tiles into TFRecord format for the multiple mutations\n",
    "## First format the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove existing TFRecord conversion results\n",
    "try:\n",
    "    shutil.rmtree(trainValid_records)\n",
    "except:\n",
    "    pass\n",
    "os.mkdir(trainValid_records)\n",
    "\n",
    "func = join(deeppath_code,'00_preprocessing/TFRecord_multi_Classes/build_image_data_multiClass.py' )\n",
    "root = func.rsplit('/',1)[1].split('.')[0]\n",
    "out_log_file = join(sorting_logs,root+'.train.out.log')\n",
    "err_log_file = join(sorting_logs,root+'.train.err.log')\n",
    "oldstderr = sys.stderr\n",
    "sys.stderr = open(err_log_file, 'w')\n",
    "oldstdout = sys.stdout\n",
    "sys.stdout = open(out_log_file, 'w')\n",
    "\n",
    "try:\n",
    "    !python $func --directory=$sorted_tiles/tiles --output_directory=$trainValid_records --train_shards=1024 \\\n",
    "        --validation_shards=128 --num_threads=4  --labels_names=$hugo_symbols_path --labels=$mutations_metadata_path \\\n",
    "        --PatientID=12\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    sys.stderr = oldstderr\n",
    "    sys.stdout = oldstdout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now format the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove existing TFRecord conversion results\n",
    "try:\n",
    "    shutil.rmtree(test_records)\n",
    "except:\n",
    "    pass\n",
    "os.mkdir(test_records)\n",
    "\n",
    "func = join(deeppath_code,'00_preprocessing/TFRecord_multi_Classes/build_TF_test_multiClass.py' )\n",
    "root = func.rsplit('/',1)[1].split('.')[0]\n",
    "out_log_file = join(sorting_logs,root+'.test.out.log')\n",
    "err_log_file = join(sorting_logs,root+'.test.err.log')\n",
    "oldstderr = sys.stderr\n",
    "sys.stderr = open(err_log_file, 'w')\n",
    "oldstdout = sys.stdout\n",
    "sys.stdout = open(out_log_file, 'w')\n",
    "\n",
    "try:\n",
    "    !python $func --directory=$sorted_tiles/tiles --output_directory=$test_records --train_shards=1024 \\\n",
    "        --validation_shards=128 --num_threads=1  --one_FT_per_Tile=False --ImageSet_basename='test' \\\n",
    "        --labels_names=$hugo_symbols_path --labels=$mutations_metadata_path \\\n",
    "        --PatientID=12\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    sys.stderr = oldstderr\n",
    "    sys.stdout = oldstdout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optionally save sorting results to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So10_Sort/data.tar [Content-Type=application/x-tar]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run\n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "\\ [1/1 files][ 31.1 GiB/ 31.1 GiB] 100% Done  70.6 MiB/s ETA 00:00:00           \n",
      "Operation completed over 1 objects/31.1 GiB.                                     \n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "# Populate the tar file\n",
    "loc = join(sorting,'data.tar')\n",
    "with tarfile.open(loc, \"w\") as tar:\n",
    "    #for name in [sorting_logs, sorted_tiles, trainValid_records, test_records]:\n",
    "    for name in [trainValid_records, test_records]:\n",
    "        tar.add(name)\n",
    "\n",
    "# Copy it to GCS\n",
    "gcs = join('gs://', deeppath_data_bucket,tiling_params, sorting_params, 'data.tar')\n",
    "!gsutil -m cp $loc $gcs \n",
    "\n",
    "# Delete the tar file\n",
    "!rm $loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optionally load previously saved training results from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://deeppath-data-whc/Px512Ol0Bg25Mg5_Tile/So3_Sort/Cl3FtTrue_Train/data.tar.gz not found\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "gcs = join('gs://', deeppath_data_bucket, tiling_params, sorting_params, training_params, 'data.tar.gz')\n",
    "loc = join(training,'data.tar.gz')\n",
    "\n",
    "result = !gsutil ls $gcs\n",
    "if result[0] ==gcs:\n",
    "    !gsutil -m cp $gcs $loc \n",
    "\n",
    "    with tarfile.open(loc) as tar:\n",
    "        tar.extractall('/')\n",
    "\n",
    "    !rm $loc\n",
    "else:\n",
    "    print('{} not found'.format(gcs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We first have to install the Bazel build tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install -y pkg-config zip g++ zlib1g-dev unzip python\n",
    "!wget https://github.com/bazelbuild/bazel/releases/download/0.24.0/bazel-0.24.0-installer-linux-x86_64.sh\n",
    "!chmod +x bazel-0.24.0-installer-linux-x86_64.sh\n",
    "!./bazel-0.24.0-installer-linux-x86_64.sh --user\n",
    "os.environ[\"PATH\"] += \":\" + join(os.getcwd(),'bin')\n",
    "!rm bazel-0.24.0-installer-linux-x86_64.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next build the Inception model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#os.chdir(join(os.environ[\"HOME\"],'DeepPATH/DeepPATH_code/s01_training/xClasses'))\n",
    "cwd = os.getcwd()\n",
    "os.chdir(join(deeppath_code,'01_training/xClasses'))\n",
    "print(os.getcwd())\n",
    "!bazel build inception/imagenet_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Fully train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!!!\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So10_Sort/TFRecord_TrainValid/train-*\n",
      "WARNING:tensorflow:From /home/bcliffor/dp/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "!!!!!!!!!!!!!!\n",
      "/mnt/disks/deeppath-data/Data/Px299Ol0Bg25Mg20_Tile/So10_Sort/TFRecord_TrainValid/train-*\n",
      "WARNING:tensorflow:From /home/bcliffor/DeepPATH/DeepPATH_code/01_training/xClasses/bazel-bin/inception/imagenet_train.runfiles/inception/inception/image_processing.py:457: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/bcliffor/dp/lib/python3.5/site-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/bcliffor/dp/lib/python3.5/site-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /home/bcliffor/dp/lib/python3.5/site-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/bcliffor/dp/lib/python3.5/site-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/bcliffor/dp/lib/python3.5/site-packages/tensorflow/python/training/input.py:202: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/bcliffor/DeepPATH/DeepPATH_code/01_training/xClasses/bazel-bin/inception/imagenet_train.runfiles/inception/inception/dataset.py:110: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n",
      "WARNING:tensorflow:From /home/bcliffor/DeepPATH/DeepPATH_code/01_training/xClasses/bazel-bin/inception/imagenet_train.runfiles/inception/inception/image_processing.py:245: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "WARNING:tensorflow:From /home/bcliffor/DeepPATH/DeepPATH_code/01_training/xClasses/bazel-bin/inception/imagenet_train.runfiles/inception/inception/image_processing.py:519: batch_join (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.interleave(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From /home/bcliffor/DeepPATH/DeepPATH_code/01_training/xClasses/bazel-bin/inception/imagenet_train.runfiles/inception/inception/slim/ops.py:419: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "signoid training\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "sigmoid loss\n",
      "sigmoid loss\n",
      "WARNING:tensorflow:From /home/bcliffor/dp/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2019-05-20 17:54:24.307926: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-05-20 17:54:26.325019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-05-20 17:54:26.325713: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x557dbe574120 executing computations on platform CUDA. Devices:\n",
      "2019-05-20 17:54:26.325767: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2019-05-20 17:54:26.329259: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-05-20 17:54:26.330058: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x557dbe5de0d0 executing computations on platform Host. Devices:\n",
      "2019-05-20 17:54:26.330112: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-05-20 17:54:26.330486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
      "2019-05-20 17:54:26.330524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2019-05-20 17:54:26.331349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-05-20 17:54:26.331379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2019-05-20 17:54:26.331388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2019-05-20 17:54:26.331637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10802 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "WARNING:tensorflow:From /home/bcliffor/DeepPATH/DeepPATH_code/01_training/xClasses/bazel-bin/inception/imagenet_train.runfiles/inception/inception/inception_train.py:339: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "2019-05-20 17:54:56.612508: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
      "2019-05-20 17:55:06.436175: step 0, loss = 4.36 (1.2 examples/sec; 25.000 sec/batch)\n",
      "2019-05-20 17:55:34.827440: step 10, loss = 4.09 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 17:55:47.069287: step 20, loss = 4.10 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-20 17:55:59.223047: step 30, loss = 4.08 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 17:56:11.403711: step 40, loss = 4.02 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 17:56:23.736977: step 50, loss = 4.09 (24.9 examples/sec; 1.205 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 17:56:35.843906: step 60, loss = 4.00 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 17:56:47.978778: step 70, loss = 3.99 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-20 17:57:00.227269: step 80, loss = 4.03 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-05-20 17:57:12.436982: step 90, loss = 4.05 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 17:57:24.432718: step 100, loss = 4.10 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 17:57:38.910257: step 110, loss = 3.97 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 17:57:51.043289: step 120, loss = 3.98 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 17:58:03.269580: step 130, loss = 4.02 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 17:58:15.480036: step 140, loss = 4.01 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 17:58:27.683085: step 150, loss = 4.00 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-20 17:58:39.844876: step 160, loss = 3.98 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-20 17:58:52.025693: step 170, loss = 3.98 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 17:59:04.234592: step 180, loss = 3.97 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 17:59:16.404750: step 190, loss = 3.93 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 17:59:28.590523: step 200, loss = 3.96 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 17:59:42.755694: step 210, loss = 3.91 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 17:59:55.003853: step 220, loss = 3.98 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-20 18:00:07.195453: step 230, loss = 3.91 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-20 18:00:19.421542: step 240, loss = 4.01 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 18:00:31.582506: step 250, loss = 3.98 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 18:00:43.692130: step 260, loss = 4.03 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 18:00:55.890613: step 270, loss = 3.97 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 18:01:08.097293: step 280, loss = 4.04 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 18:01:20.243879: step 290, loss = 3.92 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 18:01:32.417765: step 300, loss = 3.93 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 18:01:46.460013: step 310, loss = 3.97 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-20 18:01:58.549662: step 320, loss = 3.93 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 18:02:10.863554: step 330, loss = 3.94 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 18:02:23.103065: step 340, loss = 4.01 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-20 18:02:35.115516: step 350, loss = 3.93 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 18:02:47.345974: step 360, loss = 3.96 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 18:02:59.547735: step 370, loss = 3.88 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 18:03:11.696472: step 380, loss = 3.93 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 18:03:23.846006: step 390, loss = 4.00 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 18:03:36.042328: step 400, loss = 3.99 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 18:03:50.342483: step 410, loss = 3.89 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 18:04:02.531816: step 420, loss = 3.87 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 18:04:14.688230: step 430, loss = 3.87 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 18:04:26.873566: step 440, loss = 3.97 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 18:04:39.091593: step 450, loss = 3.90 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 18:04:51.253598: step 460, loss = 3.92 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 18:05:03.433372: step 470, loss = 3.88 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 18:05:15.724321: step 480, loss = 3.98 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 18:05:27.849006: step 490, loss = 3.93 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 18:05:39.999585: step 500, loss = 3.83 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 18:05:54.499976: step 510, loss = 3.81 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 18:06:06.649940: step 520, loss = 3.84 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 18:06:18.780129: step 530, loss = 3.87 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 18:06:30.912396: step 540, loss = 3.91 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 18:06:43.063940: step 550, loss = 3.85 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 18:06:55.139577: step 560, loss = 3.87 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 18:07:07.288388: step 570, loss = 3.85 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 18:07:19.429708: step 580, loss = 3.86 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 18:07:31.604538: step 590, loss = 3.92 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 18:07:43.637105: step 600, loss = 3.79 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-20 18:07:57.964743: step 610, loss = 3.95 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 18:08:10.186927: step 620, loss = 3.80 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 18:08:22.360983: step 630, loss = 3.79 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 18:08:34.645933: step 640, loss = 3.88 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 18:08:46.842076: step 650, loss = 3.83 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 18:08:58.998433: step 660, loss = 3.82 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-20 18:09:11.189614: step 670, loss = 3.83 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-20 18:09:23.374547: step 680, loss = 3.82 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 18:09:35.589536: step 690, loss = 3.82 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 18:09:47.895268: step 700, loss = 3.79 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 18:10:02.009540: step 710, loss = 3.94 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 18:10:14.153554: step 720, loss = 3.78 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 18:10:26.382661: step 730, loss = 3.91 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 18:10:38.560935: step 740, loss = 3.85 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 18:10:50.741708: step 750, loss = 3.81 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 18:11:02.927337: step 760, loss = 3.89 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 18:11:15.142253: step 770, loss = 3.79 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-20 18:11:27.329635: step 780, loss = 3.86 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 18:11:39.524119: step 790, loss = 3.86 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 18:11:51.707964: step 800, loss = 3.81 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 18:12:05.615071: step 810, loss = 3.74 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 18:12:17.824380: step 820, loss = 3.74 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 18:12:30.105294: step 830, loss = 3.78 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 18:12:42.319922: step 840, loss = 3.75 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 18:12:54.328483: step 850, loss = 3.80 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-20 18:13:06.470437: step 860, loss = 3.75 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 18:13:18.686986: step 870, loss = 3.84 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 18:13:30.856797: step 880, loss = 3.79 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 18:13:43.033217: step 890, loss = 3.75 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 18:13:55.234932: step 900, loss = 3.75 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 18:14:09.400575: step 910, loss = 3.84 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 18:14:21.556489: step 920, loss = 3.77 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-20 18:14:33.711354: step 930, loss = 3.72 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 18:14:45.970563: step 940, loss = 3.81 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 18:14:58.184381: step 950, loss = 3.76 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 18:15:10.374665: step 960, loss = 3.81 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 18:15:22.593960: step 970, loss = 3.80 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-20 18:15:34.786406: step 980, loss = 3.72 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 18:15:46.967981: step 990, loss = 3.66 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 18:15:59.145188: step 1000, loss = 3.75 (24.1 examples/sec; 1.244 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 18:16:13.520982: step 1010, loss = 3.66 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 18:16:25.718802: step 1020, loss = 3.75 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 18:16:37.875230: step 1030, loss = 3.72 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 18:16:50.051072: step 1040, loss = 3.67 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 18:17:02.287770: step 1050, loss = 3.73 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-20 18:17:14.407887: step 1060, loss = 3.74 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 18:17:26.570861: step 1070, loss = 3.75 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 18:17:38.846647: step 1080, loss = 3.77 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 18:17:51.102780: step 1090, loss = 3.76 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 18:18:03.146269: step 1100, loss = 3.76 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-20 18:18:17.521827: step 1110, loss = 3.68 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-20 18:18:29.656941: step 1120, loss = 3.71 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 18:18:41.795330: step 1130, loss = 3.65 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 18:18:53.990177: step 1140, loss = 3.70 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 18:19:06.203406: step 1150, loss = 3.81 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-20 18:19:18.472454: step 1160, loss = 3.70 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 18:19:30.704508: step 1170, loss = 3.74 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 18:19:42.855374: step 1180, loss = 3.72 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 18:19:55.148875: step 1190, loss = 3.69 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-20 18:20:07.278945: step 1200, loss = 3.71 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 18:20:21.609222: step 1210, loss = 3.68 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 18:20:33.798222: step 1220, loss = 3.70 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 18:20:46.039106: step 1230, loss = 3.71 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 18:20:58.246107: step 1240, loss = 3.63 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 18:21:10.417220: step 1250, loss = 3.75 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 18:21:22.618342: step 1260, loss = 3.73 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 18:21:34.828927: step 1270, loss = 3.75 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 18:21:46.998372: step 1280, loss = 3.77 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 18:21:59.227268: step 1290, loss = 3.65 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 18:22:11.304209: step 1300, loss = 3.60 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-20 18:22:25.385545: step 1310, loss = 3.68 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 18:22:37.608186: step 1320, loss = 3.70 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 18:22:49.774617: step 1330, loss = 3.63 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 18:23:02.048226: step 1340, loss = 3.68 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-20 18:23:14.105455: step 1350, loss = 3.65 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 18:23:26.234186: step 1360, loss = 3.73 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 18:23:38.390259: step 1370, loss = 3.64 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 18:23:50.542893: step 1380, loss = 3.63 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-20 18:24:02.743185: step 1390, loss = 3.66 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-20 18:24:14.927287: step 1400, loss = 3.71 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 18:24:29.115581: step 1410, loss = 3.63 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-05-20 18:24:41.307153: step 1420, loss = 3.62 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 18:24:53.453303: step 1430, loss = 3.69 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 18:25:05.640509: step 1440, loss = 3.65 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 18:25:17.783388: step 1450, loss = 3.64 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 18:25:29.922671: step 1460, loss = 3.62 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 18:25:42.174385: step 1470, loss = 3.63 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 18:25:54.436911: step 1480, loss = 3.69 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 18:26:06.580385: step 1490, loss = 3.61 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 18:26:18.777182: step 1500, loss = 3.63 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 18:26:33.092099: step 1510, loss = 3.60 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 18:26:45.387204: step 1520, loss = 3.68 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-05-20 18:26:57.510399: step 1530, loss = 3.64 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 18:27:09.708377: step 1540, loss = 3.59 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 18:27:21.917311: step 1550, loss = 3.65 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-05-20 18:27:34.082028: step 1560, loss = 3.71 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 18:27:46.270231: step 1570, loss = 3.60 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 18:27:58.439008: step 1580, loss = 3.57 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 18:28:10.724237: step 1590, loss = 3.58 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 18:28:22.912852: step 1600, loss = 3.61 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 18:28:36.932347: step 1610, loss = 3.60 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 18:28:49.137755: step 1620, loss = 3.61 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 18:29:01.320469: step 1630, loss = 3.58 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 18:29:13.508705: step 1640, loss = 3.63 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 18:29:25.750962: step 1650, loss = 3.58 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-20 18:29:37.923681: step 1660, loss = 3.56 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 18:29:50.202350: step 1670, loss = 3.62 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 18:30:02.380116: step 1680, loss = 3.56 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 18:30:14.545322: step 1690, loss = 3.55 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 18:30:26.792126: step 1700, loss = 3.55 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 18:30:40.917933: step 1710, loss = 3.57 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 18:30:53.142176: step 1720, loss = 3.56 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 18:31:05.468590: step 1730, loss = 3.52 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-05-20 18:31:17.659147: step 1740, loss = 3.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 18:31:29.870745: step 1750, loss = 3.63 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 18:31:42.111205: step 1760, loss = 3.46 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-20 18:31:54.371019: step 1770, loss = 3.54 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 18:32:06.542613: step 1780, loss = 3.58 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-20 18:32:18.681613: step 1790, loss = 3.63 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 18:32:30.745121: step 1800, loss = 3.52 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 18:32:45.059845: step 1810, loss = 3.54 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 18:32:57.246000: step 1820, loss = 3.53 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-20 18:33:09.553954: step 1830, loss = 3.55 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 18:33:21.703905: step 1840, loss = 3.51 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 18:33:33.903108: step 1850, loss = 3.53 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 18:33:45.982824: step 1860, loss = 3.57 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 18:33:58.170943: step 1870, loss = 3.50 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 18:34:10.368437: step 1880, loss = 3.56 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 18:34:22.561397: step 1890, loss = 3.53 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 18:34:34.849584: step 1900, loss = 3.60 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 18:34:49.342933: step 1910, loss = 3.55 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 18:35:01.560137: step 1920, loss = 3.57 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 18:35:13.713295: step 1930, loss = 3.55 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 18:35:25.895995: step 1940, loss = 3.49 (24.6 examples/sec; 1.219 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 18:35:38.100446: step 1950, loss = 3.50 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 18:35:50.274369: step 1960, loss = 3.46 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 18:36:02.445020: step 1970, loss = 3.53 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 18:36:14.710127: step 1980, loss = 3.54 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 18:36:26.839583: step 1990, loss = 3.46 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 18:36:39.141227: step 2000, loss = 3.48 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-20 18:36:53.460010: step 2010, loss = 3.51 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 18:37:05.626214: step 2020, loss = 3.53 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 18:37:17.872078: step 2030, loss = 3.47 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 18:37:30.040811: step 2040, loss = 3.43 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 18:37:42.134884: step 2050, loss = 3.51 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 18:37:54.381142: step 2060, loss = 3.42 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 18:38:06.590858: step 2070, loss = 3.45 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-20 18:38:18.770972: step 2080, loss = 3.44 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 18:38:30.959548: step 2090, loss = 3.48 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-20 18:38:43.067537: step 2100, loss = 3.46 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-20 18:38:56.981365: step 2110, loss = 3.48 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 18:39:09.152851: step 2120, loss = 3.55 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 18:39:21.310507: step 2130, loss = 3.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 18:39:33.546810: step 2140, loss = 3.54 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-20 18:39:45.814759: step 2150, loss = 3.44 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 18:39:58.030327: step 2160, loss = 3.46 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 18:40:10.209159: step 2170, loss = 3.46 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 18:40:22.365409: step 2180, loss = 3.53 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 18:40:34.623515: step 2190, loss = 3.46 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 18:40:46.892615: step 2200, loss = 3.50 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 18:41:01.210363: step 2210, loss = 3.37 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 18:41:13.391892: step 2220, loss = 3.46 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 18:41:25.685677: step 2230, loss = 3.51 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 18:41:37.876736: step 2240, loss = 3.41 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 18:41:50.060751: step 2250, loss = 3.49 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 18:42:02.192624: step 2260, loss = 3.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 18:42:14.374490: step 2270, loss = 3.44 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 18:42:26.605321: step 2280, loss = 3.42 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 18:42:38.655763: step 2290, loss = 3.46 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 18:42:50.940056: step 2300, loss = 3.40 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 18:43:04.847544: step 2310, loss = 3.46 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 18:43:17.042480: step 2320, loss = 3.55 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 18:43:29.348656: step 2330, loss = 3.46 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 18:43:41.532198: step 2340, loss = 3.42 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 18:43:53.645039: step 2350, loss = 3.42 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 18:44:05.865333: step 2360, loss = 3.42 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 18:44:17.989226: step 2370, loss = 3.42 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 18:44:30.206311: step 2380, loss = 3.38 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 18:44:42.484824: step 2390, loss = 3.45 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 18:44:54.717345: step 2400, loss = 3.45 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 18:45:09.036911: step 2410, loss = 3.38 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 18:45:21.227364: step 2420, loss = 3.40 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 18:45:33.378684: step 2430, loss = 3.41 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 18:45:45.587627: step 2440, loss = 3.34 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 18:45:57.759548: step 2450, loss = 3.32 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-20 18:46:09.950770: step 2460, loss = 3.39 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 18:46:22.170562: step 2470, loss = 3.42 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-20 18:46:34.353871: step 2480, loss = 3.36 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 18:46:46.558082: step 2490, loss = 3.41 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 18:46:58.724504: step 2500, loss = 3.40 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 18:47:13.012908: step 2510, loss = 3.35 (23.1 examples/sec; 1.300 sec/batch)\n",
      "2019-05-20 18:47:25.143659: step 2520, loss = 3.29 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 18:47:37.288960: step 2530, loss = 3.40 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 18:47:49.473948: step 2540, loss = 3.35 (23.1 examples/sec; 1.297 sec/batch)\n",
      "2019-05-20 18:48:01.647490: step 2550, loss = 3.33 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 18:48:13.832300: step 2560, loss = 3.37 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 18:48:26.077617: step 2570, loss = 3.41 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 18:48:38.270411: step 2580, loss = 3.38 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 18:48:50.527551: step 2590, loss = 3.37 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 18:49:02.646829: step 2600, loss = 3.42 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 18:49:16.730444: step 2610, loss = 3.30 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 18:49:28.886822: step 2620, loss = 3.37 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 18:49:41.025143: step 2630, loss = 3.38 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 18:49:53.221461: step 2640, loss = 3.33 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 18:50:05.482912: step 2650, loss = 3.34 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 18:50:17.750369: step 2660, loss = 3.35 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 18:50:29.984339: step 2670, loss = 3.34 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 18:50:42.212211: step 2680, loss = 3.30 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 18:50:54.401455: step 2690, loss = 3.38 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 18:51:06.590306: step 2700, loss = 3.35 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 18:51:20.819827: step 2710, loss = 3.35 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-20 18:51:32.994577: step 2720, loss = 3.32 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 18:51:45.241631: step 2730, loss = 3.36 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-20 18:51:57.484822: step 2740, loss = 3.37 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 18:52:09.651676: step 2750, loss = 3.35 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 18:52:21.784859: step 2760, loss = 3.31 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 18:52:33.990831: step 2770, loss = 3.37 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 18:52:46.111552: step 2780, loss = 3.36 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 18:52:58.378950: step 2790, loss = 3.23 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-20 18:53:10.568818: step 2800, loss = 3.35 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 18:53:24.783642: step 2810, loss = 3.27 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 18:53:36.922077: step 2820, loss = 3.33 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 18:53:49.241176: step 2830, loss = 3.32 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 18:54:01.451488: step 2840, loss = 3.29 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-20 18:54:13.689432: step 2850, loss = 3.33 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-05-20 18:54:25.700852: step 2860, loss = 3.34 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 18:54:37.837599: step 2870, loss = 3.30 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 18:54:50.115974: step 2880, loss = 3.33 (22.9 examples/sec; 1.309 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 18:55:02.289219: step 2890, loss = 3.29 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 18:55:14.440378: step 2900, loss = 3.30 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 18:55:28.598270: step 2910, loss = 3.32 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 18:55:40.830239: step 2920, loss = 3.28 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 18:55:53.041416: step 2930, loss = 3.32 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 18:56:05.365342: step 2940, loss = 3.32 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 18:56:17.519596: step 2950, loss = 3.34 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 18:56:29.675222: step 2960, loss = 3.33 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 18:56:41.857689: step 2970, loss = 3.20 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 18:56:54.062074: step 2980, loss = 3.32 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 18:57:06.232284: step 2990, loss = 3.27 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 18:57:18.407160: step 3000, loss = 3.32 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 18:57:32.396846: step 3010, loss = 3.31 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 18:57:44.632488: step 3020, loss = 3.21 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 18:57:56.745995: step 3030, loss = 3.28 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 18:58:08.922695: step 3040, loss = 3.20 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 18:58:21.104463: step 3050, loss = 3.18 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 18:58:33.259058: step 3060, loss = 3.34 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 18:58:45.463640: step 3070, loss = 3.31 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-20 18:58:57.715966: step 3080, loss = 3.21 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 18:59:09.892203: step 3090, loss = 3.23 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 18:59:22.022901: step 3100, loss = 3.23 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 18:59:35.761945: step 3110, loss = 3.25 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 18:59:47.980074: step 3120, loss = 3.24 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 19:00:00.280928: step 3130, loss = 3.18 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 19:00:12.460492: step 3140, loss = 3.24 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 19:00:24.751131: step 3150, loss = 3.24 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-05-20 19:00:36.970558: step 3160, loss = 3.28 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 19:00:49.202989: step 3170, loss = 3.26 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-20 19:01:01.408884: step 3180, loss = 3.27 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 19:01:13.650556: step 3190, loss = 3.31 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 19:01:25.797224: step 3200, loss = 3.24 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 19:01:39.663288: step 3210, loss = 3.33 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 19:01:51.839548: step 3220, loss = 3.21 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 19:02:04.088885: step 3230, loss = 3.28 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 19:02:16.274361: step 3240, loss = 3.15 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 19:02:28.431285: step 3250, loss = 3.25 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 19:02:40.642996: step 3260, loss = 3.28 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 19:02:52.882152: step 3270, loss = 3.20 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 19:03:04.929157: step 3280, loss = 3.32 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 19:03:17.097559: step 3290, loss = 3.24 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 19:03:29.310667: step 3300, loss = 3.19 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-20 19:03:43.384862: step 3310, loss = 3.17 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 19:03:55.628343: step 3320, loss = 3.21 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-05-20 19:04:07.844134: step 3330, loss = 3.19 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 19:04:19.996375: step 3340, loss = 3.15 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 19:04:32.218972: step 3350, loss = 3.21 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 19:04:44.227459: step 3360, loss = 3.27 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-20 19:04:56.353015: step 3370, loss = 3.26 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 19:05:08.512126: step 3380, loss = 3.16 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 19:05:20.853627: step 3390, loss = 3.21 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 19:05:33.043331: step 3400, loss = 3.15 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 19:05:47.122575: step 3410, loss = 3.17 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 19:05:59.292827: step 3420, loss = 3.18 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 19:06:11.506225: step 3430, loss = 3.14 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-20 19:06:23.687311: step 3440, loss = 3.19 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 19:06:35.865516: step 3450, loss = 3.18 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 19:06:48.054705: step 3460, loss = 3.22 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 19:07:00.241723: step 3470, loss = 3.15 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 19:07:12.462289: step 3480, loss = 3.20 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 19:07:24.663020: step 3490, loss = 3.14 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 19:07:36.890031: step 3500, loss = 3.18 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 19:07:50.921424: step 3510, loss = 3.18 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 19:08:03.077975: step 3520, loss = 3.18 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 19:08:15.260645: step 3530, loss = 3.19 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 19:08:27.548339: step 3540, loss = 3.18 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 19:08:39.696122: step 3550, loss = 3.19 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 19:08:51.855074: step 3560, loss = 3.21 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 19:09:04.015191: step 3570, loss = 3.17 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 19:09:16.200380: step 3580, loss = 3.22 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 19:09:28.339508: step 3590, loss = 3.18 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 19:09:40.503423: step 3600, loss = 3.15 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 19:09:54.415742: step 3610, loss = 3.18 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 19:10:06.707620: step 3620, loss = 3.20 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 19:10:18.888534: step 3630, loss = 3.11 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 19:10:31.143658: step 3640, loss = 3.18 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 19:10:43.348477: step 3650, loss = 3.14 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 19:10:55.587948: step 3660, loss = 3.13 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 19:11:07.836292: step 3670, loss = 3.12 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 19:11:20.037450: step 3680, loss = 3.15 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 19:11:32.207966: step 3690, loss = 3.09 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 19:11:44.418771: step 3700, loss = 3.14 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 19:11:58.466799: step 3710, loss = 3.12 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 19:12:10.639590: step 3720, loss = 3.18 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 19:12:22.835683: step 3730, loss = 3.19 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 19:12:34.996353: step 3740, loss = 3.20 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 19:12:47.138266: step 3750, loss = 3.11 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 19:12:59.287477: step 3760, loss = 3.15 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 19:13:11.367612: step 3770, loss = 3.10 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 19:13:23.620000: step 3780, loss = 3.18 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 19:13:35.782197: step 3790, loss = 3.09 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 19:13:47.938492: step 3800, loss = 3.08 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 19:14:02.102224: step 3810, loss = 3.13 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 19:14:14.289000: step 3820, loss = 3.08 (24.9 examples/sec; 1.206 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 19:14:26.423176: step 3830, loss = 3.14 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 19:14:38.610103: step 3840, loss = 3.14 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 19:14:50.777441: step 3850, loss = 3.12 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 19:15:02.775036: step 3860, loss = 3.10 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-20 19:15:14.957271: step 3870, loss = 3.15 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 19:15:27.168434: step 3880, loss = 3.11 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-20 19:15:39.345784: step 3890, loss = 3.14 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 19:15:51.546055: step 3900, loss = 3.11 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 19:16:05.775793: step 3910, loss = 3.15 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-20 19:16:17.890533: step 3920, loss = 3.01 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 19:16:30.047816: step 3930, loss = 3.05 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 19:16:42.301148: step 3940, loss = 3.05 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 19:16:54.452855: step 3950, loss = 3.10 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-20 19:17:06.687261: step 3960, loss = 3.08 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 19:17:19.022259: step 3970, loss = 3.05 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-20 19:17:31.215101: step 3980, loss = 3.06 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-20 19:17:43.490265: step 3990, loss = 3.05 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 19:17:55.639307: step 4000, loss = 3.11 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-20 19:18:09.953008: step 4010, loss = 3.08 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-20 19:18:22.097160: step 4020, loss = 3.01 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 19:18:34.284754: step 4030, loss = 3.02 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 19:18:46.460215: step 4040, loss = 3.11 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 19:18:58.635013: step 4050, loss = 3.10 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 19:19:10.862305: step 4060, loss = 3.01 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 19:19:23.014187: step 4070, loss = 3.02 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 19:19:35.404658: step 4080, loss = 3.11 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-20 19:19:47.613584: step 4090, loss = 3.13 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 19:19:59.816485: step 4100, loss = 3.09 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 19:20:13.752113: step 4110, loss = 3.04 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 19:20:25.978308: step 4120, loss = 3.05 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 19:20:38.125608: step 4130, loss = 3.08 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 19:20:50.393822: step 4140, loss = 3.06 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 19:21:02.557171: step 4150, loss = 3.04 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 19:21:14.713495: step 4160, loss = 3.02 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 19:21:26.880735: step 4170, loss = 3.10 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 19:21:39.028143: step 4180, loss = 3.05 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 19:21:51.231911: step 4190, loss = 3.06 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 19:22:03.391014: step 4200, loss = 3.08 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 19:22:17.421245: step 4210, loss = 3.01 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 19:22:29.532011: step 4220, loss = 3.09 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 19:22:41.716306: step 4230, loss = 3.02 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 19:22:53.874221: step 4240, loss = 3.09 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 19:23:06.062226: step 4250, loss = 3.11 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 19:23:18.259217: step 4260, loss = 3.00 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-20 19:23:30.354738: step 4270, loss = 3.06 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 19:23:42.545150: step 4280, loss = 3.02 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 19:23:54.760378: step 4290, loss = 3.03 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 19:24:07.056413: step 4300, loss = 2.98 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 19:24:20.986670: step 4310, loss = 3.01 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 19:24:33.140468: step 4320, loss = 2.98 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 19:24:45.389503: step 4330, loss = 3.06 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 19:24:57.529800: step 4340, loss = 3.03 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 19:25:09.682301: step 4350, loss = 3.05 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 19:25:21.704361: step 4360, loss = 3.00 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-20 19:25:33.800150: step 4370, loss = 3.02 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 19:25:46.018445: step 4380, loss = 2.98 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 19:25:58.242187: step 4390, loss = 3.10 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 19:26:10.377527: step 4400, loss = 3.03 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 19:26:24.858403: step 4410, loss = 2.95 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 19:26:37.128551: step 4420, loss = 2.94 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 19:26:49.311127: step 4430, loss = 3.02 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 19:27:01.458257: step 4440, loss = 3.02 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 19:27:13.612692: step 4450, loss = 2.96 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 19:27:25.876112: step 4460, loss = 2.99 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 19:27:38.029830: step 4470, loss = 3.04 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 19:27:50.203972: step 4480, loss = 2.92 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 19:28:02.369215: step 4490, loss = 2.93 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-20 19:28:14.559479: step 4500, loss = 2.97 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 19:28:28.604496: step 4510, loss = 3.04 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 19:28:40.777989: step 4520, loss = 3.04 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 19:28:52.958189: step 4530, loss = 2.93 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 19:29:05.151059: step 4540, loss = 2.93 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 19:29:17.326618: step 4550, loss = 2.97 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 19:29:29.488681: step 4560, loss = 2.95 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 19:29:41.648773: step 4570, loss = 2.98 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 19:29:53.831980: step 4580, loss = 3.04 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 19:30:05.991192: step 4590, loss = 2.98 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 19:30:18.164887: step 4600, loss = 2.94 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 19:30:32.194986: step 4610, loss = 3.03 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-20 19:30:44.265721: step 4620, loss = 2.92 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 19:30:56.416379: step 4630, loss = 2.91 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 19:31:08.672561: step 4640, loss = 2.97 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 19:31:20.838581: step 4650, loss = 2.95 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 19:31:33.009325: step 4660, loss = 2.97 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 19:31:45.162163: step 4670, loss = 2.97 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 19:31:57.378542: step 4680, loss = 3.03 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 19:32:09.555285: step 4690, loss = 2.93 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 19:32:21.735474: step 4700, loss = 2.98 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 19:32:35.907514: step 4710, loss = 2.92 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 19:32:48.074712: step 4720, loss = 2.95 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 19:33:00.332856: step 4730, loss = 2.96 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-20 19:33:12.525356: step 4740, loss = 2.89 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-20 19:33:24.670202: step 4750, loss = 2.99 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 19:33:36.767117: step 4760, loss = 2.98 (24.3 examples/sec; 1.233 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 19:33:49.037067: step 4770, loss = 2.95 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 19:34:01.217674: step 4780, loss = 2.89 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 19:34:13.404741: step 4790, loss = 2.98 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 19:34:25.561207: step 4800, loss = 2.92 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 19:34:39.588068: step 4810, loss = 3.00 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 19:34:51.767646: step 4820, loss = 2.86 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 19:35:03.935197: step 4830, loss = 2.94 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 19:35:16.073107: step 4840, loss = 2.90 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 19:35:28.240429: step 4850, loss = 2.89 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 19:35:40.322256: step 4860, loss = 2.92 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 19:35:52.368943: step 4870, loss = 2.91 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 19:36:04.701674: step 4880, loss = 2.88 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 19:36:16.911295: step 4890, loss = 2.87 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 19:36:29.169310: step 4900, loss = 2.87 (24.1 examples/sec; 1.242 sec/batch)\n",
      "2019-05-20 19:36:43.507329: step 4910, loss = 2.90 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 19:36:55.673421: step 4920, loss = 2.88 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 19:37:07.814120: step 4930, loss = 2.90 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 19:37:20.023423: step 4940, loss = 2.95 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 19:37:32.201384: step 4950, loss = 2.90 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 19:37:44.381626: step 4960, loss = 2.97 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 19:37:56.570943: step 4970, loss = 2.96 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 19:38:08.793256: step 4980, loss = 2.87 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 19:38:20.989953: step 4990, loss = 2.88 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 19:38:33.170300: step 5000, loss = 2.83 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 19:38:51.072194: step 5010, loss = 2.87 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 19:39:03.349832: step 5020, loss = 2.83 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 19:39:15.492734: step 5030, loss = 2.82 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 19:39:27.645643: step 5040, loss = 2.87 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 19:39:39.940688: step 5050, loss = 2.87 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 19:39:52.124933: step 5060, loss = 2.87 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-20 19:40:04.266669: step 5070, loss = 2.82 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 19:40:16.448641: step 5080, loss = 2.91 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 19:40:28.613874: step 5090, loss = 2.87 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 19:40:40.848668: step 5100, loss = 2.93 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 19:40:55.128898: step 5110, loss = 2.90 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-20 19:41:07.242904: step 5120, loss = 2.80 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 19:41:19.380715: step 5130, loss = 2.84 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 19:41:31.563352: step 5140, loss = 2.84 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-20 19:41:43.708218: step 5150, loss = 2.81 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 19:41:55.877700: step 5160, loss = 2.87 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 19:42:08.018905: step 5170, loss = 2.89 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-20 19:42:20.171310: step 5180, loss = 2.83 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 19:42:32.342530: step 5190, loss = 2.88 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 19:42:44.555174: step 5200, loss = 2.81 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-20 19:42:58.501595: step 5210, loss = 2.76 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 19:43:10.657063: step 5220, loss = 2.84 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 19:43:22.939271: step 5230, loss = 2.89 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 19:43:35.138528: step 5240, loss = 2.84 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 19:43:47.194215: step 5250, loss = 2.87 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 19:43:59.361743: step 5260, loss = 2.87 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 19:44:11.680169: step 5270, loss = 2.81 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 19:44:23.965359: step 5280, loss = 2.81 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 19:44:36.116101: step 5290, loss = 2.81 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 19:44:48.286390: step 5300, loss = 2.85 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 19:45:02.269188: step 5310, loss = 2.74 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 19:45:14.414173: step 5320, loss = 2.82 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-20 19:45:26.730686: step 5330, loss = 2.88 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 19:45:38.920277: step 5340, loss = 2.90 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 19:45:51.101723: step 5350, loss = 2.86 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 19:46:03.239401: step 5360, loss = 2.81 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 19:46:15.312921: step 5370, loss = 2.84 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 19:46:27.536547: step 5380, loss = 2.80 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-20 19:46:39.698855: step 5390, loss = 2.83 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 19:46:51.939163: step 5400, loss = 2.84 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 19:47:05.933086: step 5410, loss = 2.74 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 19:47:18.124800: step 5420, loss = 2.91 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 19:47:30.365201: step 5430, loss = 2.81 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 19:47:42.545531: step 5440, loss = 2.84 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 19:47:54.698311: step 5450, loss = 2.89 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 19:48:06.884477: step 5460, loss = 2.77 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 19:48:19.081383: step 5470, loss = 2.78 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 19:48:31.229850: step 5480, loss = 2.78 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 19:48:43.345905: step 5490, loss = 2.76 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-20 19:48:55.430560: step 5500, loss = 2.77 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 19:49:09.349999: step 5510, loss = 2.81 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 19:49:21.535543: step 5520, loss = 2.78 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 19:49:33.760509: step 5530, loss = 2.80 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-20 19:49:45.895028: step 5540, loss = 2.79 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 19:49:58.085810: step 5550, loss = 2.78 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 19:50:10.296621: step 5560, loss = 2.77 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-20 19:50:22.603390: step 5570, loss = 2.79 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-20 19:50:34.770888: step 5580, loss = 2.77 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-20 19:50:46.893249: step 5590, loss = 2.81 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 19:50:59.044469: step 5600, loss = 2.82 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 19:51:13.227256: step 5610, loss = 2.74 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-20 19:51:25.419291: step 5620, loss = 2.76 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 19:51:37.598993: step 5630, loss = 2.77 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 19:51:49.899149: step 5640, loss = 2.77 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 19:52:02.156783: step 5650, loss = 2.72 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 19:52:14.468571: step 5660, loss = 2.82 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 19:52:26.619105: step 5670, loss = 2.80 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 19:52:38.739092: step 5680, loss = 2.82 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 19:52:50.858019: step 5690, loss = 2.79 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-20 19:53:03.064371: step 5700, loss = 2.73 (24.3 examples/sec; 1.236 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 19:53:17.021546: step 5710, loss = 2.78 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 19:53:29.242841: step 5720, loss = 2.74 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-20 19:53:41.428348: step 5730, loss = 2.81 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 19:53:53.495502: step 5740, loss = 2.70 (25.6 examples/sec; 1.171 sec/batch)\n",
      "2019-05-20 19:54:05.659035: step 5750, loss = 2.75 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 19:54:17.839247: step 5760, loss = 2.72 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 19:54:30.034119: step 5770, loss = 2.72 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 19:54:42.263463: step 5780, loss = 2.72 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 19:54:54.416354: step 5790, loss = 2.68 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 19:55:06.554948: step 5800, loss = 2.80 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 19:55:20.734128: step 5810, loss = 2.67 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 19:55:32.891588: step 5820, loss = 2.78 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 19:55:45.162697: step 5830, loss = 2.74 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 19:55:57.352757: step 5840, loss = 2.75 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 19:56:09.509735: step 5850, loss = 2.70 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 19:56:21.761131: step 5860, loss = 2.71 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-20 19:56:33.786881: step 5870, loss = 2.67 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 19:56:45.974575: step 5880, loss = 2.83 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 19:56:58.133452: step 5890, loss = 2.71 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 19:57:10.323954: step 5900, loss = 2.73 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-20 19:57:24.446304: step 5910, loss = 2.72 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 19:57:36.696837: step 5920, loss = 2.70 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 19:57:48.840990: step 5930, loss = 2.68 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 19:58:00.974447: step 5940, loss = 2.70 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 19:58:13.108623: step 5950, loss = 2.69 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 19:58:25.290061: step 5960, loss = 2.73 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 19:58:37.511340: step 5970, loss = 2.71 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 19:58:49.689620: step 5980, loss = 2.73 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 19:59:01.700742: step 5990, loss = 2.66 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 19:59:13.906562: step 6000, loss = 2.75 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 19:59:27.812251: step 6010, loss = 2.73 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 19:59:40.048276: step 6020, loss = 2.71 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 19:59:52.211689: step 6030, loss = 2.65 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 20:00:04.390199: step 6040, loss = 2.73 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 20:00:16.567445: step 6050, loss = 2.70 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-20 20:00:28.752145: step 6060, loss = 2.73 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 20:00:40.915778: step 6070, loss = 2.75 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 20:00:53.122433: step 6080, loss = 2.67 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 20:01:05.371293: step 6090, loss = 2.74 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 20:01:17.535330: step 6100, loss = 2.76 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 20:01:31.615815: step 6110, loss = 2.68 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-20 20:01:43.609437: step 6120, loss = 2.70 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 20:01:55.799560: step 6130, loss = 2.74 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 20:02:07.984040: step 6140, loss = 2.68 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 20:02:20.149553: step 6150, loss = 2.64 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 20:02:32.320310: step 6160, loss = 2.71 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 20:02:44.530053: step 6170, loss = 2.63 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 20:02:56.815790: step 6180, loss = 2.73 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 20:03:09.123109: step 6190, loss = 2.73 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 20:03:21.317671: step 6200, loss = 2.69 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 20:03:35.179707: step 6210, loss = 2.67 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 20:03:47.357022: step 6220, loss = 2.66 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 20:03:59.551634: step 6230, loss = 2.62 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 20:04:11.616232: step 6240, loss = 2.69 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 20:04:23.816850: step 6250, loss = 2.74 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 20:04:36.066618: step 6260, loss = 2.61 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 20:04:48.236769: step 6270, loss = 2.67 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-20 20:05:00.467178: step 6280, loss = 2.72 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 20:05:12.651235: step 6290, loss = 2.61 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 20:05:24.839343: step 6300, loss = 2.59 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 20:05:38.835356: step 6310, loss = 2.76 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 20:05:50.989976: step 6320, loss = 2.66 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 20:06:03.159126: step 6330, loss = 2.57 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 20:06:15.251874: step 6340, loss = 2.64 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 20:06:27.439343: step 6350, loss = 2.61 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 20:06:39.593070: step 6360, loss = 2.66 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 20:06:51.594553: step 6370, loss = 2.65 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-20 20:07:03.751376: step 6380, loss = 2.62 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 20:07:15.864840: step 6390, loss = 2.74 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 20:07:28.003370: step 6400, loss = 2.61 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 20:07:41.940329: step 6410, loss = 2.61 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 20:07:54.104273: step 6420, loss = 2.65 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 20:08:06.202994: step 6430, loss = 2.67 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 20:08:18.368241: step 6440, loss = 2.61 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 20:08:30.554676: step 6450, loss = 2.63 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-20 20:08:42.726996: step 6460, loss = 2.70 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 20:08:54.948696: step 6470, loss = 2.60 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 20:09:07.117620: step 6480, loss = 2.66 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 20:09:19.217996: step 6490, loss = 2.62 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 20:09:31.375694: step 6500, loss = 2.60 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 20:09:45.527694: step 6510, loss = 2.63 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 20:09:57.691064: step 6520, loss = 2.69 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 20:10:09.897625: step 6530, loss = 2.64 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 20:10:22.022051: step 6540, loss = 2.66 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 20:10:34.235527: step 6550, loss = 2.66 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 20:10:46.395838: step 6560, loss = 2.65 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 20:10:58.550667: step 6570, loss = 2.60 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 20:11:10.694345: step 6580, loss = 2.62 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 20:11:22.923225: step 6590, loss = 2.60 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 20:11:35.108868: step 6600, loss = 2.59 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 20:11:49.211154: step 6610, loss = 2.62 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 20:12:01.286201: step 6620, loss = 2.61 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-20 20:12:13.429252: step 6630, loss = 2.64 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 20:12:25.613592: step 6640, loss = 2.63 (25.0 examples/sec; 1.202 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 20:12:37.793234: step 6650, loss = 2.60 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 20:12:50.036060: step 6660, loss = 2.58 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-05-20 20:13:02.231687: step 6670, loss = 2.59 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 20:13:14.391573: step 6680, loss = 2.62 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 20:13:26.548149: step 6690, loss = 2.57 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 20:13:38.680175: step 6700, loss = 2.49 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 20:13:52.863672: step 6710, loss = 2.58 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 20:14:05.026718: step 6720, loss = 2.55 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-20 20:14:17.081296: step 6730, loss = 2.56 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-20 20:14:29.259580: step 6740, loss = 2.65 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 20:14:41.441207: step 6750, loss = 2.59 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 20:14:53.614244: step 6760, loss = 2.61 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-20 20:15:05.784618: step 6770, loss = 2.57 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 20:15:17.974884: step 6780, loss = 2.60 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 20:15:30.156177: step 6790, loss = 2.60 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 20:15:42.285053: step 6800, loss = 2.54 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 20:15:56.219932: step 6810, loss = 2.59 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 20:16:08.362527: step 6820, loss = 2.56 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 20:16:20.556506: step 6830, loss = 2.51 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 20:16:32.782419: step 6840, loss = 2.60 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 20:16:44.971599: step 6850, loss = 2.58 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 20:16:57.078529: step 6860, loss = 2.57 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 20:17:09.124064: step 6870, loss = 2.60 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-20 20:17:21.246190: step 6880, loss = 2.51 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 20:17:33.422411: step 6890, loss = 2.56 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-20 20:17:45.604441: step 6900, loss = 2.56 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 20:17:59.964472: step 6910, loss = 2.54 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 20:18:12.190298: step 6920, loss = 2.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 20:18:24.367694: step 6930, loss = 2.62 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 20:18:36.525442: step 6940, loss = 2.49 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 20:18:48.787540: step 6950, loss = 2.59 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-20 20:19:00.976582: step 6960, loss = 2.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 20:19:13.246804: step 6970, loss = 2.56 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 20:19:25.383266: step 6980, loss = 2.51 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 20:19:37.560469: step 6990, loss = 2.51 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 20:19:49.711991: step 7000, loss = 2.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 20:20:03.627910: step 7010, loss = 2.50 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-20 20:20:15.830424: step 7020, loss = 2.47 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 20:20:28.045867: step 7030, loss = 2.57 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 20:20:40.340833: step 7040, loss = 2.54 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 20:20:52.588942: step 7050, loss = 2.51 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-20 20:21:04.776347: step 7060, loss = 2.56 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 20:21:16.949851: step 7070, loss = 2.62 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 20:21:29.112735: step 7080, loss = 2.62 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 20:21:41.271083: step 7090, loss = 2.59 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 20:21:53.408467: step 7100, loss = 2.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 20:22:07.538355: step 7110, loss = 2.55 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 20:22:19.616738: step 7120, loss = 2.58 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 20:22:31.812019: step 7130, loss = 2.56 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-20 20:22:44.035935: step 7140, loss = 2.53 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-20 20:22:56.328976: step 7150, loss = 2.50 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-20 20:23:08.516030: step 7160, loss = 2.61 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 20:23:20.734693: step 7170, loss = 2.51 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-20 20:23:32.920953: step 7180, loss = 2.57 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 20:23:45.121517: step 7190, loss = 2.48 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 20:23:57.353403: step 7200, loss = 2.55 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 20:24:11.484837: step 7210, loss = 2.51 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-20 20:24:23.654809: step 7220, loss = 2.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 20:24:35.746967: step 7230, loss = 2.49 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 20:24:47.958555: step 7240, loss = 2.60 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 20:25:00.105858: step 7250, loss = 2.51 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 20:25:12.370019: step 7260, loss = 2.52 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-05-20 20:25:24.525040: step 7270, loss = 2.53 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 20:25:36.701336: step 7280, loss = 2.55 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 20:25:48.868194: step 7290, loss = 2.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 20:26:01.077308: step 7300, loss = 2.55 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 20:26:15.478156: step 7310, loss = 2.49 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 20:26:27.637110: step 7320, loss = 2.48 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 20:26:39.818136: step 7330, loss = 2.48 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 20:26:51.917974: step 7340, loss = 2.46 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-20 20:27:04.061812: step 7350, loss = 2.50 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 20:27:16.198575: step 7360, loss = 2.49 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 20:27:28.235678: step 7370, loss = 2.51 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 20:27:40.291289: step 7380, loss = 2.61 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 20:27:52.515727: step 7390, loss = 2.56 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 20:28:04.698655: step 7400, loss = 2.48 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 20:28:18.837083: step 7410, loss = 2.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 20:28:31.000756: step 7420, loss = 2.59 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 20:28:43.186640: step 7430, loss = 2.47 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 20:28:55.399547: step 7440, loss = 2.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 20:29:07.582015: step 7450, loss = 2.47 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 20:29:19.746059: step 7460, loss = 2.50 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 20:29:31.853134: step 7470, loss = 2.48 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-20 20:29:43.968396: step 7480, loss = 2.51 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 20:29:56.170148: step 7490, loss = 2.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 20:30:08.357718: step 7500, loss = 2.49 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 20:30:22.511222: step 7510, loss = 2.47 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 20:30:34.678304: step 7520, loss = 2.45 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 20:30:46.904554: step 7530, loss = 2.47 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 20:30:59.061067: step 7540, loss = 2.44 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 20:31:11.249032: step 7550, loss = 2.41 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-20 20:31:23.441683: step 7560, loss = 2.41 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 20:31:35.662614: step 7570, loss = 2.49 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-20 20:31:47.833522: step 7580, loss = 2.50 (24.8 examples/sec; 1.210 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 20:32:00.016963: step 7590, loss = 2.44 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 20:32:12.162164: step 7600, loss = 2.45 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 20:32:26.126839: step 7610, loss = 2.44 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 20:32:38.185396: step 7620, loss = 2.42 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-20 20:32:50.234905: step 7630, loss = 2.42 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 20:33:02.445098: step 7640, loss = 2.39 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 20:33:14.748854: step 7650, loss = 2.40 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-05-20 20:33:26.909603: step 7660, loss = 2.45 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 20:33:39.114754: step 7670, loss = 2.47 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 20:33:51.312338: step 7680, loss = 2.39 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 20:34:03.669179: step 7690, loss = 2.39 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 20:34:15.895005: step 7700, loss = 2.46 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-20 20:34:30.119486: step 7710, loss = 2.56 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 20:34:42.199247: step 7720, loss = 2.39 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 20:34:54.361562: step 7730, loss = 2.49 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 20:35:06.521277: step 7740, loss = 2.52 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 20:35:18.866869: step 7750, loss = 2.35 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 20:35:31.017389: step 7760, loss = 2.44 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 20:35:43.176831: step 7770, loss = 2.46 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 20:35:55.366443: step 7780, loss = 2.47 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 20:36:07.524935: step 7790, loss = 2.41 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 20:36:19.777341: step 7800, loss = 2.42 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 20:36:33.944324: step 7810, loss = 2.43 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 20:36:46.226314: step 7820, loss = 2.35 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 20:36:58.476657: step 7830, loss = 2.41 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 20:37:10.662956: step 7840, loss = 2.42 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 20:37:22.812778: step 7850, loss = 2.39 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 20:37:34.992336: step 7860, loss = 2.47 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 20:37:47.169055: step 7870, loss = 2.43 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-20 20:37:59.164946: step 7880, loss = 2.42 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 20:38:11.389338: step 7890, loss = 2.38 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 20:38:23.602356: step 7900, loss = 2.44 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 20:38:37.669305: step 7910, loss = 2.39 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 20:38:49.901823: step 7920, loss = 2.38 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 20:39:02.056616: step 7930, loss = 2.41 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 20:39:14.259379: step 7940, loss = 2.38 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 20:39:26.397768: step 7950, loss = 2.41 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 20:39:38.544541: step 7960, loss = 2.36 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 20:39:50.584890: step 7970, loss = 2.45 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-20 20:40:02.752452: step 7980, loss = 2.42 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 20:40:15.067357: step 7990, loss = 2.43 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 20:40:27.267415: step 8000, loss = 2.43 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 20:40:41.197236: step 8010, loss = 2.37 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 20:40:53.326174: step 8020, loss = 2.37 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 20:41:05.518857: step 8030, loss = 2.48 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-20 20:41:17.654794: step 8040, loss = 2.40 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 20:41:29.843067: step 8050, loss = 2.38 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 20:41:42.020479: step 8060, loss = 2.35 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-20 20:41:54.284166: step 8070, loss = 2.38 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 20:42:06.459463: step 8080, loss = 2.41 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 20:42:18.746212: step 8090, loss = 2.35 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 20:42:30.899992: step 8100, loss = 2.41 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 20:42:44.872127: step 8110, loss = 2.35 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 20:42:57.064409: step 8120, loss = 2.35 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 20:43:09.090178: step 8130, loss = 2.40 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 20:43:21.265547: step 8140, loss = 2.42 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 20:43:33.408863: step 8150, loss = 2.41 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 20:43:45.599045: step 8160, loss = 2.37 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-20 20:43:57.770060: step 8170, loss = 2.37 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 20:44:09.975896: step 8180, loss = 2.45 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 20:44:22.157758: step 8190, loss = 2.40 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 20:44:34.289539: step 8200, loss = 2.40 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 20:44:48.352241: step 8210, loss = 2.36 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-20 20:45:00.526624: step 8220, loss = 2.43 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 20:45:12.708413: step 8230, loss = 2.44 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 20:45:24.950313: step 8240, loss = 2.34 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 20:45:37.144455: step 8250, loss = 2.34 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 20:45:49.327777: step 8260, loss = 2.34 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 20:46:01.532693: step 8270, loss = 2.45 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 20:46:13.741564: step 8280, loss = 2.34 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 20:46:25.917022: step 8290, loss = 2.30 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 20:46:38.086654: step 8300, loss = 2.36 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 20:46:52.178070: step 8310, loss = 2.28 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 20:47:04.314361: step 8320, loss = 2.38 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 20:47:16.457749: step 8330, loss = 2.30 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 20:47:28.658651: step 8340, loss = 2.29 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 20:47:40.932434: step 8350, loss = 2.34 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 20:47:53.149386: step 8360, loss = 2.36 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 20:48:05.456910: step 8370, loss = 2.35 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 20:48:17.425727: step 8380, loss = 2.39 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 20:48:29.638670: step 8390, loss = 2.31 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 20:48:41.793421: step 8400, loss = 2.37 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 20:48:56.192794: step 8410, loss = 2.33 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 20:49:08.351278: step 8420, loss = 2.42 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 20:49:20.617885: step 8430, loss = 2.32 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 20:49:32.715026: step 8440, loss = 2.31 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 20:49:45.011094: step 8450, loss = 2.36 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-05-20 20:49:57.027202: step 8460, loss = 2.32 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-20 20:50:09.124170: step 8470, loss = 2.34 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 20:50:21.334295: step 8480, loss = 2.27 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 20:50:33.494193: step 8490, loss = 2.32 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 20:50:45.629857: step 8500, loss = 2.32 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 20:50:59.824579: step 8510, loss = 2.30 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 20:51:11.919331: step 8520, loss = 2.36 (25.0 examples/sec; 1.200 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 20:51:24.077779: step 8530, loss = 2.41 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 20:51:36.307398: step 8540, loss = 2.38 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 20:51:48.493890: step 8550, loss = 2.28 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 20:52:00.661210: step 8560, loss = 2.37 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 20:52:12.803633: step 8570, loss = 2.33 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 20:52:24.962019: step 8580, loss = 2.32 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 20:52:37.140584: step 8590, loss = 2.43 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 20:52:49.310397: step 8600, loss = 2.33 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 20:53:03.306507: step 8610, loss = 2.28 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 20:53:15.476784: step 8620, loss = 2.29 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 20:53:27.502761: step 8630, loss = 2.31 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 20:53:39.603482: step 8640, loss = 2.31 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 20:53:51.760962: step 8650, loss = 2.35 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 20:54:03.978657: step 8660, loss = 2.30 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 20:54:16.076118: step 8670, loss = 2.29 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 20:54:28.293239: step 8680, loss = 2.31 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 20:54:40.532918: step 8690, loss = 2.29 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 20:54:52.676750: step 8700, loss = 2.36 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 20:55:06.563804: step 8710, loss = 2.27 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 20:55:18.797548: step 8720, loss = 2.31 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 20:55:30.964458: step 8730, loss = 2.27 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 20:55:43.179829: step 8740, loss = 2.32 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-20 20:55:55.438993: step 8750, loss = 2.34 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 20:56:07.597766: step 8760, loss = 2.28 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 20:56:19.811857: step 8770, loss = 2.33 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 20:56:32.001054: step 8780, loss = 2.27 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 20:56:44.253798: step 8790, loss = 2.29 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 20:56:56.438733: step 8800, loss = 2.26 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 20:57:10.279637: step 8810, loss = 2.32 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-20 20:57:22.405343: step 8820, loss = 2.32 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 20:57:34.645007: step 8830, loss = 2.29 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 20:57:47.018829: step 8840, loss = 2.22 (23.1 examples/sec; 1.300 sec/batch)\n",
      "2019-05-20 20:57:59.166690: step 8850, loss = 2.29 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 20:58:11.376039: step 8860, loss = 2.32 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-20 20:58:23.553881: step 8870, loss = 2.22 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-20 20:58:35.624478: step 8880, loss = 2.24 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-20 20:58:47.732336: step 8890, loss = 2.28 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 20:58:59.918650: step 8900, loss = 2.29 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 20:59:14.137382: step 8910, loss = 2.24 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 20:59:26.251358: step 8920, loss = 2.30 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 20:59:38.477039: step 8930, loss = 2.22 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 20:59:50.671427: step 8940, loss = 2.23 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 21:00:02.854667: step 8950, loss = 2.29 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 21:00:14.905468: step 8960, loss = 2.26 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 21:00:27.115826: step 8970, loss = 2.24 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 21:00:39.304949: step 8980, loss = 2.25 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 21:00:51.433205: step 8990, loss = 2.26 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 21:01:03.618957: step 9000, loss = 2.28 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 21:01:17.714772: step 9010, loss = 2.27 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 21:01:29.894888: step 9020, loss = 2.29 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-20 21:01:42.000095: step 9030, loss = 2.27 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-20 21:01:54.321553: step 9040, loss = 2.29 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 21:02:06.526875: step 9050, loss = 2.32 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 21:02:18.661021: step 9060, loss = 2.26 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 21:02:30.788593: step 9070, loss = 2.24 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 21:02:43.010714: step 9080, loss = 2.14 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 21:02:55.245037: step 9090, loss = 2.22 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 21:03:07.521615: step 9100, loss = 2.21 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-20 21:03:21.692414: step 9110, loss = 2.26 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 21:03:33.897069: step 9120, loss = 2.28 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 21:03:45.976612: step 9130, loss = 2.24 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 21:03:58.106335: step 9140, loss = 2.26 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 21:04:10.427574: step 9150, loss = 2.27 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 21:04:22.606002: step 9160, loss = 2.21 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 21:04:34.852849: step 9170, loss = 2.28 (23.1 examples/sec; 1.296 sec/batch)\n",
      "2019-05-20 21:04:47.033052: step 9180, loss = 2.27 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 21:04:59.221782: step 9190, loss = 2.14 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-20 21:05:11.354681: step 9200, loss = 2.26 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-20 21:05:25.384877: step 9210, loss = 2.24 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 21:05:37.538260: step 9220, loss = 2.18 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 21:05:49.869172: step 9230, loss = 2.27 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 21:06:02.052811: step 9240, loss = 2.19 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 21:06:14.244609: step 9250, loss = 2.28 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 21:06:26.381283: step 9260, loss = 2.21 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 21:06:38.536686: step 9270, loss = 2.23 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 21:06:50.804829: step 9280, loss = 2.27 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-20 21:07:02.997854: step 9290, loss = 2.27 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 21:07:15.154440: step 9300, loss = 2.21 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 21:07:29.019621: step 9310, loss = 2.27 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-05-20 21:07:41.181666: step 9320, loss = 2.22 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 21:07:53.364838: step 9330, loss = 2.17 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 21:08:05.495840: step 9340, loss = 2.28 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 21:08:17.610748: step 9350, loss = 2.24 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 21:08:29.774106: step 9360, loss = 2.11 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-20 21:08:42.063247: step 9370, loss = 2.16 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 21:08:54.184897: step 9380, loss = 2.22 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 21:09:06.306655: step 9390, loss = 2.15 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-20 21:09:18.470334: step 9400, loss = 2.22 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 21:09:32.432783: step 9410, loss = 2.22 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 21:09:44.590312: step 9420, loss = 2.25 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 21:09:56.776063: step 9430, loss = 2.21 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 21:10:09.069692: step 9440, loss = 2.19 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 21:10:21.217055: step 9450, loss = 2.16 (23.2 examples/sec; 1.295 sec/batch)\n",
      "2019-05-20 21:10:33.483280: step 9460, loss = 2.18 (24.6 examples/sec; 1.220 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 21:10:45.698753: step 9470, loss = 2.21 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-20 21:10:57.895206: step 9480, loss = 2.24 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 21:11:10.103993: step 9490, loss = 2.21 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 21:11:22.280553: step 9500, loss = 2.22 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 21:11:36.461054: step 9510, loss = 2.22 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 21:11:48.690648: step 9520, loss = 2.23 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 21:12:00.888934: step 9530, loss = 2.15 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 21:12:13.147432: step 9540, loss = 2.25 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 21:12:25.420877: step 9550, loss = 2.25 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 21:12:37.633728: step 9560, loss = 2.23 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 21:12:49.844731: step 9570, loss = 2.16 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 21:13:01.998612: step 9580, loss = 2.18 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 21:13:14.178691: step 9590, loss = 2.14 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 21:13:26.321547: step 9600, loss = 2.21 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 21:13:40.425275: step 9610, loss = 2.19 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-20 21:13:52.596050: step 9620, loss = 2.16 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-20 21:14:04.715634: step 9630, loss = 2.18 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 21:14:16.809222: step 9640, loss = 2.16 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 21:14:29.036525: step 9650, loss = 2.16 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 21:14:41.195242: step 9660, loss = 2.21 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-20 21:14:53.346848: step 9670, loss = 2.18 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-20 21:15:05.521697: step 9680, loss = 2.34 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 21:15:17.711757: step 9690, loss = 2.19 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-20 21:15:29.816135: step 9700, loss = 2.21 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-20 21:15:44.248146: step 9710, loss = 2.20 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 21:15:56.459559: step 9720, loss = 2.18 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-20 21:16:08.580136: step 9730, loss = 2.23 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 21:16:20.802827: step 9740, loss = 2.23 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 21:16:32.989968: step 9750, loss = 2.14 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 21:16:45.169592: step 9760, loss = 2.16 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 21:16:57.354701: step 9770, loss = 2.15 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-20 21:17:09.533780: step 9780, loss = 2.17 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 21:17:21.682571: step 9790, loss = 2.19 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 21:17:33.844514: step 9800, loss = 2.14 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 21:17:48.167686: step 9810, loss = 2.13 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 21:18:00.346034: step 9820, loss = 2.16 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 21:18:12.535555: step 9830, loss = 2.15 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 21:18:24.760157: step 9840, loss = 2.16 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 21:18:36.920176: step 9850, loss = 2.18 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 21:18:49.182224: step 9860, loss = 2.20 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-05-20 21:19:01.349848: step 9870, loss = 2.15 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 21:19:13.465328: step 9880, loss = 2.15 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-20 21:19:25.468382: step 9890, loss = 2.16 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 21:19:37.617748: step 9900, loss = 2.18 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 21:19:51.562801: step 9910, loss = 2.13 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-20 21:20:03.726790: step 9920, loss = 2.07 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 21:20:15.902204: step 9930, loss = 2.14 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 21:20:28.020200: step 9940, loss = 2.15 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 21:20:40.185135: step 9950, loss = 2.14 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 21:20:52.437861: step 9960, loss = 2.14 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 21:21:04.590241: step 9970, loss = 2.07 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 21:21:16.763371: step 9980, loss = 2.16 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 21:21:28.996630: step 9990, loss = 2.14 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 21:21:41.155302: step 10000, loss = 2.17 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 21:21:58.625301: step 10010, loss = 2.12 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 21:22:10.766598: step 10020, loss = 2.20 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 21:22:22.997805: step 10030, loss = 2.15 (23.2 examples/sec; 1.294 sec/batch)\n",
      "2019-05-20 21:22:35.180353: step 10040, loss = 2.18 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 21:22:47.405382: step 10050, loss = 2.11 (23.2 examples/sec; 1.292 sec/batch)\n",
      "2019-05-20 21:22:59.551345: step 10060, loss = 2.13 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 21:23:11.713687: step 10070, loss = 2.22 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 21:23:23.982855: step 10080, loss = 2.14 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 21:23:36.188593: step 10090, loss = 2.15 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 21:23:48.495785: step 10100, loss = 2.08 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 21:24:02.834399: step 10110, loss = 2.18 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 21:24:14.991939: step 10120, loss = 2.10 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 21:24:27.043538: step 10130, loss = 2.09 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-20 21:24:39.150528: step 10140, loss = 2.13 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-20 21:24:51.271009: step 10150, loss = 2.11 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-20 21:25:03.474707: step 10160, loss = 2.09 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 21:25:15.687628: step 10170, loss = 2.13 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 21:25:27.866476: step 10180, loss = 2.11 (23.1 examples/sec; 1.298 sec/batch)\n",
      "2019-05-20 21:25:39.944341: step 10190, loss = 2.08 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 21:25:52.066002: step 10200, loss = 2.11 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 21:26:06.104971: step 10210, loss = 2.15 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-20 21:26:18.319457: step 10220, loss = 2.09 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-20 21:26:30.475173: step 10230, loss = 2.12 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-20 21:26:42.595400: step 10240, loss = 2.14 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 21:26:54.784178: step 10250, loss = 2.11 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 21:27:06.997939: step 10260, loss = 2.02 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 21:27:19.188247: step 10270, loss = 2.12 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-20 21:27:31.346845: step 10280, loss = 2.06 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 21:27:43.538399: step 10290, loss = 2.14 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 21:27:55.667599: step 10300, loss = 2.13 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 21:28:09.528750: step 10310, loss = 2.18 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 21:28:21.681867: step 10320, loss = 2.03 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 21:28:33.963777: step 10330, loss = 2.12 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 21:28:46.115469: step 10340, loss = 2.11 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 21:28:58.331789: step 10350, loss = 2.06 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 21:29:10.499965: step 10360, loss = 2.13 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 21:29:22.762964: step 10370, loss = 2.08 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 21:29:34.901562: step 10380, loss = 2.07 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 21:29:46.915419: step 10390, loss = 2.11 (24.9 examples/sec; 1.207 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 21:29:59.018613: step 10400, loss = 2.13 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 21:30:13.098332: step 10410, loss = 2.13 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 21:30:25.257163: step 10420, loss = 2.12 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 21:30:37.417319: step 10430, loss = 2.11 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 21:30:49.480732: step 10440, loss = 2.12 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 21:31:01.625964: step 10450, loss = 2.12 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 21:31:13.848784: step 10460, loss = 2.09 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 21:31:26.027169: step 10470, loss = 2.11 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 21:31:38.192886: step 10480, loss = 2.09 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-20 21:31:50.372209: step 10490, loss = 2.08 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 21:32:02.644778: step 10500, loss = 2.07 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-20 21:32:16.644025: step 10510, loss = 2.09 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 21:32:28.866738: step 10520, loss = 2.00 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-05-20 21:32:41.012931: step 10530, loss = 2.10 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 21:32:53.183639: step 10540, loss = 2.08 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 21:33:05.326985: step 10550, loss = 2.03 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 21:33:17.668824: step 10560, loss = 2.16 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 21:33:29.801989: step 10570, loss = 2.07 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 21:33:41.939239: step 10580, loss = 2.04 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 21:33:54.114727: step 10590, loss = 2.03 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 21:34:06.299809: step 10600, loss = 2.09 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-20 21:34:20.300033: step 10610, loss = 2.07 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 21:34:32.481864: step 10620, loss = 2.05 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 21:34:44.676441: step 10630, loss = 2.08 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-20 21:34:56.728941: step 10640, loss = 2.08 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 21:35:08.942315: step 10650, loss = 1.99 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-20 21:35:21.158276: step 10660, loss = 2.06 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 21:35:33.285128: step 10670, loss = 2.03 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 21:35:45.448974: step 10680, loss = 2.11 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-20 21:35:57.687439: step 10690, loss = 2.03 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-20 21:36:09.878180: step 10700, loss = 2.07 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 21:36:24.027985: step 10710, loss = 2.03 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 21:36:36.198770: step 10720, loss = 1.97 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 21:36:48.421947: step 10730, loss = 2.08 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 21:37:00.632535: step 10740, loss = 2.02 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 21:37:12.785831: step 10750, loss = 2.01 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 21:37:24.923303: step 10760, loss = 2.00 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 21:37:37.102418: step 10770, loss = 2.02 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 21:37:49.352445: step 10780, loss = 2.04 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 21:38:01.564226: step 10790, loss = 2.07 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 21:38:13.736221: step 10800, loss = 2.02 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-20 21:38:27.804324: step 10810, loss = 2.03 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 21:38:40.002368: step 10820, loss = 2.08 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 21:38:52.343603: step 10830, loss = 2.08 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 21:39:04.625169: step 10840, loss = 1.99 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-20 21:39:16.781628: step 10850, loss = 2.02 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-20 21:39:29.040536: step 10860, loss = 2.02 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 21:39:41.250966: step 10870, loss = 2.00 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 21:39:53.415801: step 10880, loss = 2.06 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 21:40:05.465225: step 10890, loss = 2.01 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 21:40:17.576293: step 10900, loss = 2.04 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 21:40:31.686410: step 10910, loss = 2.02 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 21:40:43.835571: step 10920, loss = 2.03 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 21:40:55.845997: step 10930, loss = 2.01 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 21:41:08.038871: step 10940, loss = 2.00 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 21:41:20.173865: step 10950, loss = 2.05 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 21:41:32.348600: step 10960, loss = 2.06 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 21:41:44.607945: step 10970, loss = 2.10 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 21:41:56.802012: step 10980, loss = 2.03 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-20 21:42:09.000833: step 10990, loss = 2.04 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 21:42:21.270308: step 11000, loss = 1.98 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 21:42:35.314917: step 11010, loss = 2.01 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 21:42:47.646579: step 11020, loss = 2.03 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-20 21:42:59.869897: step 11030, loss = 2.04 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 21:43:12.094118: step 11040, loss = 2.04 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 21:43:24.268470: step 11050, loss = 2.04 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 21:43:36.386686: step 11060, loss = 2.02 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-20 21:43:48.580168: step 11070, loss = 1.99 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 21:44:00.757592: step 11080, loss = 1.97 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 21:44:12.911700: step 11090, loss = 1.96 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 21:44:25.075034: step 11100, loss = 2.02 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 21:44:39.329230: step 11110, loss = 1.99 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 21:44:51.619266: step 11120, loss = 1.94 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 21:45:03.779328: step 11130, loss = 2.04 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 21:45:15.740815: step 11140, loss = 1.90 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 21:45:27.854908: step 11150, loss = 2.04 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 21:45:40.045842: step 11160, loss = 2.03 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 21:45:52.266810: step 11170, loss = 1.97 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-20 21:46:04.380703: step 11180, loss = 1.96 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 21:46:16.548020: step 11190, loss = 1.96 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 21:46:28.817267: step 11200, loss = 1.96 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-05-20 21:46:42.898479: step 11210, loss = 1.98 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 21:46:55.005281: step 11220, loss = 2.00 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-20 21:47:07.154409: step 11230, loss = 1.97 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-20 21:47:19.306463: step 11240, loss = 1.99 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 21:47:31.474416: step 11250, loss = 1.99 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 21:47:43.692528: step 11260, loss = 2.03 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 21:47:55.852923: step 11270, loss = 1.93 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 21:48:08.023365: step 11280, loss = 1.96 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-20 21:48:20.172000: step 11290, loss = 2.02 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 21:48:32.394389: step 11300, loss = 1.99 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 21:48:46.516643: step 11310, loss = 1.99 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 21:48:58.700674: step 11320, loss = 2.00 (24.7 examples/sec; 1.214 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 21:49:10.863389: step 11330, loss = 1.95 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 21:49:23.013917: step 11340, loss = 1.93 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 21:49:35.174875: step 11350, loss = 1.94 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 21:49:47.367544: step 11360, loss = 2.02 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 21:49:59.598482: step 11370, loss = 1.91 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 21:50:11.755537: step 11380, loss = 1.97 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 21:50:23.758062: step 11390, loss = 2.02 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-20 21:50:35.897972: step 11400, loss = 1.92 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-20 21:50:49.967451: step 11410, loss = 1.97 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 21:51:02.075924: step 11420, loss = 2.00 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 21:51:14.175168: step 11430, loss = 2.09 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 21:51:26.379015: step 11440, loss = 1.99 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 21:51:38.620523: step 11450, loss = 1.97 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 21:51:50.889632: step 11460, loss = 1.97 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 21:52:03.081266: step 11470, loss = 1.95 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 21:52:15.285643: step 11480, loss = 1.99 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 21:52:27.494196: step 11490, loss = 1.97 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 21:52:39.639355: step 11500, loss = 1.91 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 21:52:53.813418: step 11510, loss = 2.00 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-20 21:53:05.984893: step 11520, loss = 2.01 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 21:53:18.147118: step 11530, loss = 1.97 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 21:53:30.441842: step 11540, loss = 1.97 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 21:53:42.636328: step 11550, loss = 1.97 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-20 21:53:54.836563: step 11560, loss = 1.97 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 21:54:06.961856: step 11570, loss = 1.97 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-20 21:54:19.141726: step 11580, loss = 1.92 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 21:54:31.234526: step 11590, loss = 1.98 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 21:54:43.458169: step 11600, loss = 1.91 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 21:54:57.335641: step 11610, loss = 1.98 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 21:55:09.313098: step 11620, loss = 1.93 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-20 21:55:21.408776: step 11630, loss = 1.92 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 21:55:33.425644: step 11640, loss = 1.91 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-20 21:55:45.497641: step 11650, loss = 1.94 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 21:55:57.671451: step 11660, loss = 1.90 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 21:56:09.691758: step 11670, loss = 1.94 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-20 21:56:21.796926: step 11680, loss = 1.95 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 21:56:33.856054: step 11690, loss = 1.92 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 21:56:46.055522: step 11700, loss = 1.91 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 21:56:59.961686: step 11710, loss = 1.92 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 21:57:12.065169: step 11720, loss = 1.90 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 21:57:24.122913: step 11730, loss = 1.86 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 21:57:36.300970: step 11740, loss = 1.91 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 21:57:48.390955: step 11750, loss = 1.93 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 21:58:00.516805: step 11760, loss = 1.94 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 21:58:12.620104: step 11770, loss = 1.94 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 21:58:24.703920: step 11780, loss = 1.91 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 21:58:36.723509: step 11790, loss = 1.94 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 21:58:48.791810: step 11800, loss = 1.95 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 21:59:03.073579: step 11810, loss = 1.92 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 21:59:15.096606: step 11820, loss = 1.97 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 21:59:27.136091: step 11830, loss = 1.94 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-20 21:59:39.096870: step 11840, loss = 1.93 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 21:59:51.104227: step 11850, loss = 1.93 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 22:00:03.090902: step 11860, loss = 1.93 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 22:00:15.073622: step 11870, loss = 1.86 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 22:00:27.186229: step 11880, loss = 1.93 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 22:00:39.254755: step 11890, loss = 1.94 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 22:00:51.302590: step 11900, loss = 1.92 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-20 22:01:05.360996: step 11910, loss = 1.97 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-20 22:01:17.300020: step 11920, loss = 1.92 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 22:01:29.388290: step 11930, loss = 1.90 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 22:01:41.440246: step 11940, loss = 1.93 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-20 22:01:53.520812: step 11950, loss = 1.93 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-20 22:02:05.596466: step 11960, loss = 1.95 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 22:02:17.650019: step 11970, loss = 1.96 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-20 22:02:29.717090: step 11980, loss = 1.91 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 22:02:41.791080: step 11990, loss = 1.92 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 22:02:53.882093: step 12000, loss = 1.91 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 22:03:07.670120: step 12010, loss = 1.87 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-20 22:03:19.820742: step 12020, loss = 1.95 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 22:03:31.940385: step 12030, loss = 1.86 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 22:03:44.110377: step 12040, loss = 1.86 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 22:03:56.230523: step 12050, loss = 1.89 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 22:04:08.405811: step 12060, loss = 1.92 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 22:04:20.492579: step 12070, loss = 1.89 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 22:04:32.555149: step 12080, loss = 1.91 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 22:04:44.687247: step 12090, loss = 1.89 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-20 22:04:56.812244: step 12100, loss = 1.90 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 22:05:10.956278: step 12110, loss = 1.92 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 22:05:23.102481: step 12120, loss = 1.89 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 22:05:35.115942: step 12130, loss = 1.91 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 22:05:47.208973: step 12140, loss = 1.91 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-20 22:05:59.296595: step 12150, loss = 1.89 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 22:06:11.323622: step 12160, loss = 1.89 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 22:06:23.435155: step 12170, loss = 1.89 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 22:06:35.550795: step 12180, loss = 1.89 (23.4 examples/sec; 1.281 sec/batch)\n",
      "2019-05-20 22:06:47.602225: step 12190, loss = 1.92 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 22:06:59.870357: step 12200, loss = 1.85 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 22:07:13.719542: step 12210, loss = 1.86 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 22:07:25.757541: step 12220, loss = 1.94 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 22:07:37.849667: step 12230, loss = 1.85 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-20 22:07:49.924452: step 12240, loss = 1.87 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-20 22:08:01.933456: step 12250, loss = 1.89 (24.6 examples/sec; 1.218 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 22:08:14.018866: step 12260, loss = 1.81 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-20 22:08:26.114907: step 12270, loss = 1.88 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 22:08:38.184573: step 12280, loss = 1.84 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 22:08:50.353148: step 12290, loss = 1.85 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 22:09:02.434808: step 12300, loss = 1.93 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 22:09:16.645863: step 12310, loss = 1.87 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 22:09:28.709822: step 12320, loss = 1.87 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 22:09:40.815624: step 12330, loss = 1.89 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 22:09:52.924202: step 12340, loss = 1.89 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 22:10:05.004744: step 12350, loss = 1.86 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 22:10:17.076982: step 12360, loss = 1.89 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 22:10:29.216362: step 12370, loss = 1.85 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-20 22:10:41.320694: step 12380, loss = 1.85 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 22:10:53.379338: step 12390, loss = 1.91 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 22:11:05.466570: step 12400, loss = 1.86 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 22:11:19.572472: step 12410, loss = 1.83 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 22:11:31.587430: step 12420, loss = 1.88 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 22:11:43.670932: step 12430, loss = 1.84 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 22:11:55.757867: step 12440, loss = 1.85 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 22:12:07.847616: step 12450, loss = 1.88 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 22:12:19.906636: step 12460, loss = 1.90 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 22:12:31.922064: step 12470, loss = 1.91 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-20 22:12:43.983394: step 12480, loss = 1.91 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 22:12:56.044838: step 12490, loss = 1.81 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 22:13:08.118713: step 12500, loss = 1.83 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 22:13:21.930624: step 12510, loss = 1.94 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-20 22:13:33.966435: step 12520, loss = 1.86 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-20 22:13:46.074819: step 12530, loss = 1.83 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 22:13:58.144771: step 12540, loss = 1.85 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 22:14:10.203651: step 12550, loss = 1.90 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 22:14:22.319800: step 12560, loss = 1.84 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 22:14:34.535996: step 12570, loss = 1.85 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 22:14:46.646152: step 12580, loss = 1.83 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 22:14:58.698535: step 12590, loss = 1.95 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 22:15:10.762897: step 12600, loss = 1.84 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 22:15:25.048474: step 12610, loss = 1.86 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-20 22:15:37.043810: step 12620, loss = 1.83 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-20 22:15:49.060822: step 12630, loss = 1.91 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 22:16:01.094696: step 12640, loss = 1.91 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 22:16:13.152107: step 12650, loss = 1.82 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 22:16:25.232118: step 12660, loss = 1.86 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 22:16:37.392512: step 12670, loss = 1.82 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 22:16:49.470077: step 12680, loss = 1.84 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 22:17:01.506372: step 12690, loss = 1.88 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-20 22:17:13.534730: step 12700, loss = 1.88 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 22:17:27.888252: step 12710, loss = 1.81 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 22:17:39.918787: step 12720, loss = 1.81 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-20 22:17:51.974982: step 12730, loss = 1.80 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 22:18:04.019103: step 12740, loss = 1.85 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 22:18:16.090459: step 12750, loss = 1.84 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 22:18:28.136489: step 12760, loss = 1.88 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-20 22:18:40.170369: step 12770, loss = 1.80 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-20 22:18:52.281241: step 12780, loss = 1.84 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-20 22:19:04.311091: step 12790, loss = 1.83 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 22:19:16.331804: step 12800, loss = 1.76 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-20 22:19:30.679293: step 12810, loss = 1.82 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 22:19:42.702459: step 12820, loss = 1.84 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 22:19:54.775032: step 12830, loss = 1.80 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 22:20:06.827666: step 12840, loss = 1.85 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-20 22:20:18.879714: step 12850, loss = 1.83 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 22:20:30.926023: step 12860, loss = 1.80 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 22:20:43.015302: step 12870, loss = 1.80 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 22:20:55.057711: step 12880, loss = 1.74 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 22:21:07.137570: step 12890, loss = 1.78 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-20 22:21:19.236201: step 12900, loss = 1.80 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 22:21:32.978271: step 12910, loss = 1.77 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 22:21:45.051078: step 12920, loss = 1.75 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 22:21:57.129933: step 12930, loss = 1.72 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-20 22:22:09.218257: step 12940, loss = 1.80 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 22:22:21.329895: step 12950, loss = 1.81 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 22:22:33.442259: step 12960, loss = 1.77 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-20 22:22:45.540420: step 12970, loss = 1.86 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 22:22:57.770227: step 12980, loss = 1.76 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 22:23:09.927792: step 12990, loss = 1.88 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 22:23:22.019120: step 13000, loss = 1.78 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 22:23:35.910279: step 13010, loss = 1.81 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 22:23:48.142951: step 13020, loss = 1.80 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 22:24:00.280948: step 13030, loss = 1.77 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 22:24:12.404580: step 13040, loss = 1.71 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 22:24:24.531876: step 13050, loss = 1.79 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 22:24:36.669158: step 13060, loss = 1.79 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 22:24:48.749781: step 13070, loss = 1.75 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 22:25:00.842098: step 13080, loss = 1.78 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 22:25:12.989936: step 13090, loss = 1.83 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 22:25:25.185496: step 13100, loss = 1.73 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 22:25:39.341841: step 13110, loss = 1.77 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 22:25:51.479439: step 13120, loss = 1.81 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 22:26:03.782595: step 13130, loss = 1.82 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 22:26:15.868611: step 13140, loss = 1.80 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-20 22:26:27.996956: step 13150, loss = 1.80 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 22:26:39.973397: step 13160, loss = 1.78 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-20 22:26:52.019319: step 13170, loss = 1.81 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 22:27:04.235727: step 13180, loss = 1.71 (25.0 examples/sec; 1.200 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 22:27:16.303608: step 13190, loss = 1.79 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 22:27:28.448616: step 13200, loss = 1.75 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 22:27:42.685564: step 13210, loss = 1.81 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 22:27:54.841274: step 13220, loss = 1.78 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 22:28:07.041634: step 13230, loss = 1.73 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-20 22:28:19.274508: step 13240, loss = 1.78 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 22:28:31.434949: step 13250, loss = 1.76 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 22:28:43.486313: step 13260, loss = 1.75 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 22:28:55.604633: step 13270, loss = 1.71 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 22:29:07.749930: step 13280, loss = 1.86 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 22:29:19.874954: step 13290, loss = 1.75 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 22:29:31.977605: step 13300, loss = 1.80 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-20 22:29:45.872609: step 13310, loss = 1.75 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 22:29:58.029896: step 13320, loss = 1.76 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 22:30:10.134523: step 13330, loss = 1.85 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 22:30:22.251412: step 13340, loss = 1.74 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 22:30:34.357033: step 13350, loss = 1.72 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 22:30:46.450258: step 13360, loss = 1.82 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 22:30:58.554703: step 13370, loss = 1.82 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 22:31:10.663127: step 13380, loss = 1.80 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 22:31:22.832451: step 13390, loss = 1.74 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 22:31:34.956362: step 13400, loss = 1.76 (25.5 examples/sec; 1.177 sec/batch)\n",
      "2019-05-20 22:31:48.689688: step 13410, loss = 1.77 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-20 22:32:00.715218: step 13420, loss = 1.75 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 22:32:12.739473: step 13430, loss = 1.77 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 22:32:24.903628: step 13440, loss = 1.69 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-05-20 22:32:36.954225: step 13450, loss = 1.74 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 22:32:49.059639: step 13460, loss = 1.81 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 22:33:01.148307: step 13470, loss = 1.74 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 22:33:13.196896: step 13480, loss = 1.74 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 22:33:25.237344: step 13490, loss = 1.73 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 22:33:37.333329: step 13500, loss = 1.75 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 22:33:51.285595: step 13510, loss = 1.78 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 22:34:03.454357: step 13520, loss = 1.74 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 22:34:15.488185: step 13530, loss = 1.75 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 22:34:27.541741: step 13540, loss = 1.75 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-20 22:34:39.577240: step 13550, loss = 1.69 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-20 22:34:51.619983: step 13560, loss = 1.73 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 22:35:03.707745: step 13570, loss = 1.74 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 22:35:15.734667: step 13580, loss = 1.73 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 22:35:27.939399: step 13590, loss = 1.79 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 22:35:40.025947: step 13600, loss = 1.79 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 22:35:54.288820: step 13610, loss = 1.79 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 22:36:06.441836: step 13620, loss = 1.74 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 22:36:18.694529: step 13630, loss = 1.73 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 22:36:30.761194: step 13640, loss = 1.75 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 22:36:42.809175: step 13650, loss = 1.74 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 22:36:54.830543: step 13660, loss = 1.75 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-20 22:37:06.935663: step 13670, loss = 1.71 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 22:37:18.950597: step 13680, loss = 1.73 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 22:37:30.987535: step 13690, loss = 1.81 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-20 22:37:43.070132: step 13700, loss = 1.72 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 22:37:56.943277: step 13710, loss = 1.78 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 22:38:09.035539: step 13720, loss = 1.73 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 22:38:21.115906: step 13730, loss = 1.75 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 22:38:33.161276: step 13740, loss = 1.73 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-20 22:38:45.166466: step 13750, loss = 1.75 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-20 22:38:57.218948: step 13760, loss = 1.70 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 22:39:09.245925: step 13770, loss = 1.68 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 22:39:21.292581: step 13780, loss = 1.75 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-20 22:39:33.467664: step 13790, loss = 1.69 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 22:39:45.630142: step 13800, loss = 1.67 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-05-20 22:39:59.514115: step 13810, loss = 1.76 (23.2 examples/sec; 1.296 sec/batch)\n",
      "2019-05-20 22:40:11.561523: step 13820, loss = 1.69 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 22:40:23.684125: step 13830, loss = 1.78 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 22:40:35.674559: step 13840, loss = 1.69 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 22:40:47.715225: step 13850, loss = 1.71 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-20 22:40:59.729406: step 13860, loss = 1.74 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-20 22:41:11.850929: step 13870, loss = 1.71 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 22:41:23.906425: step 13880, loss = 1.64 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-20 22:41:35.954673: step 13890, loss = 1.71 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 22:41:47.981367: step 13900, loss = 1.71 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 22:42:01.838639: step 13910, loss = 1.66 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-20 22:42:13.858329: step 13920, loss = 1.74 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 22:42:25.993201: step 13930, loss = 1.65 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-20 22:42:38.050618: step 13940, loss = 1.73 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-20 22:42:50.082568: step 13950, loss = 1.72 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 22:43:02.155309: step 13960, loss = 1.73 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 22:43:14.178745: step 13970, loss = 1.63 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-20 22:43:26.255818: step 13980, loss = 1.73 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-20 22:43:38.294304: step 13990, loss = 1.77 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 22:43:50.388797: step 14000, loss = 1.69 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 22:44:04.318945: step 14010, loss = 1.69 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-20 22:44:16.346225: step 14020, loss = 1.67 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-20 22:44:28.424543: step 14030, loss = 1.71 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 22:44:40.512540: step 14040, loss = 1.77 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-20 22:44:52.581138: step 14050, loss = 1.73 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-20 22:45:04.701494: step 14060, loss = 1.65 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 22:45:16.729354: step 14070, loss = 1.71 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 22:45:28.834167: step 14080, loss = 1.65 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 22:45:40.976494: step 14090, loss = 1.72 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 22:45:53.082947: step 14100, loss = 1.66 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-20 22:46:06.946957: step 14110, loss = 1.72 (25.1 examples/sec; 1.195 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 22:46:19.001869: step 14120, loss = 1.74 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 22:46:31.087676: step 14130, loss = 1.71 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 22:46:43.200581: step 14140, loss = 1.69 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 22:46:55.256541: step 14150, loss = 1.71 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 22:47:07.272352: step 14160, loss = 1.65 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-20 22:47:19.283677: step 14170, loss = 1.75 (25.3 examples/sec; 1.183 sec/batch)\n",
      "2019-05-20 22:47:31.288451: step 14180, loss = 1.65 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 22:47:43.338416: step 14190, loss = 1.75 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 22:47:55.428843: step 14200, loss = 1.71 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-20 22:48:09.480772: step 14210, loss = 1.73 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 22:48:21.577209: step 14220, loss = 1.67 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 22:48:33.713246: step 14230, loss = 1.66 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 22:48:45.811674: step 14240, loss = 1.71 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 22:48:57.859152: step 14250, loss = 1.68 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 22:49:10.043364: step 14260, loss = 1.67 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 22:49:22.054986: step 14270, loss = 1.64 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 22:49:34.118344: step 14280, loss = 1.67 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-20 22:49:46.209680: step 14290, loss = 1.76 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 22:49:58.349883: step 14300, loss = 1.71 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 22:50:12.607633: step 14310, loss = 1.67 (25.5 examples/sec; 1.179 sec/batch)\n",
      "2019-05-20 22:50:24.607794: step 14320, loss = 1.68 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-20 22:50:36.653707: step 14330, loss = 1.65 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 22:50:48.726487: step 14340, loss = 1.77 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 22:51:00.746892: step 14350, loss = 1.71 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-20 22:51:12.790547: step 14360, loss = 1.69 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 22:51:24.899108: step 14370, loss = 1.72 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-20 22:51:36.923124: step 14380, loss = 1.65 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-20 22:51:49.031375: step 14390, loss = 1.61 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 22:52:01.113782: step 14400, loss = 1.64 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 22:52:15.049346: step 14410, loss = 1.69 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-20 22:52:27.055223: step 14420, loss = 1.65 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 22:52:39.031254: step 14430, loss = 1.71 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 22:52:51.089194: step 14440, loss = 1.76 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 22:53:03.108036: step 14450, loss = 1.64 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-20 22:53:15.156997: step 14460, loss = 1.60 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 22:53:27.202170: step 14470, loss = 1.65 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 22:53:39.263709: step 14480, loss = 1.65 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 22:53:51.347575: step 14490, loss = 1.65 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 22:54:03.379581: step 14500, loss = 1.60 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 22:54:17.235473: step 14510, loss = 1.63 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-20 22:54:29.291741: step 14520, loss = 1.66 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 22:54:41.410144: step 14530, loss = 1.73 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 22:54:53.511803: step 14540, loss = 1.66 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-20 22:55:05.655550: step 14550, loss = 1.68 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 22:55:17.771738: step 14560, loss = 1.70 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 22:55:29.859148: step 14570, loss = 1.59 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-20 22:55:41.922272: step 14580, loss = 1.62 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 22:55:53.951478: step 14590, loss = 1.65 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 22:56:06.009543: step 14600, loss = 1.72 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 22:56:19.878279: step 14610, loss = 1.68 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 22:56:31.906461: step 14620, loss = 1.65 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 22:56:43.979491: step 14630, loss = 1.62 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 22:56:56.056616: step 14640, loss = 1.65 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 22:57:08.192027: step 14650, loss = 1.64 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 22:57:20.245098: step 14660, loss = 1.67 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 22:57:32.371444: step 14670, loss = 1.64 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 22:57:44.308589: step 14680, loss = 1.67 (25.5 examples/sec; 1.176 sec/batch)\n",
      "2019-05-20 22:57:56.462604: step 14690, loss = 1.65 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 22:58:08.573992: step 14700, loss = 1.62 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 22:58:22.564161: step 14710, loss = 1.70 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 22:58:34.680040: step 14720, loss = 1.62 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 22:58:46.708904: step 14730, loss = 1.67 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-20 22:58:58.748425: step 14740, loss = 1.60 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-20 22:59:10.824270: step 14750, loss = 1.69 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-20 22:59:22.885289: step 14760, loss = 1.67 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 22:59:34.899545: step 14770, loss = 1.68 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 22:59:46.891127: step 14780, loss = 1.66 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-20 22:59:59.001371: step 14790, loss = 1.65 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 23:00:10.990989: step 14800, loss = 1.65 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-20 23:00:25.090403: step 14810, loss = 1.62 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-20 23:00:37.095844: step 14820, loss = 1.67 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 23:00:49.161293: step 14830, loss = 1.71 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 23:01:01.240949: step 14840, loss = 1.64 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 23:01:13.344890: step 14850, loss = 1.63 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 23:01:25.481928: step 14860, loss = 1.61 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 23:01:37.588632: step 14870, loss = 1.73 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 23:01:49.653308: step 14880, loss = 1.60 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-20 23:02:01.802639: step 14890, loss = 1.61 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 23:02:13.912075: step 14900, loss = 1.66 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 23:02:27.937296: step 14910, loss = 1.63 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 23:02:40.004808: step 14920, loss = 1.65 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 23:02:52.085663: step 14930, loss = 1.63 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-20 23:03:04.158711: step 14940, loss = 1.63 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 23:03:16.227065: step 14950, loss = 1.60 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 23:03:28.283179: step 14960, loss = 1.68 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-20 23:03:40.371633: step 14970, loss = 1.62 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-20 23:03:52.487609: step 14980, loss = 1.67 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 23:04:04.592548: step 14990, loss = 1.69 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 23:04:16.696676: step 15000, loss = 1.63 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-20 23:04:34.168908: step 15010, loss = 1.63 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 23:04:46.263167: step 15020, loss = 1.63 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 23:04:58.325769: step 15030, loss = 1.64 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 23:05:10.461952: step 15040, loss = 1.64 (24.5 examples/sec; 1.225 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 23:05:22.544797: step 15050, loss = 1.59 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 23:05:34.682189: step 15060, loss = 1.66 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-20 23:05:46.823356: step 15070, loss = 1.69 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 23:05:58.966390: step 15080, loss = 1.55 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 23:06:11.048866: step 15090, loss = 1.63 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 23:06:23.181134: step 15100, loss = 1.65 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-20 23:06:37.006995: step 15110, loss = 1.62 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 23:06:49.048590: step 15120, loss = 1.60 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 23:07:01.206299: step 15130, loss = 1.64 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 23:07:13.335756: step 15140, loss = 1.70 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 23:07:25.459101: step 15150, loss = 1.55 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 23:07:37.463908: step 15160, loss = 1.63 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 23:07:49.591600: step 15170, loss = 1.61 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 23:08:01.591327: step 15180, loss = 1.60 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-20 23:08:13.661715: step 15190, loss = 1.63 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 23:08:25.808485: step 15200, loss = 1.61 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 23:08:39.771529: step 15210, loss = 1.62 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 23:08:51.970928: step 15220, loss = 1.63 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 23:09:04.135826: step 15230, loss = 1.56 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 23:09:16.308341: step 15240, loss = 1.64 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 23:09:28.466329: step 15250, loss = 1.70 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 23:09:40.576235: step 15260, loss = 1.55 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 23:09:52.728467: step 15270, loss = 1.64 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 23:10:04.823737: step 15280, loss = 1.57 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 23:10:16.946236: step 15290, loss = 1.60 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 23:10:29.051950: step 15300, loss = 1.66 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 23:10:43.150313: step 15310, loss = 1.62 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-20 23:10:55.249081: step 15320, loss = 1.67 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 23:11:07.378274: step 15330, loss = 1.59 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 23:11:19.525511: step 15340, loss = 1.61 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 23:11:31.647157: step 15350, loss = 1.61 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 23:11:43.770747: step 15360, loss = 1.63 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 23:11:55.873372: step 15370, loss = 1.67 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-20 23:12:07.981221: step 15380, loss = 1.58 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 23:12:20.079526: step 15390, loss = 1.60 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 23:12:32.146029: step 15400, loss = 1.60 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 23:12:46.173639: step 15410, loss = 1.62 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 23:12:58.273166: step 15420, loss = 1.57 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 23:13:10.318191: step 15430, loss = 1.59 (25.5 examples/sec; 1.174 sec/batch)\n",
      "2019-05-20 23:13:22.324563: step 15440, loss = 1.61 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-20 23:13:34.512571: step 15450, loss = 1.59 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-20 23:13:46.687788: step 15460, loss = 1.54 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 23:13:58.886539: step 15470, loss = 1.64 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-20 23:14:11.082485: step 15480, loss = 1.58 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 23:14:23.240568: step 15490, loss = 1.61 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 23:14:35.430894: step 15500, loss = 1.65 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 23:14:49.682950: step 15510, loss = 1.60 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 23:15:01.774845: step 15520, loss = 1.59 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 23:15:13.888390: step 15530, loss = 1.60 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-20 23:15:26.034014: step 15540, loss = 1.62 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 23:15:38.204716: step 15550, loss = 1.54 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 23:15:50.372988: step 15560, loss = 1.61 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 23:16:02.448592: step 15570, loss = 1.64 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 23:16:14.560668: step 15580, loss = 1.58 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 23:16:26.762323: step 15590, loss = 1.60 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-20 23:16:38.883225: step 15600, loss = 1.62 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 23:16:52.829410: step 15610, loss = 1.67 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 23:17:04.933964: step 15620, loss = 1.61 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 23:17:17.023174: step 15630, loss = 1.60 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 23:17:29.202777: step 15640, loss = 1.69 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-20 23:17:41.216606: step 15650, loss = 1.60 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-20 23:17:53.328133: step 15660, loss = 1.56 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 23:18:05.460785: step 15670, loss = 1.55 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 23:18:17.638617: step 15680, loss = 1.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 23:18:29.629311: step 15690, loss = 1.63 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 23:18:41.860123: step 15700, loss = 1.55 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-05-20 23:18:55.966311: step 15710, loss = 1.57 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 23:19:08.058814: step 15720, loss = 1.55 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 23:19:20.170132: step 15730, loss = 1.60 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 23:19:32.265242: step 15740, loss = 1.55 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 23:19:44.382072: step 15750, loss = 1.57 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-20 23:19:56.535816: step 15760, loss = 1.60 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 23:20:08.649750: step 15770, loss = 1.60 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-20 23:20:20.800822: step 15780, loss = 1.60 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 23:20:32.909719: step 15790, loss = 1.65 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 23:20:45.039989: step 15800, loss = 1.60 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 23:20:59.002008: step 15810, loss = 1.57 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 23:21:11.070119: step 15820, loss = 1.56 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 23:21:23.317756: step 15830, loss = 1.56 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 23:21:35.429509: step 15840, loss = 1.55 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 23:21:47.616591: step 15850, loss = 1.50 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 23:21:59.742959: step 15860, loss = 1.56 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 23:22:11.842071: step 15870, loss = 1.59 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 23:22:23.999375: step 15880, loss = 1.62 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-20 23:22:36.139978: step 15890, loss = 1.55 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 23:22:48.196136: step 15900, loss = 1.57 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 23:23:02.190142: step 15910, loss = 1.57 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 23:23:14.329633: step 15920, loss = 1.51 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 23:23:26.490432: step 15930, loss = 1.54 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 23:23:38.426263: step 15940, loss = 1.54 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-20 23:23:50.640405: step 15950, loss = 1.56 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 23:24:02.766129: step 15960, loss = 1.58 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 23:24:15.062644: step 15970, loss = 1.53 (24.9 examples/sec; 1.206 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 23:24:27.177675: step 15980, loss = 1.59 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 23:24:39.341376: step 15990, loss = 1.58 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 23:24:51.519954: step 16000, loss = 1.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 23:25:05.420701: step 16010, loss = 1.48 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 23:25:17.535696: step 16020, loss = 1.53 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 23:25:29.734872: step 16030, loss = 1.55 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 23:25:41.853681: step 16040, loss = 1.53 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 23:25:53.960709: step 16050, loss = 1.56 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 23:26:06.089258: step 16060, loss = 1.53 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 23:26:18.236521: step 16070, loss = 1.55 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-20 23:26:30.352101: step 16080, loss = 1.51 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 23:26:42.513103: step 16090, loss = 1.65 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 23:26:54.593262: step 16100, loss = 1.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 23:27:08.675947: step 16110, loss = 1.53 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 23:27:20.810568: step 16120, loss = 1.53 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 23:27:32.914367: step 16130, loss = 1.58 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 23:27:45.035618: step 16140, loss = 1.53 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-20 23:27:57.093848: step 16150, loss = 1.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-20 23:28:09.248920: step 16160, loss = 1.57 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 23:28:21.407113: step 16170, loss = 1.56 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 23:28:33.490915: step 16180, loss = 1.55 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 23:28:45.497513: step 16190, loss = 1.50 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-20 23:28:57.564588: step 16200, loss = 1.55 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 23:29:11.999066: step 16210, loss = 1.53 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 23:29:24.075137: step 16220, loss = 1.46 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 23:29:36.178031: step 16230, loss = 1.56 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 23:29:48.377717: step 16240, loss = 1.50 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 23:30:00.597906: step 16250, loss = 1.62 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 23:30:12.732309: step 16260, loss = 1.58 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 23:30:24.939843: step 16270, loss = 1.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 23:30:37.055465: step 16280, loss = 1.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 23:30:49.173848: step 16290, loss = 1.51 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-20 23:31:01.299935: step 16300, loss = 1.53 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 23:31:15.366249: step 16310, loss = 1.53 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 23:31:27.498125: step 16320, loss = 1.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 23:31:39.634328: step 16330, loss = 1.54 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 23:31:51.932248: step 16340, loss = 1.53 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 23:32:04.061628: step 16350, loss = 1.57 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 23:32:16.206000: step 16360, loss = 1.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 23:32:28.317773: step 16370, loss = 1.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 23:32:40.429502: step 16380, loss = 1.55 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-20 23:32:52.546399: step 16390, loss = 1.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 23:33:04.571003: step 16400, loss = 1.51 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 23:33:18.983297: step 16410, loss = 1.48 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-20 23:33:31.092508: step 16420, loss = 1.56 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 23:33:43.184850: step 16430, loss = 1.54 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-20 23:33:55.215358: step 16440, loss = 1.51 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 23:34:07.310943: step 16450, loss = 1.54 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-20 23:34:19.436850: step 16460, loss = 1.50 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 23:34:31.547705: step 16470, loss = 1.57 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-20 23:34:43.672872: step 16480, loss = 1.57 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 23:34:55.799990: step 16490, loss = 1.53 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 23:35:07.957117: step 16500, loss = 1.49 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 23:35:21.762701: step 16510, loss = 1.51 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-20 23:35:33.810979: step 16520, loss = 1.55 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-20 23:35:45.935021: step 16530, loss = 1.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 23:35:58.019016: step 16540, loss = 1.55 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 23:36:10.154247: step 16550, loss = 1.52 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-20 23:36:22.279190: step 16560, loss = 1.51 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 23:36:34.429201: step 16570, loss = 1.52 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-20 23:36:46.545028: step 16580, loss = 1.54 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 23:36:58.675201: step 16590, loss = 1.54 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 23:37:10.826612: step 16600, loss = 1.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 23:37:25.009758: step 16610, loss = 1.51 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-20 23:37:37.163240: step 16620, loss = 1.43 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 23:37:49.322712: step 16630, loss = 1.51 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 23:38:01.432033: step 16640, loss = 1.50 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-20 23:38:13.506307: step 16650, loss = 1.53 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-20 23:38:25.647516: step 16660, loss = 1.51 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 23:38:37.824944: step 16670, loss = 1.53 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 23:38:49.930668: step 16680, loss = 1.52 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 23:39:02.048931: step 16690, loss = 1.52 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 23:39:13.995671: step 16700, loss = 1.56 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 23:39:28.153465: step 16710, loss = 1.54 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 23:39:40.300345: step 16720, loss = 1.47 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 23:39:52.375890: step 16730, loss = 1.50 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 23:40:04.499389: step 16740, loss = 1.50 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 23:40:16.639792: step 16750, loss = 1.52 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 23:40:28.769215: step 16760, loss = 1.51 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 23:40:40.891129: step 16770, loss = 1.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 23:40:52.963496: step 16780, loss = 1.42 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 23:41:05.198379: step 16790, loss = 1.53 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 23:41:17.534694: step 16800, loss = 1.46 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-20 23:41:31.397331: step 16810, loss = 1.42 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 23:41:43.494329: step 16820, loss = 1.46 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-20 23:41:55.665328: step 16830, loss = 1.51 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-20 23:42:07.875214: step 16840, loss = 1.48 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 23:42:20.046211: step 16850, loss = 1.50 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 23:42:32.160321: step 16860, loss = 1.53 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 23:42:44.243765: step 16870, loss = 1.47 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 23:42:56.394775: step 16880, loss = 1.45 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-20 23:43:08.423491: step 16890, loss = 1.47 (25.4 examples/sec; 1.179 sec/batch)\n",
      "2019-05-20 23:43:20.643273: step 16900, loss = 1.57 (24.2 examples/sec; 1.238 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 23:43:34.775547: step 16910, loss = 1.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 23:43:46.829276: step 16920, loss = 1.47 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 23:43:58.885181: step 16930, loss = 1.47 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 23:44:11.106807: step 16940, loss = 1.45 (25.6 examples/sec; 1.174 sec/batch)\n",
      "2019-05-20 23:44:23.046529: step 16950, loss = 1.49 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 23:44:35.267292: step 16960, loss = 1.47 (22.4 examples/sec; 1.342 sec/batch)\n",
      "2019-05-20 23:44:47.272479: step 16970, loss = 1.48 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 23:44:59.338318: step 16980, loss = 1.52 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 23:45:11.366197: step 16990, loss = 1.43 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-20 23:45:23.456080: step 17000, loss = 1.49 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 23:45:37.649014: step 17010, loss = 1.48 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-20 23:45:49.753106: step 17020, loss = 1.55 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-20 23:46:01.836886: step 17030, loss = 1.44 (25.5 examples/sec; 1.176 sec/batch)\n",
      "2019-05-20 23:46:13.912430: step 17040, loss = 1.49 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-20 23:46:25.997263: step 17050, loss = 1.42 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 23:46:38.015396: step 17060, loss = 1.49 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-20 23:46:50.087275: step 17070, loss = 1.44 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 23:47:02.233271: step 17080, loss = 1.45 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 23:47:14.308410: step 17090, loss = 1.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-20 23:47:26.349266: step 17100, loss = 1.44 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-20 23:47:40.301835: step 17110, loss = 1.45 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-20 23:47:52.353018: step 17120, loss = 1.38 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 23:48:04.525886: step 17130, loss = 1.49 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 23:48:16.671341: step 17140, loss = 1.43 (23.2 examples/sec; 1.294 sec/batch)\n",
      "2019-05-20 23:48:28.756775: step 17150, loss = 1.41 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 23:48:40.818847: step 17160, loss = 1.47 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 23:48:52.960489: step 17170, loss = 1.44 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-20 23:49:05.113736: step 17180, loss = 1.47 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 23:49:17.182576: step 17190, loss = 1.53 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 23:49:29.272081: step 17200, loss = 1.50 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-20 23:49:43.539694: step 17210, loss = 1.47 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-20 23:49:55.640379: step 17220, loss = 1.45 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-20 23:50:07.726865: step 17230, loss = 1.51 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 23:50:19.845158: step 17240, loss = 1.42 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 23:50:31.943627: step 17250, loss = 1.41 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 23:50:43.956637: step 17260, loss = 1.41 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-20 23:50:56.031477: step 17270, loss = 1.48 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-20 23:51:08.102179: step 17280, loss = 1.45 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 23:51:20.210075: step 17290, loss = 1.48 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 23:51:32.323858: step 17300, loss = 1.47 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 23:51:46.075992: step 17310, loss = 1.51 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-20 23:51:58.096408: step 17320, loss = 1.44 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-20 23:52:10.165724: step 17330, loss = 1.43 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 23:52:22.255120: step 17340, loss = 1.46 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 23:52:34.372757: step 17350, loss = 1.44 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 23:52:46.418463: step 17360, loss = 1.40 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 23:52:58.568204: step 17370, loss = 1.42 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 23:53:10.615129: step 17380, loss = 1.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-20 23:53:22.635782: step 17390, loss = 1.49 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-20 23:53:34.761099: step 17400, loss = 1.42 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 23:53:48.983666: step 17410, loss = 1.48 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 23:54:01.020948: step 17420, loss = 1.47 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-20 23:54:13.123468: step 17430, loss = 1.44 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 23:54:25.252641: step 17440, loss = 1.43 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-20 23:54:37.252255: step 17450, loss = 1.42 (25.5 examples/sec; 1.178 sec/batch)\n",
      "2019-05-20 23:54:49.272445: step 17460, loss = 1.45 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 23:55:01.367750: step 17470, loss = 1.44 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-20 23:55:13.419393: step 17480, loss = 1.47 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 23:55:25.525699: step 17490, loss = 1.48 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 23:55:37.662179: step 17500, loss = 1.42 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 23:55:51.624370: step 17510, loss = 1.43 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-20 23:56:03.607289: step 17520, loss = 1.41 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-20 23:56:15.608236: step 17530, loss = 1.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-20 23:56:27.592520: step 17540, loss = 1.46 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-20 23:56:39.620349: step 17550, loss = 1.44 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-20 23:56:51.658268: step 17560, loss = 1.45 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-20 23:57:03.658262: step 17570, loss = 1.44 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-20 23:57:15.803644: step 17580, loss = 1.46 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 23:57:27.867853: step 17590, loss = 1.48 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-20 23:57:39.928155: step 17600, loss = 1.43 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 23:57:54.149994: step 17610, loss = 1.39 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-20 23:58:06.226530: step 17620, loss = 1.40 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-20 23:58:18.348513: step 17630, loss = 1.43 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-20 23:58:30.428084: step 17640, loss = 1.48 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 23:58:42.530508: step 17650, loss = 1.44 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-20 23:58:54.740511: step 17660, loss = 1.48 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-20 23:59:06.825561: step 17670, loss = 1.47 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-20 23:59:18.972361: step 17680, loss = 1.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-20 23:59:31.055608: step 17690, loss = 1.48 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-20 23:59:43.097090: step 17700, loss = 1.44 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-20 23:59:57.252006: step 17710, loss = 1.47 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 00:00:09.275871: step 17720, loss = 1.44 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 00:00:21.422519: step 17730, loss = 1.44 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 00:00:33.623629: step 17740, loss = 1.40 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 00:00:45.798327: step 17750, loss = 1.41 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 00:00:57.916818: step 17760, loss = 1.43 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 00:01:09.994638: step 17770, loss = 1.45 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 00:01:22.168832: step 17780, loss = 1.40 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 00:01:34.284138: step 17790, loss = 1.47 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 00:01:46.409255: step 17800, loss = 1.43 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 00:02:00.174928: step 17810, loss = 1.37 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 00:02:12.285922: step 17820, loss = 1.54 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 00:02:24.325332: step 17830, loss = 1.41 (24.7 examples/sec; 1.213 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 00:02:36.421607: step 17840, loss = 1.39 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 00:02:48.615291: step 17850, loss = 1.37 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 00:03:00.737448: step 17860, loss = 1.44 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 00:03:12.760987: step 17870, loss = 1.39 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 00:03:24.886514: step 17880, loss = 1.47 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 00:03:36.922649: step 17890, loss = 1.40 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 00:03:49.046811: step 17900, loss = 1.39 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 00:04:03.366333: step 17910, loss = 1.46 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 00:04:15.462442: step 17920, loss = 1.37 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 00:04:27.565661: step 17930, loss = 1.36 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 00:04:39.621153: step 17940, loss = 1.48 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-21 00:04:51.659936: step 17950, loss = 1.39 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 00:05:03.622510: step 17960, loss = 1.42 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 00:05:15.737946: step 17970, loss = 1.45 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 00:05:27.887291: step 17980, loss = 1.40 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 00:05:40.035912: step 17990, loss = 1.41 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 00:05:52.129632: step 18000, loss = 1.45 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 00:06:06.271654: step 18010, loss = 1.39 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 00:06:18.323654: step 18020, loss = 1.37 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 00:06:30.459928: step 18030, loss = 1.42 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 00:06:42.607285: step 18040, loss = 1.35 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 00:06:54.803192: step 18050, loss = 1.41 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 00:07:06.920539: step 18060, loss = 1.35 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 00:07:19.081577: step 18070, loss = 1.36 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 00:07:31.228436: step 18080, loss = 1.44 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 00:07:43.385313: step 18090, loss = 1.44 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 00:07:55.510632: step 18100, loss = 1.41 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 00:08:09.677709: step 18110, loss = 1.33 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 00:08:21.781936: step 18120, loss = 1.34 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 00:08:33.812753: step 18130, loss = 1.40 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 00:08:45.874820: step 18140, loss = 1.34 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 00:08:57.955977: step 18150, loss = 1.35 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 00:09:10.020117: step 18160, loss = 1.39 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 00:09:22.131928: step 18170, loss = 1.34 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 00:09:34.297171: step 18180, loss = 1.39 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 00:09:46.480091: step 18190, loss = 1.36 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 00:09:58.541635: step 18200, loss = 1.33 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 00:10:12.185578: step 18210, loss = 1.42 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-21 00:10:24.220281: step 18220, loss = 1.47 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 00:10:36.260499: step 18230, loss = 1.37 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 00:10:48.356605: step 18240, loss = 1.41 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 00:11:00.471395: step 18250, loss = 1.37 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 00:11:12.574863: step 18260, loss = 1.41 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 00:11:24.716765: step 18270, loss = 1.47 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 00:11:36.806134: step 18280, loss = 1.40 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 00:11:48.953758: step 18290, loss = 1.40 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 00:12:01.070178: step 18300, loss = 1.38 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 00:12:14.999987: step 18310, loss = 1.34 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 00:12:27.174880: step 18320, loss = 1.44 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 00:12:39.315544: step 18330, loss = 1.31 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 00:12:51.398409: step 18340, loss = 1.42 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 00:13:03.602350: step 18350, loss = 1.38 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 00:13:15.756176: step 18360, loss = 1.40 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 00:13:27.887861: step 18370, loss = 1.37 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 00:13:39.997555: step 18380, loss = 1.44 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 00:13:52.065294: step 18390, loss = 1.40 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 00:14:04.241166: step 18400, loss = 1.45 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 00:14:18.138329: step 18410, loss = 1.47 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 00:14:30.275595: step 18420, loss = 1.39 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 00:14:42.394927: step 18430, loss = 1.36 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 00:14:54.509307: step 18440, loss = 1.45 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 00:15:06.644536: step 18450, loss = 1.40 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 00:15:18.738321: step 18460, loss = 1.34 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 00:15:30.786646: step 18470, loss = 1.38 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 00:15:42.938112: step 18480, loss = 1.34 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 00:15:55.069308: step 18490, loss = 1.35 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 00:16:07.230474: step 18500, loss = 1.36 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 00:16:21.278457: step 18510, loss = 1.48 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 00:16:33.388182: step 18520, loss = 1.36 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 00:16:45.601556: step 18530, loss = 1.32 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 00:16:57.744179: step 18540, loss = 1.43 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 00:17:09.846505: step 18550, loss = 1.32 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 00:17:21.958911: step 18560, loss = 1.38 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 00:17:34.083789: step 18570, loss = 1.39 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 00:17:46.265445: step 18580, loss = 1.32 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 00:17:58.410043: step 18590, loss = 1.39 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 00:18:10.577610: step 18600, loss = 1.37 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 00:18:24.797825: step 18610, loss = 1.35 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 00:18:36.939459: step 18620, loss = 1.39 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 00:18:49.007259: step 18630, loss = 1.40 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 00:19:01.107781: step 18640, loss = 1.34 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 00:19:13.332182: step 18650, loss = 1.32 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 00:19:25.500247: step 18660, loss = 1.37 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 00:19:37.650020: step 18670, loss = 1.37 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 00:19:49.766526: step 18680, loss = 1.34 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 00:20:01.869708: step 18690, loss = 1.36 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 00:20:13.971757: step 18700, loss = 1.36 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 00:20:27.784692: step 18710, loss = 1.34 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 00:20:39.797300: step 18720, loss = 1.33 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 00:20:51.983190: step 18730, loss = 1.36 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 00:21:04.089107: step 18740, loss = 1.34 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 00:21:16.203496: step 18750, loss = 1.38 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 00:21:28.289120: step 18760, loss = 1.30 (24.8 examples/sec; 1.208 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 00:21:40.375333: step 18770, loss = 1.31 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 00:21:52.537042: step 18780, loss = 1.35 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 00:22:04.712626: step 18790, loss = 1.35 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 00:22:16.850898: step 18800, loss = 1.35 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 00:22:31.205010: step 18810, loss = 1.37 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 00:22:43.289659: step 18820, loss = 1.30 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 00:22:55.371491: step 18830, loss = 1.31 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 00:23:07.454318: step 18840, loss = 1.31 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 00:23:19.622265: step 18850, loss = 1.39 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 00:23:31.747502: step 18860, loss = 1.37 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 00:23:43.867431: step 18870, loss = 1.39 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 00:23:55.891537: step 18880, loss = 1.35 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 00:24:08.082809: step 18890, loss = 1.31 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-05-21 00:24:20.199307: step 18900, loss = 1.29 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 00:24:34.333504: step 18910, loss = 1.35 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 00:24:46.385075: step 18920, loss = 1.35 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 00:24:58.492689: step 18930, loss = 1.33 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 00:25:10.586960: step 18940, loss = 1.40 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 00:25:22.702286: step 18950, loss = 1.39 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 00:25:34.843763: step 18960, loss = 1.41 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 00:25:46.788806: step 18970, loss = 1.38 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 00:25:58.879868: step 18980, loss = 1.36 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 00:26:11.091608: step 18990, loss = 1.40 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 00:26:23.265928: step 19000, loss = 1.39 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 00:26:37.525348: step 19010, loss = 1.35 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 00:26:49.698439: step 19020, loss = 1.35 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 00:27:01.843900: step 19030, loss = 1.32 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 00:27:13.938345: step 19040, loss = 1.32 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 00:27:26.039707: step 19050, loss = 1.35 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 00:27:38.130877: step 19060, loss = 1.30 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 00:27:50.288296: step 19070, loss = 1.35 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 00:28:02.418456: step 19080, loss = 1.37 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 00:28:14.500140: step 19090, loss = 1.30 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 00:28:26.726064: step 19100, loss = 1.34 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 00:28:40.636947: step 19110, loss = 1.34 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 00:28:52.732027: step 19120, loss = 1.28 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 00:29:04.762151: step 19130, loss = 1.33 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 00:29:16.951117: step 19140, loss = 1.42 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 00:29:29.076812: step 19150, loss = 1.32 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 00:29:41.203140: step 19160, loss = 1.33 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 00:29:53.274847: step 19170, loss = 1.29 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 00:30:05.510591: step 19180, loss = 1.32 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 00:30:17.715088: step 19190, loss = 1.28 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 00:30:29.827690: step 19200, loss = 1.40 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 00:30:43.706197: step 19210, loss = 1.36 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 00:30:55.646336: step 19220, loss = 1.32 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 00:31:07.841237: step 19230, loss = 1.29 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 00:31:19.975958: step 19240, loss = 1.37 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 00:31:32.106004: step 19250, loss = 1.27 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 00:31:44.331194: step 19260, loss = 1.33 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 00:31:56.466179: step 19270, loss = 1.34 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 00:32:08.569595: step 19280, loss = 1.32 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 00:32:20.702092: step 19290, loss = 1.42 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 00:32:32.818371: step 19300, loss = 1.33 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 00:32:46.929515: step 19310, loss = 1.38 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 00:32:59.077408: step 19320, loss = 1.35 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 00:33:11.170082: step 19330, loss = 1.34 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 00:33:23.364641: step 19340, loss = 1.35 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 00:33:35.466768: step 19350, loss = 1.27 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 00:33:47.667512: step 19360, loss = 1.30 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-05-21 00:33:59.795962: step 19370, loss = 1.39 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 00:34:11.852264: step 19380, loss = 1.36 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 00:34:24.014105: step 19390, loss = 1.36 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 00:34:36.159437: step 19400, loss = 1.37 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 00:34:50.137721: step 19410, loss = 1.31 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 00:35:02.278793: step 19420, loss = 1.32 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 00:35:14.438545: step 19430, loss = 1.24 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 00:35:26.578608: step 19440, loss = 1.26 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 00:35:38.659559: step 19450, loss = 1.27 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 00:35:50.820008: step 19460, loss = 1.26 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 00:36:02.799589: step 19470, loss = 1.31 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-21 00:36:15.018807: step 19480, loss = 1.37 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 00:36:27.212330: step 19490, loss = 1.37 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 00:36:39.312589: step 19500, loss = 1.29 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 00:36:53.324830: step 19510, loss = 1.30 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 00:37:05.408845: step 19520, loss = 1.27 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 00:37:17.538590: step 19530, loss = 1.31 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 00:37:29.750288: step 19540, loss = 1.26 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 00:37:41.894019: step 19550, loss = 1.31 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 00:37:54.047814: step 19560, loss = 1.31 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 00:38:06.321959: step 19570, loss = 1.27 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 00:38:18.503295: step 19580, loss = 1.31 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 00:38:30.585897: step 19590, loss = 1.23 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 00:38:42.750872: step 19600, loss = 1.26 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 00:38:56.654175: step 19610, loss = 1.30 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 00:39:08.705336: step 19620, loss = 1.31 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 00:39:20.761842: step 19630, loss = 1.27 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 00:39:32.859962: step 19640, loss = 1.32 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 00:39:44.989878: step 19650, loss = 1.37 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 00:39:57.114878: step 19660, loss = 1.27 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 00:40:09.201559: step 19670, loss = 1.37 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 00:40:21.302386: step 19680, loss = 1.35 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 00:40:33.398014: step 19690, loss = 1.29 (24.7 examples/sec; 1.214 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 00:40:45.501076: step 19700, loss = 1.28 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 00:40:59.318096: step 19710, loss = 1.37 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 00:41:11.364754: step 19720, loss = 1.32 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 00:41:23.427792: step 19730, loss = 1.29 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 00:41:35.641184: step 19740, loss = 1.27 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 00:41:47.773886: step 19750, loss = 1.32 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 00:41:59.910820: step 19760, loss = 1.27 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 00:42:12.033382: step 19770, loss = 1.32 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 00:42:24.238267: step 19780, loss = 1.32 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 00:42:36.355221: step 19790, loss = 1.25 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 00:42:48.449255: step 19800, loss = 1.29 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 00:43:02.538336: step 19810, loss = 1.34 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 00:43:14.666905: step 19820, loss = 1.21 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 00:43:26.798898: step 19830, loss = 1.38 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 00:43:38.933868: step 19840, loss = 1.32 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 00:43:51.026208: step 19850, loss = 1.30 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 00:44:03.188290: step 19860, loss = 1.32 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 00:44:15.221542: step 19870, loss = 1.27 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 00:44:27.306631: step 19880, loss = 1.30 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 00:44:39.466311: step 19890, loss = 1.29 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 00:44:51.606992: step 19900, loss = 1.31 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 00:45:05.395020: step 19910, loss = 1.27 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 00:45:17.480412: step 19920, loss = 1.22 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 00:45:29.662471: step 19930, loss = 1.28 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 00:45:41.821311: step 19940, loss = 1.31 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 00:45:53.896847: step 19950, loss = 1.29 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 00:46:06.007492: step 19960, loss = 1.34 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 00:46:18.104621: step 19970, loss = 1.27 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 00:46:30.121102: step 19980, loss = 1.29 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 00:46:42.276665: step 19990, loss = 1.38 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 00:46:54.441054: step 20000, loss = 1.27 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 00:47:12.242592: step 20010, loss = 1.23 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 00:47:24.396161: step 20020, loss = 1.33 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 00:47:36.481261: step 20030, loss = 1.23 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 00:47:48.611769: step 20040, loss = 1.36 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 00:48:00.744492: step 20050, loss = 1.29 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 00:48:13.001761: step 20060, loss = 1.33 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 00:48:25.244318: step 20070, loss = 1.33 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 00:48:37.367388: step 20080, loss = 1.32 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 00:48:49.532785: step 20090, loss = 1.30 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 00:49:01.671382: step 20100, loss = 1.33 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 00:49:15.951705: step 20110, loss = 1.27 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 00:49:27.996002: step 20120, loss = 1.28 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 00:49:40.128655: step 20130, loss = 1.25 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 00:49:52.334324: step 20140, loss = 1.29 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 00:50:04.468154: step 20150, loss = 1.35 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 00:50:16.559801: step 20160, loss = 1.28 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 00:50:28.686188: step 20170, loss = 1.26 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 00:50:40.897438: step 20180, loss = 1.30 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 00:50:53.016457: step 20190, loss = 1.32 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 00:51:05.133612: step 20200, loss = 1.29 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 00:51:19.471603: step 20210, loss = 1.30 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 00:51:31.502355: step 20220, loss = 1.28 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 00:51:43.504058: step 20230, loss = 1.23 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 00:51:55.697975: step 20240, loss = 1.27 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 00:52:07.831404: step 20250, loss = 1.25 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 00:52:19.953765: step 20260, loss = 1.30 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 00:52:32.090980: step 20270, loss = 1.26 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 00:52:44.213707: step 20280, loss = 1.27 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 00:52:56.298981: step 20290, loss = 1.19 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 00:53:08.575245: step 20300, loss = 1.33 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 00:53:22.809618: step 20310, loss = 1.26 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 00:53:34.918773: step 20320, loss = 1.37 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 00:53:47.011107: step 20330, loss = 1.31 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 00:53:59.205449: step 20340, loss = 1.32 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 00:54:11.373436: step 20350, loss = 1.26 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 00:54:23.515164: step 20360, loss = 1.24 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 00:54:35.523037: step 20370, loss = 1.27 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 00:54:47.627980: step 20380, loss = 1.26 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 00:54:59.775868: step 20390, loss = 1.31 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 00:55:11.901156: step 20400, loss = 1.22 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 00:55:25.869329: step 20410, loss = 1.24 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 00:55:38.003542: step 20420, loss = 1.26 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 00:55:50.166288: step 20430, loss = 1.26 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 00:56:02.245887: step 20440, loss = 1.28 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 00:56:14.504928: step 20450, loss = 1.29 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 00:56:26.636570: step 20460, loss = 1.22 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 00:56:38.730243: step 20470, loss = 1.29 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 00:56:50.687998: step 20480, loss = 1.27 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 00:57:02.802051: step 20490, loss = 1.34 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 00:57:14.903669: step 20500, loss = 1.29 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 00:57:29.061147: step 20510, loss = 1.27 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 00:57:41.234841: step 20520, loss = 1.21 (23.1 examples/sec; 1.298 sec/batch)\n",
      "2019-05-21 00:57:53.336212: step 20530, loss = 1.26 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 00:58:05.451161: step 20540, loss = 1.21 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 00:58:17.554015: step 20550, loss = 1.23 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 00:58:29.668652: step 20560, loss = 1.23 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 00:58:41.808722: step 20570, loss = 1.20 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 00:58:54.007368: step 20580, loss = 1.29 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 00:59:06.115457: step 20590, loss = 1.25 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 00:59:18.276068: step 20600, loss = 1.25 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 00:59:32.436279: step 20610, loss = 1.27 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-21 00:59:44.557963: step 20620, loss = 1.18 (24.7 examples/sec; 1.214 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 00:59:56.682352: step 20630, loss = 1.24 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 01:00:08.854030: step 20640, loss = 1.25 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 01:00:20.935916: step 20650, loss = 1.28 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 01:00:33.035883: step 20660, loss = 1.24 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 01:00:45.183998: step 20670, loss = 1.22 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 01:00:57.324554: step 20680, loss = 1.19 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 01:01:09.404265: step 20690, loss = 1.23 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 01:01:21.547950: step 20700, loss = 1.31 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 01:01:35.685626: step 20710, loss = 1.28 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 01:01:47.760434: step 20720, loss = 1.31 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 01:01:59.728963: step 20730, loss = 1.24 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 01:02:11.837296: step 20740, loss = 1.30 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 01:02:23.944371: step 20750, loss = 1.27 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 01:02:36.055000: step 20760, loss = 1.21 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 01:02:48.135874: step 20770, loss = 1.22 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 01:03:00.259670: step 20780, loss = 1.26 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 01:03:12.405672: step 20790, loss = 1.28 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 01:03:24.634887: step 20800, loss = 1.26 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 01:03:38.509792: step 20810, loss = 1.22 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 01:03:50.746533: step 20820, loss = 1.25 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 01:04:02.924720: step 20830, loss = 1.31 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 01:04:15.049890: step 20840, loss = 1.19 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 01:04:27.215776: step 20850, loss = 1.22 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 01:04:39.301952: step 20860, loss = 1.22 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 01:04:51.414138: step 20870, loss = 1.29 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 01:05:03.491323: step 20880, loss = 1.20 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 01:05:15.683738: step 20890, loss = 1.22 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 01:05:27.882109: step 20900, loss = 1.22 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 01:05:41.749037: step 20910, loss = 1.25 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 01:05:53.833022: step 20920, loss = 1.23 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 01:06:05.948030: step 20930, loss = 1.18 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 01:06:18.049074: step 20940, loss = 1.23 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 01:06:30.157594: step 20950, loss = 1.23 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 01:06:42.286166: step 20960, loss = 1.24 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 01:06:54.394190: step 20970, loss = 1.18 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 01:07:06.410394: step 20980, loss = 1.19 (25.5 examples/sec; 1.176 sec/batch)\n",
      "2019-05-21 01:07:18.484018: step 20990, loss = 1.24 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 01:07:30.639394: step 21000, loss = 1.26 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 01:07:44.783020: step 21010, loss = 1.25 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 01:07:56.938799: step 21020, loss = 1.26 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 01:08:09.043399: step 21030, loss = 1.22 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 01:08:21.166223: step 21040, loss = 1.24 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 01:08:33.322144: step 21050, loss = 1.21 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 01:08:45.448581: step 21060, loss = 1.17 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 01:08:57.562581: step 21070, loss = 1.32 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 01:09:09.719543: step 21080, loss = 1.22 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 01:09:21.946505: step 21090, loss = 1.22 (22.9 examples/sec; 1.307 sec/batch)\n",
      "2019-05-21 01:09:34.084186: step 21100, loss = 1.16 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 01:09:48.018774: step 21110, loss = 1.31 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-05-21 01:10:00.171166: step 21120, loss = 1.18 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 01:10:12.313952: step 21130, loss = 1.16 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 01:10:24.427183: step 21140, loss = 1.21 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 01:10:36.535649: step 21150, loss = 1.24 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 01:10:48.625741: step 21160, loss = 1.20 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 01:11:00.776920: step 21170, loss = 1.24 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 01:11:12.900557: step 21180, loss = 1.25 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 01:11:25.023893: step 21190, loss = 1.22 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 01:11:37.108308: step 21200, loss = 1.30 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 01:11:51.044599: step 21210, loss = 1.24 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 01:12:03.135986: step 21220, loss = 1.21 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 01:12:15.236078: step 21230, loss = 1.24 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 01:12:27.321290: step 21240, loss = 1.23 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 01:12:39.548021: step 21250, loss = 1.23 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 01:12:51.621599: step 21260, loss = 1.22 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 01:13:03.910660: step 21270, loss = 1.16 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 01:13:16.116159: step 21280, loss = 1.19 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 01:13:28.192572: step 21290, loss = 1.25 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 01:13:40.308735: step 21300, loss = 1.25 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 01:13:54.159340: step 21310, loss = 1.21 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 01:14:06.243452: step 21320, loss = 1.22 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 01:14:18.411379: step 21330, loss = 1.18 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 01:14:30.550423: step 21340, loss = 1.20 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 01:14:42.684084: step 21350, loss = 1.27 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 01:14:54.791138: step 21360, loss = 1.19 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 01:15:06.921201: step 21370, loss = 1.21 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 01:15:19.089926: step 21380, loss = 1.17 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 01:15:31.233227: step 21390, loss = 1.22 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 01:15:43.381678: step 21400, loss = 1.20 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 01:15:57.672011: step 21410, loss = 1.26 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 01:16:09.722842: step 21420, loss = 1.20 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 01:16:21.812165: step 21430, loss = 1.22 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 01:16:33.893363: step 21440, loss = 1.22 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 01:16:45.962914: step 21450, loss = 1.21 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 01:16:58.038070: step 21460, loss = 1.29 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 01:17:10.078688: step 21470, loss = 1.24 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 01:17:22.069621: step 21480, loss = 1.25 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 01:17:34.075321: step 21490, loss = 1.27 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 01:17:46.045259: step 21500, loss = 1.15 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 01:17:59.984630: step 21510, loss = 1.20 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 01:18:12.000620: step 21520, loss = 1.19 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 01:18:24.097899: step 21530, loss = 1.27 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 01:18:36.104218: step 21540, loss = 1.16 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 01:18:48.178373: step 21550, loss = 1.25 (24.9 examples/sec; 1.203 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 01:19:00.299329: step 21560, loss = 1.27 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 01:19:12.368832: step 21570, loss = 1.23 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 01:19:24.392342: step 21580, loss = 1.20 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 01:19:36.443744: step 21590, loss = 1.24 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 01:19:48.533186: step 21600, loss = 1.21 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 01:20:02.285340: step 21610, loss = 1.20 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 01:20:14.384782: step 21620, loss = 1.23 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 01:20:26.517081: step 21630, loss = 1.26 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 01:20:38.599455: step 21640, loss = 1.22 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 01:20:50.697563: step 21650, loss = 1.17 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 01:21:02.822003: step 21660, loss = 1.20 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 01:21:14.893692: step 21670, loss = 1.24 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 01:21:27.026500: step 21680, loss = 1.17 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 01:21:39.221444: step 21690, loss = 1.26 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 01:21:51.426727: step 21700, loss = 1.21 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 01:22:05.718856: step 21710, loss = 1.20 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 01:22:17.804615: step 21720, loss = 1.16 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 01:22:29.909926: step 21730, loss = 1.23 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 01:22:41.901581: step 21740, loss = 1.16 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 01:22:54.039845: step 21750, loss = 1.23 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 01:23:06.088243: step 21760, loss = 1.16 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 01:23:18.194477: step 21770, loss = 1.21 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 01:23:30.310382: step 21780, loss = 1.20 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 01:23:42.475727: step 21790, loss = 1.11 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 01:23:54.645239: step 21800, loss = 1.22 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 01:24:08.500423: step 21810, loss = 1.21 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 01:24:20.604992: step 21820, loss = 1.15 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 01:24:32.717965: step 21830, loss = 1.25 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 01:24:44.836033: step 21840, loss = 1.29 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 01:24:56.974496: step 21850, loss = 1.21 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 01:25:08.991469: step 21860, loss = 1.14 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 01:25:21.088794: step 21870, loss = 1.15 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 01:25:33.212171: step 21880, loss = 1.14 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 01:25:45.355805: step 21890, loss = 1.15 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 01:25:57.489270: step 21900, loss = 1.17 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 01:26:11.654676: step 21910, loss = 1.21 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 01:26:23.808689: step 21920, loss = 1.18 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 01:26:35.939815: step 21930, loss = 1.15 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 01:26:48.099784: step 21940, loss = 1.17 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 01:27:00.306054: step 21950, loss = 1.25 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 01:27:12.388005: step 21960, loss = 1.22 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 01:27:24.511748: step 21970, loss = 1.20 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 01:27:36.634170: step 21980, loss = 1.18 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 01:27:48.697346: step 21990, loss = 1.19 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 01:28:00.821794: step 22000, loss = 1.19 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 01:28:14.639564: step 22010, loss = 1.20 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 01:28:26.846964: step 22020, loss = 1.23 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 01:28:38.993707: step 22030, loss = 1.19 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 01:28:51.116925: step 22040, loss = 1.20 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 01:29:03.357363: step 22050, loss = 1.18 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 01:29:15.470801: step 22060, loss = 1.20 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 01:29:27.735463: step 22070, loss = 1.19 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 01:29:39.870119: step 22080, loss = 1.10 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 01:29:51.982539: step 22090, loss = 1.19 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 01:30:04.117365: step 22100, loss = 1.21 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 01:30:18.053003: step 22110, loss = 1.14 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 01:30:30.147434: step 22120, loss = 1.15 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 01:30:42.300387: step 22130, loss = 1.19 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 01:30:54.526557: step 22140, loss = 1.19 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 01:31:06.631206: step 22150, loss = 1.16 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 01:31:18.773934: step 22160, loss = 1.16 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 01:31:31.141316: step 22170, loss = 1.21 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 01:31:43.315622: step 22180, loss = 1.19 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 01:31:55.418864: step 22190, loss = 1.23 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 01:32:07.519273: step 22200, loss = 1.12 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 01:32:21.664837: step 22210, loss = 1.21 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 01:32:33.833787: step 22220, loss = 1.17 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 01:32:46.004054: step 22230, loss = 1.19 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 01:32:58.119317: step 22240, loss = 1.19 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 01:33:10.130991: step 22250, loss = 1.19 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 01:33:22.419654: step 22260, loss = 1.10 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 01:33:34.580954: step 22270, loss = 1.20 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 01:33:46.692051: step 22280, loss = 1.18 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 01:33:58.838422: step 22290, loss = 1.20 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 01:34:10.938268: step 22300, loss = 1.22 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 01:34:25.058525: step 22310, loss = 1.22 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 01:34:37.274718: step 22320, loss = 1.17 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 01:34:49.480677: step 22330, loss = 1.23 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 01:35:01.560842: step 22340, loss = 1.13 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 01:35:13.610412: step 22350, loss = 1.14 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 01:35:25.691978: step 22360, loss = 1.14 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 01:35:37.930858: step 22370, loss = 1.14 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 01:35:50.040246: step 22380, loss = 1.13 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 01:36:02.150173: step 22390, loss = 1.14 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 01:36:14.320267: step 22400, loss = 1.14 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 01:36:28.225402: step 22410, loss = 1.10 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 01:36:40.341329: step 22420, loss = 1.24 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 01:36:52.563520: step 22430, loss = 1.16 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 01:37:04.666381: step 22440, loss = 1.15 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 01:37:16.827725: step 22450, loss = 1.20 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 01:37:28.985272: step 22460, loss = 1.16 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 01:37:41.111326: step 22470, loss = 1.10 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 01:37:53.219177: step 22480, loss = 1.20 (24.5 examples/sec; 1.225 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 01:38:05.334051: step 22490, loss = 1.15 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 01:38:17.339913: step 22500, loss = 1.19 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 01:38:31.226082: step 22510, loss = 1.23 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 01:38:43.412896: step 22520, loss = 1.18 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 01:38:55.537151: step 22530, loss = 1.15 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 01:39:07.687439: step 22540, loss = 1.16 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 01:39:19.816045: step 22550, loss = 1.19 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 01:39:31.931652: step 22560, loss = 1.12 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 01:39:44.087597: step 22570, loss = 1.22 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 01:39:56.236743: step 22580, loss = 1.14 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 01:40:08.352377: step 22590, loss = 1.18 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 01:40:20.463289: step 22600, loss = 1.12 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 01:40:34.718183: step 22610, loss = 1.23 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 01:40:46.794611: step 22620, loss = 1.17 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 01:40:59.026111: step 22630, loss = 1.16 (23.3 examples/sec; 1.286 sec/batch)\n",
      "2019-05-21 01:41:11.120862: step 22640, loss = 1.16 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 01:41:23.307670: step 22650, loss = 1.16 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 01:41:35.466320: step 22660, loss = 1.20 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 01:41:47.589958: step 22670, loss = 1.17 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 01:41:59.752861: step 22680, loss = 1.13 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 01:42:11.874723: step 22690, loss = 1.16 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 01:42:23.977934: step 22700, loss = 1.15 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 01:42:38.017056: step 22710, loss = 1.11 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 01:42:50.236889: step 22720, loss = 1.20 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 01:43:02.453249: step 22730, loss = 1.17 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 01:43:14.596780: step 22740, loss = 1.16 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 01:43:26.560996: step 22750, loss = 1.17 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 01:43:38.729549: step 22760, loss = 1.20 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 01:43:50.829144: step 22770, loss = 1.12 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 01:44:02.970051: step 22780, loss = 1.15 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 01:44:15.081315: step 22790, loss = 1.13 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 01:44:27.247344: step 22800, loss = 1.15 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 01:44:41.403873: step 22810, loss = 1.18 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 01:44:53.604960: step 22820, loss = 1.10 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 01:45:05.673903: step 22830, loss = 1.17 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 01:45:17.839781: step 22840, loss = 1.13 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 01:45:29.892303: step 22850, loss = 1.09 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 01:45:41.983540: step 22860, loss = 1.20 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 01:45:54.057432: step 22870, loss = 1.26 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 01:46:06.223869: step 22880, loss = 1.16 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 01:46:18.360348: step 22890, loss = 1.09 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 01:46:30.490006: step 22900, loss = 1.14 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 01:46:44.612694: step 22910, loss = 1.10 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 01:46:56.752954: step 22920, loss = 1.06 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 01:47:08.864730: step 22930, loss = 1.09 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 01:47:20.975314: step 22940, loss = 1.13 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 01:47:33.127737: step 22950, loss = 1.10 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 01:47:45.291025: step 22960, loss = 1.14 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 01:47:57.443683: step 22970, loss = 1.17 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 01:48:09.580161: step 22980, loss = 1.20 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 01:48:21.708676: step 22990, loss = 1.16 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 01:48:33.742717: step 23000, loss = 1.17 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-21 01:48:47.623614: step 23010, loss = 1.19 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 01:48:59.777318: step 23020, loss = 1.20 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 01:49:11.911242: step 23030, loss = 1.16 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 01:49:24.057009: step 23040, loss = 1.20 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 01:49:36.188506: step 23050, loss = 1.13 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 01:49:48.392195: step 23060, loss = 1.14 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 01:50:00.639433: step 23070, loss = 1.05 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 01:50:12.731898: step 23080, loss = 1.09 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 01:50:24.870259: step 23090, loss = 1.14 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 01:50:36.911687: step 23100, loss = 1.13 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 01:50:51.195762: step 23110, loss = 1.13 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 01:51:03.342092: step 23120, loss = 1.15 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 01:51:15.457066: step 23130, loss = 1.18 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 01:51:27.584788: step 23140, loss = 1.13 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 01:51:39.711337: step 23150, loss = 1.08 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 01:51:51.873595: step 23160, loss = 1.10 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 01:52:04.040865: step 23170, loss = 1.20 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 01:52:16.132284: step 23180, loss = 1.08 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 01:52:28.218568: step 23190, loss = 1.12 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 01:52:40.334169: step 23200, loss = 1.14 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 01:52:54.400927: step 23210, loss = 1.13 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 01:53:06.558208: step 23220, loss = 1.14 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 01:53:18.664086: step 23230, loss = 1.09 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 01:53:30.788048: step 23240, loss = 1.19 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 01:53:42.829706: step 23250, loss = 1.16 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 01:53:54.927142: step 23260, loss = 1.10 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 01:54:07.143689: step 23270, loss = 1.09 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 01:54:19.296006: step 23280, loss = 1.05 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 01:54:31.428400: step 23290, loss = 1.15 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 01:54:43.498656: step 23300, loss = 1.14 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 01:54:57.768041: step 23310, loss = 1.13 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 01:55:09.950481: step 23320, loss = 1.15 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 01:55:22.054817: step 23330, loss = 1.16 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 01:55:34.103443: step 23340, loss = 1.16 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 01:55:46.285205: step 23350, loss = 1.08 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 01:55:58.465628: step 23360, loss = 1.09 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 01:56:10.572857: step 23370, loss = 1.06 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 01:56:22.665274: step 23380, loss = 1.14 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 01:56:34.755873: step 23390, loss = 1.07 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 01:56:46.890086: step 23400, loss = 1.17 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 01:57:00.774803: step 23410, loss = 1.12 (24.8 examples/sec; 1.209 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 01:57:12.991379: step 23420, loss = 1.12 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 01:57:25.062364: step 23430, loss = 1.13 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 01:57:37.237549: step 23440, loss = 1.09 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 01:57:49.373507: step 23450, loss = 1.13 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 01:58:01.542356: step 23460, loss = 1.17 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 01:58:13.863054: step 23470, loss = 1.06 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 01:58:26.003894: step 23480, loss = 1.15 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 01:58:38.080524: step 23490, loss = 1.17 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 01:58:50.176694: step 23500, loss = 1.13 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 01:59:04.436898: step 23510, loss = 1.13 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 01:59:16.708561: step 23520, loss = 1.13 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 01:59:28.806563: step 23530, loss = 1.12 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 01:59:40.960802: step 23540, loss = 1.06 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 01:59:53.092879: step 23550, loss = 1.13 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 02:00:05.278029: step 23560, loss = 1.09 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 02:00:17.419281: step 23570, loss = 1.07 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 02:00:29.554691: step 23580, loss = 1.14 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 02:00:41.686481: step 23590, loss = 1.10 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 02:00:53.842872: step 23600, loss = 1.13 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 02:01:07.682856: step 23610, loss = 1.09 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 02:01:19.748674: step 23620, loss = 1.07 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 02:01:31.943279: step 23630, loss = 1.17 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 02:01:44.129295: step 23640, loss = 1.16 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 02:01:56.362212: step 23650, loss = 1.10 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 02:02:08.516502: step 23660, loss = 1.17 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 02:02:20.658717: step 23670, loss = 1.20 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 02:02:32.821387: step 23680, loss = 1.14 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 02:02:44.924544: step 23690, loss = 1.05 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 02:02:57.032233: step 23700, loss = 1.09 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 02:03:11.233551: step 23710, loss = 1.16 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 02:03:23.367406: step 23720, loss = 1.06 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 02:03:35.529307: step 23730, loss = 1.07 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 02:03:47.686975: step 23740, loss = 1.05 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 02:03:59.748008: step 23750, loss = 1.09 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 02:04:11.763292: step 23760, loss = 1.15 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 02:04:24.015020: step 23770, loss = 1.12 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 02:04:36.261151: step 23780, loss = 1.12 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 02:04:48.470250: step 23790, loss = 1.14 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 02:05:00.620165: step 23800, loss = 1.14 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 02:05:14.484068: step 23810, loss = 1.15 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 02:05:26.618329: step 23820, loss = 1.05 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 02:05:38.763658: step 23830, loss = 1.13 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 02:05:50.794568: step 23840, loss = 1.09 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 02:06:02.893779: step 23850, loss = 1.10 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 02:06:15.033759: step 23860, loss = 1.09 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 02:06:27.193068: step 23870, loss = 1.07 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 02:06:39.397402: step 23880, loss = 1.05 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 02:06:51.581837: step 23890, loss = 1.13 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 02:07:03.791302: step 23900, loss = 1.15 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 02:07:17.938987: step 23910, loss = 1.10 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 02:07:30.044210: step 23920, loss = 1.13 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 02:07:42.191020: step 23930, loss = 1.13 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 02:07:54.294503: step 23940, loss = 1.13 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 02:08:06.427145: step 23950, loss = 1.12 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 02:08:18.568506: step 23960, loss = 1.06 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 02:08:30.886119: step 23970, loss = 1.10 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 02:08:42.997255: step 23980, loss = 1.14 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 02:08:55.214283: step 23990, loss = 1.06 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 02:09:07.335628: step 24000, loss = 1.09 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 02:09:21.018659: step 24010, loss = 1.18 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 02:09:33.126478: step 24020, loss = 1.10 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 02:09:45.259249: step 24030, loss = 1.12 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 02:09:57.347352: step 24040, loss = 1.09 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 02:10:09.461193: step 24050, loss = 1.16 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 02:10:21.621037: step 24060, loss = 1.14 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 02:10:33.777128: step 24070, loss = 1.15 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 02:10:45.914626: step 24080, loss = 1.09 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 02:10:58.039392: step 24090, loss = 1.01 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 02:11:10.379283: step 24100, loss = 1.14 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 02:11:24.521209: step 24110, loss = 1.14 (23.2 examples/sec; 1.291 sec/batch)\n",
      "2019-05-21 02:11:36.739308: step 24120, loss = 1.12 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 02:11:48.883888: step 24130, loss = 1.09 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 02:12:01.036248: step 24140, loss = 1.13 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 02:12:13.223114: step 24150, loss = 1.01 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 02:12:25.388318: step 24160, loss = 1.06 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 02:12:37.509959: step 24170, loss = 1.10 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 02:12:49.621219: step 24180, loss = 1.12 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 02:13:01.716001: step 24190, loss = 1.11 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 02:13:13.860764: step 24200, loss = 1.12 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 02:13:27.851784: step 24210, loss = 1.10 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 02:13:39.976585: step 24220, loss = 1.10 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 02:13:52.082055: step 24230, loss = 1.12 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 02:14:04.212666: step 24240, loss = 1.09 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 02:14:16.329416: step 24250, loss = 1.04 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 02:14:28.359384: step 24260, loss = 1.13 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 02:14:40.491239: step 24270, loss = 1.10 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 02:14:52.610033: step 24280, loss = 1.05 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 02:15:04.751498: step 24290, loss = 1.04 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 02:15:16.860859: step 24300, loss = 1.10 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 02:15:30.809198: step 24310, loss = 1.11 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 02:15:42.899147: step 24320, loss = 1.03 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 02:15:55.012045: step 24330, loss = 1.10 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 02:16:07.090652: step 24340, loss = 1.05 (24.6 examples/sec; 1.220 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 02:16:19.236844: step 24350, loss = 1.13 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 02:16:31.347607: step 24360, loss = 1.11 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 02:16:43.430920: step 24370, loss = 1.02 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 02:16:55.597227: step 24380, loss = 1.08 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 02:17:07.679474: step 24390, loss = 1.11 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 02:17:19.782050: step 24400, loss = 1.10 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 02:17:33.531608: step 24410, loss = 1.06 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 02:17:45.652119: step 24420, loss = 1.04 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 02:17:57.761954: step 24430, loss = 1.18 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 02:18:09.919430: step 24440, loss = 1.02 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 02:18:22.045708: step 24450, loss = 1.04 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 02:18:34.161930: step 24460, loss = 1.13 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 02:18:46.283629: step 24470, loss = 1.16 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 02:18:58.426383: step 24480, loss = 1.16 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 02:19:10.541972: step 24490, loss = 1.11 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 02:19:22.677017: step 24500, loss = 1.06 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 02:19:36.716123: step 24510, loss = 1.14 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-21 02:19:48.827837: step 24520, loss = 1.08 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 02:20:00.993882: step 24530, loss = 1.09 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 02:20:13.168567: step 24540, loss = 1.08 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 02:20:25.271863: step 24550, loss = 1.07 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 02:20:37.339882: step 24560, loss = 1.05 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 02:20:49.539194: step 24570, loss = 1.13 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 02:21:01.710737: step 24580, loss = 1.07 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 02:21:13.755494: step 24590, loss = 1.05 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 02:21:25.799974: step 24600, loss = 1.07 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 02:21:39.767940: step 24610, loss = 1.08 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 02:21:51.908027: step 24620, loss = 1.11 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 02:22:04.055818: step 24630, loss = 1.13 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 02:22:16.300956: step 24640, loss = 1.11 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 02:22:28.428789: step 24650, loss = 1.09 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 02:22:40.666859: step 24660, loss = 1.04 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 02:22:52.812195: step 24670, loss = 1.09 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 02:23:04.934252: step 24680, loss = 1.15 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 02:23:17.086662: step 24690, loss = 1.15 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 02:23:29.290714: step 24700, loss = 1.06 (22.9 examples/sec; 1.307 sec/batch)\n",
      "2019-05-21 02:23:43.607025: step 24710, loss = 1.04 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 02:23:55.748261: step 24720, loss = 1.21 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 02:24:07.871105: step 24730, loss = 1.03 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 02:24:20.165530: step 24740, loss = 1.06 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 02:24:32.300891: step 24750, loss = 1.06 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 02:24:44.433134: step 24760, loss = 1.03 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 02:24:56.449738: step 24770, loss = 1.06 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 02:25:08.573027: step 24780, loss = 1.04 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 02:25:20.749790: step 24790, loss = 1.08 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 02:25:32.907290: step 24800, loss = 1.08 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 02:25:47.311393: step 24810, loss = 0.99 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 02:25:59.434277: step 24820, loss = 1.04 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 02:26:11.558493: step 24830, loss = 1.07 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 02:26:23.651422: step 24840, loss = 1.05 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 02:26:35.755253: step 24850, loss = 1.06 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 02:26:47.871837: step 24860, loss = 1.08 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 02:27:00.040330: step 24870, loss = 1.13 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 02:27:12.174740: step 24880, loss = 1.09 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 02:27:24.375286: step 24890, loss = 1.09 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 02:27:36.517259: step 24900, loss = 1.05 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 02:27:50.750052: step 24910, loss = 0.99 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 02:28:02.850605: step 24920, loss = 1.14 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 02:28:15.019219: step 24930, loss = 1.08 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 02:28:27.137265: step 24940, loss = 1.05 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 02:28:39.252470: step 24950, loss = 1.07 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 02:28:51.318159: step 24960, loss = 1.03 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 02:29:03.441021: step 24970, loss = 1.03 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 02:29:15.591391: step 24980, loss = 1.04 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 02:29:27.688002: step 24990, loss = 1.05 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 02:29:39.879890: step 25000, loss = 1.12 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 02:29:57.360252: step 25010, loss = 0.98 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 02:30:09.355543: step 25020, loss = 1.04 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 02:30:21.443279: step 25030, loss = 1.10 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 02:30:33.595311: step 25040, loss = 1.03 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 02:30:45.712562: step 25050, loss = 1.09 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 02:30:57.830437: step 25060, loss = 1.00 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 02:31:09.912692: step 25070, loss = 1.06 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 02:31:21.955735: step 25080, loss = 1.06 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 02:31:34.040286: step 25090, loss = 1.09 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 02:31:46.196201: step 25100, loss = 1.06 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 02:32:00.120232: step 25110, loss = 0.99 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 02:32:12.252643: step 25120, loss = 1.11 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 02:32:24.385937: step 25130, loss = 1.02 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 02:32:36.538944: step 25140, loss = 1.05 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 02:32:48.619237: step 25150, loss = 1.11 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 02:33:00.777726: step 25160, loss = 1.00 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 02:33:12.968136: step 25170, loss = 1.08 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 02:33:25.140187: step 25180, loss = 1.03 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 02:33:37.336596: step 25190, loss = 1.02 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 02:33:49.465266: step 25200, loss = 1.05 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 02:34:03.692355: step 25210, loss = 1.11 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 02:34:15.881399: step 25220, loss = 1.02 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 02:34:28.023203: step 25230, loss = 1.11 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 02:34:40.208778: step 25240, loss = 1.08 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 02:34:52.477999: step 25250, loss = 1.08 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 02:35:04.604647: step 25260, loss = 1.04 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 02:35:16.624431: step 25270, loss = 1.00 (24.9 examples/sec; 1.207 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 02:35:28.786014: step 25280, loss = 1.05 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 02:35:40.953379: step 25290, loss = 0.96 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 02:35:53.090964: step 25300, loss = 1.09 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 02:36:07.086355: step 25310, loss = 1.06 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 02:36:19.235819: step 25320, loss = 1.06 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 02:36:31.333848: step 25330, loss = 1.03 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 02:36:43.489704: step 25340, loss = 0.97 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 02:36:55.697803: step 25350, loss = 0.98 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 02:37:07.834666: step 25360, loss = 1.07 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 02:37:19.990970: step 25370, loss = 1.08 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 02:37:32.179251: step 25380, loss = 1.10 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 02:37:44.291136: step 25390, loss = 1.09 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 02:37:56.420181: step 25400, loss = 1.05 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 02:38:10.744450: step 25410, loss = 1.14 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 02:38:22.834044: step 25420, loss = 1.01 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 02:38:35.064405: step 25430, loss = 1.04 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 02:38:47.218185: step 25440, loss = 1.01 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 02:38:59.459540: step 25450, loss = 1.04 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 02:39:11.599735: step 25460, loss = 1.05 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 02:39:23.806890: step 25470, loss = 1.03 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 02:39:35.927722: step 25480, loss = 1.09 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 02:39:48.079318: step 25490, loss = 1.05 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 02:40:00.237417: step 25500, loss = 1.10 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 02:40:14.005134: step 25510, loss = 1.06 (25.5 examples/sec; 1.178 sec/batch)\n",
      "2019-05-21 02:40:25.982027: step 25520, loss = 1.07 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 02:40:38.125898: step 25530, loss = 1.12 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 02:40:50.256612: step 25540, loss = 1.03 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 02:41:02.411959: step 25550, loss = 1.04 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 02:41:14.557884: step 25560, loss = 1.00 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 02:41:26.603733: step 25570, loss = 1.06 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 02:41:38.775400: step 25580, loss = 1.16 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 02:41:50.878411: step 25590, loss = 1.09 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 02:42:03.026318: step 25600, loss = 1.06 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 02:42:16.852773: step 25610, loss = 1.04 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 02:42:29.085406: step 25620, loss = 1.05 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 02:42:41.234676: step 25630, loss = 1.05 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 02:42:53.356951: step 25640, loss = 1.02 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 02:43:05.491255: step 25650, loss = 1.07 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 02:43:17.742878: step 25660, loss = 0.98 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 02:43:30.011687: step 25670, loss = 1.06 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 02:43:42.105864: step 25680, loss = 1.10 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 02:43:54.359799: step 25690, loss = 1.02 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 02:44:06.536996: step 25700, loss = 1.04 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 02:44:20.407931: step 25710, loss = 1.10 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 02:44:32.563442: step 25720, loss = 1.02 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 02:44:44.724158: step 25730, loss = 1.11 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 02:44:56.893155: step 25740, loss = 1.03 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 02:45:09.061982: step 25750, loss = 1.05 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 02:45:21.216612: step 25760, loss = 1.05 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 02:45:33.275271: step 25770, loss = 1.06 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 02:45:45.427421: step 25780, loss = 0.95 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 02:45:57.586806: step 25790, loss = 1.09 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 02:46:09.761626: step 25800, loss = 1.02 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 02:46:23.593561: step 25810, loss = 1.06 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 02:46:35.717179: step 25820, loss = 1.05 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 02:46:48.070479: step 25830, loss = 1.07 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 02:47:00.398279: step 25840, loss = 0.99 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 02:47:12.536303: step 25850, loss = 1.01 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 02:47:24.694695: step 25860, loss = 1.03 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 02:47:36.851137: step 25870, loss = 1.00 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 02:47:49.031634: step 25880, loss = 1.00 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 02:48:01.195404: step 25890, loss = 1.03 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 02:48:13.376019: step 25900, loss = 0.99 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 02:48:27.288958: step 25910, loss = 1.08 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 02:48:39.428785: step 25920, loss = 1.06 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 02:48:51.576810: step 25930, loss = 1.04 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 02:49:03.813434: step 25940, loss = 1.07 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 02:49:15.959536: step 25950, loss = 0.97 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 02:49:28.163103: step 25960, loss = 1.05 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 02:49:40.354713: step 25970, loss = 1.13 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 02:49:52.514785: step 25980, loss = 1.04 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 02:50:04.767001: step 25990, loss = 1.06 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 02:50:16.897862: step 26000, loss = 0.99 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 02:50:31.246047: step 26010, loss = 0.99 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 02:50:43.226584: step 26020, loss = 1.02 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 02:50:55.353334: step 26030, loss = 0.99 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 02:51:07.500066: step 26040, loss = 1.04 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 02:51:19.839464: step 26050, loss = 0.97 (23.2 examples/sec; 1.293 sec/batch)\n",
      "2019-05-21 02:51:32.018675: step 26060, loss = 1.07 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 02:51:44.078950: step 26070, loss = 1.01 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 02:51:56.234980: step 26080, loss = 0.98 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 02:52:08.377133: step 26090, loss = 1.02 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 02:52:20.522023: step 26100, loss = 1.01 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 02:52:34.743842: step 26110, loss = 0.96 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 02:52:46.847069: step 26120, loss = 1.08 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 02:52:58.968366: step 26130, loss = 0.94 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 02:53:11.150360: step 26140, loss = 1.01 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 02:53:23.316007: step 26150, loss = 1.00 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 02:53:35.485163: step 26160, loss = 0.99 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 02:53:47.627996: step 26170, loss = 1.02 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 02:53:59.785628: step 26180, loss = 1.01 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 02:54:11.970848: step 26190, loss = 1.03 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 02:54:24.116013: step 26200, loss = 1.05 (24.9 examples/sec; 1.204 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 02:54:37.992393: step 26210, loss = 1.01 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 02:54:50.077554: step 26220, loss = 1.02 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 02:55:02.206404: step 26230, loss = 1.04 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 02:55:14.382356: step 26240, loss = 1.00 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 02:55:26.521999: step 26250, loss = 0.99 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 02:55:38.732566: step 26260, loss = 1.03 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 02:55:50.775403: step 26270, loss = 1.07 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 02:56:02.879305: step 26280, loss = 1.01 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 02:56:15.045500: step 26290, loss = 1.03 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 02:56:27.219036: step 26300, loss = 0.98 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 02:56:41.519827: step 26310, loss = 1.02 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 02:56:53.626759: step 26320, loss = 0.98 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 02:57:05.748047: step 26330, loss = 1.08 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 02:57:17.876287: step 26340, loss = 1.09 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 02:57:30.020205: step 26350, loss = 1.05 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 02:57:42.171258: step 26360, loss = 1.03 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 02:57:54.306738: step 26370, loss = 0.93 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 02:58:06.591581: step 26380, loss = 1.03 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 02:58:18.778485: step 26390, loss = 1.06 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 02:58:30.965451: step 26400, loss = 1.08 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 02:58:45.259530: step 26410, loss = 1.03 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 02:58:57.367437: step 26420, loss = 1.02 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 02:59:09.591327: step 26430, loss = 1.04 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 02:59:21.735175: step 26440, loss = 1.07 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 02:59:34.027765: step 26450, loss = 1.00 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 02:59:46.170117: step 26460, loss = 1.05 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 02:59:58.318238: step 26470, loss = 0.96 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 03:00:10.474661: step 26480, loss = 1.08 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 03:00:22.661892: step 26490, loss = 0.99 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 03:00:34.771478: step 26500, loss = 0.96 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 03:00:48.625241: step 26510, loss = 0.96 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 03:01:00.717294: step 26520, loss = 1.01 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 03:01:12.763493: step 26530, loss = 1.03 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 03:01:24.919670: step 26540, loss = 1.05 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 03:01:37.121335: step 26550, loss = 1.02 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 03:01:49.189641: step 26560, loss = 1.04 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 03:02:01.308132: step 26570, loss = 0.99 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 03:02:13.488474: step 26580, loss = 0.97 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 03:02:25.622903: step 26590, loss = 0.93 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 03:02:37.770159: step 26600, loss = 1.05 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 03:02:51.719725: step 26610, loss = 0.95 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 03:03:03.751560: step 26620, loss = 1.01 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 03:03:15.870842: step 26630, loss = 1.03 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 03:03:27.969934: step 26640, loss = 1.08 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 03:03:40.032001: step 26650, loss = 1.01 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 03:03:52.062977: step 26660, loss = 1.01 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 03:04:04.180161: step 26670, loss = 1.05 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 03:04:16.193148: step 26680, loss = 1.03 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 03:04:28.212982: step 26690, loss = 0.95 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 03:04:40.240126: step 26700, loss = 1.03 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 03:04:54.199960: step 26710, loss = 1.05 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 03:05:06.188288: step 26720, loss = 1.03 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 03:05:18.201279: step 26730, loss = 1.01 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 03:05:30.317740: step 26740, loss = 0.97 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 03:05:42.381217: step 26750, loss = 1.00 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 03:05:54.409090: step 26760, loss = 1.00 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 03:06:06.483830: step 26770, loss = 1.00 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 03:06:18.460753: step 26780, loss = 1.08 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 03:06:30.529028: step 26790, loss = 1.01 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 03:06:42.645956: step 26800, loss = 1.01 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 03:06:56.718583: step 26810, loss = 1.00 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 03:07:08.738782: step 26820, loss = 0.99 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 03:07:20.965396: step 26830, loss = 1.05 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 03:07:33.057207: step 26840, loss = 1.02 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 03:07:45.181045: step 26850, loss = 0.94 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 03:07:57.275543: step 26860, loss = 1.01 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 03:08:09.374125: step 26870, loss = 0.99 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 03:08:21.427647: step 26880, loss = 1.03 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 03:08:33.560204: step 26890, loss = 0.96 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 03:08:45.674548: step 26900, loss = 1.01 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 03:08:59.648711: step 26910, loss = 1.00 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 03:09:11.787698: step 26920, loss = 1.00 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 03:09:23.968621: step 26930, loss = 0.96 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 03:09:36.092933: step 26940, loss = 0.96 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 03:09:48.231228: step 26950, loss = 0.97 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 03:10:00.342244: step 26960, loss = 0.98 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 03:10:12.556992: step 26970, loss = 0.96 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 03:10:24.701551: step 26980, loss = 0.97 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 03:10:36.809192: step 26990, loss = 0.95 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 03:10:49.017958: step 27000, loss = 0.98 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 03:11:03.387432: step 27010, loss = 0.99 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 03:11:15.498128: step 27020, loss = 1.00 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 03:11:27.498976: step 27030, loss = 0.97 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 03:11:39.742630: step 27040, loss = 0.98 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 03:11:51.880197: step 27050, loss = 1.01 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 03:12:04.032450: step 27060, loss = 0.97 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 03:12:16.216485: step 27070, loss = 1.02 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 03:12:28.407632: step 27080, loss = 0.98 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 03:12:40.539036: step 27090, loss = 1.03 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 03:12:52.679162: step 27100, loss = 0.91 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 03:13:06.835033: step 27110, loss = 0.94 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 03:13:19.006551: step 27120, loss = 0.98 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 03:13:31.170415: step 27130, loss = 1.02 (24.8 examples/sec; 1.210 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 03:13:43.313044: step 27140, loss = 0.95 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 03:13:55.511558: step 27150, loss = 0.96 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 03:14:07.683865: step 27160, loss = 0.98 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 03:14:19.852498: step 27170, loss = 0.93 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 03:14:32.024535: step 27180, loss = 0.95 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 03:14:44.292789: step 27190, loss = 0.97 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 03:14:56.508643: step 27200, loss = 0.99 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 03:15:10.356263: step 27210, loss = 0.98 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 03:15:22.497858: step 27220, loss = 0.99 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 03:15:34.653749: step 27230, loss = 0.94 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 03:15:46.788936: step 27240, loss = 0.98 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 03:15:59.016613: step 27250, loss = 1.00 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 03:16:11.205414: step 27260, loss = 0.94 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 03:16:23.404766: step 27270, loss = 0.99 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 03:16:35.418899: step 27280, loss = 1.00 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 03:16:47.466558: step 27290, loss = 0.95 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 03:16:59.706540: step 27300, loss = 0.93 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 03:17:13.593579: step 27310, loss = 0.91 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 03:17:25.751841: step 27320, loss = 0.98 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 03:17:37.901105: step 27330, loss = 0.93 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 03:17:50.089055: step 27340, loss = 0.98 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 03:18:02.206935: step 27350, loss = 1.03 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 03:18:14.384614: step 27360, loss = 1.02 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 03:18:26.555670: step 27370, loss = 0.95 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 03:18:38.686443: step 27380, loss = 0.92 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 03:18:51.008457: step 27390, loss = 0.94 (23.3 examples/sec; 1.285 sec/batch)\n",
      "2019-05-21 03:19:03.211039: step 27400, loss = 0.97 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 03:19:17.118611: step 27410, loss = 0.98 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 03:19:29.287371: step 27420, loss = 0.99 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 03:19:41.526503: step 27430, loss = 0.94 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 03:19:53.814077: step 27440, loss = 0.93 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 03:20:05.973561: step 27450, loss = 0.93 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 03:20:18.205927: step 27460, loss = 0.91 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 03:20:30.315913: step 27470, loss = 0.99 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 03:20:42.500312: step 27480, loss = 1.00 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 03:20:54.769340: step 27490, loss = 0.98 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 03:21:06.896638: step 27500, loss = 1.01 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 03:21:20.992149: step 27510, loss = 1.03 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 03:21:33.256081: step 27520, loss = 0.93 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 03:21:45.291841: step 27530, loss = 1.02 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 03:21:57.385816: step 27540, loss = 0.98 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 03:22:09.556246: step 27550, loss = 1.04 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 03:22:21.702978: step 27560, loss = 0.99 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 03:22:33.855772: step 27570, loss = 0.97 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 03:22:46.097370: step 27580, loss = 0.94 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 03:22:58.239953: step 27590, loss = 1.01 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 03:23:10.477589: step 27600, loss = 0.93 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 03:23:24.517264: step 27610, loss = 1.00 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 03:23:36.662495: step 27620, loss = 0.94 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 03:23:48.836613: step 27630, loss = 0.98 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 03:24:01.031433: step 27640, loss = 0.99 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 03:24:13.165298: step 27650, loss = 1.07 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 03:24:25.311623: step 27660, loss = 1.04 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 03:24:37.434051: step 27670, loss = 0.93 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 03:24:49.623092: step 27680, loss = 0.92 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 03:25:01.744073: step 27690, loss = 0.99 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 03:25:13.895574: step 27700, loss = 1.00 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 03:25:27.900792: step 27710, loss = 0.99 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 03:25:40.015224: step 27720, loss = 0.96 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 03:25:52.178510: step 27730, loss = 0.94 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 03:26:04.373135: step 27740, loss = 0.95 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 03:26:16.534078: step 27750, loss = 1.07 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 03:26:28.691653: step 27760, loss = 0.98 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-21 03:26:40.701936: step 27770, loss = 0.97 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 03:26:52.666711: step 27780, loss = 0.97 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 03:27:04.641458: step 27790, loss = 0.95 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 03:27:16.616848: step 27800, loss = 0.94 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 03:27:30.516418: step 27810, loss = 0.95 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 03:27:42.529353: step 27820, loss = 0.97 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 03:27:54.634937: step 27830, loss = 0.96 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 03:28:06.757784: step 27840, loss = 0.99 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 03:28:18.799164: step 27850, loss = 0.94 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 03:28:30.933000: step 27860, loss = 1.03 (23.6 examples/sec; 1.271 sec/batch)\n",
      "2019-05-21 03:28:43.016988: step 27870, loss = 0.93 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 03:28:55.185267: step 27880, loss = 0.99 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 03:29:07.279559: step 27890, loss = 1.01 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 03:29:19.513136: step 27900, loss = 0.94 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 03:29:33.468233: step 27910, loss = 0.95 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 03:29:45.586584: step 27920, loss = 0.96 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 03:29:57.652437: step 27930, loss = 0.97 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 03:30:09.753120: step 27940, loss = 1.00 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 03:30:21.898294: step 27950, loss = 1.09 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 03:30:34.198035: step 27960, loss = 0.94 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 03:30:46.304354: step 27970, loss = 0.95 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 03:30:58.439885: step 27980, loss = 1.03 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 03:31:10.562685: step 27990, loss = 1.04 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 03:31:22.727087: step 28000, loss = 0.94 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 03:31:36.647069: step 28010, loss = 0.98 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 03:31:48.872734: step 28020, loss = 0.98 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 03:32:01.009492: step 28030, loss = 0.97 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 03:32:13.013339: step 28040, loss = 0.96 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 03:32:25.092090: step 28050, loss = 0.99 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 03:32:37.271387: step 28060, loss = 0.99 (24.6 examples/sec; 1.218 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 03:32:49.412094: step 28070, loss = 0.97 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 03:33:01.572945: step 28080, loss = 0.94 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 03:33:13.792584: step 28090, loss = 1.00 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 03:33:25.992022: step 28100, loss = 0.92 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 03:33:40.187976: step 28110, loss = 0.90 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 03:33:52.345723: step 28120, loss = 0.96 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 03:34:04.462182: step 28130, loss = 0.95 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 03:34:16.594157: step 28140, loss = 1.04 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 03:34:28.859018: step 28150, loss = 0.87 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 03:34:41.031268: step 28160, loss = 1.01 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 03:34:53.156936: step 28170, loss = 1.04 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 03:35:05.347865: step 28180, loss = 0.95 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 03:35:17.712772: step 28190, loss = 1.02 (23.1 examples/sec; 1.300 sec/batch)\n",
      "2019-05-21 03:35:29.873430: step 28200, loss = 0.96 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 03:35:43.617526: step 28210, loss = 0.94 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 03:35:55.786430: step 28220, loss = 0.92 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 03:36:07.945870: step 28230, loss = 0.95 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 03:36:20.096432: step 28240, loss = 0.95 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 03:36:32.323013: step 28250, loss = 0.95 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 03:36:44.510164: step 28260, loss = 0.88 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 03:36:56.663557: step 28270, loss = 0.93 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 03:37:08.848543: step 28280, loss = 0.94 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 03:37:20.845502: step 28290, loss = 0.98 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 03:37:32.884128: step 28300, loss = 0.96 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 03:37:46.796759: step 28310, loss = 0.95 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 03:37:59.008371: step 28320, loss = 0.92 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 03:38:11.172596: step 28330, loss = 0.87 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 03:38:23.332183: step 28340, loss = 0.95 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 03:38:35.672480: step 28350, loss = 1.02 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 03:38:47.755842: step 28360, loss = 0.96 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 03:38:59.895088: step 28370, loss = 0.96 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 03:39:11.971646: step 28380, loss = 0.98 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 03:39:24.388656: step 28390, loss = 1.01 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-05-21 03:39:36.744610: step 28400, loss = 0.93 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 03:39:51.056523: step 28410, loss = 0.95 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 03:40:03.236032: step 28420, loss = 0.92 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 03:40:15.347814: step 28430, loss = 0.94 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 03:40:27.512442: step 28440, loss = 0.96 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 03:40:39.636107: step 28450, loss = 0.96 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 03:40:51.753336: step 28460, loss = 0.95 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 03:41:03.819097: step 28470, loss = 1.03 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 03:41:16.081059: step 28480, loss = 0.91 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 03:41:28.244055: step 28490, loss = 0.98 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 03:41:40.448710: step 28500, loss = 0.92 (23.3 examples/sec; 1.288 sec/batch)\n",
      "2019-05-21 03:41:54.452405: step 28510, loss = 0.92 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 03:42:06.594969: step 28520, loss = 0.92 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 03:42:18.667201: step 28530, loss = 1.04 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 03:42:30.676235: step 28540, loss = 0.97 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 03:42:42.749253: step 28550, loss = 0.96 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 03:42:54.878523: step 28560, loss = 0.95 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 03:43:06.985169: step 28570, loss = 0.92 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 03:43:19.164935: step 28580, loss = 0.89 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 03:43:31.289561: step 28590, loss = 0.98 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 03:43:43.441185: step 28600, loss = 0.89 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 03:43:57.186028: step 28610, loss = 0.88 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 03:44:09.296196: step 28620, loss = 1.07 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 03:44:21.383788: step 28630, loss = 0.91 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 03:44:33.536006: step 28640, loss = 0.91 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 03:44:45.659516: step 28650, loss = 0.94 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 03:44:57.766154: step 28660, loss = 0.87 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 03:45:09.936061: step 28670, loss = 0.99 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 03:45:22.071649: step 28680, loss = 0.97 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 03:45:34.261400: step 28690, loss = 0.93 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 03:45:46.475858: step 28700, loss = 0.98 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 03:46:00.563870: step 28710, loss = 0.98 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 03:46:12.760299: step 28720, loss = 0.94 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 03:46:24.939302: step 28730, loss = 0.91 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 03:46:37.018585: step 28740, loss = 0.96 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 03:46:49.142922: step 28750, loss = 0.93 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 03:47:01.282646: step 28760, loss = 0.95 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 03:47:13.413026: step 28770, loss = 0.93 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 03:47:25.749619: step 28780, loss = 0.92 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 03:47:37.882178: step 28790, loss = 0.94 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 03:47:50.046260: step 28800, loss = 0.96 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 03:48:03.952023: step 28810, loss = 0.98 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 03:48:16.163485: step 28820, loss = 0.99 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 03:48:28.328383: step 28830, loss = 0.99 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 03:48:40.485361: step 28840, loss = 0.97 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 03:48:52.650548: step 28850, loss = 0.96 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 03:49:04.819958: step 28860, loss = 1.04 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 03:49:17.009930: step 28870, loss = 0.90 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 03:49:29.138979: step 28880, loss = 0.95 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 03:49:41.268580: step 28890, loss = 0.86 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 03:49:53.439935: step 28900, loss = 0.92 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 03:50:07.616563: step 28910, loss = 0.91 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 03:50:19.764437: step 28920, loss = 0.90 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 03:50:31.899395: step 28930, loss = 0.84 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 03:50:44.056786: step 28940, loss = 0.93 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 03:50:56.161871: step 28950, loss = 0.94 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 03:51:08.409301: step 28960, loss = 0.94 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 03:51:20.407061: step 28970, loss = 0.94 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 03:51:32.509271: step 28980, loss = 0.93 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 03:51:44.535205: step 28990, loss = 0.98 (25.3 examples/sec; 1.185 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 03:51:56.510349: step 29000, loss = 0.94 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 03:52:10.515763: step 29010, loss = 0.85 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 03:52:22.525214: step 29020, loss = 0.90 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 03:52:34.567235: step 29030, loss = 0.94 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 03:52:46.529632: step 29040, loss = 0.97 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 03:52:58.482724: step 29050, loss = 0.98 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 03:53:10.450349: step 29060, loss = 0.92 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 03:53:22.478422: step 29070, loss = 0.99 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 03:53:34.585907: step 29080, loss = 0.99 (23.7 examples/sec; 1.268 sec/batch)\n",
      "2019-05-21 03:53:46.534689: step 29090, loss = 0.97 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 03:53:58.482872: step 29100, loss = 0.98 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 03:54:12.610301: step 29110, loss = 0.93 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 03:54:24.541443: step 29120, loss = 0.91 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 03:54:36.561036: step 29130, loss = 0.96 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 03:54:48.503655: step 29140, loss = 0.91 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 03:55:00.609929: step 29150, loss = 0.95 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 03:55:12.629509: step 29160, loss = 0.94 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 03:55:24.739204: step 29170, loss = 0.88 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 03:55:36.921090: step 29180, loss = 0.93 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 03:55:48.931177: step 29190, loss = 0.95 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 03:56:00.958083: step 29200, loss = 0.86 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 03:56:14.752009: step 29210, loss = 0.94 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 03:56:26.713845: step 29220, loss = 0.95 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 03:56:38.707220: step 29230, loss = 0.96 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 03:56:50.772946: step 29240, loss = 0.90 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 03:57:02.878156: step 29250, loss = 0.93 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 03:57:14.998076: step 29260, loss = 0.94 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 03:57:27.160933: step 29270, loss = 0.88 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 03:57:39.306211: step 29280, loss = 0.99 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 03:57:51.436961: step 29290, loss = 0.87 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 03:58:03.426148: step 29300, loss = 0.91 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 03:58:17.608967: step 29310, loss = 0.91 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 03:58:29.773967: step 29320, loss = 0.90 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 03:58:41.923177: step 29330, loss = 0.95 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 03:58:54.007884: step 29340, loss = 0.92 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 03:59:06.077570: step 29350, loss = 0.94 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 03:59:18.126340: step 29360, loss = 0.89 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 03:59:30.211317: step 29370, loss = 0.91 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 03:59:42.386079: step 29380, loss = 0.96 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 03:59:54.612233: step 29390, loss = 1.00 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 04:00:06.746600: step 29400, loss = 0.96 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 04:00:21.074899: step 29410, loss = 0.98 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 04:00:33.158798: step 29420, loss = 0.94 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 04:00:45.310516: step 29430, loss = 1.00 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 04:00:57.457454: step 29440, loss = 0.91 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 04:01:09.612220: step 29450, loss = 0.94 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 04:01:21.657240: step 29460, loss = 0.89 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 04:01:33.699048: step 29470, loss = 0.95 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 04:01:45.737299: step 29480, loss = 0.92 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 04:01:57.819253: step 29490, loss = 0.88 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 04:02:09.851710: step 29500, loss = 0.88 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 04:02:23.534932: step 29510, loss = 0.89 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 04:02:35.692120: step 29520, loss = 0.89 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 04:02:47.821323: step 29530, loss = 0.96 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 04:02:59.804653: step 29540, loss = 0.90 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 04:03:11.826234: step 29550, loss = 0.86 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 04:03:23.961976: step 29560, loss = 0.92 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 04:03:35.976316: step 29570, loss = 0.95 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 04:03:47.959738: step 29580, loss = 0.89 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 04:03:59.990090: step 29590, loss = 0.91 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 04:04:12.005407: step 29600, loss = 0.96 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 04:04:25.914010: step 29610, loss = 0.93 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 04:04:38.054521: step 29620, loss = 0.86 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 04:04:50.113846: step 29630, loss = 0.92 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 04:05:02.232895: step 29640, loss = 0.92 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 04:05:14.320026: step 29650, loss = 1.00 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 04:05:26.378624: step 29660, loss = 0.90 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 04:05:38.373188: step 29670, loss = 0.90 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 04:05:50.371457: step 29680, loss = 0.95 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 04:06:02.354553: step 29690, loss = 0.93 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 04:06:14.385680: step 29700, loss = 0.96 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 04:06:28.171235: step 29710, loss = 0.93 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 04:06:40.312333: step 29720, loss = 0.86 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 04:06:52.393445: step 29730, loss = 0.90 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 04:07:04.433629: step 29740, loss = 0.96 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 04:07:16.572336: step 29750, loss = 0.93 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 04:07:28.600581: step 29760, loss = 0.87 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 04:07:40.636484: step 29770, loss = 0.88 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 04:07:52.743698: step 29780, loss = 0.83 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 04:08:04.796291: step 29790, loss = 0.86 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 04:08:16.825657: step 29800, loss = 0.97 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 04:08:31.181305: step 29810, loss = 0.98 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 04:08:43.396853: step 29820, loss = 0.94 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 04:08:55.410068: step 29830, loss = 0.89 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 04:09:07.465682: step 29840, loss = 0.92 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 04:09:19.517646: step 29850, loss = 0.95 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 04:09:31.564413: step 29860, loss = 0.92 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 04:09:43.597419: step 29870, loss = 0.92 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 04:09:55.750900: step 29880, loss = 0.88 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 04:10:07.861548: step 29890, loss = 0.97 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 04:10:19.990363: step 29900, loss = 0.87 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 04:10:33.844964: step 29910, loss = 0.91 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 04:10:45.991189: step 29920, loss = 0.91 (24.8 examples/sec; 1.211 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 04:10:58.233568: step 29930, loss = 0.90 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 04:11:10.356750: step 29940, loss = 0.89 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 04:11:22.539806: step 29950, loss = 0.90 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 04:11:34.655984: step 29960, loss = 0.89 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 04:11:46.874382: step 29970, loss = 0.88 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 04:11:59.096102: step 29980, loss = 0.94 (23.0 examples/sec; 1.307 sec/batch)\n",
      "2019-05-21 04:12:11.251938: step 29990, loss = 0.93 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 04:12:23.380028: step 30000, loss = 0.89 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 04:12:41.086846: step 30010, loss = 0.89 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 04:12:53.296776: step 30020, loss = 0.88 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 04:13:05.362486: step 30030, loss = 0.96 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 04:13:17.439989: step 30040, loss = 0.92 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 04:13:29.551887: step 30050, loss = 0.89 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 04:13:41.691581: step 30060, loss = 0.93 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 04:13:53.862773: step 30070, loss = 0.88 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 04:14:06.024176: step 30080, loss = 0.89 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 04:14:18.158751: step 30090, loss = 0.96 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 04:14:30.375456: step 30100, loss = 0.92 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 04:14:44.284416: step 30110, loss = 0.94 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 04:14:56.389855: step 30120, loss = 0.96 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 04:15:08.391961: step 30130, loss = 0.96 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 04:15:20.510826: step 30140, loss = 0.90 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 04:15:32.525511: step 30150, loss = 1.01 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 04:15:44.556552: step 30160, loss = 0.91 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 04:15:56.606552: step 30170, loss = 0.90 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 04:16:08.597628: step 30180, loss = 0.93 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 04:16:20.614277: step 30190, loss = 0.90 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 04:16:32.784909: step 30200, loss = 0.89 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 04:16:47.147972: step 30210, loss = 0.89 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 04:16:59.286619: step 30220, loss = 0.97 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 04:17:11.456213: step 30230, loss = 0.94 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 04:17:23.455269: step 30240, loss = 0.87 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 04:17:35.526384: step 30250, loss = 0.98 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 04:17:47.610277: step 30260, loss = 0.89 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 04:17:59.722386: step 30270, loss = 0.87 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 04:18:11.759936: step 30280, loss = 0.88 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 04:18:23.806340: step 30290, loss = 0.91 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 04:18:35.819353: step 30300, loss = 0.88 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 04:18:49.633024: step 30310, loss = 0.93 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 04:19:01.617544: step 30320, loss = 0.87 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 04:19:13.753347: step 30330, loss = 1.00 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 04:19:25.764339: step 30340, loss = 0.84 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 04:19:37.817863: step 30350, loss = 0.98 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 04:19:49.814545: step 30360, loss = 0.83 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 04:20:01.830962: step 30370, loss = 0.96 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 04:20:13.883249: step 30380, loss = 0.88 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 04:20:26.057117: step 30390, loss = 0.94 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 04:20:38.074512: step 30400, loss = 0.89 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 04:20:52.426147: step 30410, loss = 0.87 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 04:21:04.481642: step 30420, loss = 0.91 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 04:21:16.545427: step 30430, loss = 0.87 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 04:21:28.687202: step 30440, loss = 0.92 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 04:21:40.794428: step 30450, loss = 0.86 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 04:21:52.820527: step 30460, loss = 0.84 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 04:22:04.870327: step 30470, loss = 1.00 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 04:22:16.854230: step 30480, loss = 0.86 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 04:22:28.848385: step 30490, loss = 0.88 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 04:22:40.931120: step 30500, loss = 0.85 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 04:22:54.969708: step 30510, loss = 0.97 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 04:23:07.125042: step 30520, loss = 0.93 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 04:23:19.159417: step 30530, loss = 0.85 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 04:23:31.239934: step 30540, loss = 0.93 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 04:23:43.397544: step 30550, loss = 0.92 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 04:23:55.474812: step 30560, loss = 0.96 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 04:24:07.610407: step 30570, loss = 0.85 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 04:24:19.728495: step 30580, loss = 0.91 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 04:24:31.851665: step 30590, loss = 0.94 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 04:24:44.006146: step 30600, loss = 0.94 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 04:24:57.835908: step 30610, loss = 0.96 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 04:25:09.833993: step 30620, loss = 0.88 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 04:25:21.925306: step 30630, loss = 0.87 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 04:25:34.030414: step 30640, loss = 0.93 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 04:25:46.076888: step 30650, loss = 0.88 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 04:25:58.059573: step 30660, loss = 0.88 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 04:26:10.098049: step 30670, loss = 0.89 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 04:26:22.187178: step 30680, loss = 0.88 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 04:26:34.255729: step 30690, loss = 0.92 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 04:26:46.413983: step 30700, loss = 0.86 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 04:27:00.211262: step 30710, loss = 0.91 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 04:27:12.236529: step 30720, loss = 0.90 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 04:27:24.278380: step 30730, loss = 0.84 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 04:27:36.560575: step 30740, loss = 0.90 (23.1 examples/sec; 1.296 sec/batch)\n",
      "2019-05-21 04:27:48.644687: step 30750, loss = 0.92 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 04:28:00.668695: step 30760, loss = 0.90 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 04:28:12.744025: step 30770, loss = 0.90 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 04:28:24.799617: step 30780, loss = 0.85 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 04:28:36.845830: step 30790, loss = 0.86 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 04:28:48.906472: step 30800, loss = 0.87 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 04:29:03.050498: step 30810, loss = 0.90 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-21 04:29:15.157711: step 30820, loss = 0.95 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 04:29:27.310304: step 30830, loss = 0.92 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 04:29:39.391428: step 30840, loss = 0.94 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 04:29:51.438493: step 30850, loss = 0.89 (24.8 examples/sec; 1.209 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 04:30:03.555242: step 30860, loss = 0.94 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 04:30:15.664592: step 30870, loss = 0.88 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 04:30:27.685253: step 30880, loss = 0.89 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 04:30:39.745770: step 30890, loss = 0.89 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 04:30:51.760360: step 30900, loss = 0.85 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 04:31:05.758288: step 30910, loss = 0.83 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 04:31:17.844362: step 30920, loss = 0.94 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 04:31:29.880519: step 30930, loss = 0.88 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 04:31:41.979686: step 30940, loss = 0.88 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 04:31:54.158216: step 30950, loss = 0.90 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 04:32:06.326765: step 30960, loss = 0.91 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 04:32:18.556102: step 30970, loss = 0.90 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 04:32:30.763318: step 30980, loss = 0.95 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 04:32:42.972210: step 30990, loss = 0.82 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 04:32:55.079606: step 31000, loss = 0.91 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 04:33:09.204186: step 31010, loss = 0.82 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 04:33:21.226828: step 31020, loss = 0.89 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 04:33:33.361180: step 31030, loss = 0.91 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 04:33:45.402149: step 31040, loss = 0.89 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 04:33:57.414110: step 31050, loss = 0.93 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 04:34:09.388406: step 31060, loss = 0.92 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 04:34:21.378093: step 31070, loss = 0.87 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 04:34:33.488544: step 31080, loss = 0.89 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 04:34:45.566378: step 31090, loss = 0.91 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 04:34:57.724144: step 31100, loss = 0.85 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 04:35:11.627960: step 31110, loss = 0.87 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 04:35:23.752267: step 31120, loss = 0.84 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 04:35:35.967411: step 31130, loss = 0.89 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 04:35:48.055091: step 31140, loss = 0.86 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 04:36:00.178979: step 31150, loss = 0.84 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 04:36:12.297617: step 31160, loss = 0.92 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 04:36:24.478552: step 31170, loss = 0.87 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 04:36:36.653396: step 31180, loss = 0.86 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 04:36:48.782942: step 31190, loss = 1.02 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 04:37:00.891365: step 31200, loss = 0.89 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 04:37:14.734406: step 31210, loss = 0.88 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 04:37:26.923307: step 31220, loss = 0.94 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 04:37:38.965657: step 31230, loss = 0.95 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 04:37:51.016566: step 31240, loss = 0.89 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 04:38:03.081388: step 31250, loss = 0.86 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 04:38:15.159441: step 31260, loss = 0.91 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 04:38:27.242560: step 31270, loss = 0.89 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 04:38:39.305406: step 31280, loss = 0.86 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 04:38:51.393748: step 31290, loss = 0.85 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 04:39:03.438253: step 31300, loss = 0.91 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 04:39:17.375593: step 31310, loss = 0.95 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 04:39:29.344639: step 31320, loss = 0.88 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 04:39:41.327823: step 31330, loss = 0.84 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 04:39:53.411454: step 31340, loss = 0.90 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 04:40:05.475557: step 31350, loss = 0.94 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 04:40:17.570190: step 31360, loss = 0.92 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 04:40:29.633333: step 31370, loss = 0.88 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 04:40:41.709888: step 31380, loss = 0.94 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 04:40:53.866155: step 31390, loss = 0.81 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 04:41:06.123747: step 31400, loss = 0.86 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 04:41:19.910713: step 31410, loss = 0.83 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 04:41:31.962659: step 31420, loss = 0.89 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 04:41:44.017832: step 31430, loss = 0.88 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 04:41:56.061334: step 31440, loss = 0.85 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 04:42:08.133503: step 31450, loss = 0.89 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 04:42:20.230970: step 31460, loss = 0.85 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 04:42:32.297453: step 31470, loss = 0.87 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 04:42:44.438202: step 31480, loss = 0.85 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 04:42:56.536975: step 31490, loss = 0.89 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 04:43:08.630175: step 31500, loss = 0.90 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 04:43:22.340621: step 31510, loss = 0.96 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 04:43:34.343308: step 31520, loss = 0.94 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 04:43:46.387537: step 31530, loss = 0.84 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 04:43:58.549089: step 31540, loss = 0.86 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 04:44:10.635490: step 31550, loss = 0.82 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 04:44:22.666399: step 31560, loss = 0.87 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 04:44:34.680860: step 31570, loss = 0.93 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 04:44:46.777204: step 31580, loss = 0.78 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 04:44:58.843917: step 31590, loss = 0.91 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 04:45:10.926084: step 31600, loss = 0.84 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 04:45:25.355495: step 31610, loss = 0.85 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 04:45:37.455125: step 31620, loss = 0.87 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 04:45:49.530153: step 31630, loss = 0.88 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 04:46:01.639841: step 31640, loss = 0.84 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 04:46:13.766890: step 31650, loss = 0.81 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 04:46:25.881538: step 31660, loss = 0.82 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 04:46:38.012776: step 31670, loss = 0.92 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 04:46:50.280114: step 31680, loss = 0.89 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 04:47:02.485293: step 31690, loss = 0.89 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 04:47:14.538887: step 31700, loss = 0.82 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 04:47:28.681460: step 31710, loss = 0.84 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 04:47:40.773776: step 31720, loss = 0.87 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 04:47:52.819555: step 31730, loss = 0.93 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 04:48:04.889549: step 31740, loss = 0.84 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 04:48:16.967533: step 31750, loss = 0.87 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 04:48:29.060077: step 31760, loss = 0.84 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 04:48:41.145598: step 31770, loss = 0.79 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 04:48:53.168684: step 31780, loss = 0.88 (24.5 examples/sec; 1.227 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 04:49:05.191127: step 31790, loss = 0.89 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 04:49:17.262698: step 31800, loss = 0.91 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 04:49:31.129285: step 31810, loss = 0.91 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 04:49:43.211307: step 31820, loss = 0.83 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 04:49:55.274909: step 31830, loss = 0.83 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 04:50:07.368742: step 31840, loss = 0.88 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 04:50:19.423200: step 31850, loss = 0.83 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 04:50:31.590484: step 31860, loss = 0.86 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 04:50:43.829348: step 31870, loss = 0.80 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 04:50:55.950954: step 31880, loss = 0.83 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 04:51:08.022163: step 31890, loss = 0.83 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 04:51:20.081781: step 31900, loss = 0.80 (23.5 examples/sec; 1.274 sec/batch)\n",
      "2019-05-21 04:51:34.236987: step 31910, loss = 0.82 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 04:51:46.297793: step 31920, loss = 0.91 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 04:51:58.350966: step 31930, loss = 0.84 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 04:52:10.516810: step 31940, loss = 0.88 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 04:52:22.559965: step 31950, loss = 0.89 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 04:52:34.636384: step 31960, loss = 0.87 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 04:52:46.754399: step 31970, loss = 0.84 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 04:52:58.811072: step 31980, loss = 0.88 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 04:53:10.852381: step 31990, loss = 0.86 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 04:53:22.956902: step 32000, loss = 0.88 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 04:53:37.381735: step 32010, loss = 0.91 (23.3 examples/sec; 1.285 sec/batch)\n",
      "2019-05-21 04:53:49.433835: step 32020, loss = 0.89 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 04:54:01.496399: step 32030, loss = 0.90 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 04:54:13.466712: step 32040, loss = 0.84 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 04:54:25.442857: step 32050, loss = 0.84 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 04:54:37.418170: step 32060, loss = 0.90 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 04:54:49.375037: step 32070, loss = 0.83 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 04:55:01.352299: step 32080, loss = 0.93 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 04:55:13.357924: step 32090, loss = 0.80 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 04:55:25.365960: step 32100, loss = 0.87 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 04:55:39.189265: step 32110, loss = 0.93 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 04:55:51.149693: step 32120, loss = 0.89 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 04:56:03.136929: step 32130, loss = 0.89 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 04:56:15.175895: step 32140, loss = 0.84 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 04:56:27.208367: step 32150, loss = 0.84 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 04:56:39.245975: step 32160, loss = 0.87 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 04:56:51.259499: step 32170, loss = 0.86 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 04:57:03.323004: step 32180, loss = 0.89 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 04:57:15.389522: step 32190, loss = 0.88 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 04:57:27.471251: step 32200, loss = 0.92 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 04:57:41.302884: step 32210, loss = 0.90 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 04:57:53.443256: step 32220, loss = 0.89 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 04:58:05.496110: step 32230, loss = 0.85 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 04:58:17.570889: step 32240, loss = 0.86 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 04:58:29.668463: step 32250, loss = 0.90 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 04:58:41.795708: step 32260, loss = 0.83 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 04:58:53.948518: step 32270, loss = 0.80 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 04:59:05.975934: step 32280, loss = 0.87 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 04:59:18.105306: step 32290, loss = 0.84 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 04:59:30.285253: step 32300, loss = 0.81 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 04:59:44.302018: step 32310, loss = 0.84 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 04:59:56.396413: step 32320, loss = 0.86 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 05:00:08.414745: step 32330, loss = 0.87 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 05:00:20.464279: step 32340, loss = 0.81 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 05:00:32.617076: step 32350, loss = 0.87 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 05:00:44.771925: step 32360, loss = 0.85 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 05:00:56.950780: step 32370, loss = 0.91 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 05:01:09.145705: step 32380, loss = 0.86 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 05:01:21.345235: step 32390, loss = 0.88 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 05:01:33.540946: step 32400, loss = 0.77 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 05:01:47.441915: step 32410, loss = 0.86 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 05:01:59.602213: step 32420, loss = 0.91 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 05:02:11.733136: step 32430, loss = 0.86 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 05:02:23.932283: step 32440, loss = 0.88 (23.3 examples/sec; 1.290 sec/batch)\n",
      "2019-05-21 05:02:36.090202: step 32450, loss = 0.84 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 05:02:48.248659: step 32460, loss = 0.95 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 05:03:00.282894: step 32470, loss = 0.84 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 05:03:12.440156: step 32480, loss = 0.89 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 05:03:24.591281: step 32490, loss = 0.85 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 05:03:36.739726: step 32500, loss = 0.87 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 05:03:50.806530: step 32510, loss = 0.88 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 05:04:03.041634: step 32520, loss = 0.83 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 05:04:15.154366: step 32530, loss = 0.88 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 05:04:27.274900: step 32540, loss = 0.83 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 05:04:39.465866: step 32550, loss = 0.88 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 05:04:51.616655: step 32560, loss = 0.88 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 05:05:03.693459: step 32570, loss = 0.84 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 05:05:15.752644: step 32580, loss = 0.89 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 05:05:27.755760: step 32590, loss = 0.90 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 05:05:39.975798: step 32600, loss = 0.78 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 05:05:54.081758: step 32610, loss = 0.84 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 05:06:06.158438: step 32620, loss = 0.91 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 05:06:18.392660: step 32630, loss = 0.84 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 05:06:30.655179: step 32640, loss = 0.84 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 05:06:42.793453: step 32650, loss = 0.85 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 05:06:54.953974: step 32660, loss = 0.87 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 05:07:07.176846: step 32670, loss = 0.80 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 05:07:19.362041: step 32680, loss = 0.87 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 05:07:31.544054: step 32690, loss = 0.84 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 05:07:43.770437: step 32700, loss = 0.82 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 05:07:57.902087: step 32710, loss = 0.87 (24.8 examples/sec; 1.209 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 05:08:10.130650: step 32720, loss = 0.90 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 05:08:22.263146: step 32730, loss = 0.86 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 05:08:34.485513: step 32740, loss = 0.86 (22.7 examples/sec; 1.320 sec/batch)\n",
      "2019-05-21 05:08:46.644838: step 32750, loss = 0.84 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 05:08:58.838600: step 32760, loss = 0.87 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 05:09:10.975374: step 32770, loss = 0.81 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 05:09:23.133852: step 32780, loss = 0.80 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 05:09:35.361005: step 32790, loss = 0.84 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 05:09:47.542859: step 32800, loss = 0.86 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 05:10:01.432516: step 32810, loss = 0.87 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 05:10:13.573326: step 32820, loss = 0.87 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 05:10:25.691127: step 32830, loss = 0.80 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 05:10:37.777235: step 32840, loss = 0.84 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 05:10:49.916234: step 32850, loss = 0.82 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 05:11:02.055858: step 32860, loss = 0.84 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 05:11:14.149284: step 32870, loss = 0.86 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 05:11:26.319861: step 32880, loss = 0.85 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 05:11:38.552436: step 32890, loss = 0.88 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 05:11:50.685688: step 32900, loss = 0.79 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 05:12:04.863995: step 32910, loss = 0.83 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 05:12:17.107651: step 32920, loss = 0.92 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 05:12:29.269410: step 32930, loss = 0.78 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 05:12:41.415165: step 32940, loss = 0.82 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 05:12:53.593326: step 32950, loss = 0.85 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 05:13:05.733394: step 32960, loss = 0.87 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 05:13:18.068030: step 32970, loss = 0.82 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 05:13:30.298903: step 32980, loss = 0.79 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 05:13:42.505319: step 32990, loss = 0.84 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 05:13:54.699749: step 33000, loss = 0.78 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 05:14:08.681900: step 33010, loss = 0.81 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 05:14:20.908080: step 33020, loss = 0.89 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 05:14:33.089322: step 33030, loss = 0.82 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 05:14:45.226476: step 33040, loss = 0.92 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 05:14:57.425746: step 33050, loss = 0.88 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 05:15:09.601558: step 33060, loss = 0.80 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 05:15:21.779024: step 33070, loss = 0.89 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 05:15:34.048388: step 33080, loss = 0.80 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 05:15:46.062074: step 33090, loss = 0.77 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 05:15:58.201514: step 33100, loss = 0.73 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 05:16:12.199465: step 33110, loss = 0.93 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 05:16:24.355296: step 33120, loss = 0.82 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 05:16:36.517880: step 33130, loss = 0.88 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 05:16:48.648349: step 33140, loss = 0.81 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 05:17:00.826567: step 33150, loss = 0.84 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 05:17:13.009661: step 33160, loss = 0.80 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 05:17:25.221998: step 33170, loss = 0.86 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 05:17:37.382786: step 33180, loss = 0.88 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 05:17:49.638002: step 33190, loss = 0.85 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 05:18:01.775995: step 33200, loss = 0.87 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 05:18:15.644052: step 33210, loss = 0.86 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 05:18:27.905976: step 33220, loss = 0.81 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 05:18:40.074875: step 33230, loss = 0.84 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 05:18:52.268723: step 33240, loss = 0.85 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 05:19:04.428788: step 33250, loss = 0.91 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 05:19:16.566414: step 33260, loss = 0.89 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 05:19:28.631632: step 33270, loss = 0.79 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 05:19:40.793916: step 33280, loss = 0.79 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 05:19:52.946346: step 33290, loss = 0.80 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 05:20:05.088501: step 33300, loss = 0.80 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 05:20:18.968330: step 33310, loss = 0.84 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 05:20:31.021747: step 33320, loss = 0.83 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 05:20:43.167097: step 33330, loss = 0.88 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 05:20:55.311205: step 33340, loss = 0.80 (23.3 examples/sec; 1.286 sec/batch)\n",
      "2019-05-21 05:21:07.447829: step 33350, loss = 0.82 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 05:21:19.524362: step 33360, loss = 0.89 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 05:21:31.654348: step 33370, loss = 0.84 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 05:21:43.737448: step 33380, loss = 0.84 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 05:21:55.879653: step 33390, loss = 0.80 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 05:22:08.049939: step 33400, loss = 0.86 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 05:22:22.191868: step 33410, loss = 0.88 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 05:22:34.397295: step 33420, loss = 0.82 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 05:22:46.490629: step 33430, loss = 0.79 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 05:22:58.581167: step 33440, loss = 0.88 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 05:23:10.848262: step 33450, loss = 0.80 (23.1 examples/sec; 1.297 sec/batch)\n",
      "2019-05-21 05:23:22.998254: step 33460, loss = 0.88 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 05:23:35.253634: step 33470, loss = 0.89 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-05-21 05:23:47.375977: step 33480, loss = 0.80 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 05:23:59.603715: step 33490, loss = 0.79 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 05:24:11.832932: step 33500, loss = 0.82 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 05:24:26.012302: step 33510, loss = 0.77 (25.6 examples/sec; 1.170 sec/batch)\n",
      "2019-05-21 05:24:38.090546: step 33520, loss = 0.83 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 05:24:50.309425: step 33530, loss = 0.83 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 05:25:02.521201: step 33540, loss = 0.81 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 05:25:14.655617: step 33550, loss = 0.87 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 05:25:26.808905: step 33560, loss = 0.86 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 05:25:38.988202: step 33570, loss = 0.81 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 05:25:51.162112: step 33580, loss = 0.88 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 05:26:03.237502: step 33590, loss = 0.79 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 05:26:15.282521: step 33600, loss = 0.85 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 05:26:29.462602: step 33610, loss = 0.87 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 05:26:41.646507: step 33620, loss = 0.81 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 05:26:53.772407: step 33630, loss = 0.79 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 05:27:05.803868: step 33640, loss = 0.78 (24.8 examples/sec; 1.210 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 05:27:17.954529: step 33650, loss = 0.81 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 05:27:30.137892: step 33660, loss = 0.82 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 05:27:42.241918: step 33670, loss = 0.86 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 05:27:54.348090: step 33680, loss = 0.91 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 05:28:06.531781: step 33690, loss = 0.84 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 05:28:18.666700: step 33700, loss = 0.93 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 05:28:32.560014: step 33710, loss = 0.83 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 05:28:44.778724: step 33720, loss = 0.85 (22.8 examples/sec; 1.317 sec/batch)\n",
      "2019-05-21 05:28:56.953582: step 33730, loss = 0.79 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 05:29:09.131514: step 33740, loss = 0.80 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 05:29:21.305631: step 33750, loss = 0.85 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 05:29:33.346342: step 33760, loss = 0.84 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-21 05:29:45.491342: step 33770, loss = 0.84 (23.5 examples/sec; 1.278 sec/batch)\n",
      "2019-05-21 05:29:57.659025: step 33780, loss = 0.86 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 05:30:09.846197: step 33790, loss = 0.84 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 05:30:21.963800: step 33800, loss = 0.76 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 05:30:36.040725: step 33810, loss = 0.76 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 05:30:48.171528: step 33820, loss = 0.82 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 05:31:00.296903: step 33830, loss = 0.80 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 05:31:12.452089: step 33840, loss = 0.94 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 05:31:24.498948: step 33850, loss = 0.84 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 05:31:36.643319: step 33860, loss = 0.77 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 05:31:48.820921: step 33870, loss = 0.82 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 05:32:01.077229: step 33880, loss = 0.76 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 05:32:13.230530: step 33890, loss = 0.83 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 05:32:25.409549: step 33900, loss = 0.77 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 05:32:39.414959: step 33910, loss = 0.79 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 05:32:51.632368: step 33920, loss = 0.84 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 05:33:03.812787: step 33930, loss = 0.78 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 05:33:15.971369: step 33940, loss = 0.83 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 05:33:28.124320: step 33950, loss = 0.95 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 05:33:40.324953: step 33960, loss = 0.79 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 05:33:52.437958: step 33970, loss = 0.87 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 05:34:04.562739: step 33980, loss = 0.78 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 05:34:16.708815: step 33990, loss = 0.81 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 05:34:29.000479: step 34000, loss = 0.84 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 05:34:43.045665: step 34010, loss = 0.78 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 05:34:55.152134: step 34020, loss = 0.82 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 05:35:07.280297: step 34030, loss = 0.86 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 05:35:19.539239: step 34040, loss = 0.80 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 05:35:31.688616: step 34050, loss = 0.81 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 05:35:43.839072: step 34060, loss = 0.85 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 05:35:56.000541: step 34070, loss = 0.81 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 05:36:08.127836: step 34080, loss = 0.77 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 05:36:20.254905: step 34090, loss = 0.87 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 05:36:32.350779: step 34100, loss = 0.80 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 05:36:46.432940: step 34110, loss = 0.78 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 05:36:58.600260: step 34120, loss = 0.85 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 05:37:10.792208: step 34130, loss = 0.79 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 05:37:22.963724: step 34140, loss = 0.73 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 05:37:35.067975: step 34150, loss = 0.88 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 05:37:47.209349: step 34160, loss = 0.76 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 05:37:59.418752: step 34170, loss = 0.85 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 05:38:11.732642: step 34180, loss = 0.83 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 05:38:23.817358: step 34190, loss = 0.90 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 05:38:36.168789: step 34200, loss = 0.78 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-05-21 05:38:50.145995: step 34210, loss = 0.83 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 05:39:02.263541: step 34220, loss = 0.82 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 05:39:14.458339: step 34230, loss = 0.79 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 05:39:26.733951: step 34240, loss = 0.87 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 05:39:38.872441: step 34250, loss = 0.81 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 05:39:50.948614: step 34260, loss = 0.86 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 05:40:03.095841: step 34270, loss = 0.81 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 05:40:15.333852: step 34280, loss = 0.81 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 05:40:27.487670: step 34290, loss = 0.78 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 05:40:39.671443: step 34300, loss = 0.84 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 05:40:53.547294: step 34310, loss = 0.84 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 05:41:05.674412: step 34320, loss = 0.83 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 05:41:17.944591: step 34330, loss = 0.80 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 05:41:30.106223: step 34340, loss = 0.79 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 05:41:42.224910: step 34350, loss = 0.76 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-05-21 05:41:54.347088: step 34360, loss = 0.82 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 05:42:06.484844: step 34370, loss = 0.78 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 05:42:18.659359: step 34380, loss = 0.81 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 05:42:30.812593: step 34390, loss = 0.86 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 05:42:42.981327: step 34400, loss = 0.85 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 05:42:57.133572: step 34410, loss = 0.85 (23.5 examples/sec; 1.277 sec/batch)\n",
      "2019-05-21 05:43:09.461608: step 34420, loss = 0.79 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 05:43:21.695090: step 34430, loss = 0.75 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 05:43:33.830282: step 34440, loss = 0.81 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 05:43:46.015028: step 34450, loss = 0.87 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 05:43:58.156336: step 34460, loss = 0.78 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 05:44:10.332245: step 34470, loss = 0.86 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 05:44:22.484713: step 34480, loss = 0.80 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 05:44:34.553013: step 34490, loss = 0.80 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 05:44:46.696528: step 34500, loss = 0.80 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 05:45:00.477868: step 34510, loss = 0.80 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 05:45:12.711581: step 34520, loss = 0.83 (22.8 examples/sec; 1.319 sec/batch)\n",
      "2019-05-21 05:45:24.892381: step 34530, loss = 0.82 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 05:45:37.121623: step 34540, loss = 0.82 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 05:45:49.326395: step 34550, loss = 0.84 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 05:46:01.458539: step 34560, loss = 0.77 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 05:46:13.538736: step 34570, loss = 0.82 (24.8 examples/sec; 1.209 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 05:46:25.720071: step 34580, loss = 0.82 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 05:46:37.926743: step 34590, loss = 0.72 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 05:46:50.137414: step 34600, loss = 0.82 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 05:47:04.524933: step 34610, loss = 0.76 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 05:47:16.604242: step 34620, loss = 0.76 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 05:47:28.711868: step 34630, loss = 0.85 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 05:47:40.889974: step 34640, loss = 0.87 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 05:47:52.983726: step 34650, loss = 0.75 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 05:48:05.056106: step 34660, loss = 0.82 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 05:48:17.172590: step 34670, loss = 0.83 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 05:48:29.430212: step 34680, loss = 0.85 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 05:48:41.545618: step 34690, loss = 0.78 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 05:48:53.718478: step 34700, loss = 0.84 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 05:49:07.692726: step 34710, loss = 0.83 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 05:49:19.844900: step 34720, loss = 0.76 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 05:49:32.062948: step 34730, loss = 0.84 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 05:49:44.191477: step 34740, loss = 0.81 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 05:49:56.242015: step 34750, loss = 0.82 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 05:50:08.265341: step 34760, loss = 0.83 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 05:50:20.354593: step 34770, loss = 0.79 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 05:50:32.465576: step 34780, loss = 0.82 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 05:50:44.552761: step 34790, loss = 0.81 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 05:50:56.691878: step 34800, loss = 0.85 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 05:51:10.472077: step 34810, loss = 0.90 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 05:51:22.566737: step 34820, loss = 0.85 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 05:51:34.764879: step 34830, loss = 0.72 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 05:51:46.878119: step 34840, loss = 0.86 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 05:51:58.870136: step 34850, loss = 0.80 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 05:52:10.929957: step 34860, loss = 0.74 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 05:52:23.051996: step 34870, loss = 0.79 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 05:52:35.259450: step 34880, loss = 0.88 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 05:52:47.382836: step 34890, loss = 0.78 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 05:52:59.456956: step 34900, loss = 0.84 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 05:53:13.246002: step 34910, loss = 0.77 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 05:53:25.328425: step 34920, loss = 0.83 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 05:53:37.447050: step 34930, loss = 0.74 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 05:53:49.543922: step 34940, loss = 0.87 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 05:54:01.650187: step 34950, loss = 0.82 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 05:54:13.783226: step 34960, loss = 0.80 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 05:54:25.883443: step 34970, loss = 0.80 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 05:54:38.031921: step 34980, loss = 0.78 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 05:54:50.107073: step 34990, loss = 0.77 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 05:55:02.255769: step 35000, loss = 0.83 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 05:55:19.957131: step 35010, loss = 0.82 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 05:55:32.062160: step 35020, loss = 0.82 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 05:55:44.106950: step 35030, loss = 0.84 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 05:55:56.237765: step 35040, loss = 0.76 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 05:56:08.376284: step 35050, loss = 0.83 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 05:56:20.507202: step 35060, loss = 0.71 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 05:56:32.603866: step 35070, loss = 0.74 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 05:56:44.714012: step 35080, loss = 0.82 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 05:56:57.017617: step 35090, loss = 0.79 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 05:57:09.043999: step 35100, loss = 0.80 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 05:57:23.085760: step 35110, loss = 0.83 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 05:57:35.187824: step 35120, loss = 0.77 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 05:57:47.289090: step 35130, loss = 0.82 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 05:57:59.443928: step 35140, loss = 0.80 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 05:58:11.562114: step 35150, loss = 0.82 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 05:58:23.720450: step 35160, loss = 0.86 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 05:58:35.842248: step 35170, loss = 0.86 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 05:58:47.995692: step 35180, loss = 0.81 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 05:59:00.135180: step 35190, loss = 0.78 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 05:59:12.380229: step 35200, loss = 0.80 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 05:59:26.138813: step 35210, loss = 0.75 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 05:59:38.278397: step 35220, loss = 0.87 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 05:59:50.391976: step 35230, loss = 0.84 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 06:00:02.527564: step 35240, loss = 0.80 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 06:00:14.564082: step 35250, loss = 0.77 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 06:00:26.667927: step 35260, loss = 0.73 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 06:00:38.881041: step 35270, loss = 0.81 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 06:00:51.183983: step 35280, loss = 0.77 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 06:01:03.334765: step 35290, loss = 0.78 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 06:01:15.525957: step 35300, loss = 0.78 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 06:01:29.428608: step 35310, loss = 0.79 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 06:01:41.554348: step 35320, loss = 0.76 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 06:01:53.686018: step 35330, loss = 0.76 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 06:02:05.808820: step 35340, loss = 0.80 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 06:02:17.883735: step 35350, loss = 0.80 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 06:02:29.955599: step 35360, loss = 0.80 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 06:02:42.001611: step 35370, loss = 0.76 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 06:02:54.175000: step 35380, loss = 0.80 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 06:03:06.269885: step 35390, loss = 0.81 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 06:03:18.376567: step 35400, loss = 0.82 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 06:03:32.406883: step 35410, loss = 0.87 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-05-21 06:03:44.514533: step 35420, loss = 0.78 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 06:03:56.786241: step 35430, loss = 0.81 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 06:04:08.934610: step 35440, loss = 0.81 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 06:04:21.168215: step 35450, loss = 0.85 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 06:04:33.301726: step 35460, loss = 0.84 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 06:04:45.476315: step 35470, loss = 0.80 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 06:04:57.622022: step 35480, loss = 0.82 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 06:05:09.798380: step 35490, loss = 0.77 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 06:05:21.829597: step 35500, loss = 0.82 (24.6 examples/sec; 1.221 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 06:05:35.800636: step 35510, loss = 0.85 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 06:05:47.866978: step 35520, loss = 0.78 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 06:05:59.998126: step 35530, loss = 0.81 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 06:06:12.174288: step 35540, loss = 0.81 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 06:06:24.315575: step 35550, loss = 0.87 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 06:06:36.450419: step 35560, loss = 0.80 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 06:06:48.573065: step 35570, loss = 0.78 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 06:07:00.693789: step 35580, loss = 0.72 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 06:07:12.863160: step 35590, loss = 0.78 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 06:07:24.985226: step 35600, loss = 0.83 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 06:07:38.803065: step 35610, loss = 0.77 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 06:07:51.038350: step 35620, loss = 0.76 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-05-21 06:08:03.188789: step 35630, loss = 0.83 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 06:08:15.355911: step 35640, loss = 0.78 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 06:08:27.450673: step 35650, loss = 0.79 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 06:08:39.674888: step 35660, loss = 0.75 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 06:08:51.820362: step 35670, loss = 0.78 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 06:09:04.047182: step 35680, loss = 0.74 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 06:09:16.161217: step 35690, loss = 0.80 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 06:09:28.253803: step 35700, loss = 0.85 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 06:09:42.109284: step 35710, loss = 0.79 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 06:09:54.272274: step 35720, loss = 0.83 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 06:10:06.395467: step 35730, loss = 0.79 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 06:10:18.584125: step 35740, loss = 0.78 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 06:10:30.630547: step 35750, loss = 0.78 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 06:10:42.770518: step 35760, loss = 0.76 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 06:10:54.857156: step 35770, loss = 0.79 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 06:11:07.025347: step 35780, loss = 0.74 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 06:11:19.138090: step 35790, loss = 0.81 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 06:11:31.308786: step 35800, loss = 0.82 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 06:11:45.682347: step 35810, loss = 0.77 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 06:11:57.781707: step 35820, loss = 0.81 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 06:12:09.876844: step 35830, loss = 0.78 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 06:12:22.021956: step 35840, loss = 0.74 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 06:12:34.189638: step 35850, loss = 0.78 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 06:12:46.139043: step 35860, loss = 0.72 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 06:12:58.334314: step 35870, loss = 0.84 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 06:13:10.467177: step 35880, loss = 0.75 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 06:13:22.624461: step 35890, loss = 0.77 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 06:13:34.763553: step 35900, loss = 0.79 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 06:13:48.975820: step 35910, loss = 0.85 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 06:14:01.082853: step 35920, loss = 0.82 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 06:14:13.328221: step 35930, loss = 0.81 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 06:14:25.437314: step 35940, loss = 0.75 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 06:14:37.559009: step 35950, loss = 0.75 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 06:14:49.724913: step 35960, loss = 0.75 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 06:15:01.836153: step 35970, loss = 0.82 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 06:15:13.976725: step 35980, loss = 0.79 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 06:15:26.030667: step 35990, loss = 0.80 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 06:15:38.180256: step 36000, loss = 0.78 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 06:15:52.194176: step 36010, loss = 0.75 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 06:16:04.299645: step 36020, loss = 0.79 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 06:16:16.407975: step 36030, loss = 0.82 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 06:16:28.626655: step 36040, loss = 0.74 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 06:16:40.748698: step 36050, loss = 0.71 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 06:16:52.974980: step 36060, loss = 0.82 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 06:17:05.069988: step 36070, loss = 0.78 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 06:17:17.236963: step 36080, loss = 0.81 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 06:17:29.394306: step 36090, loss = 0.80 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 06:17:41.536743: step 36100, loss = 0.82 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 06:17:55.293650: step 36110, loss = 0.81 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 06:18:07.373538: step 36120, loss = 0.76 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 06:18:19.502842: step 36130, loss = 0.87 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 06:18:31.675886: step 36140, loss = 0.76 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 06:18:43.792733: step 36150, loss = 0.83 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 06:18:55.972431: step 36160, loss = 0.78 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 06:19:08.080940: step 36170, loss = 0.83 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 06:19:20.223941: step 36180, loss = 0.73 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 06:19:32.301314: step 36190, loss = 0.74 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 06:19:44.415941: step 36200, loss = 0.81 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 06:19:58.372753: step 36210, loss = 0.84 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 06:20:10.590079: step 36220, loss = 0.79 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 06:20:22.728385: step 36230, loss = 0.73 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 06:20:34.716240: step 36240, loss = 0.77 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 06:20:46.805124: step 36250, loss = 0.85 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 06:20:58.946046: step 36260, loss = 0.80 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 06:21:11.197260: step 36270, loss = 0.80 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-05-21 06:21:23.501571: step 36280, loss = 0.80 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 06:21:35.636658: step 36290, loss = 0.77 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 06:21:47.808587: step 36300, loss = 0.78 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 06:22:01.711819: step 36310, loss = 0.81 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 06:22:13.872056: step 36320, loss = 0.75 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 06:22:26.023540: step 36330, loss = 0.79 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 06:22:38.200989: step 36340, loss = 0.72 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 06:22:50.276096: step 36350, loss = 0.76 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 06:23:02.344329: step 36360, loss = 0.84 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 06:23:14.468273: step 36370, loss = 0.84 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 06:23:26.562383: step 36380, loss = 0.77 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 06:23:38.692171: step 36390, loss = 0.80 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 06:23:50.788592: step 36400, loss = 0.78 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 06:24:04.851235: step 36410, loss = 0.74 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 06:24:16.971603: step 36420, loss = 0.80 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 06:24:29.130496: step 36430, loss = 0.75 (24.7 examples/sec; 1.216 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 06:24:41.264508: step 36440, loss = 0.77 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 06:24:53.428294: step 36450, loss = 0.83 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 06:25:05.678914: step 36460, loss = 0.72 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 06:25:17.607113: step 36470, loss = 0.84 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 06:25:29.599658: step 36480, loss = 0.80 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 06:25:41.575012: step 36490, loss = 0.75 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 06:25:53.731153: step 36500, loss = 0.77 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 06:26:08.061586: step 36510, loss = 0.76 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 06:26:20.159741: step 36520, loss = 0.75 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 06:26:32.284847: step 36530, loss = 0.83 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 06:26:44.438890: step 36540, loss = 0.78 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 06:26:56.735906: step 36550, loss = 0.75 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 06:27:08.833822: step 36560, loss = 0.75 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 06:27:20.986745: step 36570, loss = 0.71 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 06:27:33.031192: step 36580, loss = 0.75 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 06:27:45.138158: step 36590, loss = 0.79 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 06:27:57.339678: step 36600, loss = 0.78 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 06:28:11.476886: step 36610, loss = 0.75 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-21 06:28:23.666552: step 36620, loss = 0.82 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 06:28:35.784835: step 36630, loss = 0.74 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 06:28:47.919959: step 36640, loss = 0.80 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 06:29:00.030159: step 36650, loss = 0.80 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 06:29:12.175123: step 36660, loss = 0.82 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 06:29:24.329008: step 36670, loss = 0.75 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 06:29:36.449500: step 36680, loss = 0.81 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 06:29:48.543464: step 36690, loss = 0.83 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 06:30:00.728532: step 36700, loss = 0.81 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 06:30:14.916142: step 36710, loss = 0.78 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 06:30:27.092929: step 36720, loss = 0.81 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 06:30:39.242232: step 36730, loss = 0.79 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 06:30:51.327446: step 36740, loss = 0.71 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 06:31:03.550612: step 36750, loss = 0.80 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 06:31:15.714494: step 36760, loss = 0.81 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 06:31:27.810576: step 36770, loss = 0.76 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 06:31:39.982414: step 36780, loss = 0.82 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 06:31:52.113264: step 36790, loss = 0.80 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 06:32:04.282346: step 36800, loss = 0.81 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 06:32:18.353739: step 36810, loss = 0.85 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 06:32:30.484228: step 36820, loss = 0.77 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 06:32:42.636954: step 36830, loss = 0.79 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 06:32:54.781195: step 36840, loss = 0.79 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 06:33:06.948034: step 36850, loss = 0.75 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 06:33:19.054034: step 36860, loss = 0.76 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 06:33:31.059789: step 36870, loss = 0.76 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 06:33:43.150584: step 36880, loss = 0.76 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 06:33:55.309789: step 36890, loss = 0.75 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 06:34:07.441502: step 36900, loss = 0.73 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 06:34:21.510902: step 36910, loss = 0.79 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 06:34:33.617461: step 36920, loss = 0.77 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 06:34:45.694518: step 36930, loss = 0.77 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 06:34:57.868488: step 36940, loss = 0.76 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 06:35:09.965681: step 36950, loss = 0.76 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 06:35:22.182747: step 36960, loss = 0.83 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-05-21 06:35:34.312174: step 36970, loss = 0.75 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 06:35:46.463962: step 36980, loss = 0.80 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 06:35:58.530144: step 36990, loss = 0.70 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 06:36:10.669954: step 37000, loss = 0.78 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 06:36:25.061180: step 37010, loss = 0.88 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 06:36:37.208397: step 37020, loss = 0.75 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 06:36:49.421399: step 37030, loss = 0.75 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 06:37:01.577860: step 37040, loss = 0.80 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 06:37:13.713666: step 37050, loss = 0.72 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 06:37:25.880781: step 37060, loss = 0.76 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 06:37:38.046915: step 37070, loss = 0.82 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 06:37:50.193946: step 37080, loss = 0.78 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 06:38:02.342338: step 37090, loss = 0.72 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 06:38:14.511351: step 37100, loss = 0.74 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 06:38:28.634160: step 37110, loss = 0.77 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 06:38:40.718710: step 37120, loss = 0.87 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 06:38:52.845756: step 37130, loss = 0.78 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 06:39:04.981549: step 37140, loss = 0.74 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 06:39:17.109364: step 37150, loss = 0.74 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 06:39:29.353587: step 37160, loss = 0.77 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 06:39:41.516239: step 37170, loss = 0.80 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 06:39:53.653726: step 37180, loss = 0.71 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 06:40:05.831803: step 37190, loss = 0.80 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 06:40:17.983086: step 37200, loss = 0.75 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 06:40:32.009690: step 37210, loss = 0.83 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 06:40:44.130203: step 37220, loss = 0.77 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 06:40:56.195485: step 37230, loss = 0.77 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 06:41:08.318829: step 37240, loss = 0.71 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 06:41:20.451692: step 37250, loss = 0.72 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 06:41:32.545255: step 37260, loss = 0.77 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 06:41:44.658564: step 37270, loss = 0.72 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 06:41:56.790836: step 37280, loss = 0.70 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 06:42:08.979370: step 37290, loss = 0.76 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 06:42:21.120163: step 37300, loss = 0.70 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 06:42:35.104340: step 37310, loss = 0.71 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 06:42:47.126466: step 37320, loss = 0.83 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 06:42:59.311978: step 37330, loss = 0.76 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 06:43:11.481693: step 37340, loss = 0.78 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 06:43:23.593776: step 37350, loss = 0.74 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 06:43:35.775357: step 37360, loss = 0.75 (25.0 examples/sec; 1.199 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 06:43:47.762593: step 37370, loss = 0.71 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 06:43:59.907735: step 37380, loss = 0.76 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 06:44:11.983019: step 37390, loss = 0.72 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 06:44:24.148977: step 37400, loss = 0.81 (23.4 examples/sec; 1.283 sec/batch)\n",
      "2019-05-21 06:44:38.306689: step 37410, loss = 0.79 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 06:44:50.376748: step 37420, loss = 0.73 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 06:45:02.430286: step 37430, loss = 0.87 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 06:45:14.481137: step 37440, loss = 0.73 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 06:45:26.580915: step 37450, loss = 0.83 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 06:45:38.652873: step 37460, loss = 0.79 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 06:45:50.741754: step 37470, loss = 0.77 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 06:46:02.757067: step 37480, loss = 0.77 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 06:46:14.800622: step 37490, loss = 0.84 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 06:46:26.937203: step 37500, loss = 0.68 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 06:46:40.825403: step 37510, loss = 0.68 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 06:46:52.914354: step 37520, loss = 0.78 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 06:47:05.063946: step 37530, loss = 0.83 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 06:47:17.176832: step 37540, loss = 0.80 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 06:47:29.293711: step 37550, loss = 0.76 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 06:47:41.400122: step 37560, loss = 0.78 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 06:47:53.458363: step 37570, loss = 0.76 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 06:48:05.514753: step 37580, loss = 0.91 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 06:48:17.622412: step 37590, loss = 0.82 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 06:48:29.689806: step 37600, loss = 0.79 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 06:48:43.725400: step 37610, loss = 0.77 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 06:48:55.838511: step 37620, loss = 0.80 (23.2 examples/sec; 1.296 sec/batch)\n",
      "2019-05-21 06:49:07.836906: step 37630, loss = 0.74 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 06:49:19.945640: step 37640, loss = 0.78 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 06:49:32.044487: step 37650, loss = 0.72 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 06:49:44.215356: step 37660, loss = 0.71 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 06:49:56.390823: step 37670, loss = 0.74 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 06:50:08.452310: step 37680, loss = 0.83 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 06:50:20.511471: step 37690, loss = 0.74 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 06:50:32.616281: step 37700, loss = 0.78 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 06:50:46.501723: step 37710, loss = 0.76 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 06:50:58.553885: step 37720, loss = 0.72 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 06:51:10.549436: step 37730, loss = 0.77 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 06:51:22.788251: step 37740, loss = 0.76 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 06:51:34.895828: step 37750, loss = 0.80 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 06:51:47.078110: step 37760, loss = 0.81 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 06:51:59.139100: step 37770, loss = 0.74 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 06:52:11.193424: step 37780, loss = 0.76 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 06:52:23.382198: step 37790, loss = 0.79 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 06:52:35.501375: step 37800, loss = 0.82 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 06:52:49.588388: step 37810, loss = 0.74 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 06:53:01.750532: step 37820, loss = 0.83 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 06:53:13.916655: step 37830, loss = 0.76 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 06:53:25.942016: step 37840, loss = 0.77 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 06:53:38.101796: step 37850, loss = 0.76 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 06:53:50.279132: step 37860, loss = 0.78 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 06:54:02.401750: step 37870, loss = 0.77 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 06:54:14.442554: step 37880, loss = 0.74 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 06:54:26.618144: step 37890, loss = 0.71 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 06:54:38.812885: step 37900, loss = 0.77 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 06:54:52.954921: step 37910, loss = 0.77 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 06:55:04.981884: step 37920, loss = 0.74 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 06:55:17.261138: step 37930, loss = 0.75 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-05-21 06:55:29.395073: step 37940, loss = 0.91 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 06:55:41.556348: step 37950, loss = 0.77 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 06:55:53.685975: step 37960, loss = 0.72 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 06:56:05.808322: step 37970, loss = 0.78 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 06:56:17.875561: step 37980, loss = 0.77 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 06:56:30.085538: step 37990, loss = 0.80 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 06:56:42.315461: step 38000, loss = 0.76 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 06:56:56.140227: step 38010, loss = 0.77 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 06:57:08.315488: step 38020, loss = 0.77 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 06:57:20.533269: step 38030, loss = 0.80 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 06:57:32.741867: step 38040, loss = 0.77 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 06:57:44.987948: step 38050, loss = 0.83 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 06:57:57.138429: step 38060, loss = 0.78 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 06:58:09.284489: step 38070, loss = 0.82 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 06:58:21.378717: step 38080, loss = 0.77 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 06:58:33.557328: step 38090, loss = 0.74 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 06:58:45.649376: step 38100, loss = 0.86 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 06:58:59.571703: step 38110, loss = 0.77 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 06:59:11.640881: step 38120, loss = 0.83 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 06:59:23.590664: step 38130, loss = 0.81 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 06:59:35.720560: step 38140, loss = 0.74 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 06:59:47.860969: step 38150, loss = 0.76 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 07:00:00.134680: step 38160, loss = 0.77 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 07:00:12.377362: step 38170, loss = 0.73 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 07:00:24.524529: step 38180, loss = 0.74 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 07:00:36.716103: step 38190, loss = 0.72 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 07:00:48.895421: step 38200, loss = 0.73 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 07:01:02.828713: step 38210, loss = 0.75 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 07:01:14.956651: step 38220, loss = 0.73 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 07:01:27.043420: step 38230, loss = 0.77 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 07:01:39.196559: step 38240, loss = 0.71 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 07:01:51.302733: step 38250, loss = 0.73 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 07:02:03.445163: step 38260, loss = 0.81 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 07:02:15.576107: step 38270, loss = 0.72 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 07:02:27.740563: step 38280, loss = 0.73 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 07:02:39.802492: step 38290, loss = 0.71 (25.1 examples/sec; 1.194 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 07:02:51.970002: step 38300, loss = 0.72 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 07:03:05.919321: step 38310, loss = 0.72 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 07:03:18.017956: step 38320, loss = 0.78 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 07:03:30.171018: step 38330, loss = 0.78 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 07:03:42.361725: step 38340, loss = 0.77 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 07:03:54.515754: step 38350, loss = 0.76 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 07:04:06.638347: step 38360, loss = 0.76 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 07:04:18.727903: step 38370, loss = 0.74 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 07:04:30.692011: step 38380, loss = 0.74 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 07:04:42.856718: step 38390, loss = 0.81 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 07:04:54.959377: step 38400, loss = 0.77 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 07:05:08.983732: step 38410, loss = 0.76 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 07:05:21.115470: step 38420, loss = 0.82 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 07:05:33.276074: step 38430, loss = 0.68 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 07:05:45.406212: step 38440, loss = 0.76 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 07:05:57.539618: step 38450, loss = 0.74 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 07:06:09.683370: step 38460, loss = 0.72 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 07:06:21.791771: step 38470, loss = 0.69 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 07:06:34.049109: step 38480, loss = 0.80 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 07:06:46.156422: step 38490, loss = 0.81 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 07:06:58.300462: step 38500, loss = 0.76 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 07:07:12.106856: step 38510, loss = 0.66 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 07:07:24.193342: step 38520, loss = 0.80 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 07:07:36.325387: step 38530, loss = 0.75 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 07:07:48.441854: step 38540, loss = 0.76 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 07:08:00.557647: step 38550, loss = 0.70 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 07:08:12.674512: step 38560, loss = 0.75 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 07:08:24.812326: step 38570, loss = 0.74 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 07:08:36.991236: step 38580, loss = 0.75 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 07:08:49.229655: step 38590, loss = 0.85 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 07:09:01.320211: step 38600, loss = 0.79 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 07:09:15.229186: step 38610, loss = 0.77 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 07:09:27.300819: step 38620, loss = 0.79 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 07:09:39.371372: step 38630, loss = 0.79 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 07:09:51.470527: step 38640, loss = 0.79 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 07:10:03.583576: step 38650, loss = 0.73 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 07:10:15.696890: step 38660, loss = 0.73 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 07:10:27.928982: step 38670, loss = 0.76 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 07:10:40.086261: step 38680, loss = 0.83 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 07:10:52.179005: step 38690, loss = 0.73 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 07:11:04.277647: step 38700, loss = 0.73 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 07:11:18.180679: step 38710, loss = 0.78 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 07:11:30.165849: step 38720, loss = 0.80 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 07:11:42.264653: step 38730, loss = 0.73 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 07:11:54.411185: step 38740, loss = 0.73 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 07:12:06.552459: step 38750, loss = 0.74 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 07:12:18.632967: step 38760, loss = 0.70 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 07:12:30.754301: step 38770, loss = 0.77 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 07:12:42.893960: step 38780, loss = 0.80 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 07:12:54.989768: step 38790, loss = 0.73 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 07:13:07.089833: step 38800, loss = 0.74 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 07:13:21.055449: step 38810, loss = 0.75 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 07:13:33.147725: step 38820, loss = 0.79 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 07:13:45.328517: step 38830, loss = 0.77 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 07:13:57.414360: step 38840, loss = 0.77 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 07:14:09.585998: step 38850, loss = 0.73 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 07:14:21.723746: step 38860, loss = 0.71 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 07:14:33.830557: step 38870, loss = 0.79 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 07:14:45.936145: step 38880, loss = 0.78 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 07:14:58.049210: step 38890, loss = 0.74 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 07:15:10.164492: step 38900, loss = 0.83 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 07:15:24.413217: step 38910, loss = 0.69 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 07:15:36.511286: step 38920, loss = 0.83 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 07:15:48.654455: step 38930, loss = 0.75 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 07:16:00.874472: step 38940, loss = 0.77 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-05-21 07:16:13.012566: step 38950, loss = 0.70 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 07:16:25.228473: step 38960, loss = 0.76 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 07:16:37.324279: step 38970, loss = 0.70 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 07:16:49.462220: step 38980, loss = 0.75 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 07:17:01.588591: step 38990, loss = 0.81 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 07:17:13.718924: step 39000, loss = 0.73 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 07:17:27.743887: step 39010, loss = 0.77 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 07:17:39.881894: step 39020, loss = 0.77 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 07:17:52.050903: step 39030, loss = 0.80 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 07:18:04.181226: step 39040, loss = 0.72 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 07:18:16.314203: step 39050, loss = 0.73 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 07:18:28.444648: step 39060, loss = 0.73 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 07:18:40.595347: step 39070, loss = 0.68 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 07:18:52.692237: step 39080, loss = 0.77 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 07:19:04.799296: step 39090, loss = 0.71 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 07:19:16.920729: step 39100, loss = 0.74 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 07:19:30.908035: step 39110, loss = 0.74 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 07:19:43.024365: step 39120, loss = 0.72 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 07:19:55.196157: step 39130, loss = 0.70 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 07:20:07.171827: step 39140, loss = 0.75 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 07:20:19.319742: step 39150, loss = 0.72 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 07:20:31.437089: step 39160, loss = 0.73 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 07:20:43.594685: step 39170, loss = 0.71 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 07:20:55.679762: step 39180, loss = 0.68 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 07:21:07.814514: step 39190, loss = 0.71 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 07:21:19.911214: step 39200, loss = 0.73 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 07:21:33.972427: step 39210, loss = 0.73 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 07:21:45.984340: step 39220, loss = 0.74 (24.9 examples/sec; 1.205 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 07:21:58.095653: step 39230, loss = 0.72 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 07:22:10.323235: step 39240, loss = 0.68 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 07:22:22.549913: step 39250, loss = 0.77 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 07:22:34.677006: step 39260, loss = 0.79 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 07:22:46.812205: step 39270, loss = 0.78 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 07:22:58.922306: step 39280, loss = 0.73 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 07:23:11.057934: step 39290, loss = 0.74 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 07:23:23.249637: step 39300, loss = 0.71 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 07:23:37.410916: step 39310, loss = 0.70 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 07:23:49.544274: step 39320, loss = 0.68 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 07:24:01.735327: step 39330, loss = 0.81 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 07:24:13.935891: step 39340, loss = 0.80 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 07:24:26.045422: step 39350, loss = 0.75 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 07:24:38.223491: step 39360, loss = 0.69 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 07:24:50.327223: step 39370, loss = 0.81 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 07:25:02.421188: step 39380, loss = 0.75 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 07:25:14.365359: step 39390, loss = 0.73 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 07:25:26.571580: step 39400, loss = 0.72 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 07:25:40.539807: step 39410, loss = 0.72 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 07:25:52.695605: step 39420, loss = 0.77 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 07:26:04.835569: step 39430, loss = 0.69 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 07:26:16.973040: step 39440, loss = 0.75 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 07:26:29.130662: step 39450, loss = 0.71 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 07:26:41.231251: step 39460, loss = 0.76 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 07:26:53.248270: step 39470, loss = 0.76 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 07:27:05.402286: step 39480, loss = 0.74 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 07:27:17.546001: step 39490, loss = 0.73 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 07:27:29.688306: step 39500, loss = 0.65 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 07:27:43.783299: step 39510, loss = 0.84 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 07:27:55.864109: step 39520, loss = 0.70 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 07:28:08.027863: step 39530, loss = 0.79 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 07:28:20.232967: step 39540, loss = 0.74 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-05-21 07:28:32.344972: step 39550, loss = 0.68 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 07:28:44.439443: step 39560, loss = 0.79 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 07:28:56.575814: step 39570, loss = 0.76 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 07:29:08.736296: step 39580, loss = 0.76 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 07:29:20.905965: step 39590, loss = 0.81 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 07:29:33.080393: step 39600, loss = 0.79 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 07:29:46.969545: step 39610, loss = 0.70 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 07:29:59.128037: step 39620, loss = 0.79 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 07:30:11.229378: step 39630, loss = 0.74 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 07:30:23.306342: step 39640, loss = 0.73 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 07:30:35.349827: step 39650, loss = 0.75 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 07:30:47.473858: step 39660, loss = 0.68 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 07:30:59.560946: step 39670, loss = 0.67 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 07:31:11.669680: step 39680, loss = 0.69 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 07:31:23.804834: step 39690, loss = 0.73 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 07:31:36.059150: step 39700, loss = 0.77 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 07:31:50.039012: step 39710, loss = 0.78 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 07:32:02.100040: step 39720, loss = 0.78 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 07:32:14.259158: step 39730, loss = 0.70 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 07:32:26.449704: step 39740, loss = 0.75 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 07:32:38.598202: step 39750, loss = 0.69 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 07:32:50.702766: step 39760, loss = 0.69 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 07:33:02.866509: step 39770, loss = 0.81 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 07:33:15.124010: step 39780, loss = 0.74 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 07:33:27.222052: step 39790, loss = 0.76 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 07:33:39.372823: step 39800, loss = 0.74 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 07:33:53.767590: step 39810, loss = 0.74 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 07:34:05.888220: step 39820, loss = 0.78 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 07:34:18.180848: step 39830, loss = 0.74 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 07:34:30.222925: step 39840, loss = 0.75 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 07:34:42.402990: step 39850, loss = 0.74 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 07:34:54.485129: step 39860, loss = 0.67 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 07:35:06.624797: step 39870, loss = 0.78 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 07:35:18.759772: step 39880, loss = 0.70 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 07:35:30.802628: step 39890, loss = 0.71 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 07:35:42.856630: step 39900, loss = 0.78 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 07:35:56.924130: step 39910, loss = 0.71 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 07:36:09.046295: step 39920, loss = 0.70 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 07:36:21.210539: step 39930, loss = 0.70 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 07:36:33.312938: step 39940, loss = 0.73 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 07:36:45.427074: step 39950, loss = 0.67 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 07:36:57.478100: step 39960, loss = 0.80 (25.6 examples/sec; 1.171 sec/batch)\n",
      "2019-05-21 07:37:09.619712: step 39970, loss = 0.71 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 07:37:21.731908: step 39980, loss = 0.74 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 07:37:33.876033: step 39990, loss = 0.75 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 07:37:45.989910: step 40000, loss = 0.67 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 07:38:03.365204: step 40010, loss = 0.78 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 07:38:15.586124: step 40020, loss = 0.65 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 07:38:27.666625: step 40030, loss = 0.77 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 07:38:39.851757: step 40040, loss = 0.71 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 07:38:51.979585: step 40050, loss = 0.72 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 07:39:04.111302: step 40060, loss = 0.76 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 07:39:16.335324: step 40070, loss = 0.78 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 07:39:28.486402: step 40080, loss = 0.73 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 07:39:40.627521: step 40090, loss = 0.75 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 07:39:52.773182: step 40100, loss = 0.76 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 07:40:06.666746: step 40110, loss = 0.74 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 07:40:18.816875: step 40120, loss = 0.74 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 07:40:30.902745: step 40130, loss = 0.74 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 07:40:43.077739: step 40140, loss = 0.80 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 07:40:55.133005: step 40150, loss = 0.75 (24.4 examples/sec; 1.230 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 07:41:07.257635: step 40160, loss = 0.73 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 07:41:19.432603: step 40170, loss = 0.74 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 07:41:31.480545: step 40180, loss = 0.69 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 07:41:43.631581: step 40190, loss = 0.73 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 07:41:55.720115: step 40200, loss = 0.70 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 07:42:09.875366: step 40210, loss = 0.68 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 07:42:21.968822: step 40220, loss = 0.68 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 07:42:34.124648: step 40230, loss = 0.68 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 07:42:46.284732: step 40240, loss = 0.73 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 07:42:58.552228: step 40250, loss = 0.72 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 07:43:10.661831: step 40260, loss = 0.71 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 07:43:22.750010: step 40270, loss = 0.71 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 07:43:34.919760: step 40280, loss = 0.73 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 07:43:47.072711: step 40290, loss = 0.81 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 07:43:59.237376: step 40300, loss = 0.71 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 07:44:13.170890: step 40310, loss = 0.75 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 07:44:25.256012: step 40320, loss = 0.75 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 07:44:37.360320: step 40330, loss = 0.73 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 07:44:49.497178: step 40340, loss = 0.78 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 07:45:01.654994: step 40350, loss = 0.76 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 07:45:13.865671: step 40360, loss = 0.78 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 07:45:26.049702: step 40370, loss = 0.71 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 07:45:38.157228: step 40380, loss = 0.69 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 07:45:50.304142: step 40390, loss = 0.72 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 07:46:02.338842: step 40400, loss = 0.78 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 07:46:16.664765: step 40410, loss = 0.70 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 07:46:28.800019: step 40420, loss = 0.75 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 07:46:40.912147: step 40430, loss = 0.76 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 07:46:53.047398: step 40440, loss = 0.73 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 07:47:05.157159: step 40450, loss = 0.73 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 07:47:17.229229: step 40460, loss = 0.70 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 07:47:29.318363: step 40470, loss = 0.70 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 07:47:41.454483: step 40480, loss = 0.68 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 07:47:53.605170: step 40490, loss = 0.75 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 07:48:05.756213: step 40500, loss = 0.66 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 07:48:19.820072: step 40510, loss = 0.72 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 07:48:31.978956: step 40520, loss = 0.69 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 07:48:44.090861: step 40530, loss = 0.69 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 07:48:56.290844: step 40540, loss = 0.73 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 07:49:08.438769: step 40550, loss = 0.71 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 07:49:20.591366: step 40560, loss = 0.71 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 07:49:32.755928: step 40570, loss = 0.73 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 07:49:44.882034: step 40580, loss = 0.73 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 07:49:57.056749: step 40590, loss = 0.73 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 07:50:09.306039: step 40600, loss = 0.75 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 07:50:23.222276: step 40610, loss = 0.75 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 07:50:35.307915: step 40620, loss = 0.79 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 07:50:47.484330: step 40630, loss = 0.76 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 07:50:59.701428: step 40640, loss = 0.71 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 07:51:11.788642: step 40650, loss = 0.71 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 07:51:23.921781: step 40660, loss = 0.70 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 07:51:36.074001: step 40670, loss = 0.71 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 07:51:48.217060: step 40680, loss = 0.76 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 07:52:00.339134: step 40690, loss = 0.65 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 07:52:12.453772: step 40700, loss = 0.78 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 07:52:26.825319: step 40710, loss = 0.71 (23.3 examples/sec; 1.285 sec/batch)\n",
      "2019-05-21 07:52:38.936896: step 40720, loss = 0.76 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 07:52:51.104359: step 40730, loss = 0.75 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 07:53:03.251323: step 40740, loss = 0.74 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 07:53:15.397227: step 40750, loss = 0.72 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 07:53:27.476701: step 40760, loss = 0.75 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 07:53:39.618478: step 40770, loss = 0.74 (23.3 examples/sec; 1.286 sec/batch)\n",
      "2019-05-21 07:53:51.780228: step 40780, loss = 0.74 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 07:54:03.991988: step 40790, loss = 0.74 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 07:54:16.108837: step 40800, loss = 0.74 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 07:54:30.010748: step 40810, loss = 0.67 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 07:54:42.124752: step 40820, loss = 0.71 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 07:54:54.309633: step 40830, loss = 0.64 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 07:55:06.389549: step 40840, loss = 0.74 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 07:55:18.522372: step 40850, loss = 0.71 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 07:55:30.706737: step 40860, loss = 0.74 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 07:55:42.685171: step 40870, loss = 0.80 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 07:55:54.636427: step 40880, loss = 0.72 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 07:56:06.596603: step 40890, loss = 0.73 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-21 07:56:18.590680: step 40900, loss = 0.78 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 07:56:32.455318: step 40910, loss = 0.70 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 07:56:44.489548: step 40920, loss = 0.73 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 07:56:56.539729: step 40930, loss = 0.72 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 07:57:08.532366: step 40940, loss = 0.74 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 07:57:20.509522: step 40950, loss = 0.74 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 07:57:32.501274: step 40960, loss = 0.66 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 07:57:44.542628: step 40970, loss = 0.74 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 07:57:56.643678: step 40980, loss = 0.71 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 07:58:08.713001: step 40990, loss = 0.78 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 07:58:20.698202: step 41000, loss = 0.73 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 07:58:34.977953: step 41010, loss = 0.77 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 07:58:47.013002: step 41020, loss = 0.68 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 07:58:58.931526: step 41030, loss = 0.75 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 07:59:10.881371: step 41040, loss = 0.77 (25.5 examples/sec; 1.177 sec/batch)\n",
      "2019-05-21 07:59:22.970688: step 41050, loss = 0.75 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 07:59:35.024653: step 41060, loss = 0.73 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 07:59:47.047262: step 41070, loss = 0.67 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 07:59:59.077891: step 41080, loss = 0.70 (25.3 examples/sec; 1.185 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 08:00:11.117923: step 41090, loss = 0.77 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 08:00:23.129998: step 41100, loss = 0.68 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 08:00:37.165951: step 41110, loss = 0.70 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 08:00:49.247881: step 41120, loss = 0.77 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 08:01:01.344317: step 41130, loss = 0.75 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 08:01:13.469413: step 41140, loss = 0.71 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 08:01:25.522943: step 41150, loss = 0.71 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 08:01:37.522405: step 41160, loss = 0.74 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 08:01:49.596421: step 41170, loss = 0.71 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 08:02:01.658458: step 41180, loss = 0.76 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 08:02:13.737402: step 41190, loss = 0.74 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 08:02:25.737977: step 41200, loss = 0.75 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 08:02:39.749964: step 41210, loss = 0.72 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 08:02:51.769991: step 41220, loss = 0.73 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 08:03:03.831093: step 41230, loss = 0.73 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 08:03:15.834228: step 41240, loss = 0.74 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 08:03:27.895234: step 41250, loss = 0.71 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 08:03:39.958189: step 41260, loss = 0.78 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 08:03:51.971649: step 41270, loss = 0.72 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 08:04:04.026117: step 41280, loss = 0.78 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 08:04:16.149862: step 41290, loss = 0.80 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 08:04:28.222399: step 41300, loss = 0.66 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 08:04:42.010734: step 41310, loss = 0.73 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 08:04:54.013878: step 41320, loss = 0.69 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 08:05:06.074374: step 41330, loss = 0.75 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 08:05:18.279939: step 41340, loss = 0.67 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 08:05:30.366701: step 41350, loss = 0.69 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 08:05:42.522904: step 41360, loss = 0.70 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 08:05:54.654679: step 41370, loss = 0.73 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 08:06:06.769877: step 41380, loss = 0.68 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 08:06:18.833151: step 41390, loss = 0.70 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 08:06:30.925601: step 41400, loss = 0.80 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 08:06:45.320801: step 41410, loss = 0.71 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 08:06:57.358104: step 41420, loss = 0.69 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-21 08:07:09.406567: step 41430, loss = 0.69 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 08:07:21.495200: step 41440, loss = 0.74 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 08:07:33.540241: step 41450, loss = 0.82 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 08:07:45.688016: step 41460, loss = 0.81 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 08:07:57.820676: step 41470, loss = 0.75 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 08:08:09.922099: step 41480, loss = 0.82 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 08:08:22.029152: step 41490, loss = 0.71 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 08:08:34.111852: step 41500, loss = 0.74 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 08:08:48.054005: step 41510, loss = 0.65 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 08:09:00.126464: step 41520, loss = 0.76 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 08:09:12.126571: step 41530, loss = 0.77 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 08:09:24.061601: step 41540, loss = 0.74 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 08:09:36.069339: step 41550, loss = 0.72 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 08:09:48.048362: step 41560, loss = 0.71 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 08:10:00.046956: step 41570, loss = 0.71 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 08:10:12.046827: step 41580, loss = 0.74 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 08:10:24.005056: step 41590, loss = 0.80 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 08:10:36.115607: step 41600, loss = 0.77 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 08:10:49.850851: step 41610, loss = 0.74 (25.5 examples/sec; 1.177 sec/batch)\n",
      "2019-05-21 08:11:01.883730: step 41620, loss = 0.72 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 08:11:13.949543: step 41630, loss = 0.74 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 08:11:26.029245: step 41640, loss = 0.70 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 08:11:38.016487: step 41650, loss = 0.70 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 08:11:50.119379: step 41660, loss = 0.75 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 08:12:02.129774: step 41670, loss = 0.69 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 08:12:14.162461: step 41680, loss = 0.72 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 08:12:26.148273: step 41690, loss = 0.73 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 08:12:38.185222: step 41700, loss = 0.71 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 08:12:52.221171: step 41710, loss = 0.71 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 08:13:04.288960: step 41720, loss = 0.72 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 08:13:16.323206: step 41730, loss = 0.74 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 08:13:28.351884: step 41740, loss = 0.75 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 08:13:40.407204: step 41750, loss = 0.72 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 08:13:52.495506: step 41760, loss = 0.66 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 08:14:04.554066: step 41770, loss = 0.67 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 08:14:16.611113: step 41780, loss = 0.73 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 08:14:28.697600: step 41790, loss = 0.68 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 08:14:40.769299: step 41800, loss = 0.72 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 08:14:54.577201: step 41810, loss = 0.71 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 08:15:06.626636: step 41820, loss = 0.78 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 08:15:18.683225: step 41830, loss = 0.65 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 08:15:30.782368: step 41840, loss = 0.77 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 08:15:42.890704: step 41850, loss = 0.68 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 08:15:55.050687: step 41860, loss = 0.66 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 08:16:07.269289: step 41870, loss = 0.67 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 08:16:19.397840: step 41880, loss = 0.67 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 08:16:31.530492: step 41890, loss = 0.63 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 08:16:43.606486: step 41900, loss = 0.76 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 08:16:57.439212: step 41910, loss = 0.68 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 08:17:09.458112: step 41920, loss = 0.64 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 08:17:21.617100: step 41930, loss = 0.75 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 08:17:33.692611: step 41940, loss = 0.71 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 08:17:45.717686: step 41950, loss = 0.71 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 08:17:57.862168: step 41960, loss = 0.67 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 08:18:09.965257: step 41970, loss = 0.74 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 08:18:22.072698: step 41980, loss = 0.76 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 08:18:34.249023: step 41990, loss = 0.70 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 08:18:46.349757: step 42000, loss = 0.76 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 08:19:00.120749: step 42010, loss = 0.65 (24.9 examples/sec; 1.203 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 08:19:12.116933: step 42020, loss = 0.81 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 08:19:24.185674: step 42030, loss = 0.65 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 08:19:36.308229: step 42040, loss = 0.69 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 08:19:48.354956: step 42050, loss = 0.69 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 08:20:00.450954: step 42060, loss = 0.72 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 08:20:12.522826: step 42070, loss = 0.76 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 08:20:24.607382: step 42080, loss = 0.78 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 08:20:36.673688: step 42090, loss = 0.71 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 08:20:48.855376: step 42100, loss = 0.74 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 08:21:02.889211: step 42110, loss = 0.68 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 08:21:15.047560: step 42120, loss = 0.71 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 08:21:27.156961: step 42130, loss = 0.65 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 08:21:39.294500: step 42140, loss = 0.81 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 08:21:51.368531: step 42150, loss = 0.73 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 08:22:03.463196: step 42160, loss = 0.73 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 08:22:15.414095: step 42170, loss = 0.72 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 08:22:27.542520: step 42180, loss = 0.78 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 08:22:39.704001: step 42190, loss = 0.73 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 08:22:51.894220: step 42200, loss = 0.70 (23.5 examples/sec; 1.275 sec/batch)\n",
      "2019-05-21 08:23:06.162149: step 42210, loss = 0.74 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 08:23:18.216766: step 42220, loss = 0.70 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 08:23:30.287009: step 42230, loss = 0.69 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 08:23:42.417420: step 42240, loss = 0.69 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 08:23:54.549323: step 42250, loss = 0.67 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 08:24:06.696809: step 42260, loss = 0.70 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 08:24:18.712831: step 42270, loss = 0.72 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 08:24:30.869269: step 42280, loss = 0.69 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 08:24:43.037269: step 42290, loss = 0.68 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 08:24:55.248623: step 42300, loss = 0.70 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 08:25:09.203334: step 42310, loss = 0.73 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 08:25:21.282440: step 42320, loss = 0.80 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 08:25:33.411715: step 42330, loss = 0.70 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 08:25:45.666209: step 42340, loss = 0.64 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 08:25:57.762816: step 42350, loss = 0.70 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 08:26:09.957807: step 42360, loss = 0.70 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 08:26:22.122742: step 42370, loss = 0.76 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 08:26:34.239898: step 42380, loss = 0.80 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 08:26:46.357035: step 42390, loss = 0.74 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 08:26:58.509038: step 42400, loss = 0.68 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 08:27:12.923329: step 42410, loss = 0.81 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 08:27:24.858829: step 42420, loss = 0.67 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 08:27:36.996396: step 42430, loss = 0.68 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 08:27:49.071284: step 42440, loss = 0.72 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 08:28:01.072534: step 42450, loss = 0.75 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 08:28:13.256760: step 42460, loss = 0.78 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 08:28:25.385537: step 42470, loss = 0.73 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 08:28:37.516701: step 42480, loss = 0.77 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 08:28:49.654978: step 42490, loss = 0.66 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 08:29:01.750300: step 42500, loss = 0.72 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 08:29:15.695149: step 42510, loss = 0.63 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 08:29:27.807634: step 42520, loss = 0.70 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 08:29:39.805910: step 42530, loss = 0.68 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 08:29:51.910406: step 42540, loss = 0.65 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 08:30:04.099374: step 42550, loss = 0.68 (23.1 examples/sec; 1.299 sec/batch)\n",
      "2019-05-21 08:30:16.235556: step 42560, loss = 0.69 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 08:30:28.416600: step 42570, loss = 0.77 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 08:30:40.474049: step 42580, loss = 0.73 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 08:30:52.566527: step 42590, loss = 0.79 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 08:31:04.706150: step 42600, loss = 0.67 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 08:31:18.676842: step 42610, loss = 0.66 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 08:31:30.636227: step 42620, loss = 0.69 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 08:31:42.597224: step 42630, loss = 0.69 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 08:31:54.627036: step 42640, loss = 0.80 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 08:32:06.609224: step 42650, loss = 0.72 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 08:32:18.689309: step 42660, loss = 0.72 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 08:32:30.697853: step 42670, loss = 0.71 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 08:32:42.721298: step 42680, loss = 0.73 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 08:32:54.756187: step 42690, loss = 0.72 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 08:33:06.779297: step 42700, loss = 0.73 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 08:33:20.580319: step 42710, loss = 0.78 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 08:33:32.581136: step 42720, loss = 0.73 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 08:33:44.578348: step 42730, loss = 0.73 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 08:33:56.634059: step 42740, loss = 0.63 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 08:34:08.652630: step 42750, loss = 0.68 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 08:34:20.682711: step 42760, loss = 0.68 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 08:34:32.757105: step 42770, loss = 0.70 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 08:34:44.832299: step 42780, loss = 0.63 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 08:34:56.924573: step 42790, loss = 0.71 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 08:35:08.976214: step 42800, loss = 0.70 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 08:35:22.924984: step 42810, loss = 0.68 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 08:35:35.003901: step 42820, loss = 0.71 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 08:35:47.180739: step 42830, loss = 0.71 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 08:35:59.298336: step 42840, loss = 0.70 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 08:36:11.440693: step 42850, loss = 0.80 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 08:36:23.612530: step 42860, loss = 0.68 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 08:36:35.699424: step 42870, loss = 0.66 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 08:36:47.825060: step 42880, loss = 0.75 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 08:36:59.873146: step 42890, loss = 0.68 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 08:37:12.115664: step 42900, loss = 0.67 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 08:37:26.065136: step 42910, loss = 0.68 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 08:37:38.123273: step 42920, loss = 0.73 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 08:37:50.163295: step 42930, loss = 0.75 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 08:38:02.240958: step 42940, loss = 0.73 (24.7 examples/sec; 1.216 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 08:38:14.281968: step 42950, loss = 0.77 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 08:38:26.368076: step 42960, loss = 0.73 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 08:38:38.474398: step 42970, loss = 0.71 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 08:38:50.526233: step 42980, loss = 0.75 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 08:39:02.617824: step 42990, loss = 0.73 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 08:39:14.806753: step 43000, loss = 0.66 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 08:39:28.685377: step 43010, loss = 0.71 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 08:39:40.750384: step 43020, loss = 0.70 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 08:39:52.844796: step 43030, loss = 0.72 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 08:40:04.989169: step 43040, loss = 0.68 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 08:40:17.055024: step 43050, loss = 0.66 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 08:40:29.160543: step 43060, loss = 0.72 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 08:40:41.229591: step 43070, loss = 0.68 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 08:40:53.287846: step 43080, loss = 0.75 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 08:41:05.368009: step 43090, loss = 0.68 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 08:41:17.466951: step 43100, loss = 0.68 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 08:41:31.837394: step 43110, loss = 0.71 (23.3 examples/sec; 1.288 sec/batch)\n",
      "2019-05-21 08:41:44.048143: step 43120, loss = 0.70 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 08:41:56.179257: step 43130, loss = 0.71 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 08:42:08.255812: step 43140, loss = 0.72 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 08:42:20.348524: step 43150, loss = 0.70 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 08:42:32.502354: step 43160, loss = 0.72 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 08:42:44.650307: step 43170, loss = 0.68 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 08:42:56.637570: step 43180, loss = 0.76 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 08:43:08.712582: step 43190, loss = 0.73 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 08:43:20.742153: step 43200, loss = 0.76 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 08:43:35.069873: step 43210, loss = 0.62 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 08:43:47.153428: step 43220, loss = 0.68 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 08:43:59.295335: step 43230, loss = 0.70 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 08:44:11.437527: step 43240, loss = 0.66 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 08:44:23.541997: step 43250, loss = 0.67 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 08:44:35.672908: step 43260, loss = 0.73 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 08:44:47.859659: step 43270, loss = 0.63 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 08:44:59.921558: step 43280, loss = 0.65 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 08:45:12.196324: step 43290, loss = 0.71 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 08:45:24.335075: step 43300, loss = 0.63 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 08:45:38.262437: step 43310, loss = 0.71 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 08:45:50.344666: step 43320, loss = 0.65 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 08:46:02.575411: step 43330, loss = 0.69 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 08:46:14.708705: step 43340, loss = 0.65 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 08:46:26.838134: step 43350, loss = 0.68 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 08:46:38.987749: step 43360, loss = 0.72 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 08:46:51.100342: step 43370, loss = 0.80 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 08:47:03.195064: step 43380, loss = 0.70 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 08:47:15.336203: step 43390, loss = 0.75 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 08:47:27.440454: step 43400, loss = 0.69 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 08:47:41.680161: step 43410, loss = 0.76 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 08:47:53.786835: step 43420, loss = 0.70 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 08:48:05.833812: step 43430, loss = 0.73 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 08:48:17.910197: step 43440, loss = 0.70 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 08:48:30.033024: step 43450, loss = 0.70 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 08:48:42.153734: step 43460, loss = 0.72 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 08:48:54.232735: step 43470, loss = 0.66 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 08:49:06.345445: step 43480, loss = 0.79 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 08:49:18.510159: step 43490, loss = 0.73 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 08:49:30.728177: step 43500, loss = 0.68 (23.2 examples/sec; 1.293 sec/batch)\n",
      "2019-05-21 08:49:44.702014: step 43510, loss = 0.69 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 08:49:56.877320: step 43520, loss = 0.65 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 08:50:09.078648: step 43530, loss = 0.66 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 08:50:21.192002: step 43540, loss = 0.67 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 08:50:33.285443: step 43550, loss = 0.72 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 08:50:45.626219: step 43560, loss = 0.71 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 08:50:57.860334: step 43570, loss = 0.61 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 08:51:10.019774: step 43580, loss = 0.63 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 08:51:22.148505: step 43590, loss = 0.72 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 08:51:34.387454: step 43600, loss = 0.75 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 08:51:48.264083: step 43610, loss = 0.65 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 08:52:00.398138: step 43620, loss = 0.70 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 08:52:12.563384: step 43630, loss = 0.68 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 08:52:24.698855: step 43640, loss = 0.72 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 08:52:36.793212: step 43650, loss = 0.67 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 08:52:48.981659: step 43660, loss = 0.68 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 08:53:01.152320: step 43670, loss = 0.73 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 08:53:13.290840: step 43680, loss = 0.66 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 08:53:25.271754: step 43690, loss = 0.75 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 08:53:37.364702: step 43700, loss = 0.69 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 08:53:51.296425: step 43710, loss = 0.65 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 08:54:03.443901: step 43720, loss = 0.74 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 08:54:15.605270: step 43730, loss = 0.64 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 08:54:27.762708: step 43740, loss = 0.69 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 08:54:40.017032: step 43750, loss = 0.67 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 08:54:52.220953: step 43760, loss = 0.69 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 08:55:04.385372: step 43770, loss = 0.69 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 08:55:16.545009: step 43780, loss = 0.66 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 08:55:28.641868: step 43790, loss = 0.68 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 08:55:40.769087: step 43800, loss = 0.75 (23.2 examples/sec; 1.294 sec/batch)\n",
      "2019-05-21 08:55:54.884703: step 43810, loss = 0.66 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 08:56:06.974690: step 43820, loss = 0.63 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 08:56:19.010478: step 43830, loss = 0.64 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 08:56:31.126566: step 43840, loss = 0.69 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 08:56:43.147299: step 43850, loss = 0.72 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 08:56:55.201450: step 43860, loss = 0.66 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 08:57:07.245984: step 43870, loss = 0.68 (24.9 examples/sec; 1.206 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 08:57:19.365559: step 43880, loss = 0.66 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 08:57:31.462478: step 43890, loss = 0.71 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 08:57:43.648146: step 43900, loss = 0.69 (23.3 examples/sec; 1.290 sec/batch)\n",
      "2019-05-21 08:57:57.724248: step 43910, loss = 0.69 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 08:58:09.811741: step 43920, loss = 0.66 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 08:58:21.877858: step 43930, loss = 0.69 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 08:58:33.962797: step 43940, loss = 0.70 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 08:58:46.043114: step 43950, loss = 0.68 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 08:58:58.090082: step 43960, loss = 0.67 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 08:59:10.215438: step 43970, loss = 0.69 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 08:59:22.318429: step 43980, loss = 0.73 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 08:59:34.438717: step 43990, loss = 0.72 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 08:59:46.571504: step 44000, loss = 0.74 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 09:00:00.720410: step 44010, loss = 0.74 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 09:00:12.862623: step 44020, loss = 0.67 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 09:00:24.958846: step 44030, loss = 0.68 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 09:00:37.048536: step 44040, loss = 0.72 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 09:00:49.167088: step 44050, loss = 0.67 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 09:01:01.227483: step 44060, loss = 0.68 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 09:01:13.297668: step 44070, loss = 0.67 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 09:01:25.381905: step 44080, loss = 0.69 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 09:01:37.496187: step 44090, loss = 0.72 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 09:01:49.611719: step 44100, loss = 0.74 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 09:02:03.487950: step 44110, loss = 0.73 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 09:02:15.600070: step 44120, loss = 0.72 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 09:02:27.695846: step 44130, loss = 0.71 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 09:02:39.904311: step 44140, loss = 0.79 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 09:02:52.017385: step 44150, loss = 0.66 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 09:03:04.131743: step 44160, loss = 0.67 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 09:03:16.249175: step 44170, loss = 0.67 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 09:03:28.352507: step 44180, loss = 0.63 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 09:03:40.341750: step 44190, loss = 0.82 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 09:03:52.478731: step 44200, loss = 0.68 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 09:04:06.391811: step 44210, loss = 0.72 (23.1 examples/sec; 1.296 sec/batch)\n",
      "2019-05-21 09:04:18.471723: step 44220, loss = 0.68 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 09:04:30.555337: step 44230, loss = 0.66 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 09:04:42.678061: step 44240, loss = 0.79 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 09:04:54.812374: step 44250, loss = 0.67 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 09:05:06.867206: step 44260, loss = 0.70 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 09:05:18.930795: step 44270, loss = 0.65 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 09:05:30.999881: step 44280, loss = 0.64 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 09:05:43.126482: step 44290, loss = 0.77 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 09:05:55.204648: step 44300, loss = 0.64 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 09:06:09.133587: step 44310, loss = 0.73 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 09:06:21.286385: step 44320, loss = 0.65 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 09:06:33.223253: step 44330, loss = 0.71 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 09:06:45.277646: step 44340, loss = 0.65 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 09:06:57.303946: step 44350, loss = 0.68 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 09:07:09.434436: step 44360, loss = 0.71 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 09:07:21.518883: step 44370, loss = 0.72 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 09:07:33.596444: step 44380, loss = 0.73 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 09:07:45.711028: step 44390, loss = 0.68 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 09:07:57.839865: step 44400, loss = 0.70 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 09:08:11.699468: step 44410, loss = 0.69 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 09:08:23.794488: step 44420, loss = 0.69 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 09:08:35.891961: step 44430, loss = 0.68 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 09:08:47.912814: step 44440, loss = 0.69 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 09:09:00.015470: step 44450, loss = 0.64 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 09:09:12.153425: step 44460, loss = 0.71 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 09:09:24.365172: step 44470, loss = 0.63 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 09:09:36.450705: step 44480, loss = 0.74 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 09:09:48.617497: step 44490, loss = 0.64 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 09:10:00.839622: step 44500, loss = 0.70 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 09:10:14.610022: step 44510, loss = 0.78 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 09:10:26.751694: step 44520, loss = 0.68 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 09:10:38.882179: step 44530, loss = 0.63 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 09:10:51.147207: step 44540, loss = 0.74 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 09:11:03.343722: step 44550, loss = 0.70 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 09:11:15.493851: step 44560, loss = 0.70 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 09:11:27.684902: step 44570, loss = 0.69 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 09:11:39.808864: step 44580, loss = 0.67 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 09:11:51.919663: step 44590, loss = 0.68 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 09:12:04.035888: step 44600, loss = 0.67 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 09:12:17.938317: step 44610, loss = 0.76 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 09:12:30.015998: step 44620, loss = 0.64 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 09:12:42.194823: step 44630, loss = 0.68 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 09:12:54.360072: step 44640, loss = 0.74 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 09:13:06.497758: step 44650, loss = 0.72 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 09:13:18.711526: step 44660, loss = 0.66 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 09:13:30.840473: step 44670, loss = 0.67 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 09:13:42.965281: step 44680, loss = 0.76 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 09:13:54.918649: step 44690, loss = 0.67 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 09:14:06.965566: step 44700, loss = 0.66 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 09:14:21.302962: step 44710, loss = 0.65 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 09:14:33.418834: step 44720, loss = 0.69 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 09:14:45.560410: step 44730, loss = 0.76 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 09:14:57.783689: step 44740, loss = 0.68 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 09:15:09.875364: step 44750, loss = 0.70 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 09:15:21.977025: step 44760, loss = 0.72 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 09:15:34.161123: step 44770, loss = 0.77 (23.2 examples/sec; 1.295 sec/batch)\n",
      "2019-05-21 09:15:46.242169: step 44780, loss = 0.71 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 09:15:58.291190: step 44790, loss = 0.64 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 09:16:10.420458: step 44800, loss = 0.65 (24.9 examples/sec; 1.206 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 09:16:24.311021: step 44810, loss = 0.66 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 09:16:36.349692: step 44820, loss = 0.66 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 09:16:48.449226: step 44830, loss = 0.72 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 09:17:00.602802: step 44840, loss = 0.76 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 09:17:12.700990: step 44850, loss = 0.68 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 09:17:24.763847: step 44860, loss = 0.69 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 09:17:37.025847: step 44870, loss = 0.64 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 09:17:49.128126: step 44880, loss = 0.63 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 09:18:01.277729: step 44890, loss = 0.70 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 09:18:13.445525: step 44900, loss = 0.64 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 09:18:27.361462: step 44910, loss = 0.68 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 09:18:39.465656: step 44920, loss = 0.66 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 09:18:51.580189: step 44930, loss = 0.70 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 09:19:03.593574: step 44940, loss = 0.68 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 09:19:15.714477: step 44950, loss = 0.69 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 09:19:27.843260: step 44960, loss = 0.67 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 09:19:40.048516: step 44970, loss = 0.64 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 09:19:52.168979: step 44980, loss = 0.79 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 09:20:04.282442: step 44990, loss = 0.73 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 09:20:16.425755: step 45000, loss = 0.69 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 09:20:34.034405: step 45010, loss = 0.73 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 09:20:46.183981: step 45020, loss = 0.73 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 09:20:58.244635: step 45030, loss = 0.74 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 09:21:10.428952: step 45040, loss = 0.64 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 09:21:22.640429: step 45050, loss = 0.68 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 09:21:34.784855: step 45060, loss = 0.67 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 09:21:46.884089: step 45070, loss = 0.61 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 09:21:59.044724: step 45080, loss = 0.63 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 09:22:11.213653: step 45090, loss = 0.63 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 09:22:23.304301: step 45100, loss = 0.72 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 09:22:37.197186: step 45110, loss = 0.64 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 09:22:49.336383: step 45120, loss = 0.68 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 09:23:01.493458: step 45130, loss = 0.65 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 09:23:13.715832: step 45140, loss = 0.70 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 09:23:25.902465: step 45150, loss = 0.68 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 09:23:38.054574: step 45160, loss = 0.71 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 09:23:50.278770: step 45170, loss = 0.67 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 09:24:02.377432: step 45180, loss = 0.72 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 09:24:14.550224: step 45190, loss = 0.72 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 09:24:26.575347: step 45200, loss = 0.70 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 09:24:40.508409: step 45210, loss = 0.73 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 09:24:52.574992: step 45220, loss = 0.64 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 09:25:04.659457: step 45230, loss = 0.69 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 09:25:16.782372: step 45240, loss = 0.62 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 09:25:28.885176: step 45250, loss = 0.66 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 09:25:41.112604: step 45260, loss = 0.64 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 09:25:53.230002: step 45270, loss = 0.73 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 09:26:05.381712: step 45280, loss = 0.73 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 09:26:17.590795: step 45290, loss = 0.72 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 09:26:29.796057: step 45300, loss = 0.69 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 09:26:43.803833: step 45310, loss = 0.77 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 09:26:55.943473: step 45320, loss = 0.67 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 09:27:08.074957: step 45330, loss = 0.67 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 09:27:20.199070: step 45340, loss = 0.68 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 09:27:32.348140: step 45350, loss = 0.69 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 09:27:44.494755: step 45360, loss = 0.67 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 09:27:56.600131: step 45370, loss = 0.67 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 09:28:08.726836: step 45380, loss = 0.69 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 09:28:20.911136: step 45390, loss = 0.70 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 09:28:33.046813: step 45400, loss = 0.64 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 09:28:47.174864: step 45410, loss = 0.68 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 09:28:59.323185: step 45420, loss = 0.71 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 09:29:11.462505: step 45430, loss = 0.70 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 09:29:23.646341: step 45440, loss = 0.69 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 09:29:35.609346: step 45450, loss = 0.67 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 09:29:47.698314: step 45460, loss = 0.69 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 09:29:59.687993: step 45470, loss = 0.75 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 09:30:11.707483: step 45480, loss = 0.65 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 09:30:23.819944: step 45490, loss = 0.73 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 09:30:35.924855: step 45500, loss = 0.67 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 09:30:50.186925: step 45510, loss = 0.68 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 09:31:02.391320: step 45520, loss = 0.73 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 09:31:14.499513: step 45530, loss = 0.69 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 09:31:26.624484: step 45540, loss = 0.67 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 09:31:38.747777: step 45550, loss = 0.65 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 09:31:50.887720: step 45560, loss = 0.71 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 09:32:02.989603: step 45570, loss = 0.77 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 09:32:15.074995: step 45580, loss = 0.68 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 09:32:27.215426: step 45590, loss = 0.66 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 09:32:39.291505: step 45600, loss = 0.76 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 09:32:53.192309: step 45610, loss = 0.71 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 09:33:05.293789: step 45620, loss = 0.75 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 09:33:17.434975: step 45630, loss = 0.72 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 09:33:29.545468: step 45640, loss = 0.75 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 09:33:41.666705: step 45650, loss = 0.66 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 09:33:53.841578: step 45660, loss = 0.67 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 09:34:05.986900: step 45670, loss = 0.70 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 09:34:18.011410: step 45680, loss = 0.72 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 09:34:30.105584: step 45690, loss = 0.68 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 09:34:42.074806: step 45700, loss = 0.66 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 09:34:56.223416: step 45710, loss = 0.66 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 09:35:08.311016: step 45720, loss = 0.69 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 09:35:20.402065: step 45730, loss = 0.63 (24.9 examples/sec; 1.204 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 09:35:32.523060: step 45740, loss = 0.71 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 09:35:44.661392: step 45750, loss = 0.66 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 09:35:56.762168: step 45760, loss = 0.69 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 09:36:08.875593: step 45770, loss = 0.64 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 09:36:20.994104: step 45780, loss = 0.71 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 09:36:33.097538: step 45790, loss = 0.67 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 09:36:45.254449: step 45800, loss = 0.66 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 09:36:59.558061: step 45810, loss = 0.68 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 09:37:11.675713: step 45820, loss = 0.69 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 09:37:23.815695: step 45830, loss = 0.62 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 09:37:35.902322: step 45840, loss = 0.69 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 09:37:48.039773: step 45850, loss = 0.71 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 09:38:00.186350: step 45860, loss = 0.64 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 09:38:12.314212: step 45870, loss = 0.66 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 09:38:24.475937: step 45880, loss = 0.64 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 09:38:36.627318: step 45890, loss = 0.67 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 09:38:48.833515: step 45900, loss = 0.65 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 09:39:03.255489: step 45910, loss = 0.65 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 09:39:15.325555: step 45920, loss = 0.61 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 09:39:27.476253: step 45930, loss = 0.66 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 09:39:39.648977: step 45940, loss = 0.72 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 09:39:51.674171: step 45950, loss = 0.78 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 09:40:03.763954: step 45960, loss = 0.69 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 09:40:15.910804: step 45970, loss = 0.66 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 09:40:28.185917: step 45980, loss = 0.65 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 09:40:40.335209: step 45990, loss = 0.69 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 09:40:52.506140: step 46000, loss = 0.66 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 09:41:06.469327: step 46010, loss = 0.68 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 09:41:18.605448: step 46020, loss = 0.68 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 09:41:30.766754: step 46030, loss = 0.69 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 09:41:42.933703: step 46040, loss = 0.65 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 09:41:55.090367: step 46050, loss = 0.68 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 09:42:07.190423: step 46060, loss = 0.72 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 09:42:19.312250: step 46070, loss = 0.66 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 09:42:31.575672: step 46080, loss = 0.65 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 09:42:43.730807: step 46090, loss = 0.67 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 09:42:55.894831: step 46100, loss = 0.63 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 09:43:10.045825: step 46110, loss = 0.69 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 09:43:22.213356: step 46120, loss = 0.69 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 09:43:34.311127: step 46130, loss = 0.67 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 09:43:46.428294: step 46140, loss = 0.67 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 09:43:58.574590: step 46150, loss = 0.76 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 09:44:10.658762: step 46160, loss = 0.65 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 09:44:22.716754: step 46170, loss = 0.68 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 09:44:34.749916: step 46180, loss = 0.63 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 09:44:46.896456: step 46190, loss = 0.69 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 09:44:59.063731: step 46200, loss = 0.67 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 09:45:13.184948: step 46210, loss = 0.62 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 09:45:25.296532: step 46220, loss = 0.67 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 09:45:37.499567: step 46230, loss = 0.67 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 09:45:49.671204: step 46240, loss = 0.68 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 09:46:01.825696: step 46250, loss = 0.70 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 09:46:13.959563: step 46260, loss = 0.68 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 09:46:26.142365: step 46270, loss = 0.69 (23.1 examples/sec; 1.296 sec/batch)\n",
      "2019-05-21 09:46:38.248838: step 46280, loss = 0.66 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 09:46:50.375372: step 46290, loss = 0.66 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 09:47:02.477787: step 46300, loss = 0.66 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 09:47:16.513789: step 46310, loss = 0.70 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 09:47:28.600114: step 46320, loss = 0.70 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 09:47:40.668383: step 46330, loss = 0.71 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 09:47:52.780392: step 46340, loss = 0.62 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 09:48:05.010619: step 46350, loss = 0.69 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 09:48:17.116148: step 46360, loss = 0.73 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 09:48:29.201228: step 46370, loss = 0.65 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 09:48:41.312827: step 46380, loss = 0.63 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 09:48:53.472381: step 46390, loss = 0.73 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 09:49:05.650992: step 46400, loss = 0.65 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 09:49:19.628180: step 46410, loss = 0.61 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 09:49:31.692937: step 46420, loss = 0.71 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 09:49:43.885108: step 46430, loss = 0.70 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 09:49:56.074049: step 46440, loss = 0.67 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 09:50:08.240292: step 46450, loss = 0.66 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 09:50:20.210063: step 46460, loss = 0.76 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 09:50:32.364463: step 46470, loss = 0.73 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 09:50:44.491400: step 46480, loss = 0.61 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 09:50:56.610221: step 46490, loss = 0.73 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 09:51:08.721691: step 46500, loss = 0.74 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 09:51:22.557623: step 46510, loss = 0.66 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 09:51:34.803591: step 46520, loss = 0.74 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 09:51:46.942737: step 46530, loss = 0.66 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 09:51:59.124549: step 46540, loss = 0.65 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 09:52:11.299039: step 46550, loss = 0.66 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 09:52:23.453115: step 46560, loss = 0.66 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 09:52:35.669552: step 46570, loss = 0.68 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 09:52:47.797598: step 46580, loss = 0.65 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 09:52:59.967277: step 46590, loss = 0.68 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 09:53:12.129607: step 46600, loss = 0.71 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 09:53:26.173958: step 46610, loss = 0.66 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 09:53:38.336273: step 46620, loss = 0.65 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 09:53:50.458922: step 46630, loss = 0.68 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 09:54:02.643251: step 46640, loss = 0.70 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 09:54:14.761072: step 46650, loss = 0.70 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 09:54:26.885994: step 46660, loss = 0.69 (24.8 examples/sec; 1.207 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 09:54:38.956215: step 46670, loss = 0.65 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 09:54:51.089500: step 46680, loss = 0.66 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 09:55:03.265503: step 46690, loss = 0.78 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 09:55:15.328034: step 46700, loss = 0.69 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 09:55:29.152722: step 46710, loss = 0.61 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 09:55:41.314258: step 46720, loss = 0.67 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 09:55:53.551967: step 46730, loss = 0.63 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 09:56:05.673756: step 46740, loss = 0.65 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 09:56:17.867949: step 46750, loss = 0.75 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 09:56:30.054181: step 46760, loss = 0.64 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 09:56:42.212445: step 46770, loss = 0.69 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 09:56:54.365322: step 46780, loss = 0.70 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 09:57:06.588608: step 46790, loss = 0.64 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 09:57:18.650242: step 46800, loss = 0.58 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 09:57:32.460779: step 46810, loss = 0.67 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 09:57:44.558063: step 46820, loss = 0.68 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 09:57:56.696678: step 46830, loss = 0.61 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 09:58:08.816936: step 46840, loss = 0.67 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 09:58:20.921844: step 46850, loss = 0.67 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 09:58:33.019243: step 46860, loss = 0.68 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 09:58:45.173607: step 46870, loss = 0.67 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 09:58:57.214745: step 46880, loss = 0.75 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 09:59:09.333371: step 46890, loss = 0.61 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 09:59:21.433531: step 46900, loss = 0.64 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 09:59:35.378900: step 46910, loss = 0.73 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 09:59:47.409895: step 46920, loss = 0.60 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 09:59:59.536622: step 46930, loss = 0.67 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 10:00:11.679739: step 46940, loss = 0.70 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 10:00:23.794488: step 46950, loss = 0.61 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 10:00:35.822732: step 46960, loss = 0.66 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 10:00:47.990281: step 46970, loss = 0.65 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 10:01:00.165679: step 46980, loss = 0.66 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 10:01:12.267102: step 46990, loss = 0.61 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 10:01:24.374243: step 47000, loss = 0.67 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 10:01:38.427217: step 47010, loss = 0.75 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 10:01:50.577110: step 47020, loss = 0.69 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 10:02:02.726311: step 47030, loss = 0.66 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 10:02:14.847815: step 47040, loss = 0.64 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 10:02:26.998376: step 47050, loss = 0.66 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 10:02:39.100055: step 47060, loss = 0.62 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 10:02:51.238262: step 47070, loss = 0.66 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 10:03:03.379308: step 47080, loss = 0.71 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 10:03:15.518847: step 47090, loss = 0.68 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 10:03:27.676198: step 47100, loss = 0.69 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 10:03:41.839795: step 47110, loss = 0.59 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 10:03:53.994250: step 47120, loss = 0.67 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 10:04:06.070469: step 47130, loss = 0.65 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 10:04:18.157217: step 47140, loss = 0.69 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 10:04:30.336623: step 47150, loss = 0.68 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-05-21 10:04:42.395484: step 47160, loss = 0.61 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 10:04:54.427458: step 47170, loss = 0.64 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 10:05:06.500720: step 47180, loss = 0.70 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 10:05:18.775848: step 47190, loss = 0.62 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 10:05:30.835030: step 47200, loss = 0.64 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 10:05:44.548786: step 47210, loss = 0.64 (25.5 examples/sec; 1.176 sec/batch)\n",
      "2019-05-21 10:05:56.591460: step 47220, loss = 0.69 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 10:06:08.827242: step 47230, loss = 0.66 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 10:06:20.926684: step 47240, loss = 0.65 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 10:06:33.058583: step 47250, loss = 0.60 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 10:06:45.157271: step 47260, loss = 0.70 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 10:06:57.304706: step 47270, loss = 0.63 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 10:07:09.409810: step 47280, loss = 0.70 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 10:07:21.519222: step 47290, loss = 0.78 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 10:07:33.651586: step 47300, loss = 0.65 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 10:07:47.564166: step 47310, loss = 0.72 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 10:07:59.655339: step 47320, loss = 0.70 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 10:08:11.840013: step 47330, loss = 0.62 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 10:08:23.977838: step 47340, loss = 0.68 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 10:08:36.140154: step 47350, loss = 0.64 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 10:08:48.239615: step 47360, loss = 0.69 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 10:09:00.371506: step 47370, loss = 0.64 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 10:09:12.448727: step 47380, loss = 0.70 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 10:09:24.699617: step 47390, loss = 0.68 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 10:09:36.783507: step 47400, loss = 0.67 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 10:09:50.981749: step 47410, loss = 0.68 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 10:10:03.071747: step 47420, loss = 0.66 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 10:10:15.180116: step 47430, loss = 0.70 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 10:10:27.292970: step 47440, loss = 0.68 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 10:10:39.465222: step 47450, loss = 0.68 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 10:10:51.561750: step 47460, loss = 0.64 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 10:11:03.519103: step 47470, loss = 0.68 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 10:11:15.559488: step 47480, loss = 0.68 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 10:11:27.746457: step 47490, loss = 0.65 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 10:11:39.812400: step 47500, loss = 0.66 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 10:11:53.733417: step 47510, loss = 0.74 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 10:12:05.785729: step 47520, loss = 0.68 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 10:12:17.912597: step 47530, loss = 0.65 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 10:12:30.025652: step 47540, loss = 0.63 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 10:12:42.115813: step 47550, loss = 0.64 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 10:12:54.214002: step 47560, loss = 0.62 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 10:13:06.415091: step 47570, loss = 0.68 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 10:13:18.659999: step 47580, loss = 0.64 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 10:13:30.773374: step 47590, loss = 0.69 (24.6 examples/sec; 1.222 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 10:13:42.926517: step 47600, loss = 0.73 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 10:13:56.883708: step 47610, loss = 0.60 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 10:14:08.938897: step 47620, loss = 0.72 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 10:14:21.131454: step 47630, loss = 0.67 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 10:14:33.330988: step 47640, loss = 0.67 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 10:14:45.481532: step 47650, loss = 0.68 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 10:14:57.692287: step 47660, loss = 0.67 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 10:15:09.788020: step 47670, loss = 0.61 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 10:15:21.895238: step 47680, loss = 0.63 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 10:15:34.010229: step 47690, loss = 0.67 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 10:15:46.150282: step 47700, loss = 0.71 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 10:16:00.183588: step 47710, loss = 0.68 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 10:16:12.236865: step 47720, loss = 0.62 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 10:16:24.342881: step 47730, loss = 0.66 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 10:16:36.453131: step 47740, loss = 0.70 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 10:16:48.567331: step 47750, loss = 0.63 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 10:17:00.698167: step 47760, loss = 0.60 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 10:17:12.794972: step 47770, loss = 0.70 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 10:17:24.996712: step 47780, loss = 0.66 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-05-21 10:17:37.122608: step 47790, loss = 0.67 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 10:17:49.263571: step 47800, loss = 0.65 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 10:18:03.506264: step 47810, loss = 0.63 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 10:18:15.662319: step 47820, loss = 0.68 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 10:18:27.956563: step 47830, loss = 0.66 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 10:18:40.071210: step 47840, loss = 0.64 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 10:18:52.251471: step 47850, loss = 0.65 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 10:19:04.372659: step 47860, loss = 0.70 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 10:19:16.500899: step 47870, loss = 0.68 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 10:19:28.656437: step 47880, loss = 0.70 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 10:19:40.773296: step 47890, loss = 0.69 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 10:19:52.909578: step 47900, loss = 0.65 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 10:20:06.893528: step 47910, loss = 0.73 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 10:20:19.103065: step 47920, loss = 0.65 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 10:20:31.230868: step 47930, loss = 0.64 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 10:20:43.328949: step 47940, loss = 0.59 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 10:20:55.430157: step 47950, loss = 0.70 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 10:21:07.596866: step 47960, loss = 0.67 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 10:21:19.597719: step 47970, loss = 0.68 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 10:21:31.655385: step 47980, loss = 0.67 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 10:21:43.777605: step 47990, loss = 0.69 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 10:21:55.873666: step 48000, loss = 0.67 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 10:22:10.007444: step 48010, loss = 0.65 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 10:22:22.183573: step 48020, loss = 0.72 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 10:22:34.304735: step 48030, loss = 0.61 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 10:22:46.379414: step 48040, loss = 0.71 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 10:22:58.474746: step 48050, loss = 0.65 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 10:23:10.569197: step 48060, loss = 0.65 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 10:23:22.676895: step 48070, loss = 0.58 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 10:23:34.784498: step 48080, loss = 0.64 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 10:23:46.896830: step 48090, loss = 0.64 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 10:23:59.016149: step 48100, loss = 0.68 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 10:24:13.000295: step 48110, loss = 0.76 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 10:24:25.117297: step 48120, loss = 0.75 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 10:24:37.222669: step 48130, loss = 0.65 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 10:24:49.343579: step 48140, loss = 0.73 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 10:25:01.463928: step 48150, loss = 0.71 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 10:25:13.486450: step 48160, loss = 0.73 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 10:25:25.583928: step 48170, loss = 0.60 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 10:25:37.759357: step 48180, loss = 0.68 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 10:25:49.890195: step 48190, loss = 0.70 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 10:26:02.049403: step 48200, loss = 0.64 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 10:26:16.230551: step 48210, loss = 0.67 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 10:26:28.258299: step 48220, loss = 0.66 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 10:26:40.355867: step 48230, loss = 0.67 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 10:26:52.636514: step 48240, loss = 0.60 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 10:27:04.869697: step 48250, loss = 0.59 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 10:27:16.980288: step 48260, loss = 0.59 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 10:27:29.098494: step 48270, loss = 0.72 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 10:27:41.261465: step 48280, loss = 0.67 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 10:27:53.513862: step 48290, loss = 0.68 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 10:28:05.674774: step 48300, loss = 0.62 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 10:28:19.835212: step 48310, loss = 0.62 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 10:28:31.922455: step 48320, loss = 0.73 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 10:28:44.054765: step 48330, loss = 0.67 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 10:28:56.219004: step 48340, loss = 0.68 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 10:29:08.430409: step 48350, loss = 0.73 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 10:29:20.534116: step 48360, loss = 0.70 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 10:29:32.626634: step 48370, loss = 0.66 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 10:29:44.780757: step 48380, loss = 0.69 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 10:29:56.911134: step 48390, loss = 0.66 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 10:30:09.031449: step 48400, loss = 0.67 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 10:30:22.837279: step 48410, loss = 0.59 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 10:30:34.943175: step 48420, loss = 0.73 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 10:30:47.076749: step 48430, loss = 0.67 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 10:30:59.177057: step 48440, loss = 0.65 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 10:31:11.303393: step 48450, loss = 0.62 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 10:31:23.441062: step 48460, loss = 0.67 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 10:31:35.488790: step 48470, loss = 0.67 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 10:31:47.641532: step 48480, loss = 0.71 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 10:31:59.760562: step 48490, loss = 0.71 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 10:32:11.845654: step 48500, loss = 0.70 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 10:32:25.716459: step 48510, loss = 0.63 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 10:32:37.845491: step 48520, loss = 0.65 (24.8 examples/sec; 1.208 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 10:32:49.969416: step 48530, loss = 0.64 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 10:33:02.121362: step 48540, loss = 0.64 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 10:33:14.259466: step 48550, loss = 0.61 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 10:33:26.414652: step 48560, loss = 0.67 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 10:33:38.573730: step 48570, loss = 0.69 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 10:33:50.713774: step 48580, loss = 0.71 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 10:34:02.869040: step 48590, loss = 0.59 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 10:34:15.042818: step 48600, loss = 0.71 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 10:34:29.021110: step 48610, loss = 0.68 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 10:34:41.204647: step 48620, loss = 0.64 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 10:34:53.359423: step 48630, loss = 0.61 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 10:35:05.543395: step 48640, loss = 0.59 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 10:35:17.704148: step 48650, loss = 0.68 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 10:35:29.822806: step 48660, loss = 0.62 (23.2 examples/sec; 1.295 sec/batch)\n",
      "2019-05-21 10:35:41.925463: step 48670, loss = 0.69 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 10:35:54.032477: step 48680, loss = 0.65 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 10:36:06.133871: step 48690, loss = 0.63 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 10:36:18.255607: step 48700, loss = 0.62 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 10:36:32.148423: step 48710, loss = 0.60 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 10:36:44.233369: step 48720, loss = 0.68 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 10:36:56.263607: step 48730, loss = 0.64 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 10:37:08.362737: step 48740, loss = 0.62 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 10:37:20.527683: step 48750, loss = 0.60 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 10:37:32.586207: step 48760, loss = 0.74 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 10:37:44.773348: step 48770, loss = 0.64 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 10:37:56.871507: step 48780, loss = 0.72 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 10:38:09.089920: step 48790, loss = 0.67 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 10:38:21.184625: step 48800, loss = 0.66 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 10:38:35.016772: step 48810, loss = 0.70 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 10:38:47.135424: step 48820, loss = 0.62 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 10:38:59.239013: step 48830, loss = 0.70 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 10:39:11.422253: step 48840, loss = 0.69 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 10:39:23.529100: step 48850, loss = 0.70 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 10:39:35.804132: step 48860, loss = 0.64 (23.1 examples/sec; 1.299 sec/batch)\n",
      "2019-05-21 10:39:48.057257: step 48870, loss = 0.65 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 10:40:00.241755: step 48880, loss = 0.69 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 10:40:12.361716: step 48890, loss = 0.66 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 10:40:24.437466: step 48900, loss = 0.66 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 10:40:38.472283: step 48910, loss = 0.72 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 10:40:50.621540: step 48920, loss = 0.69 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 10:41:02.735121: step 48930, loss = 0.60 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 10:41:14.849186: step 48940, loss = 0.67 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 10:41:26.973198: step 48950, loss = 0.71 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 10:41:39.035349: step 48960, loss = 0.69 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 10:41:51.158806: step 48970, loss = 0.67 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 10:42:03.185034: step 48980, loss = 0.63 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 10:42:15.294792: step 48990, loss = 0.73 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 10:42:27.385351: step 49000, loss = 0.63 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 10:42:41.769673: step 49010, loss = 0.61 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 10:42:53.933886: step 49020, loss = 0.64 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 10:43:06.058949: step 49030, loss = 0.64 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 10:43:18.180656: step 49040, loss = 0.63 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 10:43:30.315353: step 49050, loss = 0.69 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 10:43:42.530947: step 49060, loss = 0.62 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 10:43:54.673211: step 49070, loss = 0.64 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 10:44:06.799224: step 49080, loss = 0.68 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 10:44:18.956417: step 49090, loss = 0.67 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 10:44:31.070697: step 49100, loss = 0.66 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 10:44:45.187088: step 49110, loss = 0.63 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 10:44:57.268267: step 49120, loss = 0.64 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 10:45:09.417316: step 49130, loss = 0.66 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 10:45:21.546655: step 49140, loss = 0.71 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 10:45:33.633170: step 49150, loss = 0.70 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 10:45:45.804015: step 49160, loss = 0.73 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 10:45:57.904862: step 49170, loss = 0.65 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 10:46:10.078517: step 49180, loss = 0.65 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 10:46:22.281515: step 49190, loss = 0.72 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 10:46:34.445880: step 49200, loss = 0.68 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 10:46:48.995167: step 49210, loss = 0.67 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 10:47:01.121366: step 49220, loss = 0.69 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 10:47:13.239905: step 49230, loss = 0.71 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 10:47:25.330106: step 49240, loss = 0.64 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 10:47:37.533312: step 49250, loss = 0.68 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 10:47:49.684888: step 49260, loss = 0.68 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 10:48:01.919154: step 49270, loss = 0.69 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 10:48:14.068835: step 49280, loss = 0.65 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 10:48:26.285612: step 49290, loss = 0.69 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 10:48:38.375801: step 49300, loss = 0.72 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 10:48:52.627483: step 49310, loss = 0.61 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 10:49:04.750670: step 49320, loss = 0.71 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 10:49:16.985191: step 49330, loss = 0.61 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 10:49:29.069207: step 49340, loss = 0.71 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 10:49:41.343736: step 49350, loss = 0.67 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 10:49:53.391334: step 49360, loss = 0.61 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 10:50:05.488768: step 49370, loss = 0.70 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 10:50:17.652468: step 49380, loss = 0.65 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 10:50:29.788340: step 49390, loss = 0.70 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 10:50:41.970995: step 49400, loss = 0.66 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 10:50:56.090319: step 49410, loss = 0.66 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 10:51:08.308023: step 49420, loss = 0.66 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 10:51:20.399873: step 49430, loss = 0.68 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 10:51:32.523288: step 49440, loss = 0.67 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 10:51:44.684911: step 49450, loss = 0.62 (24.7 examples/sec; 1.214 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 10:51:56.816125: step 49460, loss = 0.64 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 10:52:08.997545: step 49470, loss = 0.67 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 10:52:21.072878: step 49480, loss = 0.66 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 10:52:33.104395: step 49490, loss = 0.73 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 10:52:45.261336: step 49500, loss = 0.71 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 10:52:59.477466: step 49510, loss = 0.66 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 10:53:11.630234: step 49520, loss = 0.62 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 10:53:23.786110: step 49530, loss = 0.68 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 10:53:35.935409: step 49540, loss = 0.66 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 10:53:48.086592: step 49550, loss = 0.66 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 10:54:00.180224: step 49560, loss = 0.73 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 10:54:12.308631: step 49570, loss = 0.68 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 10:54:24.448435: step 49580, loss = 0.61 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 10:54:36.538580: step 49590, loss = 0.65 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 10:54:48.671648: step 49600, loss = 0.74 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 10:55:02.743809: step 49610, loss = 0.57 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 10:55:14.896622: step 49620, loss = 0.68 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 10:55:27.123030: step 49630, loss = 0.68 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 10:55:39.292988: step 49640, loss = 0.62 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-21 10:55:51.387499: step 49650, loss = 0.67 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 10:56:03.522997: step 49660, loss = 0.69 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 10:56:15.660260: step 49670, loss = 0.73 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 10:56:27.735424: step 49680, loss = 0.60 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 10:56:39.839886: step 49690, loss = 0.65 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 10:56:51.963345: step 49700, loss = 0.64 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 10:57:05.995506: step 49710, loss = 0.65 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 10:57:18.164108: step 49720, loss = 0.64 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 10:57:30.237287: step 49730, loss = 0.68 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 10:57:42.249543: step 49740, loss = 0.61 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 10:57:54.444370: step 49750, loss = 0.73 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 10:58:06.568251: step 49760, loss = 0.67 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 10:58:18.681024: step 49770, loss = 0.63 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 10:58:30.794365: step 49780, loss = 0.64 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 10:58:42.880178: step 49790, loss = 0.63 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 10:58:55.041560: step 49800, loss = 0.71 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 10:59:09.367900: step 49810, loss = 0.66 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 10:59:21.424975: step 49820, loss = 0.59 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 10:59:33.661135: step 49830, loss = 0.64 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 10:59:45.771485: step 49840, loss = 0.67 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 10:59:57.888807: step 49850, loss = 0.69 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 11:00:10.021722: step 49860, loss = 0.66 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 11:00:22.135890: step 49870, loss = 0.69 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 11:00:34.451977: step 49880, loss = 0.68 (23.2 examples/sec; 1.295 sec/batch)\n",
      "2019-05-21 11:00:46.565560: step 49890, loss = 0.66 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 11:00:58.596913: step 49900, loss = 0.68 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 11:01:12.557945: step 49910, loss = 0.72 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 11:01:24.759351: step 49920, loss = 0.62 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 11:01:36.854164: step 49930, loss = 0.60 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 11:01:48.960836: step 49940, loss = 0.66 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 11:02:01.112799: step 49950, loss = 0.61 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 11:02:13.232508: step 49960, loss = 0.65 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 11:02:25.430016: step 49970, loss = 0.62 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 11:02:37.592752: step 49980, loss = 0.62 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 11:02:49.542434: step 49990, loss = 0.61 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 11:03:01.710284: step 50000, loss = 0.66 (23.2 examples/sec; 1.291 sec/batch)\n",
      "2019-05-21 11:03:19.730528: step 50010, loss = 0.60 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 11:03:31.759440: step 50020, loss = 0.66 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 11:03:43.817760: step 50030, loss = 0.63 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 11:03:56.007024: step 50040, loss = 0.74 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 11:04:08.063249: step 50050, loss = 0.69 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 11:04:20.093930: step 50060, loss = 0.62 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 11:04:32.223459: step 50070, loss = 0.66 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 11:04:44.392704: step 50080, loss = 0.70 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 11:04:56.499468: step 50090, loss = 0.69 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 11:05:08.640185: step 50100, loss = 0.65 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 11:05:22.737452: step 50110, loss = 0.74 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 11:05:34.889236: step 50120, loss = 0.66 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 11:05:46.983892: step 50130, loss = 0.69 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 11:05:59.010252: step 50140, loss = 0.67 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 11:06:11.106324: step 50150, loss = 0.76 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 11:06:23.218520: step 50160, loss = 0.64 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 11:06:35.323862: step 50170, loss = 0.61 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 11:06:47.483033: step 50180, loss = 0.68 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 11:06:59.582597: step 50190, loss = 0.63 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 11:07:11.691568: step 50200, loss = 0.67 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 11:07:25.719935: step 50210, loss = 0.61 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 11:07:37.866365: step 50220, loss = 0.63 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 11:07:50.000136: step 50230, loss = 0.63 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 11:08:02.020911: step 50240, loss = 0.60 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 11:08:14.180190: step 50250, loss = 0.67 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 11:08:26.295636: step 50260, loss = 0.69 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 11:08:38.414343: step 50270, loss = 0.60 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 11:08:50.542173: step 50280, loss = 0.68 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 11:09:02.694353: step 50290, loss = 0.66 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 11:09:14.812230: step 50300, loss = 0.65 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 11:09:29.101367: step 50310, loss = 0.66 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 11:09:41.166834: step 50320, loss = 0.70 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 11:09:53.329293: step 50330, loss = 0.64 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 11:10:05.573286: step 50340, loss = 0.66 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 11:10:17.716302: step 50350, loss = 0.70 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 11:10:29.878774: step 50360, loss = 0.64 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 11:10:42.126370: step 50370, loss = 0.68 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 11:10:54.277618: step 50380, loss = 0.64 (24.3 examples/sec; 1.234 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 11:11:06.308511: step 50390, loss = 0.64 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 11:11:18.398887: step 50400, loss = 0.56 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 11:11:32.707787: step 50410, loss = 0.63 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 11:11:44.916741: step 50420, loss = 0.69 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 11:11:57.058632: step 50430, loss = 0.64 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 11:12:09.153715: step 50440, loss = 0.66 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 11:12:21.294256: step 50450, loss = 0.65 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 11:12:33.413323: step 50460, loss = 0.65 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 11:12:45.548525: step 50470, loss = 0.64 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 11:12:57.626795: step 50480, loss = 0.69 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 11:13:09.622236: step 50490, loss = 0.61 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 11:13:21.854987: step 50500, loss = 0.63 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 11:13:35.977261: step 50510, loss = 0.58 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 11:13:48.108693: step 50520, loss = 0.70 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 11:14:00.272429: step 50530, loss = 0.70 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 11:14:12.361765: step 50540, loss = 0.62 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 11:14:24.491431: step 50550, loss = 0.63 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 11:14:36.618715: step 50560, loss = 0.65 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 11:14:48.805838: step 50570, loss = 0.72 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 11:15:00.948226: step 50580, loss = 0.68 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 11:15:13.112646: step 50590, loss = 0.71 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 11:15:25.275780: step 50600, loss = 0.64 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 11:15:39.137549: step 50610, loss = 0.59 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 11:15:51.256178: step 50620, loss = 0.65 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 11:16:03.360308: step 50630, loss = 0.67 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 11:16:15.471604: step 50640, loss = 0.58 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 11:16:27.663902: step 50650, loss = 0.67 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 11:16:39.826622: step 50660, loss = 0.61 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 11:16:51.915672: step 50670, loss = 0.75 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 11:17:03.967006: step 50680, loss = 0.60 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 11:17:16.099513: step 50690, loss = 0.64 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 11:17:28.164371: step 50700, loss = 0.54 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 11:17:42.537209: step 50710, loss = 0.73 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 11:17:54.683139: step 50720, loss = 0.66 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 11:18:06.734494: step 50730, loss = 0.69 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 11:18:18.778860: step 50740, loss = 0.63 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 11:18:30.854440: step 50750, loss = 0.63 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 11:18:42.986890: step 50760, loss = 0.62 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 11:18:55.158354: step 50770, loss = 0.64 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 11:19:07.325293: step 50780, loss = 0.63 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 11:19:19.442317: step 50790, loss = 0.65 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 11:19:31.510543: step 50800, loss = 0.66 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 11:19:45.510205: step 50810, loss = 0.62 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 11:19:57.638775: step 50820, loss = 0.64 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 11:20:09.885162: step 50830, loss = 0.66 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 11:20:21.968745: step 50840, loss = 0.57 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 11:20:34.100703: step 50850, loss = 0.72 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 11:20:46.203079: step 50860, loss = 0.59 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 11:20:58.503305: step 50870, loss = 0.63 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 11:21:10.569508: step 50880, loss = 0.69 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 11:21:22.660723: step 50890, loss = 0.64 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 11:21:34.861977: step 50900, loss = 0.66 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 11:21:48.771419: step 50910, loss = 0.63 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 11:22:00.880764: step 50920, loss = 0.59 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 11:22:12.982493: step 50930, loss = 0.64 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 11:22:25.143565: step 50940, loss = 0.64 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 11:22:37.283598: step 50950, loss = 0.59 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 11:22:49.429329: step 50960, loss = 0.64 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 11:23:01.570986: step 50970, loss = 0.62 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 11:23:13.736126: step 50980, loss = 0.63 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 11:23:25.783736: step 50990, loss = 0.59 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 11:23:37.968693: step 51000, loss = 0.59 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 11:23:51.865696: step 51010, loss = 0.64 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 11:24:03.935993: step 51020, loss = 0.62 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 11:24:16.154323: step 51030, loss = 0.69 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 11:24:28.294865: step 51040, loss = 0.68 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 11:24:40.454801: step 51050, loss = 0.63 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 11:24:52.574387: step 51060, loss = 0.64 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 11:25:04.715414: step 51070, loss = 0.61 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 11:25:16.790792: step 51080, loss = 0.72 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 11:25:28.928753: step 51090, loss = 0.69 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 11:25:41.012517: step 51100, loss = 0.59 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 11:25:54.927891: step 51110, loss = 0.72 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 11:26:07.038396: step 51120, loss = 0.69 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 11:26:19.127678: step 51130, loss = 0.64 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 11:26:31.222204: step 51140, loss = 0.56 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 11:26:43.435662: step 51150, loss = 0.66 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 11:26:55.610705: step 51160, loss = 0.62 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 11:27:07.637336: step 51170, loss = 0.63 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 11:27:19.864105: step 51180, loss = 0.63 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 11:27:31.925422: step 51190, loss = 0.59 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 11:27:44.035946: step 51200, loss = 0.74 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 11:27:57.820385: step 51210, loss = 0.56 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 11:28:09.916248: step 51220, loss = 0.59 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 11:28:22.000248: step 51230, loss = 0.66 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 11:28:34.012039: step 51240, loss = 0.74 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 11:28:46.012541: step 51250, loss = 0.62 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 11:28:58.150041: step 51260, loss = 0.74 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 11:29:10.230174: step 51270, loss = 0.69 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 11:29:22.316354: step 51280, loss = 0.64 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 11:29:34.352571: step 51290, loss = 0.68 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 11:29:46.462071: step 51300, loss = 0.66 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 11:30:00.710636: step 51310, loss = 0.60 (25.0 examples/sec; 1.201 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 11:30:12.741767: step 51320, loss = 0.67 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 11:30:24.787277: step 51330, loss = 0.67 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 11:30:36.824329: step 51340, loss = 0.60 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 11:30:48.895038: step 51350, loss = 0.66 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 11:31:00.976402: step 51360, loss = 0.68 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 11:31:13.064758: step 51370, loss = 0.65 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 11:31:25.091447: step 51380, loss = 0.59 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 11:31:37.237335: step 51390, loss = 0.76 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 11:31:49.277431: step 51400, loss = 0.64 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 11:32:03.357700: step 51410, loss = 0.71 (23.4 examples/sec; 1.279 sec/batch)\n",
      "2019-05-21 11:32:15.400100: step 51420, loss = 0.68 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 11:32:27.481032: step 51430, loss = 0.66 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 11:32:39.529972: step 51440, loss = 0.55 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 11:32:51.629394: step 51450, loss = 0.60 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 11:33:03.729173: step 51460, loss = 0.69 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 11:33:15.856575: step 51470, loss = 0.67 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 11:33:27.920462: step 51480, loss = 0.64 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 11:33:40.014823: step 51490, loss = 0.62 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 11:33:51.923754: step 51500, loss = 0.62 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 11:34:05.955200: step 51510, loss = 0.70 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 11:34:18.093248: step 51520, loss = 0.61 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 11:34:30.168183: step 51530, loss = 0.66 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 11:34:42.243511: step 51540, loss = 0.65 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 11:34:54.337588: step 51550, loss = 0.58 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 11:35:06.479468: step 51560, loss = 0.68 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 11:35:18.618158: step 51570, loss = 0.62 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 11:35:30.826230: step 51580, loss = 0.62 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 11:35:42.918198: step 51590, loss = 0.61 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 11:35:55.053825: step 51600, loss = 0.66 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 11:36:08.979934: step 51610, loss = 0.64 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 11:36:21.112621: step 51620, loss = 0.65 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 11:36:33.140153: step 51630, loss = 0.65 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 11:36:45.287500: step 51640, loss = 0.60 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 11:36:57.375483: step 51650, loss = 0.65 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 11:37:09.506144: step 51660, loss = 0.69 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 11:37:21.698457: step 51670, loss = 0.67 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-21 11:37:33.781778: step 51680, loss = 0.65 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 11:37:45.900226: step 51690, loss = 0.70 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 11:37:58.104216: step 51700, loss = 0.60 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 11:38:12.464143: step 51710, loss = 0.64 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 11:38:24.591366: step 51720, loss = 0.64 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 11:38:36.698732: step 51730, loss = 0.68 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 11:38:48.781377: step 51740, loss = 0.67 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 11:39:00.747604: step 51750, loss = 0.66 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 11:39:12.821021: step 51760, loss = 0.59 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 11:39:24.963626: step 51770, loss = 0.69 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 11:39:37.136255: step 51780, loss = 0.63 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 11:39:49.245005: step 51790, loss = 0.62 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 11:40:01.393718: step 51800, loss = 0.66 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 11:40:15.462960: step 51810, loss = 0.67 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 11:40:27.574578: step 51820, loss = 0.69 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 11:40:39.706844: step 51830, loss = 0.61 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 11:40:51.828057: step 51840, loss = 0.59 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 11:41:03.979712: step 51850, loss = 0.57 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 11:41:16.059303: step 51860, loss = 0.66 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 11:41:28.191984: step 51870, loss = 0.64 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 11:41:40.174877: step 51880, loss = 0.68 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 11:41:52.277360: step 51890, loss = 0.59 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 11:42:04.353455: step 51900, loss = 0.60 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 11:42:18.251690: step 51910, loss = 0.66 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 11:42:30.365044: step 51920, loss = 0.59 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 11:42:42.488318: step 51930, loss = 0.69 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 11:42:54.606271: step 51940, loss = 0.65 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 11:43:06.752501: step 51950, loss = 0.69 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 11:43:18.972769: step 51960, loss = 0.70 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 11:43:31.048123: step 51970, loss = 0.63 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 11:43:43.093746: step 51980, loss = 0.73 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 11:43:55.221720: step 51990, loss = 0.70 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 11:44:07.265950: step 52000, loss = 0.69 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 11:44:21.440020: step 52010, loss = 0.62 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 11:44:33.475153: step 52020, loss = 0.59 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 11:44:45.524884: step 52030, loss = 0.68 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 11:44:57.664960: step 52040, loss = 0.64 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 11:45:09.712956: step 52050, loss = 0.69 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 11:45:21.819929: step 52060, loss = 0.63 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 11:45:33.960438: step 52070, loss = 0.57 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 11:45:46.022899: step 52080, loss = 0.71 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 11:45:58.080373: step 52090, loss = 0.60 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 11:46:10.182659: step 52100, loss = 0.60 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 11:46:24.093127: step 52110, loss = 0.62 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 11:46:36.222378: step 52120, loss = 0.63 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 11:46:48.250788: step 52130, loss = 0.67 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 11:47:00.390531: step 52140, loss = 0.69 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 11:47:12.487018: step 52150, loss = 0.66 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 11:47:24.554815: step 52160, loss = 0.66 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 11:47:36.614708: step 52170, loss = 0.64 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 11:47:48.772523: step 52180, loss = 0.66 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 11:48:00.810845: step 52190, loss = 0.64 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 11:48:12.816740: step 52200, loss = 0.63 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 11:48:26.677043: step 52210, loss = 0.61 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 11:48:38.713788: step 52220, loss = 0.58 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 11:48:50.789599: step 52230, loss = 0.62 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 11:49:02.832946: step 52240, loss = 0.68 (25.1 examples/sec; 1.195 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 11:49:14.902689: step 52250, loss = 0.62 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 11:49:26.906824: step 52260, loss = 0.59 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 11:49:38.980680: step 52270, loss = 0.62 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 11:49:51.061909: step 52280, loss = 0.73 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 11:50:03.129572: step 52290, loss = 0.65 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 11:50:15.185906: step 52300, loss = 0.59 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 11:50:29.180275: step 52310, loss = 0.61 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 11:50:41.194267: step 52320, loss = 0.67 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 11:50:53.312262: step 52330, loss = 0.63 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 11:51:05.350822: step 52340, loss = 0.64 (25.5 examples/sec; 1.178 sec/batch)\n",
      "2019-05-21 11:51:17.433056: step 52350, loss = 0.66 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 11:51:29.512845: step 52360, loss = 0.75 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 11:51:41.591032: step 52370, loss = 0.55 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 11:51:53.597594: step 52380, loss = 0.60 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 11:52:05.726039: step 52390, loss = 0.68 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 11:52:17.774279: step 52400, loss = 0.57 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 11:52:31.586204: step 52410, loss = 0.58 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 11:52:43.623587: step 52420, loss = 0.66 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 11:52:55.723726: step 52430, loss = 0.58 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 11:53:07.814143: step 52440, loss = 0.60 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 11:53:19.920766: step 52450, loss = 0.73 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 11:53:32.051357: step 52460, loss = 0.63 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 11:53:44.047310: step 52470, loss = 0.63 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 11:53:56.036293: step 52480, loss = 0.68 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 11:54:08.087059: step 52490, loss = 0.67 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 11:54:20.163986: step 52500, loss = 0.70 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 11:54:34.438806: step 52510, loss = 0.57 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 11:54:46.517078: step 52520, loss = 0.64 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 11:54:58.579536: step 52530, loss = 0.68 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 11:55:10.646886: step 52540, loss = 0.66 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 11:55:22.775768: step 52550, loss = 0.67 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 11:55:34.819025: step 52560, loss = 0.61 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 11:55:46.951233: step 52570, loss = 0.63 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 11:55:58.992660: step 52580, loss = 0.70 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 11:56:11.046530: step 52590, loss = 0.69 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 11:56:23.197409: step 52600, loss = 0.63 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 11:56:37.152765: step 52610, loss = 0.68 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 11:56:49.155900: step 52620, loss = 0.68 (25.6 examples/sec; 1.172 sec/batch)\n",
      "2019-05-21 11:57:01.154700: step 52630, loss = 0.58 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 11:57:13.261841: step 52640, loss = 0.65 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 11:57:25.259729: step 52650, loss = 0.64 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 11:57:37.318762: step 52660, loss = 0.62 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 11:57:49.561836: step 52670, loss = 0.63 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 11:58:01.681307: step 52680, loss = 0.64 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 11:58:13.749370: step 52690, loss = 0.65 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 11:58:25.814302: step 52700, loss = 0.69 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 11:58:39.644251: step 52710, loss = 0.67 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 11:58:51.812953: step 52720, loss = 0.71 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 11:59:03.924528: step 52730, loss = 0.59 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 11:59:15.976623: step 52740, loss = 0.63 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 11:59:28.111057: step 52750, loss = 0.62 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 11:59:40.146660: step 52760, loss = 0.72 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 11:59:52.230433: step 52770, loss = 0.72 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 12:00:04.339196: step 52780, loss = 0.71 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 12:00:16.441548: step 52790, loss = 0.64 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 12:00:28.471955: step 52800, loss = 0.65 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 12:00:42.293257: step 52810, loss = 0.58 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 12:00:54.412610: step 52820, loss = 0.60 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 12:01:06.515623: step 52830, loss = 0.60 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 12:01:18.586182: step 52840, loss = 0.68 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 12:01:30.670020: step 52850, loss = 0.61 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 12:01:42.734699: step 52860, loss = 0.64 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 12:01:54.846986: step 52870, loss = 0.65 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 12:02:06.891378: step 52880, loss = 0.62 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 12:02:18.910032: step 52890, loss = 0.61 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 12:02:30.957595: step 52900, loss = 0.57 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 12:02:45.193559: step 52910, loss = 0.62 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 12:02:57.242998: step 52920, loss = 0.76 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 12:03:09.362921: step 52930, loss = 0.66 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 12:03:21.468236: step 52940, loss = 0.68 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 12:03:33.580291: step 52950, loss = 0.69 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 12:03:45.642768: step 52960, loss = 0.69 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 12:03:57.763818: step 52970, loss = 0.68 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 12:04:09.910587: step 52980, loss = 0.68 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 12:04:22.130692: step 52990, loss = 0.74 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 12:04:34.210133: step 53000, loss = 0.64 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 12:04:48.045744: step 53010, loss = 0.68 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 12:04:59.984677: step 53020, loss = 0.62 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 12:05:12.087277: step 53030, loss = 0.71 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 12:05:24.168982: step 53040, loss = 0.70 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 12:05:36.254360: step 53050, loss = 0.73 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 12:05:48.443436: step 53060, loss = 0.65 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 12:06:00.731051: step 53070, loss = 0.68 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 12:06:12.821789: step 53080, loss = 0.65 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 12:06:24.890692: step 53090, loss = 0.62 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 12:06:36.993741: step 53100, loss = 0.64 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 12:06:50.964863: step 53110, loss = 0.66 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 12:07:02.977413: step 53120, loss = 0.65 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 12:07:15.050020: step 53130, loss = 0.59 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 12:07:27.126853: step 53140, loss = 0.74 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 12:07:39.228687: step 53150, loss = 0.61 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 12:07:51.328531: step 53160, loss = 0.65 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 12:08:03.433026: step 53170, loss = 0.60 (24.9 examples/sec; 1.204 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 12:08:15.520361: step 53180, loss = 0.65 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 12:08:27.635235: step 53190, loss = 0.72 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 12:08:39.743964: step 53200, loss = 0.65 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 12:08:53.642791: step 53210, loss = 0.63 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 12:09:05.683286: step 53220, loss = 0.62 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 12:09:17.758085: step 53230, loss = 0.69 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 12:09:29.805815: step 53240, loss = 0.58 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 12:09:41.895067: step 53250, loss = 0.70 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 12:09:54.026206: step 53260, loss = 0.65 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 12:10:06.036602: step 53270, loss = 0.57 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 12:10:18.101096: step 53280, loss = 0.68 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 12:10:30.239804: step 53290, loss = 0.62 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 12:10:42.325119: step 53300, loss = 0.64 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 12:10:56.542588: step 53310, loss = 0.74 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 12:11:08.622375: step 53320, loss = 0.72 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 12:11:20.874944: step 53330, loss = 0.67 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 12:11:32.947153: step 53340, loss = 0.66 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 12:11:45.046903: step 53350, loss = 0.66 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 12:11:57.188921: step 53360, loss = 0.67 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 12:12:09.285652: step 53370, loss = 0.68 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 12:12:21.329921: step 53380, loss = 0.65 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 12:12:33.442721: step 53390, loss = 0.64 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 12:12:45.678324: step 53400, loss = 0.62 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 12:12:59.455633: step 53410, loss = 0.61 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 12:13:11.542404: step 53420, loss = 0.67 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 12:13:23.674665: step 53430, loss = 0.64 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 12:13:35.765676: step 53440, loss = 0.73 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 12:13:47.860629: step 53450, loss = 0.64 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 12:14:00.063332: step 53460, loss = 0.69 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 12:14:12.216923: step 53470, loss = 0.64 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 12:14:24.249532: step 53480, loss = 0.62 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 12:14:36.301598: step 53490, loss = 0.63 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 12:14:48.413942: step 53500, loss = 0.63 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 12:15:02.216944: step 53510, loss = 0.60 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 12:15:14.203352: step 53520, loss = 0.69 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 12:15:26.259632: step 53530, loss = 0.70 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 12:15:38.372134: step 53540, loss = 0.67 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 12:15:50.540510: step 53550, loss = 0.63 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 12:16:02.645140: step 53560, loss = 0.65 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 12:16:14.710529: step 53570, loss = 0.62 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 12:16:26.762565: step 53580, loss = 0.60 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 12:16:38.809393: step 53590, loss = 0.68 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 12:16:50.864939: step 53600, loss = 0.63 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 12:17:05.327400: step 53610, loss = 0.73 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 12:17:17.323896: step 53620, loss = 0.71 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 12:17:29.519967: step 53630, loss = 0.67 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 12:17:41.683093: step 53640, loss = 0.72 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 12:17:53.797931: step 53650, loss = 0.68 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 12:18:05.942622: step 53660, loss = 0.70 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 12:18:18.039175: step 53670, loss = 0.65 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 12:18:30.140843: step 53680, loss = 0.70 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 12:18:42.257848: step 53690, loss = 0.70 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 12:18:54.383022: step 53700, loss = 0.69 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 12:19:08.277562: step 53710, loss = 0.72 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 12:19:20.316933: step 53720, loss = 0.60 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 12:19:32.415327: step 53730, loss = 0.59 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 12:19:44.527131: step 53740, loss = 0.72 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 12:19:56.755199: step 53750, loss = 0.68 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 12:20:08.853833: step 53760, loss = 0.58 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 12:20:20.929696: step 53770, loss = 0.69 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 12:20:32.951790: step 53780, loss = 0.64 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 12:20:45.171513: step 53790, loss = 0.66 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 12:20:57.346560: step 53800, loss = 0.69 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 12:21:11.303725: step 53810, loss = 0.59 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 12:21:23.403137: step 53820, loss = 0.70 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 12:21:35.508461: step 53830, loss = 0.64 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 12:21:47.575373: step 53840, loss = 0.60 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 12:21:59.717872: step 53850, loss = 0.69 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 12:22:11.791515: step 53860, loss = 0.70 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 12:22:23.906010: step 53870, loss = 0.67 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 12:22:36.000677: step 53880, loss = 0.61 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 12:22:48.123987: step 53890, loss = 0.65 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 12:23:00.234956: step 53900, loss = 0.60 (23.2 examples/sec; 1.295 sec/batch)\n",
      "2019-05-21 12:23:14.537763: step 53910, loss = 0.65 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 12:23:26.604213: step 53920, loss = 0.67 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 12:23:38.697596: step 53930, loss = 0.61 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 12:23:50.750020: step 53940, loss = 0.61 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 12:24:02.814008: step 53950, loss = 0.57 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 12:24:14.886795: step 53960, loss = 0.58 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 12:24:26.952186: step 53970, loss = 0.65 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 12:24:39.020165: step 53980, loss = 0.68 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 12:24:51.074766: step 53990, loss = 0.67 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 12:25:03.104518: step 54000, loss = 0.69 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 12:25:17.548393: step 54010, loss = 0.73 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 12:25:29.610892: step 54020, loss = 0.71 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 12:25:41.548591: step 54030, loss = 0.63 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 12:25:53.618842: step 54040, loss = 0.57 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 12:26:05.733841: step 54050, loss = 0.70 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 12:26:17.867538: step 54060, loss = 0.64 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 12:26:30.083277: step 54070, loss = 0.64 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-05-21 12:26:42.237407: step 54080, loss = 0.57 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 12:26:54.439967: step 54090, loss = 0.60 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 12:27:06.580378: step 54100, loss = 0.67 (24.7 examples/sec; 1.213 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 12:27:20.926319: step 54110, loss = 0.69 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 12:27:32.934339: step 54120, loss = 0.65 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 12:27:45.091567: step 54130, loss = 0.63 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 12:27:57.246851: step 54140, loss = 0.72 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 12:28:09.316589: step 54150, loss = 0.76 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 12:28:21.513821: step 54160, loss = 0.74 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 12:28:33.623420: step 54170, loss = 0.66 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 12:28:45.694219: step 54180, loss = 0.63 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 12:28:57.786354: step 54190, loss = 0.61 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 12:29:10.015298: step 54200, loss = 0.60 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 12:29:24.042295: step 54210, loss = 0.64 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 12:29:36.230292: step 54220, loss = 0.63 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 12:29:48.443150: step 54230, loss = 0.62 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 12:30:00.522536: step 54240, loss = 0.58 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 12:30:12.672299: step 54250, loss = 0.63 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 12:30:24.854643: step 54260, loss = 0.66 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 12:30:37.018813: step 54270, loss = 0.68 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 12:30:49.126490: step 54280, loss = 0.66 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 12:31:01.192399: step 54290, loss = 0.63 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 12:31:13.305393: step 54300, loss = 0.72 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 12:31:27.216204: step 54310, loss = 0.66 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 12:31:39.332023: step 54320, loss = 0.63 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 12:31:51.472236: step 54330, loss = 0.66 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 12:32:03.603406: step 54340, loss = 0.63 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 12:32:15.749982: step 54350, loss = 0.60 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 12:32:27.844873: step 54360, loss = 0.63 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 12:32:39.817427: step 54370, loss = 0.65 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 12:32:51.919942: step 54380, loss = 0.62 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 12:33:04.167173: step 54390, loss = 0.68 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 12:33:16.324079: step 54400, loss = 0.65 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 12:33:30.562419: step 54410, loss = 0.59 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 12:33:42.708192: step 54420, loss = 0.60 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 12:33:54.903095: step 54430, loss = 0.60 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 12:34:07.041827: step 54440, loss = 0.70 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 12:34:19.139400: step 54450, loss = 0.68 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 12:34:31.247548: step 54460, loss = 0.64 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 12:34:43.360712: step 54470, loss = 0.62 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 12:34:55.574847: step 54480, loss = 0.65 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 12:35:07.786589: step 54490, loss = 0.61 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 12:35:19.926082: step 54500, loss = 0.62 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 12:35:34.075816: step 54510, loss = 0.64 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 12:35:46.184741: step 54520, loss = 0.65 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 12:35:58.203039: step 54530, loss = 0.57 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 12:36:10.250825: step 54540, loss = 0.64 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 12:36:22.470590: step 54550, loss = 0.61 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 12:36:34.573962: step 54560, loss = 0.66 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 12:36:46.808197: step 54570, loss = 0.65 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 12:36:58.908732: step 54580, loss = 0.63 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 12:37:11.039577: step 54590, loss = 0.61 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 12:37:23.195967: step 54600, loss = 0.63 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 12:37:37.617961: step 54610, loss = 0.67 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 12:37:49.691077: step 54620, loss = 0.60 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 12:38:01.817219: step 54630, loss = 0.65 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 12:38:13.962979: step 54640, loss = 0.67 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 12:38:26.104571: step 54650, loss = 0.65 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 12:38:38.213437: step 54660, loss = 0.66 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 12:38:50.376634: step 54670, loss = 0.67 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 12:39:02.525699: step 54680, loss = 0.64 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 12:39:14.680870: step 54690, loss = 0.62 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 12:39:26.804596: step 54700, loss = 0.61 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 12:39:41.385714: step 54710, loss = 0.64 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 12:39:53.541870: step 54720, loss = 0.68 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 12:40:05.656833: step 54730, loss = 0.66 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 12:40:17.925971: step 54740, loss = 0.63 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 12:40:30.086025: step 54750, loss = 0.65 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 12:40:42.198863: step 54760, loss = 0.73 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 12:40:54.371006: step 54770, loss = 0.63 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 12:41:06.428923: step 54780, loss = 0.64 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-21 12:41:18.465829: step 54790, loss = 0.67 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 12:41:30.597206: step 54800, loss = 0.69 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 12:41:44.747443: step 54810, loss = 0.71 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 12:41:56.924654: step 54820, loss = 0.65 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 12:42:09.113331: step 54830, loss = 0.61 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 12:42:21.262324: step 54840, loss = 0.66 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 12:42:33.470012: step 54850, loss = 0.65 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 12:42:45.556932: step 54860, loss = 0.66 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 12:42:57.701173: step 54870, loss = 0.64 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 12:43:09.876021: step 54880, loss = 0.55 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 12:43:22.120818: step 54890, loss = 0.64 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 12:43:34.243074: step 54900, loss = 0.59 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 12:43:48.482375: step 54910, loss = 0.64 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 12:44:00.637668: step 54920, loss = 0.66 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 12:44:12.782450: step 54930, loss = 0.64 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 12:44:24.961385: step 54940, loss = 0.65 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 12:44:37.047157: step 54950, loss = 0.64 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 12:44:49.191256: step 54960, loss = 0.69 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 12:45:01.426329: step 54970, loss = 0.58 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 12:45:13.635350: step 54980, loss = 0.66 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 12:45:25.746174: step 54990, loss = 0.63 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 12:45:37.877916: step 55000, loss = 0.66 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 12:45:55.194435: step 55010, loss = 0.63 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 12:46:07.337988: step 55020, loss = 0.68 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 12:46:19.395286: step 55030, loss = 0.67 (24.9 examples/sec; 1.204 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 12:46:31.424784: step 55040, loss = 0.60 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 12:46:43.544387: step 55050, loss = 0.63 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 12:46:55.626529: step 55060, loss = 0.70 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 12:47:07.842832: step 55070, loss = 0.70 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-05-21 12:47:19.965561: step 55080, loss = 0.64 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 12:47:32.128285: step 55090, loss = 0.67 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 12:47:44.237620: step 55100, loss = 0.67 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 12:47:58.201075: step 55110, loss = 0.64 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 12:48:10.305870: step 55120, loss = 0.66 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 12:48:22.554417: step 55130, loss = 0.66 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 12:48:34.695628: step 55140, loss = 0.67 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 12:48:46.823744: step 55150, loss = 0.67 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 12:48:58.982888: step 55160, loss = 0.65 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 12:49:11.205501: step 55170, loss = 0.61 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 12:49:23.313839: step 55180, loss = 0.59 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 12:49:35.436476: step 55190, loss = 0.66 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 12:49:47.629081: step 55200, loss = 0.67 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 12:50:01.833017: step 55210, loss = 0.61 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 12:50:14.014560: step 55220, loss = 0.63 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 12:50:26.179020: step 55230, loss = 0.67 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 12:50:38.388560: step 55240, loss = 0.60 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 12:50:50.604981: step 55250, loss = 0.70 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 12:51:02.782745: step 55260, loss = 0.70 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 12:51:14.904773: step 55270, loss = 0.64 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 12:51:26.982053: step 55280, loss = 0.59 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 12:51:39.032536: step 55290, loss = 0.66 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 12:51:51.256351: step 55300, loss = 0.63 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 12:52:05.507922: step 55310, loss = 0.60 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 12:52:17.599865: step 55320, loss = 0.62 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 12:52:29.713778: step 55330, loss = 0.65 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 12:52:41.831093: step 55340, loss = 0.61 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 12:52:53.973331: step 55350, loss = 0.63 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 12:53:06.114675: step 55360, loss = 0.66 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 12:53:18.383121: step 55370, loss = 0.59 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 12:53:30.587539: step 55380, loss = 0.61 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 12:53:42.699008: step 55390, loss = 0.62 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 12:53:54.866820: step 55400, loss = 0.75 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 12:54:08.837365: step 55410, loss = 0.64 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 12:54:20.961532: step 55420, loss = 0.65 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 12:54:33.167598: step 55430, loss = 0.60 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 12:54:45.283059: step 55440, loss = 0.71 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 12:54:57.402955: step 55450, loss = 0.71 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 12:55:09.533001: step 55460, loss = 0.62 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 12:55:21.649548: step 55470, loss = 0.61 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 12:55:33.795818: step 55480, loss = 0.61 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 12:55:45.958926: step 55490, loss = 0.66 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 12:55:58.127332: step 55500, loss = 0.63 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 12:56:12.172744: step 55510, loss = 0.59 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 12:56:24.280613: step 55520, loss = 0.58 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 12:56:36.379189: step 55530, loss = 0.63 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 12:56:48.430617: step 55540, loss = 0.57 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 12:57:00.584055: step 55550, loss = 0.60 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 12:57:12.702439: step 55560, loss = 0.59 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 12:57:24.856257: step 55570, loss = 0.54 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 12:57:36.969315: step 55580, loss = 0.61 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 12:57:49.061375: step 55590, loss = 0.61 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 12:58:01.091830: step 55600, loss = 0.62 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 12:58:15.108046: step 55610, loss = 0.60 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 12:58:27.240947: step 55620, loss = 0.62 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 12:58:39.308894: step 55630, loss = 0.58 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 12:58:51.470127: step 55640, loss = 0.60 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 12:59:03.597401: step 55650, loss = 0.62 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 12:59:15.747462: step 55660, loss = 0.61 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 12:59:27.917461: step 55670, loss = 0.57 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 12:59:40.037921: step 55680, loss = 0.65 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 12:59:52.166189: step 55690, loss = 0.62 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 13:00:04.398902: step 55700, loss = 0.63 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 13:00:18.442387: step 55710, loss = 0.57 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 13:00:30.614053: step 55720, loss = 0.70 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 13:00:42.758550: step 55730, loss = 0.65 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 13:00:54.974753: step 55740, loss = 0.68 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 13:01:07.112811: step 55750, loss = 0.59 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 13:01:19.225800: step 55760, loss = 0.62 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 13:01:31.387951: step 55770, loss = 0.70 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 13:01:43.499984: step 55780, loss = 0.58 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 13:01:55.477854: step 55790, loss = 0.65 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 13:02:07.590000: step 55800, loss = 0.62 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 13:02:21.606383: step 55810, loss = 0.68 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 13:02:33.696237: step 55820, loss = 0.63 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 13:02:45.808719: step 55830, loss = 0.63 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 13:02:57.981046: step 55840, loss = 0.72 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 13:03:10.016329: step 55850, loss = 0.61 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 13:03:22.189408: step 55860, loss = 0.58 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 13:03:34.421741: step 55870, loss = 0.56 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 13:03:46.459353: step 55880, loss = 0.56 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 13:03:58.604673: step 55890, loss = 0.66 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 13:04:10.710026: step 55900, loss = 0.67 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 13:04:25.066053: step 55910, loss = 0.59 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 13:04:37.193289: step 55920, loss = 0.65 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 13:04:49.333848: step 55930, loss = 0.63 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 13:05:01.404723: step 55940, loss = 0.73 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 13:05:13.497623: step 55950, loss = 0.64 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 13:05:25.665597: step 55960, loss = 0.57 (24.8 examples/sec; 1.212 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 13:05:37.747581: step 55970, loss = 0.60 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 13:05:49.872171: step 55980, loss = 0.60 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 13:06:01.960409: step 55990, loss = 0.61 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 13:06:14.103773: step 56000, loss = 0.59 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 13:06:28.589288: step 56010, loss = 0.65 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 13:06:40.692603: step 56020, loss = 0.62 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 13:06:52.879860: step 56030, loss = 0.68 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 13:07:04.950339: step 56040, loss = 0.62 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 13:07:17.035451: step 56050, loss = 0.57 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 13:07:29.157901: step 56060, loss = 0.63 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 13:07:41.281530: step 56070, loss = 0.60 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 13:07:53.390636: step 56080, loss = 0.65 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 13:08:05.514507: step 56090, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 13:08:17.562155: step 56100, loss = 0.69 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 13:08:31.519642: step 56110, loss = 0.63 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 13:08:43.579236: step 56120, loss = 0.67 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 13:08:55.689732: step 56130, loss = 0.66 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 13:09:07.863902: step 56140, loss = 0.62 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 13:09:20.009544: step 56150, loss = 0.55 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 13:09:32.160694: step 56160, loss = 0.67 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 13:09:44.400743: step 56170, loss = 0.62 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 13:09:56.635987: step 56180, loss = 0.73 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 13:10:08.838994: step 56190, loss = 0.67 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 13:10:20.970128: step 56200, loss = 0.55 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 13:10:35.130547: step 56210, loss = 0.62 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 13:10:47.331425: step 56220, loss = 0.62 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 13:10:59.489660: step 56230, loss = 0.62 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 13:11:11.659041: step 56240, loss = 0.64 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 13:11:23.852186: step 56250, loss = 0.62 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 13:11:36.021381: step 56260, loss = 0.60 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 13:11:48.261645: step 56270, loss = 0.70 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 13:12:00.346287: step 56280, loss = 0.68 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 13:12:12.431534: step 56290, loss = 0.60 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 13:12:24.502372: step 56300, loss = 0.64 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 13:12:38.691563: step 56310, loss = 0.59 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-05-21 13:12:50.867855: step 56320, loss = 0.68 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 13:13:03.023419: step 56330, loss = 0.62 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 13:13:15.137923: step 56340, loss = 0.68 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 13:13:27.196671: step 56350, loss = 0.60 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 13:13:39.282327: step 56360, loss = 0.62 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 13:13:51.473895: step 56370, loss = 0.59 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 13:14:03.569765: step 56380, loss = 0.62 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 13:14:15.655599: step 56390, loss = 0.67 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 13:14:27.728854: step 56400, loss = 0.70 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 13:14:41.892425: step 56410, loss = 0.65 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 13:14:53.964506: step 56420, loss = 0.64 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 13:15:06.073614: step 56430, loss = 0.64 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 13:15:18.158717: step 56440, loss = 0.60 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 13:15:30.205231: step 56450, loss = 0.59 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 13:15:42.368820: step 56460, loss = 0.68 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 13:15:54.440434: step 56470, loss = 0.65 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 13:16:06.507614: step 56480, loss = 0.68 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 13:16:18.593123: step 56490, loss = 0.71 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 13:16:30.598136: step 56500, loss = 0.61 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 13:16:44.550540: step 56510, loss = 0.61 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 13:16:56.630911: step 56520, loss = 0.58 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 13:17:08.810214: step 56530, loss = 0.61 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 13:17:20.867383: step 56540, loss = 0.69 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 13:17:32.844608: step 56550, loss = 0.58 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 13:17:44.933353: step 56560, loss = 0.59 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 13:17:56.985162: step 56570, loss = 0.62 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 13:18:09.116392: step 56580, loss = 0.71 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 13:18:21.161419: step 56590, loss = 0.62 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 13:18:33.193555: step 56600, loss = 0.67 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 13:18:46.997121: step 56610, loss = 0.61 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 13:18:59.093682: step 56620, loss = 0.66 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 13:19:11.109223: step 56630, loss = 0.59 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 13:19:23.102246: step 56640, loss = 0.63 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 13:19:35.168023: step 56650, loss = 0.59 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 13:19:47.179775: step 56660, loss = 0.60 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 13:19:59.160505: step 56670, loss = 0.64 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 13:20:11.197853: step 56680, loss = 0.61 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 13:20:23.183226: step 56690, loss = 0.61 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 13:20:35.200942: step 56700, loss = 0.64 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 13:20:49.342076: step 56710, loss = 0.62 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 13:21:01.268116: step 56720, loss = 0.58 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 13:21:13.370043: step 56730, loss = 0.62 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 13:21:25.428905: step 56740, loss = 0.66 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-21 13:21:37.420736: step 56750, loss = 0.59 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 13:21:49.555976: step 56760, loss = 0.63 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-21 13:22:01.577451: step 56770, loss = 0.60 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 13:22:13.540753: step 56780, loss = 0.65 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 13:22:25.659527: step 56790, loss = 0.62 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 13:22:37.694475: step 56800, loss = 0.73 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 13:22:51.987582: step 56810, loss = 0.61 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 13:23:04.126741: step 56820, loss = 0.64 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 13:23:16.214884: step 56830, loss = 0.57 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 13:23:28.237680: step 56840, loss = 0.64 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 13:23:40.308718: step 56850, loss = 0.68 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 13:23:52.359228: step 56860, loss = 0.66 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 13:24:04.428871: step 56870, loss = 0.61 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 13:24:16.510759: step 56880, loss = 0.65 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 13:24:28.669153: step 56890, loss = 0.67 (25.0 examples/sec; 1.200 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 13:24:40.685601: step 56900, loss = 0.63 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 13:24:54.712860: step 56910, loss = 0.61 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 13:25:06.884128: step 56920, loss = 0.62 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 13:25:18.993236: step 56930, loss = 0.62 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 13:25:31.042964: step 56940, loss = 0.58 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 13:25:43.112427: step 56950, loss = 0.62 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 13:25:55.116320: step 56960, loss = 0.60 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 13:26:07.127206: step 56970, loss = 0.64 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 13:26:19.192918: step 56980, loss = 0.61 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 13:26:31.176074: step 56990, loss = 0.56 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 13:26:43.234030: step 57000, loss = 0.64 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 13:26:57.438535: step 57010, loss = 0.60 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 13:27:09.506563: step 57020, loss = 0.61 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 13:27:21.532667: step 57030, loss = 0.62 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 13:27:33.655273: step 57040, loss = 0.71 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 13:27:45.666811: step 57050, loss = 0.68 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 13:27:57.683018: step 57060, loss = 0.63 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 13:28:09.789805: step 57070, loss = 0.58 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 13:28:21.829188: step 57080, loss = 0.66 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 13:28:33.840762: step 57090, loss = 0.61 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 13:28:45.829876: step 57100, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 13:28:59.622093: step 57110, loss = 0.64 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 13:29:11.617402: step 57120, loss = 0.60 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 13:29:23.723457: step 57130, loss = 0.65 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 13:29:35.721920: step 57140, loss = 0.57 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 13:29:47.709529: step 57150, loss = 0.54 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 13:29:59.699818: step 57160, loss = 0.62 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 13:30:11.688217: step 57170, loss = 0.67 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 13:30:23.707359: step 57180, loss = 0.62 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 13:30:35.751752: step 57190, loss = 0.61 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 13:30:47.829926: step 57200, loss = 0.66 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 13:31:02.120682: step 57210, loss = 0.66 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 13:31:14.060869: step 57220, loss = 0.61 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 13:31:26.120988: step 57230, loss = 0.65 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 13:31:38.132074: step 57240, loss = 0.60 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 13:31:50.190849: step 57250, loss = 0.68 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 13:32:02.274804: step 57260, loss = 0.59 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 13:32:14.319540: step 57270, loss = 0.66 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 13:32:26.369122: step 57280, loss = 0.61 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 13:32:38.437504: step 57290, loss = 0.59 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 13:32:50.465847: step 57300, loss = 0.60 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 13:33:04.312441: step 57310, loss = 0.58 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 13:33:16.288657: step 57320, loss = 0.63 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 13:33:28.307669: step 57330, loss = 0.67 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 13:33:40.272173: step 57340, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 13:33:52.307189: step 57350, loss = 0.60 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 13:34:04.260253: step 57360, loss = 0.64 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 13:34:16.341436: step 57370, loss = 0.64 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 13:34:28.385751: step 57380, loss = 0.67 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 13:34:40.426987: step 57390, loss = 0.67 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 13:34:52.487359: step 57400, loss = 0.69 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 13:35:06.585678: step 57410, loss = 0.65 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 13:35:18.526428: step 57420, loss = 0.59 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 13:35:30.464507: step 57430, loss = 0.57 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 13:35:42.406995: step 57440, loss = 0.61 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 13:35:54.428286: step 57450, loss = 0.59 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 13:36:06.440138: step 57460, loss = 0.65 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 13:36:18.470630: step 57470, loss = 0.59 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 13:36:30.457512: step 57480, loss = 0.53 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 13:36:42.454647: step 57490, loss = 0.63 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 13:36:54.427439: step 57500, loss = 0.62 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 13:37:08.620278: step 57510, loss = 0.62 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 13:37:20.594038: step 57520, loss = 0.54 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 13:37:32.642315: step 57530, loss = 0.58 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 13:37:44.599508: step 57540, loss = 0.62 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 13:37:56.561361: step 57550, loss = 0.59 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 13:38:08.515185: step 57560, loss = 0.64 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 13:38:20.485448: step 57570, loss = 0.57 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 13:38:32.451182: step 57580, loss = 0.66 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 13:38:44.415777: step 57590, loss = 0.70 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-21 13:38:56.435385: step 57600, loss = 0.58 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 13:39:10.567161: step 57610, loss = 0.69 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 13:39:22.554389: step 57620, loss = 0.63 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 13:39:34.575773: step 57630, loss = 0.64 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 13:39:46.726840: step 57640, loss = 0.58 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 13:39:58.858478: step 57650, loss = 0.64 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 13:40:10.839886: step 57660, loss = 0.61 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 13:40:22.893714: step 57670, loss = 0.64 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 13:40:34.875911: step 57680, loss = 0.67 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 13:40:46.877618: step 57690, loss = 0.66 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 13:40:58.953573: step 57700, loss = 0.60 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 13:41:13.125714: step 57710, loss = 0.67 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 13:41:25.166851: step 57720, loss = 0.58 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 13:41:37.121844: step 57730, loss = 0.60 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 13:41:49.224331: step 57740, loss = 0.64 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 13:42:01.206963: step 57750, loss = 0.60 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 13:42:13.171660: step 57760, loss = 0.57 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 13:42:25.125921: step 57770, loss = 0.63 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 13:42:37.243026: step 57780, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 13:42:49.399881: step 57790, loss = 0.68 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 13:43:01.503284: step 57800, loss = 0.57 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 13:43:15.292393: step 57810, loss = 0.72 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 13:43:27.331771: step 57820, loss = 0.64 (25.1 examples/sec; 1.193 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 13:43:39.385916: step 57830, loss = 0.61 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-21 13:43:51.344998: step 57840, loss = 0.60 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 13:44:03.321280: step 57850, loss = 0.66 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 13:44:15.304724: step 57860, loss = 0.58 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 13:44:27.290923: step 57870, loss = 0.63 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 13:44:39.246519: step 57880, loss = 0.73 (25.5 examples/sec; 1.176 sec/batch)\n",
      "2019-05-21 13:44:51.262950: step 57890, loss = 0.61 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 13:45:03.281793: step 57900, loss = 0.65 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 13:45:17.242152: step 57910, loss = 0.67 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 13:45:29.279423: step 57920, loss = 0.66 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 13:45:41.264510: step 57930, loss = 0.68 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 13:45:53.355325: step 57940, loss = 0.64 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 13:46:05.413918: step 57950, loss = 0.63 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 13:46:17.547829: step 57960, loss = 0.60 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 13:46:29.562386: step 57970, loss = 0.74 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 13:46:41.513306: step 57980, loss = 0.72 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 13:46:53.511342: step 57990, loss = 0.63 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 13:47:05.509898: step 58000, loss = 0.53 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 13:47:19.588765: step 58010, loss = 0.69 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 13:47:31.585983: step 58020, loss = 0.60 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 13:47:43.599867: step 58030, loss = 0.60 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 13:47:55.688881: step 58040, loss = 0.61 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 13:48:07.733957: step 58050, loss = 0.67 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 13:48:19.801231: step 58060, loss = 0.61 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 13:48:31.790344: step 58070, loss = 0.64 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 13:48:43.799844: step 58080, loss = 0.65 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 13:48:55.830479: step 58090, loss = 0.65 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 13:49:07.841244: step 58100, loss = 0.63 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 13:49:21.576953: step 58110, loss = 0.57 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 13:49:33.542219: step 58120, loss = 0.62 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 13:49:45.512173: step 58130, loss = 0.62 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 13:49:57.612830: step 58140, loss = 0.63 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 13:50:09.735662: step 58150, loss = 0.58 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 13:50:21.817830: step 58160, loss = 0.63 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 13:50:33.798589: step 58170, loss = 0.59 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 13:50:45.786399: step 58180, loss = 0.59 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 13:50:57.796336: step 58190, loss = 0.59 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 13:51:09.818264: step 58200, loss = 0.59 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 13:51:24.068031: step 58210, loss = 0.60 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 13:51:36.105561: step 58220, loss = 0.61 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 13:51:48.224464: step 58230, loss = 0.63 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 13:52:00.314473: step 58240, loss = 0.57 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 13:52:12.417584: step 58250, loss = 0.60 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 13:52:24.488698: step 58260, loss = 0.59 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 13:52:36.611230: step 58270, loss = 0.59 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 13:52:48.677369: step 58280, loss = 0.60 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 13:53:00.767441: step 58290, loss = 0.66 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 13:53:12.896781: step 58300, loss = 0.56 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 13:53:27.069108: step 58310, loss = 0.61 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 13:53:39.095445: step 58320, loss = 0.65 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 13:53:51.080122: step 58330, loss = 0.64 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 13:54:03.192980: step 58340, loss = 0.59 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 13:54:15.247777: step 58350, loss = 0.58 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 13:54:27.448207: step 58360, loss = 0.63 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 13:54:39.549001: step 58370, loss = 0.58 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 13:54:51.602764: step 58380, loss = 0.69 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 13:55:03.738151: step 58390, loss = 0.56 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 13:55:15.817535: step 58400, loss = 0.64 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 13:55:29.988273: step 58410, loss = 0.66 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 13:55:42.087477: step 58420, loss = 0.60 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 13:55:54.209410: step 58430, loss = 0.60 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 13:56:06.393961: step 58440, loss = 0.59 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 13:56:18.453321: step 58450, loss = 0.60 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 13:56:30.532961: step 58460, loss = 0.58 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 13:56:42.710076: step 58470, loss = 0.69 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 13:56:54.901506: step 58480, loss = 0.57 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 13:57:06.994593: step 58490, loss = 0.62 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 13:57:19.131175: step 58500, loss = 0.59 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 13:57:33.097617: step 58510, loss = 0.64 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 13:57:45.224627: step 58520, loss = 0.60 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 13:57:57.300577: step 58530, loss = 0.61 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 13:58:09.457295: step 58540, loss = 0.62 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 13:58:21.674777: step 58550, loss = 0.66 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 13:58:33.770211: step 58560, loss = 0.62 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 13:58:45.846289: step 58570, loss = 0.64 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 13:58:57.864536: step 58580, loss = 0.62 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 13:59:10.002151: step 58590, loss = 0.61 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 13:59:22.174262: step 58600, loss = 0.67 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 13:59:36.497583: step 58610, loss = 0.66 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 13:59:48.602769: step 58620, loss = 0.65 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 14:00:00.717045: step 58630, loss = 0.65 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 14:00:12.953567: step 58640, loss = 0.62 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 14:00:25.043730: step 58650, loss = 0.63 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 14:00:37.186020: step 58660, loss = 0.57 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 14:00:49.360264: step 58670, loss = 0.58 (23.1 examples/sec; 1.298 sec/batch)\n",
      "2019-05-21 14:01:01.389725: step 58680, loss = 0.54 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 14:01:13.580271: step 58690, loss = 0.59 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 14:01:25.689152: step 58700, loss = 0.53 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 14:01:39.831284: step 58710, loss = 0.67 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 14:01:51.891952: step 58720, loss = 0.62 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 14:02:04.007008: step 58730, loss = 0.65 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 14:02:16.184554: step 58740, loss = 0.69 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 14:02:28.295809: step 58750, loss = 0.66 (24.8 examples/sec; 1.208 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 14:02:40.389504: step 58760, loss = 0.56 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 14:02:52.500835: step 58770, loss = 0.64 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 14:03:04.625739: step 58780, loss = 0.59 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 14:03:16.713045: step 58790, loss = 0.59 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 14:03:28.740673: step 58800, loss = 0.57 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 14:03:42.774312: step 58810, loss = 0.63 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 14:03:54.939958: step 58820, loss = 0.58 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 14:04:06.970941: step 58830, loss = 0.60 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 14:04:19.055526: step 58840, loss = 0.62 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 14:04:31.245913: step 58850, loss = 0.64 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 14:04:43.386232: step 58860, loss = 0.59 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 14:04:55.494414: step 58870, loss = 0.62 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 14:05:07.571896: step 58880, loss = 0.62 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 14:05:19.705225: step 58890, loss = 0.63 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 14:05:31.898396: step 58900, loss = 0.69 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 14:05:45.813076: step 58910, loss = 0.67 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 14:05:57.961444: step 58920, loss = 0.66 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 14:06:10.076021: step 58930, loss = 0.61 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 14:06:22.246355: step 58940, loss = 0.59 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 14:06:34.331763: step 58950, loss = 0.56 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 14:06:46.517763: step 58960, loss = 0.57 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 14:06:58.635363: step 58970, loss = 0.58 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 14:07:10.719871: step 58980, loss = 0.66 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 14:07:22.831980: step 58990, loss = 0.58 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 14:07:34.892533: step 59000, loss = 0.59 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 14:07:48.748274: step 59010, loss = 0.62 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 14:08:00.839384: step 59020, loss = 0.62 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 14:08:12.907171: step 59030, loss = 0.59 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 14:08:24.978482: step 59040, loss = 0.60 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 14:08:37.077550: step 59050, loss = 0.63 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 14:08:49.185378: step 59060, loss = 0.62 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 14:09:01.204044: step 59070, loss = 0.60 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 14:09:13.222952: step 59080, loss = 0.64 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 14:09:25.228425: step 59090, loss = 0.60 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 14:09:37.299387: step 59100, loss = 0.60 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 14:09:51.777974: step 59110, loss = 0.65 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 14:10:03.973098: step 59120, loss = 0.60 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 14:10:16.150144: step 59130, loss = 0.68 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 14:10:28.349938: step 59140, loss = 0.62 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 14:10:40.530321: step 59150, loss = 0.62 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 14:10:52.702474: step 59160, loss = 0.62 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 14:11:04.885225: step 59170, loss = 0.65 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 14:11:17.073083: step 59180, loss = 0.61 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 14:11:29.371215: step 59190, loss = 0.58 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 14:11:41.547627: step 59200, loss = 0.65 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 14:11:56.075620: step 59210, loss = 0.63 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 14:12:08.237777: step 59220, loss = 0.68 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 14:12:20.449355: step 59230, loss = 0.62 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 14:12:32.718992: step 59240, loss = 0.65 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 14:12:44.962496: step 59250, loss = 0.63 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 14:12:57.153184: step 59260, loss = 0.59 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 14:13:09.201372: step 59270, loss = 0.63 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 14:13:21.254737: step 59280, loss = 0.65 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 14:13:33.336866: step 59290, loss = 0.59 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 14:13:45.455062: step 59300, loss = 0.57 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 14:13:59.407037: step 59310, loss = 0.65 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 14:14:11.494689: step 59320, loss = 0.65 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 14:14:23.464268: step 59330, loss = 0.56 (25.3 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 14:14:35.570457: step 59340, loss = 0.61 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 14:14:47.646093: step 59350, loss = 0.56 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 14:14:59.689056: step 59360, loss = 0.62 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 14:15:11.728213: step 59370, loss = 0.58 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 14:15:23.780038: step 59380, loss = 0.59 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 14:15:35.847249: step 59390, loss = 0.56 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 14:15:47.927838: step 59400, loss = 0.63 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 14:16:02.039910: step 59410, loss = 0.62 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 14:16:14.081879: step 59420, loss = 0.58 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 14:16:26.158543: step 59430, loss = 0.61 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 14:16:38.229830: step 59440, loss = 0.58 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 14:16:50.310692: step 59450, loss = 0.63 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 14:17:02.382602: step 59460, loss = 0.63 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 14:17:14.500364: step 59470, loss = 0.66 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 14:17:26.560688: step 59480, loss = 0.63 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 14:17:38.632841: step 59490, loss = 0.66 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 14:17:50.764474: step 59500, loss = 0.58 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 14:18:04.704108: step 59510, loss = 0.67 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 14:18:16.860034: step 59520, loss = 0.60 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 14:18:28.892509: step 59530, loss = 0.64 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 14:18:41.013084: step 59540, loss = 0.60 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 14:18:53.158591: step 59550, loss = 0.62 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 14:19:05.276332: step 59560, loss = 0.62 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 14:19:17.369522: step 59570, loss = 0.64 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 14:19:29.396937: step 59580, loss = 0.52 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 14:19:41.375057: step 59590, loss = 0.59 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 14:19:53.413015: step 59600, loss = 0.59 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 14:20:07.683700: step 59610, loss = 0.64 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 14:20:19.734219: step 59620, loss = 0.58 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 14:20:31.882409: step 59630, loss = 0.62 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 14:20:43.930969: step 59640, loss = 0.63 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 14:20:56.031294: step 59650, loss = 0.69 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 14:21:08.227851: step 59660, loss = 0.61 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 14:21:20.294758: step 59670, loss = 0.56 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 14:21:32.364172: step 59680, loss = 0.59 (25.1 examples/sec; 1.197 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 14:21:44.416619: step 59690, loss = 0.58 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 14:21:56.541599: step 59700, loss = 0.56 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 14:22:10.502569: step 59710, loss = 0.61 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 14:22:22.535303: step 59720, loss = 0.59 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 14:22:34.625569: step 59730, loss = 0.65 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 14:22:46.631127: step 59740, loss = 0.58 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 14:22:58.685953: step 59750, loss = 0.64 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 14:23:10.792301: step 59760, loss = 0.63 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 14:23:22.825898: step 59770, loss = 0.70 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 14:23:34.955644: step 59780, loss = 0.63 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 14:23:46.997561: step 59790, loss = 0.63 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 14:23:59.124063: step 59800, loss = 0.65 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 14:24:13.125928: step 59810, loss = 0.57 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 14:24:25.191802: step 59820, loss = 0.64 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 14:24:37.180653: step 59830, loss = 0.62 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 14:24:49.270315: step 59840, loss = 0.67 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 14:25:01.283404: step 59850, loss = 0.57 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 14:25:13.315888: step 59860, loss = 0.63 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 14:25:25.346957: step 59870, loss = 0.65 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 14:25:37.390925: step 59880, loss = 0.61 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 14:25:49.468801: step 59890, loss = 0.60 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 14:26:01.569622: step 59900, loss = 0.58 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 14:26:15.786662: step 59910, loss = 0.53 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 14:26:27.951952: step 59920, loss = 0.59 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 14:26:40.025068: step 59930, loss = 0.69 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 14:26:52.072236: step 59940, loss = 0.58 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 14:27:04.109774: step 59950, loss = 0.70 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 14:27:16.227636: step 59960, loss = 0.69 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 14:27:28.439464: step 59970, loss = 0.58 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 14:27:40.453302: step 59980, loss = 0.62 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 14:27:52.482091: step 59990, loss = 0.61 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 14:28:04.547928: step 60000, loss = 0.61 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 14:28:21.989936: step 60010, loss = 0.62 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 14:28:34.194205: step 60020, loss = 0.70 (23.3 examples/sec; 1.289 sec/batch)\n",
      "2019-05-21 14:28:46.212795: step 60030, loss = 0.62 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 14:28:58.294996: step 60040, loss = 0.61 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 14:29:10.519441: step 60050, loss = 0.61 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 14:29:22.650188: step 60060, loss = 0.67 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 14:29:34.665614: step 60070, loss = 0.65 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 14:29:46.824412: step 60080, loss = 0.59 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 14:29:58.881875: step 60090, loss = 0.63 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 14:30:10.893929: step 60100, loss = 0.55 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 14:30:25.040917: step 60110, loss = 0.64 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 14:30:37.065298: step 60120, loss = 0.68 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 14:30:49.126431: step 60130, loss = 0.52 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 14:31:01.114164: step 60140, loss = 0.56 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-21 14:31:13.137039: step 60150, loss = 0.61 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 14:31:25.109913: step 60160, loss = 0.62 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 14:31:37.103623: step 60170, loss = 0.65 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 14:31:49.054898: step 60180, loss = 0.61 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 14:32:01.062340: step 60190, loss = 0.55 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 14:32:13.069189: step 60200, loss = 0.56 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 14:32:27.421122: step 60210, loss = 0.57 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 14:32:39.705248: step 60220, loss = 0.59 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 14:32:51.790831: step 60230, loss = 0.62 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 14:33:03.777007: step 60240, loss = 0.62 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 14:33:15.776409: step 60250, loss = 0.57 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 14:33:27.777901: step 60260, loss = 0.62 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 14:33:39.784200: step 60270, loss = 0.65 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 14:33:51.796182: step 60280, loss = 0.65 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 14:34:03.787387: step 60290, loss = 0.64 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 14:34:15.807709: step 60300, loss = 0.57 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 14:34:29.819903: step 60310, loss = 0.59 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 14:34:41.774668: step 60320, loss = 0.61 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 14:34:53.744750: step 60330, loss = 0.64 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 14:35:05.642151: step 60340, loss = 0.59 (25.6 examples/sec; 1.170 sec/batch)\n",
      "2019-05-21 14:35:17.711105: step 60350, loss = 0.53 (23.3 examples/sec; 1.289 sec/batch)\n",
      "2019-05-21 14:35:29.689439: step 60360, loss = 0.53 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 14:35:41.746000: step 60370, loss = 0.63 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 14:35:53.857731: step 60380, loss = 0.66 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 14:36:05.907763: step 60390, loss = 0.55 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 14:36:18.072151: step 60400, loss = 0.62 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 14:36:32.285930: step 60410, loss = 0.58 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 14:36:44.281885: step 60420, loss = 0.65 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 14:36:56.445846: step 60430, loss = 0.62 (23.5 examples/sec; 1.279 sec/batch)\n",
      "2019-05-21 14:37:08.447456: step 60440, loss = 0.54 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 14:37:20.530177: step 60450, loss = 0.60 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 14:37:32.546520: step 60460, loss = 0.63 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 14:37:44.677612: step 60470, loss = 0.67 (23.3 examples/sec; 1.290 sec/batch)\n",
      "2019-05-21 14:37:56.711944: step 60480, loss = 0.62 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 14:38:08.679660: step 60490, loss = 0.62 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 14:38:20.659728: step 60500, loss = 0.69 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 14:38:34.996139: step 60510, loss = 0.62 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 14:38:46.958975: step 60520, loss = 0.66 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 14:38:58.971729: step 60530, loss = 0.57 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 14:39:11.043876: step 60540, loss = 0.63 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 14:39:23.018349: step 60550, loss = 0.66 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 14:39:35.012453: step 60560, loss = 0.63 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 14:39:47.028480: step 60570, loss = 0.65 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 14:39:58.986186: step 60580, loss = 0.57 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 14:40:11.040984: step 60590, loss = 0.51 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 14:40:22.962548: step 60600, loss = 0.60 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 14:40:37.179364: step 60610, loss = 0.62 (24.8 examples/sec; 1.208 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 14:40:49.296081: step 60620, loss = 0.61 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 14:41:01.361974: step 60630, loss = 0.66 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 14:41:13.403502: step 60640, loss = 0.59 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 14:41:25.442214: step 60650, loss = 0.56 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 14:41:37.428219: step 60660, loss = 0.62 (25.6 examples/sec; 1.174 sec/batch)\n",
      "2019-05-21 14:41:49.465754: step 60670, loss = 0.65 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 14:42:01.516905: step 60680, loss = 0.61 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 14:42:13.627826: step 60690, loss = 0.62 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 14:42:25.636093: step 60700, loss = 0.60 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 14:42:39.504772: step 60710, loss = 0.57 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 14:42:51.599683: step 60720, loss = 0.67 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 14:43:03.779261: step 60730, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 14:43:15.835190: step 60740, loss = 0.59 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 14:43:27.871704: step 60750, loss = 0.66 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 14:43:39.943785: step 60760, loss = 0.66 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 14:43:52.075583: step 60770, loss = 0.58 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 14:44:04.091824: step 60780, loss = 0.53 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 14:44:16.094178: step 60790, loss = 0.68 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 14:44:28.116472: step 60800, loss = 0.70 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 14:44:42.080515: step 60810, loss = 0.62 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 14:44:54.041524: step 60820, loss = 0.62 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 14:45:05.985698: step 60830, loss = 0.56 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 14:45:17.956206: step 60840, loss = 0.65 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 14:45:29.931662: step 60850, loss = 0.58 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 14:45:41.962199: step 60860, loss = 0.53 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 14:45:53.980305: step 60870, loss = 0.56 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 14:46:05.946309: step 60880, loss = 0.53 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 14:46:17.932310: step 60890, loss = 0.61 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 14:46:29.885678: step 60900, loss = 0.64 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 14:46:44.237388: step 60910, loss = 0.58 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 14:46:56.258597: step 60920, loss = 0.60 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 14:47:08.258859: step 60930, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 14:47:20.307269: step 60940, loss = 0.60 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 14:47:32.414900: step 60950, loss = 0.70 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 14:47:44.444981: step 60960, loss = 0.62 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 14:47:56.523845: step 60970, loss = 0.65 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 14:48:08.606942: step 60980, loss = 0.60 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 14:48:20.623290: step 60990, loss = 0.65 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 14:48:32.609349: step 61000, loss = 0.62 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 14:48:46.879465: step 61010, loss = 0.69 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 14:48:58.981926: step 61020, loss = 0.61 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 14:49:11.026400: step 61030, loss = 0.59 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 14:49:23.085411: step 61040, loss = 0.63 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 14:49:35.236902: step 61050, loss = 0.61 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 14:49:47.368723: step 61060, loss = 0.60 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 14:49:59.457874: step 61070, loss = 0.59 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 14:50:11.430746: step 61080, loss = 0.60 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 14:50:23.521537: step 61090, loss = 0.60 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 14:50:35.546622: step 61100, loss = 0.67 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 14:50:49.612532: step 61110, loss = 0.61 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 14:51:01.638842: step 61120, loss = 0.60 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 14:51:13.670379: step 61130, loss = 0.64 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 14:51:25.787870: step 61140, loss = 0.57 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 14:51:37.816442: step 61150, loss = 0.56 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 14:51:49.854040: step 61160, loss = 0.58 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 14:52:01.867683: step 61170, loss = 0.69 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-21 14:52:13.882898: step 61180, loss = 0.65 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 14:52:25.895742: step 61190, loss = 0.60 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 14:52:37.913356: step 61200, loss = 0.61 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 14:52:51.734325: step 61210, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 14:53:03.725172: step 61220, loss = 0.58 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 14:53:15.740174: step 61230, loss = 0.68 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 14:53:27.825118: step 61240, loss = 0.58 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 14:53:39.894413: step 61250, loss = 0.61 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 14:53:51.959862: step 61260, loss = 0.64 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 14:54:04.019761: step 61270, loss = 0.54 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 14:54:16.034548: step 61280, loss = 0.66 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 14:54:28.054927: step 61290, loss = 0.71 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 14:54:40.113772: step 61300, loss = 0.64 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 14:54:54.462545: step 61310, loss = 0.58 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 14:55:06.460369: step 61320, loss = 0.59 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 14:55:18.439745: step 61330, loss = 0.62 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 14:55:30.458092: step 61340, loss = 0.57 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 14:55:42.489342: step 61350, loss = 0.62 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 14:55:54.515159: step 61360, loss = 0.61 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 14:56:06.543231: step 61370, loss = 0.65 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 14:56:18.547825: step 61380, loss = 0.56 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 14:56:30.629332: step 61390, loss = 0.63 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 14:56:42.644996: step 61400, loss = 0.63 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 14:56:56.976301: step 61410, loss = 0.61 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 14:57:09.083243: step 61420, loss = 0.64 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 14:57:21.091230: step 61430, loss = 0.60 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 14:57:33.044728: step 61440, loss = 0.57 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 14:57:45.035279: step 61450, loss = 0.60 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 14:57:57.006095: step 61460, loss = 0.64 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-21 14:58:09.048450: step 61470, loss = 0.71 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 14:58:21.124735: step 61480, loss = 0.61 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 14:58:33.277427: step 61490, loss = 0.67 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 14:58:45.268593: step 61500, loss = 0.63 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 14:58:59.555949: step 61510, loss = 0.67 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 14:59:11.576776: step 61520, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 14:59:23.666380: step 61530, loss = 0.61 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 14:59:35.752321: step 61540, loss = 0.66 (24.6 examples/sec; 1.218 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 14:59:47.879546: step 61550, loss = 0.58 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 14:59:59.908613: step 61560, loss = 0.59 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 15:00:11.986269: step 61570, loss = 0.64 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 15:00:23.995385: step 61580, loss = 0.59 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 15:00:36.061220: step 61590, loss = 0.59 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 15:00:48.061032: step 61600, loss = 0.59 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 15:01:02.450175: step 61610, loss = 0.69 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 15:01:14.446462: step 61620, loss = 0.57 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 15:01:26.472851: step 61630, loss = 0.66 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 15:01:38.557972: step 61640, loss = 0.64 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 15:01:50.594451: step 61650, loss = 0.60 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 15:02:02.622409: step 61660, loss = 0.60 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 15:02:14.590691: step 61670, loss = 0.56 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 15:02:26.626443: step 61680, loss = 0.71 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 15:02:38.676368: step 61690, loss = 0.64 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 15:02:50.869526: step 61700, loss = 0.64 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 15:03:04.839180: step 61710, loss = 0.57 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-21 15:03:16.911358: step 61720, loss = 0.60 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 15:03:28.992987: step 61730, loss = 0.54 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 15:03:41.072617: step 61740, loss = 0.58 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 15:03:53.166066: step 61750, loss = 0.57 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 15:04:05.374787: step 61760, loss = 0.73 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 15:04:17.444472: step 61770, loss = 0.57 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 15:04:29.584870: step 61780, loss = 0.57 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 15:04:41.677734: step 61790, loss = 0.65 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 15:04:53.902173: step 61800, loss = 0.66 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 15:05:08.116230: step 61810, loss = 0.58 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 15:05:20.124271: step 61820, loss = 0.56 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 15:05:32.150717: step 61830, loss = 0.56 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 15:05:44.178829: step 61840, loss = 0.56 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 15:05:56.165212: step 61850, loss = 0.63 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 15:06:08.159379: step 61860, loss = 0.61 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-21 15:06:20.138506: step 61870, loss = 0.63 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 15:06:32.196624: step 61880, loss = 0.64 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 15:06:44.215053: step 61890, loss = 0.58 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 15:06:56.217624: step 61900, loss = 0.53 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 15:07:10.476602: step 61910, loss = 0.58 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 15:07:22.503130: step 61920, loss = 0.73 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 15:07:34.561772: step 61930, loss = 0.71 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 15:07:46.547328: step 61940, loss = 0.66 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 15:07:58.529067: step 61950, loss = 0.57 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 15:08:10.523554: step 61960, loss = 0.61 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 15:08:22.616356: step 61970, loss = 0.64 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 15:08:34.635020: step 61980, loss = 0.61 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 15:08:46.735548: step 61990, loss = 0.61 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 15:08:58.763844: step 62000, loss = 0.67 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 15:09:12.669703: step 62010, loss = 0.59 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 15:09:24.715753: step 62020, loss = 0.58 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 15:09:36.707871: step 62030, loss = 0.63 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 15:09:48.730546: step 62040, loss = 0.62 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 15:10:00.876931: step 62050, loss = 0.61 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 15:10:12.873200: step 62060, loss = 0.58 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 15:10:24.966936: step 62070, loss = 0.62 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 15:10:37.015046: step 62080, loss = 0.58 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 15:10:49.181161: step 62090, loss = 0.58 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-05-21 15:11:01.295725: step 62100, loss = 0.54 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 15:11:15.302849: step 62110, loss = 0.58 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 15:11:27.259728: step 62120, loss = 0.57 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 15:11:39.310719: step 62130, loss = 0.66 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 15:11:51.405651: step 62140, loss = 0.63 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 15:12:03.508160: step 62150, loss = 0.63 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 15:12:15.591995: step 62160, loss = 0.58 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 15:12:27.715047: step 62170, loss = 0.57 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 15:12:39.693967: step 62180, loss = 0.65 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 15:12:51.696716: step 62190, loss = 0.60 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 15:13:03.719192: step 62200, loss = 0.62 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 15:13:17.610407: step 62210, loss = 0.66 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 15:13:29.667386: step 62220, loss = 0.53 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 15:13:41.653598: step 62230, loss = 0.64 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 15:13:53.684246: step 62240, loss = 0.58 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 15:14:05.727749: step 62250, loss = 0.59 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 15:14:17.803270: step 62260, loss = 0.62 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 15:14:29.842687: step 62270, loss = 0.54 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 15:14:41.906144: step 62280, loss = 0.66 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 15:14:53.992735: step 62290, loss = 0.58 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 15:15:06.209258: step 62300, loss = 0.63 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 15:15:20.440661: step 62310, loss = 0.63 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 15:15:32.473808: step 62320, loss = 0.63 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 15:15:44.522733: step 62330, loss = 0.61 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 15:15:56.605674: step 62340, loss = 0.61 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 15:16:08.639692: step 62350, loss = 0.65 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 15:16:20.758545: step 62360, loss = 0.60 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 15:16:32.860728: step 62370, loss = 0.65 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 15:16:44.853626: step 62380, loss = 0.61 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 15:16:56.848244: step 62390, loss = 0.57 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 15:17:08.781436: step 62400, loss = 0.65 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 15:17:22.550705: step 62410, loss = 0.70 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 15:17:34.549430: step 62420, loss = 0.51 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 15:17:46.437031: step 62430, loss = 0.58 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 15:17:58.403760: step 62440, loss = 0.63 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 15:18:10.407067: step 62450, loss = 0.63 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 15:18:22.364661: step 62460, loss = 0.58 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 15:18:34.289140: step 62470, loss = 0.62 (25.2 examples/sec; 1.192 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 15:18:46.226363: step 62480, loss = 0.60 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-21 15:18:58.255302: step 62490, loss = 0.63 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 15:19:10.323278: step 62500, loss = 0.63 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 15:19:24.509654: step 62510, loss = 0.55 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 15:19:36.507653: step 62520, loss = 0.60 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 15:19:48.442579: step 62530, loss = 0.59 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 15:20:00.449439: step 62540, loss = 0.57 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 15:20:12.506433: step 62550, loss = 0.58 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 15:20:24.548748: step 62560, loss = 0.60 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 15:20:36.646051: step 62570, loss = 0.57 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 15:20:48.646269: step 62580, loss = 0.64 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 15:21:00.707678: step 62590, loss = 0.57 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 15:21:12.759213: step 62600, loss = 0.60 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 15:21:27.032133: step 62610, loss = 0.66 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 15:21:39.038420: step 62620, loss = 0.66 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 15:21:51.009065: step 62630, loss = 0.66 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 15:22:03.040797: step 62640, loss = 0.69 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 15:22:15.138207: step 62650, loss = 0.62 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 15:22:27.182197: step 62660, loss = 0.63 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 15:22:39.207941: step 62670, loss = 0.58 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 15:22:51.178923: step 62680, loss = 0.60 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-21 15:23:03.076099: step 62690, loss = 0.64 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 15:23:15.067802: step 62700, loss = 0.57 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 15:23:29.253500: step 62710, loss = 0.60 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 15:23:41.211985: step 62720, loss = 0.63 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 15:23:53.182469: step 62730, loss = 0.52 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 15:24:05.148769: step 62740, loss = 0.65 (25.5 examples/sec; 1.179 sec/batch)\n",
      "2019-05-21 15:24:17.096286: step 62750, loss = 0.63 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 15:24:29.071076: step 62760, loss = 0.67 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 15:24:41.027354: step 62770, loss = 0.61 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 15:24:53.024693: step 62780, loss = 0.59 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 15:25:04.995811: step 62790, loss = 0.58 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 15:25:16.991516: step 62800, loss = 0.63 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 15:25:31.005501: step 62810, loss = 0.64 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 15:25:42.897042: step 62820, loss = 0.66 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 15:25:54.825099: step 62830, loss = 0.65 (25.6 examples/sec; 1.172 sec/batch)\n",
      "2019-05-21 15:26:06.796604: step 62840, loss = 0.66 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 15:26:18.792071: step 62850, loss = 0.51 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 15:26:30.725978: step 62860, loss = 0.54 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-21 15:26:42.788355: step 62870, loss = 0.59 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 15:26:54.822168: step 62880, loss = 0.64 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 15:27:06.946932: step 62890, loss = 0.65 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-05-21 15:27:19.051803: step 62900, loss = 0.61 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 15:27:33.006898: step 62910, loss = 0.68 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 15:27:45.138626: step 62920, loss = 0.60 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 15:27:57.243676: step 62930, loss = 0.57 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 15:28:09.358396: step 62940, loss = 0.61 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 15:28:21.401954: step 62950, loss = 0.69 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 15:28:33.444438: step 62960, loss = 0.59 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 15:28:45.516698: step 62970, loss = 0.57 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 15:28:57.559138: step 62980, loss = 0.61 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 15:29:09.596031: step 62990, loss = 0.56 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 15:29:21.718712: step 63000, loss = 0.68 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 15:29:35.853952: step 63010, loss = 0.62 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 15:29:47.858394: step 63020, loss = 0.62 (25.7 examples/sec; 1.167 sec/batch)\n",
      "2019-05-21 15:29:59.910985: step 63030, loss = 0.59 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 15:30:12.119733: step 63040, loss = 0.68 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 15:30:24.269944: step 63050, loss = 0.60 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 15:30:36.426146: step 63060, loss = 0.55 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 15:30:48.539538: step 63070, loss = 0.55 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 15:31:00.519327: step 63080, loss = 0.57 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 15:31:12.530012: step 63090, loss = 0.63 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 15:31:24.536950: step 63100, loss = 0.62 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 15:31:38.550113: step 63110, loss = 0.68 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 15:31:50.560382: step 63120, loss = 0.63 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 15:32:02.500951: step 63130, loss = 0.62 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 15:32:14.490076: step 63140, loss = 0.58 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 15:32:26.558192: step 63150, loss = 0.58 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 15:32:38.607341: step 63160, loss = 0.59 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 15:32:50.699479: step 63170, loss = 0.61 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 15:33:02.709819: step 63180, loss = 0.71 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 15:33:14.701032: step 63190, loss = 0.58 (25.4 examples/sec; 1.179 sec/batch)\n",
      "2019-05-21 15:33:26.724221: step 63200, loss = 0.58 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 15:33:41.044037: step 63210, loss = 0.57 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 15:33:53.050554: step 63220, loss = 0.57 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 15:34:05.029511: step 63230, loss = 0.63 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 15:34:17.081287: step 63240, loss = 0.66 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 15:34:29.208249: step 63250, loss = 0.59 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 15:34:41.280846: step 63260, loss = 0.65 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 15:34:53.257676: step 63270, loss = 0.62 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 15:35:05.318098: step 63280, loss = 0.60 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 15:35:17.354678: step 63290, loss = 0.57 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 15:35:29.365108: step 63300, loss = 0.56 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 15:35:43.133063: step 63310, loss = 0.57 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 15:35:55.123038: step 63320, loss = 0.60 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 15:36:07.087479: step 63330, loss = 0.59 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 15:36:19.093960: step 63340, loss = 0.61 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 15:36:31.128316: step 63350, loss = 0.57 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 15:36:43.140527: step 63360, loss = 0.57 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 15:36:55.129937: step 63370, loss = 0.60 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 15:37:07.103394: step 63380, loss = 0.62 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-21 15:37:19.030485: step 63390, loss = 0.67 (25.5 examples/sec; 1.175 sec/batch)\n",
      "2019-05-21 15:37:31.099539: step 63400, loss = 0.58 (24.6 examples/sec; 1.221 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 15:37:44.966441: step 63410, loss = 0.60 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 15:37:56.904265: step 63420, loss = 0.56 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 15:38:08.918096: step 63430, loss = 0.64 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 15:38:20.896972: step 63440, loss = 0.67 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 15:38:32.919654: step 63450, loss = 0.56 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 15:38:44.953424: step 63460, loss = 0.59 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 15:38:56.969837: step 63470, loss = 0.66 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 15:39:08.973019: step 63480, loss = 0.61 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 15:39:21.022299: step 63490, loss = 0.65 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 15:39:33.022689: step 63500, loss = 0.59 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 15:39:46.949896: step 63510, loss = 0.66 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 15:39:58.979191: step 63520, loss = 0.57 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 15:40:11.036553: step 63530, loss = 0.55 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 15:40:23.112430: step 63540, loss = 0.63 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 15:40:35.137130: step 63550, loss = 0.64 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 15:40:47.185220: step 63560, loss = 0.60 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 15:40:59.193013: step 63570, loss = 0.63 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 15:41:11.185576: step 63580, loss = 0.60 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 15:41:23.384489: step 63590, loss = 0.62 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 15:41:35.466404: step 63600, loss = 0.53 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 15:41:49.471976: step 63610, loss = 0.56 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 15:42:01.534934: step 63620, loss = 0.62 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 15:42:13.546990: step 63630, loss = 0.62 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 15:42:25.596778: step 63640, loss = 0.61 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 15:42:37.555951: step 63650, loss = 0.56 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 15:42:49.688486: step 63660, loss = 0.57 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 15:43:01.757800: step 63670, loss = 0.64 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 15:43:13.792638: step 63680, loss = 0.62 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 15:43:25.870168: step 63690, loss = 0.61 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 15:43:37.893622: step 63700, loss = 0.61 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 15:43:52.263281: step 63710, loss = 0.63 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 15:44:04.394325: step 63720, loss = 0.59 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 15:44:16.493541: step 63730, loss = 0.61 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 15:44:28.605813: step 63740, loss = 0.63 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 15:44:40.765019: step 63750, loss = 0.56 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 15:44:52.828579: step 63760, loss = 0.57 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 15:45:04.905310: step 63770, loss = 0.67 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 15:45:17.006977: step 63780, loss = 0.67 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 15:45:29.182606: step 63790, loss = 0.58 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 15:45:41.296558: step 63800, loss = 0.60 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 15:45:55.597382: step 63810, loss = 0.57 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 15:46:07.643589: step 63820, loss = 0.62 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 15:46:19.702946: step 63830, loss = 0.63 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 15:46:31.848493: step 63840, loss = 0.54 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 15:46:43.963598: step 63850, loss = 0.56 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 15:46:56.085106: step 63860, loss = 0.60 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 15:47:08.232372: step 63870, loss = 0.62 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 15:47:20.278126: step 63880, loss = 0.60 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 15:47:32.337465: step 63890, loss = 0.61 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-21 15:47:44.315278: step 63900, loss = 0.57 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 15:47:58.412770: step 63910, loss = 0.55 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 15:48:10.481966: step 63920, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 15:48:22.619339: step 63930, loss = 0.59 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 15:48:34.768151: step 63940, loss = 0.58 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 15:48:46.819367: step 63950, loss = 0.55 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 15:48:58.897234: step 63960, loss = 0.59 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 15:49:10.979061: step 63970, loss = 0.61 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 15:49:23.070838: step 63980, loss = 0.63 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 15:49:35.116420: step 63990, loss = 0.58 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 15:49:47.197187: step 64000, loss = 0.64 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 15:50:01.265534: step 64010, loss = 0.64 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 15:50:13.338813: step 64020, loss = 0.69 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 15:50:25.499842: step 64030, loss = 0.65 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 15:50:37.667490: step 64040, loss = 0.54 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 15:50:49.721699: step 64050, loss = 0.58 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 15:51:01.860013: step 64060, loss = 0.63 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 15:51:13.986080: step 64070, loss = 0.67 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 15:51:25.986581: step 64080, loss = 0.59 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 15:51:38.120780: step 64090, loss = 0.57 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 15:51:50.230310: step 64100, loss = 0.54 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 15:52:04.169535: step 64110, loss = 0.61 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 15:52:16.246726: step 64120, loss = 0.68 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 15:52:28.368596: step 64130, loss = 0.55 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 15:52:40.475939: step 64140, loss = 0.58 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 15:52:52.420594: step 64150, loss = 0.61 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 15:53:04.527228: step 64160, loss = 0.67 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 15:53:16.644659: step 64170, loss = 0.62 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 15:53:28.767561: step 64180, loss = 0.56 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 15:53:40.803726: step 64190, loss = 0.59 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 15:53:52.880251: step 64200, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 15:54:06.980172: step 64210, loss = 0.58 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 15:54:19.083924: step 64220, loss = 0.54 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 15:54:31.245307: step 64230, loss = 0.53 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 15:54:43.318218: step 64240, loss = 0.57 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 15:54:55.299666: step 64250, loss = 0.59 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 15:55:07.428194: step 64260, loss = 0.57 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 15:55:19.483984: step 64270, loss = 0.58 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 15:55:31.566926: step 64280, loss = 0.56 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 15:55:43.632208: step 64290, loss = 0.65 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 15:55:55.679502: step 64300, loss = 0.55 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 15:56:09.464882: step 64310, loss = 0.63 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 15:56:21.408349: step 64320, loss = 0.63 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 15:56:33.363037: step 64330, loss = 0.58 (25.2 examples/sec; 1.191 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 15:56:45.401910: step 64340, loss = 0.55 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 15:56:57.425945: step 64350, loss = 0.57 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 15:57:09.477624: step 64360, loss = 0.60 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 15:57:21.535174: step 64370, loss = 0.58 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 15:57:33.595779: step 64380, loss = 0.60 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 15:57:45.652662: step 64390, loss = 0.62 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 15:57:57.678551: step 64400, loss = 0.56 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 15:58:11.536659: step 64410, loss = 0.57 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 15:58:23.522542: step 64420, loss = 0.51 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 15:58:35.607693: step 64430, loss = 0.60 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 15:58:47.769157: step 64440, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 15:58:59.917169: step 64450, loss = 0.62 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 15:59:11.976006: step 64460, loss = 0.61 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 15:59:24.107349: step 64470, loss = 0.62 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 15:59:36.313133: step 64480, loss = 0.62 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 15:59:48.453333: step 64490, loss = 0.59 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 16:00:00.533092: step 64500, loss = 0.61 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 16:00:14.654727: step 64510, loss = 0.56 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 16:00:26.737381: step 64520, loss = 0.63 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 16:00:38.774006: step 64530, loss = 0.64 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 16:00:50.928041: step 64540, loss = 0.62 (24.1 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 16:01:03.082883: step 64550, loss = 0.58 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 16:01:15.148963: step 64560, loss = 0.62 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 16:01:27.173567: step 64570, loss = 0.63 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 16:01:39.165943: step 64580, loss = 0.63 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 16:01:51.106011: step 64590, loss = 0.76 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 16:02:03.136023: step 64600, loss = 0.66 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 16:02:17.237864: step 64610, loss = 0.64 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 16:02:29.199055: step 64620, loss = 0.61 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 16:02:41.174974: step 64630, loss = 0.60 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 16:02:53.294015: step 64640, loss = 0.60 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 16:03:05.301977: step 64650, loss = 0.76 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 16:03:17.294847: step 64660, loss = 0.61 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 16:03:29.365653: step 64670, loss = 0.58 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-21 16:03:41.375723: step 64680, loss = 0.59 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 16:03:53.411693: step 64690, loss = 0.65 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 16:04:05.430297: step 64700, loss = 0.55 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 16:04:19.760481: step 64710, loss = 0.59 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 16:04:31.775648: step 64720, loss = 0.55 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 16:04:43.779783: step 64730, loss = 0.57 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 16:04:55.692455: step 64740, loss = 0.62 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 16:05:07.666191: step 64750, loss = 0.64 (25.3 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 16:05:19.738018: step 64760, loss = 0.65 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 16:05:31.715626: step 64770, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 16:05:43.694786: step 64780, loss = 0.59 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 16:05:55.691749: step 64790, loss = 0.58 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 16:06:07.688832: step 64800, loss = 0.55 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 16:06:21.509357: step 64810, loss = 0.59 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-21 16:06:33.427889: step 64820, loss = 0.54 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 16:06:45.412718: step 64830, loss = 0.54 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 16:06:57.406914: step 64840, loss = 0.54 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 16:07:09.363963: step 64850, loss = 0.62 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-21 16:07:21.373391: step 64860, loss = 0.59 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 16:07:33.372972: step 64870, loss = 0.56 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 16:07:45.377670: step 64880, loss = 0.64 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 16:07:57.345064: step 64890, loss = 0.61 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 16:08:09.492522: step 64900, loss = 0.55 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 16:08:23.315238: step 64910, loss = 0.57 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 16:08:35.521004: step 64920, loss = 0.61 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 16:08:47.561024: step 64930, loss = 0.66 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 16:08:59.617926: step 64940, loss = 0.59 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 16:09:11.658762: step 64950, loss = 0.62 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 16:09:23.707871: step 64960, loss = 0.58 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 16:09:35.817236: step 64970, loss = 0.63 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 16:09:47.867485: step 64980, loss = 0.62 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 16:09:59.897020: step 64990, loss = 0.59 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 16:10:11.831223: step 65000, loss = 0.65 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 16:10:29.561130: step 65010, loss = 0.61 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 16:10:41.525087: step 65020, loss = 0.54 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 16:10:53.495619: step 65030, loss = 0.59 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 16:11:05.513870: step 65040, loss = 0.58 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 16:11:17.544090: step 65050, loss = 0.68 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 16:11:29.564754: step 65060, loss = 0.64 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 16:11:41.510667: step 65070, loss = 0.56 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-21 16:11:53.611710: step 65080, loss = 0.67 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 16:12:05.630961: step 65090, loss = 0.55 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 16:12:17.763276: step 65100, loss = 0.58 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 16:12:31.928198: step 65110, loss = 0.61 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 16:12:43.978076: step 65120, loss = 0.58 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 16:12:56.045304: step 65130, loss = 0.59 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 16:13:08.037336: step 65140, loss = 0.60 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 16:13:20.073236: step 65150, loss = 0.61 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 16:13:32.076056: step 65160, loss = 0.59 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 16:13:44.140655: step 65170, loss = 0.58 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 16:13:56.239703: step 65180, loss = 0.59 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 16:14:08.337619: step 65190, loss = 0.64 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 16:14:20.383737: step 65200, loss = 0.66 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 16:14:34.506212: step 65210, loss = 0.66 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 16:14:46.573775: step 65220, loss = 0.61 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 16:14:58.634811: step 65230, loss = 0.61 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 16:15:10.722261: step 65240, loss = 0.60 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 16:15:22.892129: step 65250, loss = 0.53 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 16:15:35.012036: step 65260, loss = 0.54 (25.0 examples/sec; 1.198 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 16:15:47.100176: step 65270, loss = 0.65 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 16:15:59.298921: step 65280, loss = 0.52 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 16:16:11.465213: step 65290, loss = 0.59 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 16:16:23.419268: step 65300, loss = 0.56 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 16:16:37.414745: step 65310, loss = 0.61 (23.6 examples/sec; 1.272 sec/batch)\n",
      "2019-05-21 16:16:49.396559: step 65320, loss = 0.68 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 16:17:01.497351: step 65330, loss = 0.57 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 16:17:13.549323: step 65340, loss = 0.60 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 16:17:25.639235: step 65350, loss = 0.60 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 16:17:37.722654: step 65360, loss = 0.63 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 16:17:49.777269: step 65370, loss = 0.61 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 16:18:01.831046: step 65380, loss = 0.57 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 16:18:13.856851: step 65390, loss = 0.53 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 16:18:25.865694: step 65400, loss = 0.57 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 16:18:39.840981: step 65410, loss = 0.60 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 16:18:51.876035: step 65420, loss = 0.66 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 16:19:03.815407: step 65430, loss = 0.62 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 16:19:15.870323: step 65440, loss = 0.57 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 16:19:27.847156: step 65450, loss = 0.59 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 16:19:39.884166: step 65460, loss = 0.65 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 16:19:51.862227: step 65470, loss = 0.62 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 16:20:03.831892: step 65480, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 16:20:15.839450: step 65490, loss = 0.58 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 16:20:27.888291: step 65500, loss = 0.59 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 16:20:41.690142: step 65510, loss = 0.63 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 16:20:53.816062: step 65520, loss = 0.57 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-05-21 16:21:05.876186: step 65530, loss = 0.60 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 16:21:17.988280: step 65540, loss = 0.56 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 16:21:30.122815: step 65550, loss = 0.65 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 16:21:42.206842: step 65560, loss = 0.57 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 16:21:54.225326: step 65570, loss = 0.56 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 16:22:06.331626: step 65580, loss = 0.65 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 16:22:18.400274: step 65590, loss = 0.53 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 16:22:30.490376: step 65600, loss = 0.63 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 16:22:44.844294: step 65610, loss = 0.59 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 16:22:57.022267: step 65620, loss = 0.60 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 16:23:09.150327: step 65630, loss = 0.59 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 16:23:21.237707: step 65640, loss = 0.60 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 16:23:33.338348: step 65650, loss = 0.52 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 16:23:45.495779: step 65660, loss = 0.58 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 16:23:57.461758: step 65670, loss = 0.60 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 16:24:09.553589: step 65680, loss = 0.69 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 16:24:21.639433: step 65690, loss = 0.64 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 16:24:33.721958: step 65700, loss = 0.54 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 16:24:47.794912: step 65710, loss = 0.61 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 16:24:59.937351: step 65720, loss = 0.60 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 16:25:12.066288: step 65730, loss = 0.63 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 16:25:24.194166: step 65740, loss = 0.60 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 16:25:36.402813: step 65750, loss = 0.67 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 16:25:48.511254: step 65760, loss = 0.64 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 16:26:00.581339: step 65770, loss = 0.64 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 16:26:12.713379: step 65780, loss = 0.54 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 16:26:24.852284: step 65790, loss = 0.52 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 16:26:36.969634: step 65800, loss = 0.61 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 16:26:51.215387: step 65810, loss = 0.59 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 16:27:03.261940: step 65820, loss = 0.55 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 16:27:15.332268: step 65830, loss = 0.65 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 16:27:27.552695: step 65840, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 16:27:39.696171: step 65850, loss = 0.64 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 16:27:51.816246: step 65860, loss = 0.60 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 16:28:03.964016: step 65870, loss = 0.60 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 16:28:16.104057: step 65880, loss = 0.61 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 16:28:28.243829: step 65890, loss = 0.59 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 16:28:40.356711: step 65900, loss = 0.62 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 16:28:54.408559: step 65910, loss = 0.67 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 16:29:06.351005: step 65920, loss = 0.63 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 16:29:18.437905: step 65930, loss = 0.64 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 16:29:30.531920: step 65940, loss = 0.57 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 16:29:42.663746: step 65950, loss = 0.60 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 16:29:54.742312: step 65960, loss = 0.58 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 16:30:06.854890: step 65970, loss = 0.60 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 16:30:18.951433: step 65980, loss = 0.62 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 16:30:31.014844: step 65990, loss = 0.57 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 16:30:43.225554: step 66000, loss = 0.63 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 16:30:57.444342: step 66010, loss = 0.63 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 16:31:09.571360: step 66020, loss = 0.68 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 16:31:21.667532: step 66030, loss = 0.64 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 16:31:33.800187: step 66040, loss = 0.56 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 16:31:45.956597: step 66050, loss = 0.64 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 16:31:58.112148: step 66060, loss = 0.60 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 16:32:10.167885: step 66070, loss = 0.68 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 16:32:22.310387: step 66080, loss = 0.67 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 16:32:34.435082: step 66090, loss = 0.60 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 16:32:46.517290: step 66100, loss = 0.59 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 16:33:00.853965: step 66110, loss = 0.64 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 16:33:13.023069: step 66120, loss = 0.65 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 16:33:25.136589: step 66130, loss = 0.62 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 16:33:37.249617: step 66140, loss = 0.58 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 16:33:49.403175: step 66150, loss = 0.53 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 16:34:01.516722: step 66160, loss = 0.54 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 16:34:13.525970: step 66170, loss = 0.56 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 16:34:25.571097: step 66180, loss = 0.63 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 16:34:37.697079: step 66190, loss = 0.56 (24.4 examples/sec; 1.230 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 16:34:49.831975: step 66200, loss = 0.52 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 16:35:04.123011: step 66210, loss = 0.59 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 16:35:16.211119: step 66220, loss = 0.65 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 16:35:28.292523: step 66230, loss = 0.57 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 16:35:40.387725: step 66240, loss = 0.58 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 16:35:52.512450: step 66250, loss = 0.57 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 16:36:04.598802: step 66260, loss = 0.58 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 16:36:16.761397: step 66270, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 16:36:28.952398: step 66280, loss = 0.64 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 16:36:41.004548: step 66290, loss = 0.56 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 16:36:53.030151: step 66300, loss = 0.59 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 16:37:07.070131: step 66310, loss = 0.62 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 16:37:19.042411: step 66320, loss = 0.63 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 16:37:30.986383: step 66330, loss = 0.56 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 16:37:43.007669: step 66340, loss = 0.51 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 16:37:55.029777: step 66350, loss = 0.61 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 16:38:07.045890: step 66360, loss = 0.60 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 16:38:19.058034: step 66370, loss = 0.60 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 16:38:31.142207: step 66380, loss = 0.62 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 16:38:43.206917: step 66390, loss = 0.61 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 16:38:55.307538: step 66400, loss = 0.60 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 16:39:09.347227: step 66410, loss = 0.57 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 16:39:21.377001: step 66420, loss = 0.61 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 16:39:33.359397: step 66430, loss = 0.59 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 16:39:45.432158: step 66440, loss = 0.66 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 16:39:57.484443: step 66450, loss = 0.62 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 16:40:09.513577: step 66460, loss = 0.56 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 16:40:21.591137: step 66470, loss = 0.54 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 16:40:33.616450: step 66480, loss = 0.69 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 16:40:45.710367: step 66490, loss = 0.56 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 16:40:57.790527: step 66500, loss = 0.54 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 16:41:11.777207: step 66510, loss = 0.60 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 16:41:23.933906: step 66520, loss = 0.57 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 16:41:36.005105: step 66530, loss = 0.53 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 16:41:48.362793: step 66540, loss = 0.64 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 16:42:00.429538: step 66550, loss = 0.61 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 16:42:12.429456: step 66560, loss = 0.55 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 16:42:24.456067: step 66570, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 16:42:36.430367: step 66580, loss = 0.64 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 16:42:48.393881: step 66590, loss = 0.62 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 16:43:00.421358: step 66600, loss = 0.57 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 16:43:14.396453: step 66610, loss = 0.61 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 16:43:26.459597: step 66620, loss = 0.53 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 16:43:38.498617: step 66630, loss = 0.55 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 16:43:50.548848: step 66640, loss = 0.56 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 16:44:02.561627: step 66650, loss = 0.54 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 16:44:14.574467: step 66660, loss = 0.56 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 16:44:26.607414: step 66670, loss = 0.59 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 16:44:38.631435: step 66680, loss = 0.58 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 16:44:50.646882: step 66690, loss = 0.68 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 16:45:02.808622: step 66700, loss = 0.58 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 16:45:16.728065: step 66710, loss = 0.68 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 16:45:28.738141: step 66720, loss = 0.56 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 16:45:40.665817: step 66730, loss = 0.63 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 16:45:52.628090: step 66740, loss = 0.49 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 16:46:04.635935: step 66750, loss = 0.58 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 16:46:16.574749: step 66760, loss = 0.66 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 16:46:28.566000: step 66770, loss = 0.60 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 16:46:40.576842: step 66780, loss = 0.66 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 16:46:52.642210: step 66790, loss = 0.50 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 16:47:04.738472: step 66800, loss = 0.60 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 16:47:18.657680: step 66810, loss = 0.59 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 16:47:30.699711: step 66820, loss = 0.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 16:47:42.775578: step 66830, loss = 0.62 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 16:47:54.837651: step 66840, loss = 0.57 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 16:48:06.883279: step 66850, loss = 0.61 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 16:48:18.949171: step 66860, loss = 0.60 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 16:48:31.013843: step 66870, loss = 0.58 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 16:48:43.162689: step 66880, loss = 0.56 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 16:48:55.747088: step 66890, loss = 0.57 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 16:49:08.219536: step 66900, loss = 0.55 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 16:49:22.598875: step 66910, loss = 0.59 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 16:49:35.192512: step 66920, loss = 0.63 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 16:49:47.544072: step 66930, loss = 0.54 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 16:50:00.004832: step 66940, loss = 0.52 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 16:50:12.463628: step 66950, loss = 0.57 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 16:50:24.970324: step 66960, loss = 0.62 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 16:50:37.545325: step 66970, loss = 0.59 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 16:50:50.121113: step 66980, loss = 0.63 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-05-21 16:51:02.822281: step 66990, loss = 0.61 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 16:51:15.321133: step 67000, loss = 0.55 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 16:51:29.813685: step 67010, loss = 0.59 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 16:51:42.396878: step 67020, loss = 0.65 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 16:51:54.905184: step 67030, loss = 0.55 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-05-21 16:52:07.478215: step 67040, loss = 0.63 (23.4 examples/sec; 1.280 sec/batch)\n",
      "2019-05-21 16:52:19.999616: step 67050, loss = 0.61 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 16:52:32.401428: step 67060, loss = 0.54 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 16:52:45.010079: step 67070, loss = 0.58 (21.9 examples/sec; 1.370 sec/batch)\n",
      "2019-05-21 16:52:57.535869: step 67080, loss = 0.56 (23.6 examples/sec; 1.272 sec/batch)\n",
      "2019-05-21 16:53:10.042847: step 67090, loss = 0.46 (24.0 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 16:53:22.556546: step 67100, loss = 0.61 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 16:53:36.984269: step 67110, loss = 0.60 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 16:53:49.482892: step 67120, loss = 0.62 (24.3 examples/sec; 1.232 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 16:54:01.973901: step 67130, loss = 0.57 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 16:54:14.503240: step 67140, loss = 0.63 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-05-21 16:54:27.015148: step 67150, loss = 0.67 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 16:54:39.555786: step 67160, loss = 0.58 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 16:54:52.109726: step 67170, loss = 0.56 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 16:55:04.532622: step 67180, loss = 0.58 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 16:55:17.065201: step 67190, loss = 0.59 (23.6 examples/sec; 1.269 sec/batch)\n",
      "2019-05-21 16:55:29.582262: step 67200, loss = 0.65 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 16:55:44.115725: step 67210, loss = 0.59 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 16:55:56.712670: step 67220, loss = 0.58 (22.2 examples/sec; 1.352 sec/batch)\n",
      "2019-05-21 16:56:09.419582: step 67230, loss = 0.63 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 16:56:21.936027: step 67240, loss = 0.54 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 16:56:34.490865: step 67250, loss = 0.58 (23.8 examples/sec; 1.258 sec/batch)\n",
      "2019-05-21 16:56:46.945114: step 67260, loss = 0.61 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 16:56:59.414441: step 67270, loss = 0.56 (24.1 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 16:57:11.884984: step 67280, loss = 0.62 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 16:57:24.450949: step 67290, loss = 0.73 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 16:57:36.807147: step 67300, loss = 0.59 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 16:57:51.743773: step 67310, loss = 0.62 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 16:58:04.266315: step 67320, loss = 0.49 (23.8 examples/sec; 1.263 sec/batch)\n",
      "2019-05-21 16:58:16.828951: step 67330, loss = 0.51 (23.7 examples/sec; 1.263 sec/batch)\n",
      "2019-05-21 16:58:29.336390: step 67340, loss = 0.60 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 16:58:41.934742: step 67350, loss = 0.61 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 16:58:54.527919: step 67360, loss = 0.61 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 16:59:07.251903: step 67370, loss = 0.57 (22.1 examples/sec; 1.356 sec/batch)\n",
      "2019-05-21 16:59:19.761803: step 67380, loss = 0.58 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 16:59:32.172548: step 67390, loss = 0.58 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 16:59:44.340697: step 67400, loss = 0.55 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 16:59:58.360002: step 67410, loss = 0.59 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 17:00:10.352116: step 67420, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 17:00:22.384085: step 67430, loss = 0.66 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 17:00:34.426529: step 67440, loss = 0.56 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 17:00:46.473646: step 67450, loss = 0.62 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 17:00:58.560945: step 67460, loss = 0.63 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 17:01:10.584826: step 67470, loss = 0.58 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 17:01:22.668744: step 67480, loss = 0.62 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 17:01:35.209235: step 67490, loss = 0.57 (23.1 examples/sec; 1.299 sec/batch)\n",
      "2019-05-21 17:01:47.694995: step 67500, loss = 0.53 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 17:02:02.152545: step 67510, loss = 0.60 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 17:02:14.682595: step 67520, loss = 0.54 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 17:02:27.254276: step 67530, loss = 0.59 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 17:02:39.846603: step 67540, loss = 0.54 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 17:02:52.471888: step 67550, loss = 0.62 (23.4 examples/sec; 1.281 sec/batch)\n",
      "2019-05-21 17:03:05.094038: step 67560, loss = 0.57 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 17:03:17.743800: step 67570, loss = 0.56 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 17:03:30.413084: step 67580, loss = 0.57 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 17:03:42.980766: step 67590, loss = 0.58 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 17:03:55.526624: step 67600, loss = 0.61 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 17:04:10.191508: step 67610, loss = 0.58 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 17:04:22.756159: step 67620, loss = 0.57 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-05-21 17:04:35.332919: step 67630, loss = 0.56 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 17:04:47.970291: step 67640, loss = 0.60 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 17:05:00.553171: step 67650, loss = 0.54 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 17:05:12.981573: step 67660, loss = 0.49 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 17:05:25.453915: step 67670, loss = 0.51 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 17:05:38.076809: step 67680, loss = 0.51 (23.7 examples/sec; 1.266 sec/batch)\n",
      "2019-05-21 17:05:50.701894: step 67690, loss = 0.56 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 17:06:03.285988: step 67700, loss = 0.59 (23.5 examples/sec; 1.279 sec/batch)\n",
      "2019-05-21 17:06:18.214176: step 67710, loss = 0.58 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 17:06:30.859550: step 67720, loss = 0.63 (22.4 examples/sec; 1.337 sec/batch)\n",
      "2019-05-21 17:06:43.522657: step 67730, loss = 0.62 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 17:06:56.141790: step 67740, loss = 0.60 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 17:07:08.705959: step 67750, loss = 0.64 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-05-21 17:07:21.246878: step 67760, loss = 0.58 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 17:07:33.827343: step 67770, loss = 0.54 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 17:07:46.290963: step 67780, loss = 0.53 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 17:07:58.866850: step 67790, loss = 0.57 (23.6 examples/sec; 1.269 sec/batch)\n",
      "2019-05-21 17:08:11.455280: step 67800, loss = 0.62 (22.5 examples/sec; 1.336 sec/batch)\n",
      "2019-05-21 17:08:26.242426: step 67810, loss = 0.62 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 17:08:38.808279: step 67820, loss = 0.63 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 17:08:51.320951: step 67830, loss = 0.68 (23.7 examples/sec; 1.263 sec/batch)\n",
      "2019-05-21 17:09:03.903475: step 67840, loss = 0.59 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-05-21 17:09:16.392431: step 67850, loss = 0.59 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 17:09:28.986529: step 67860, loss = 0.56 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 17:09:41.510121: step 67870, loss = 0.55 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 17:09:54.046913: step 67880, loss = 0.60 (23.6 examples/sec; 1.269 sec/batch)\n",
      "2019-05-21 17:10:06.605171: step 67890, loss = 0.56 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 17:10:19.247376: step 67900, loss = 0.66 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 17:10:34.008694: step 67910, loss = 0.61 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 17:10:46.571112: step 67920, loss = 0.62 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 17:10:59.103096: step 67930, loss = 0.61 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 17:11:11.662833: step 67940, loss = 0.54 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 17:11:24.278105: step 67950, loss = 0.65 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 17:11:36.951601: step 67960, loss = 0.65 (23.6 examples/sec; 1.271 sec/batch)\n",
      "2019-05-21 17:11:49.537049: step 67970, loss = 0.56 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 17:12:02.113264: step 67980, loss = 0.62 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 17:12:14.642816: step 67990, loss = 0.65 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 17:12:27.181188: step 68000, loss = 0.56 (23.7 examples/sec; 1.266 sec/batch)\n",
      "2019-05-21 17:12:41.568991: step 68010, loss = 0.52 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 17:12:53.984368: step 68020, loss = 0.57 (23.7 examples/sec; 1.268 sec/batch)\n",
      "2019-05-21 17:13:06.387501: step 68030, loss = 0.71 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 17:13:18.911376: step 68040, loss = 0.62 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 17:13:31.486603: step 68050, loss = 0.56 (24.2 examples/sec; 1.239 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 17:13:44.030661: step 68060, loss = 0.57 (23.6 examples/sec; 1.272 sec/batch)\n",
      "2019-05-21 17:13:56.567666: step 68070, loss = 0.51 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 17:14:09.061868: step 68080, loss = 0.73 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 17:14:21.576666: step 68090, loss = 0.51 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 17:14:34.105029: step 68100, loss = 0.67 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 17:14:48.434380: step 68110, loss = 0.59 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 17:15:00.984719: step 68120, loss = 0.57 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 17:15:13.600487: step 68130, loss = 0.52 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 17:15:26.227516: step 68140, loss = 0.65 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 17:15:38.695979: step 68150, loss = 0.66 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 17:15:51.087439: step 68160, loss = 0.57 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 17:16:03.591691: step 68170, loss = 0.57 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 17:16:16.104190: step 68180, loss = 0.60 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 17:16:28.615200: step 68190, loss = 0.60 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 17:16:41.002664: step 68200, loss = 0.59 (23.8 examples/sec; 1.263 sec/batch)\n",
      "2019-05-21 17:16:56.048320: step 68210, loss = 0.58 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 17:17:08.402009: step 68220, loss = 0.58 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 17:17:20.906828: step 68230, loss = 0.58 (23.7 examples/sec; 1.266 sec/batch)\n",
      "2019-05-21 17:17:33.408649: step 68240, loss = 0.63 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 17:17:45.936635: step 68250, loss = 0.60 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 17:17:58.246863: step 68260, loss = 0.54 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 17:18:10.811840: step 68270, loss = 0.62 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 17:18:23.334704: step 68280, loss = 0.54 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 17:18:35.846551: step 68290, loss = 0.68 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 17:18:48.271547: step 68300, loss = 0.62 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 17:19:02.907388: step 68310, loss = 0.61 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 17:19:15.219142: step 68320, loss = 0.62 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 17:19:27.651180: step 68330, loss = 0.56 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 17:19:39.949506: step 68340, loss = 0.57 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 17:19:52.494562: step 68350, loss = 0.60 (23.6 examples/sec; 1.269 sec/batch)\n",
      "2019-05-21 17:20:04.869382: step 68360, loss = 0.60 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 17:20:17.264163: step 68370, loss = 0.56 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 17:20:29.943990: step 68380, loss = 0.57 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 17:20:42.382666: step 68390, loss = 0.55 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 17:20:54.684300: step 68400, loss = 0.68 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 17:21:09.312535: step 68410, loss = 0.51 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 17:21:21.879313: step 68420, loss = 0.61 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 17:21:34.316678: step 68430, loss = 0.54 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 17:21:46.819278: step 68440, loss = 0.55 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 17:21:59.237395: step 68450, loss = 0.57 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 17:22:11.807149: step 68460, loss = 0.56 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 17:22:24.366158: step 68470, loss = 0.54 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 17:22:36.900797: step 68480, loss = 0.64 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 17:22:49.498914: step 68490, loss = 0.62 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 17:23:01.870345: step 68500, loss = 0.59 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 17:23:16.508585: step 68510, loss = 0.64 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 17:23:29.003530: step 68520, loss = 0.54 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 17:23:41.486783: step 68530, loss = 0.56 (23.8 examples/sec; 1.258 sec/batch)\n",
      "2019-05-21 17:23:53.946582: step 68540, loss = 0.58 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 17:24:06.389618: step 68550, loss = 0.61 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 17:24:18.830811: step 68560, loss = 0.64 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 17:24:31.311784: step 68570, loss = 0.57 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 17:24:43.741421: step 68580, loss = 0.60 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 17:24:56.164016: step 68590, loss = 0.66 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 17:25:08.569438: step 68600, loss = 0.58 (24.0 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 17:25:22.993211: step 68610, loss = 0.58 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-05-21 17:25:35.415120: step 68620, loss = 0.65 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 17:25:48.012144: step 68630, loss = 0.64 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 17:26:00.539674: step 68640, loss = 0.62 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 17:26:13.178037: step 68650, loss = 0.60 (22.3 examples/sec; 1.344 sec/batch)\n",
      "2019-05-21 17:26:25.835939: step 68660, loss = 0.56 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 17:26:38.427326: step 68670, loss = 0.61 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 17:26:51.013413: step 68680, loss = 0.63 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 17:27:03.604719: step 68690, loss = 0.50 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-05-21 17:27:16.187433: step 68700, loss = 0.59 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 17:27:30.548514: step 68710, loss = 0.60 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 17:27:43.071996: step 68720, loss = 0.56 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 17:27:55.552950: step 68730, loss = 0.58 (23.4 examples/sec; 1.284 sec/batch)\n",
      "2019-05-21 17:28:07.926581: step 68740, loss = 0.63 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 17:28:20.367188: step 68750, loss = 0.58 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 17:28:32.982836: step 68760, loss = 0.54 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 17:28:45.383112: step 68770, loss = 0.57 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 17:28:57.867061: step 68780, loss = 0.64 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 17:29:10.285660: step 68790, loss = 0.60 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 17:29:22.912620: step 68800, loss = 0.65 (23.2 examples/sec; 1.293 sec/batch)\n",
      "2019-05-21 17:29:37.632912: step 68810, loss = 0.56 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 17:29:50.150168: step 68820, loss = 0.58 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 17:30:02.608096: step 68830, loss = 0.54 (23.6 examples/sec; 1.271 sec/batch)\n",
      "2019-05-21 17:30:15.055347: step 68840, loss = 0.58 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 17:30:27.596233: step 68850, loss = 0.57 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 17:30:40.146167: step 68860, loss = 0.61 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 17:30:52.579226: step 68870, loss = 0.61 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 17:31:05.127963: step 68880, loss = 0.64 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 17:31:17.527069: step 68890, loss = 0.54 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 17:31:30.261601: step 68900, loss = 0.55 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 17:31:45.020993: step 68910, loss = 0.63 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 17:31:57.446589: step 68920, loss = 0.53 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 17:32:09.874628: step 68930, loss = 0.55 (23.8 examples/sec; 1.263 sec/batch)\n",
      "2019-05-21 17:32:22.352739: step 68940, loss = 0.67 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 17:32:34.860814: step 68950, loss = 0.55 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 17:32:47.363894: step 68960, loss = 0.60 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-05-21 17:33:00.027109: step 68970, loss = 0.61 (23.3 examples/sec; 1.288 sec/batch)\n",
      "2019-05-21 17:33:12.441125: step 68980, loss = 0.62 (24.1 examples/sec; 1.243 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 17:33:25.098634: step 68990, loss = 0.61 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 17:33:37.626184: step 69000, loss = 0.61 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 17:33:52.468445: step 69010, loss = 0.58 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 17:34:04.940071: step 69020, loss = 0.59 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 17:34:17.661999: step 69030, loss = 0.57 (23.3 examples/sec; 1.287 sec/batch)\n",
      "2019-05-21 17:34:30.268281: step 69040, loss = 0.53 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 17:34:42.808209: step 69050, loss = 0.54 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 17:34:55.259993: step 69060, loss = 0.51 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 17:35:07.770330: step 69070, loss = 0.65 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 17:35:20.360499: step 69080, loss = 0.59 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 17:35:33.068088: step 69090, loss = 0.59 (22.2 examples/sec; 1.353 sec/batch)\n",
      "2019-05-21 17:35:45.575341: step 69100, loss = 0.60 (23.6 examples/sec; 1.271 sec/batch)\n",
      "2019-05-21 17:36:00.145561: step 69110, loss = 0.56 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 17:36:12.667953: step 69120, loss = 0.60 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 17:36:25.047018: step 69130, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 17:36:37.546095: step 69140, loss = 0.69 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 17:36:49.990110: step 69150, loss = 0.60 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 17:37:02.568276: step 69160, loss = 0.54 (23.4 examples/sec; 1.284 sec/batch)\n",
      "2019-05-21 17:37:14.991918: step 69170, loss = 0.53 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 17:37:27.655830: step 69180, loss = 0.61 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-05-21 17:37:40.229188: step 69190, loss = 0.53 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 17:37:52.650451: step 69200, loss = 0.51 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 17:38:07.004331: step 69210, loss = 0.56 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 17:38:19.388248: step 69220, loss = 0.61 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 17:38:31.880771: step 69230, loss = 0.61 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 17:38:44.378240: step 69240, loss = 0.62 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 17:38:56.811886: step 69250, loss = 0.53 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 17:39:09.211748: step 69260, loss = 0.57 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 17:39:21.730344: step 69270, loss = 0.55 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 17:39:34.328389: step 69280, loss = 0.55 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 17:39:46.854545: step 69290, loss = 0.60 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 17:39:59.301792: step 69300, loss = 0.62 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 17:40:13.629848: step 69310, loss = 0.58 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 17:40:26.092696: step 69320, loss = 0.60 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 17:40:38.531419: step 69330, loss = 0.62 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 17:40:51.083941: step 69340, loss = 0.63 (22.1 examples/sec; 1.356 sec/batch)\n",
      "2019-05-21 17:41:03.649294: step 69350, loss = 0.59 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 17:41:16.113521: step 69360, loss = 0.56 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 17:41:28.639944: step 69370, loss = 0.63 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 17:41:41.068863: step 69380, loss = 0.59 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 17:41:53.516768: step 69390, loss = 0.63 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 17:42:05.949018: step 69400, loss = 0.63 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 17:42:20.362474: step 69410, loss = 0.54 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 17:42:32.670891: step 69420, loss = 0.59 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 17:42:45.334925: step 69430, loss = 0.58 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 17:42:57.828347: step 69440, loss = 0.54 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-05-21 17:43:10.298076: step 69450, loss = 0.56 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 17:43:22.788099: step 69460, loss = 0.57 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 17:43:35.307662: step 69470, loss = 0.58 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 17:43:48.020720: step 69480, loss = 0.60 (22.4 examples/sec; 1.340 sec/batch)\n",
      "2019-05-21 17:44:00.723185: step 69490, loss = 0.59 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 17:44:13.315411: step 69500, loss = 0.55 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 17:44:28.014189: step 69510, loss = 0.59 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 17:44:40.637953: step 69520, loss = 0.57 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 17:44:53.288130: step 69530, loss = 0.61 (23.5 examples/sec; 1.278 sec/batch)\n",
      "2019-05-21 17:45:05.874399: step 69540, loss = 0.62 (23.5 examples/sec; 1.278 sec/batch)\n",
      "2019-05-21 17:45:18.561650: step 69550, loss = 0.57 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 17:45:31.201391: step 69560, loss = 0.56 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 17:45:43.865365: step 69570, loss = 0.56 (23.2 examples/sec; 1.292 sec/batch)\n",
      "2019-05-21 17:45:56.580192: step 69580, loss = 0.59 (23.5 examples/sec; 1.279 sec/batch)\n",
      "2019-05-21 17:46:09.205453: step 69590, loss = 0.60 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 17:46:21.778904: step 69600, loss = 0.59 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 17:46:36.240335: step 69610, loss = 0.61 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 17:46:48.661398: step 69620, loss = 0.58 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 17:47:01.172565: step 69630, loss = 0.58 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 17:47:13.782312: step 69640, loss = 0.64 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 17:47:26.485934: step 69650, loss = 0.56 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 17:47:39.213033: step 69660, loss = 0.57 (21.8 examples/sec; 1.379 sec/batch)\n",
      "2019-05-21 17:47:51.879829: step 69670, loss = 0.61 (23.6 examples/sec; 1.274 sec/batch)\n",
      "2019-05-21 17:48:04.500651: step 69680, loss = 0.52 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 17:48:17.174615: step 69690, loss = 0.55 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 17:48:29.677750: step 69700, loss = 0.52 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-05-21 17:48:44.137948: step 69710, loss = 0.61 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 17:48:56.723972: step 69720, loss = 0.51 (23.4 examples/sec; 1.282 sec/batch)\n",
      "2019-05-21 17:49:09.388934: step 69730, loss = 0.59 (23.3 examples/sec; 1.286 sec/batch)\n",
      "2019-05-21 17:49:22.084183: step 69740, loss = 0.60 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 17:49:34.731878: step 69750, loss = 0.54 (23.5 examples/sec; 1.278 sec/batch)\n",
      "2019-05-21 17:49:47.432160: step 69760, loss = 0.61 (23.2 examples/sec; 1.292 sec/batch)\n",
      "2019-05-21 17:50:00.023962: step 69770, loss = 0.54 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 17:50:12.572984: step 69780, loss = 0.62 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 17:50:25.145451: step 69790, loss = 0.48 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 17:50:37.832555: step 69800, loss = 0.59 (23.5 examples/sec; 1.274 sec/batch)\n",
      "2019-05-21 17:50:52.472005: step 69810, loss = 0.58 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-05-21 17:51:05.072481: step 69820, loss = 0.62 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-05-21 17:51:17.666503: step 69830, loss = 0.59 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 17:51:30.274711: step 69840, loss = 0.49 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 17:51:42.949835: step 69850, loss = 0.54 (23.5 examples/sec; 1.275 sec/batch)\n",
      "2019-05-21 17:51:55.674202: step 69860, loss = 0.57 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 17:52:08.149295: step 69870, loss = 0.57 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-05-21 17:52:20.850982: step 69880, loss = 0.61 (24.1 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 17:52:33.592009: step 69890, loss = 0.59 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 17:52:46.382542: step 69900, loss = 0.64 (23.6 examples/sec; 1.273 sec/batch)\n",
      "2019-05-21 17:53:00.997976: step 69910, loss = 0.53 (23.6 examples/sec; 1.272 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 17:53:13.667756: step 69920, loss = 0.59 (22.0 examples/sec; 1.361 sec/batch)\n",
      "2019-05-21 17:53:26.311735: step 69930, loss = 0.56 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 17:53:38.893660: step 69940, loss = 0.53 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-05-21 17:53:51.620868: step 69950, loss = 0.57 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 17:54:04.319086: step 69960, loss = 0.59 (23.6 examples/sec; 1.272 sec/batch)\n",
      "2019-05-21 17:54:16.941245: step 69970, loss = 0.55 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 17:54:29.664578: step 69980, loss = 0.53 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 17:54:42.317092: step 69990, loss = 0.56 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 17:54:54.918137: step 70000, loss = 0.63 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 17:55:13.676515: step 70010, loss = 0.66 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 17:55:26.349996: step 70020, loss = 0.64 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 17:55:38.955929: step 70030, loss = 0.63 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 17:55:51.714458: step 70040, loss = 0.57 (22.0 examples/sec; 1.366 sec/batch)\n",
      "2019-05-21 17:56:04.231703: step 70050, loss = 0.56 (23.5 examples/sec; 1.277 sec/batch)\n",
      "2019-05-21 17:56:16.875603: step 70060, loss = 0.56 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 17:56:29.592567: step 70070, loss = 0.61 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-05-21 17:56:42.206170: step 70080, loss = 0.57 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-05-21 17:56:54.717731: step 70090, loss = 0.57 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 17:57:07.200803: step 70100, loss = 0.55 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 17:57:21.644273: step 70110, loss = 0.61 (23.7 examples/sec; 1.268 sec/batch)\n",
      "2019-05-21 17:57:34.243923: step 70120, loss = 0.56 (23.7 examples/sec; 1.263 sec/batch)\n",
      "2019-05-21 17:57:46.986074: step 70130, loss = 0.57 (21.9 examples/sec; 1.368 sec/batch)\n",
      "2019-05-21 17:57:59.635546: step 70140, loss = 0.63 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-05-21 17:58:12.315146: step 70150, loss = 0.56 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 17:58:25.066811: step 70160, loss = 0.64 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 17:58:37.723444: step 70170, loss = 0.59 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 17:58:50.254238: step 70180, loss = 0.57 (23.5 examples/sec; 1.276 sec/batch)\n",
      "2019-05-21 17:59:02.970586: step 70190, loss = 0.58 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 17:59:15.532962: step 70200, loss = 0.61 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 17:59:30.409884: step 70210, loss = 0.57 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 17:59:42.958410: step 70220, loss = 0.58 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 17:59:55.549718: step 70230, loss = 0.60 (23.3 examples/sec; 1.288 sec/batch)\n",
      "2019-05-21 18:00:08.216643: step 70240, loss = 0.52 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 18:00:20.846760: step 70250, loss = 0.64 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 18:00:33.540950: step 70260, loss = 0.65 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 18:00:46.169976: step 70270, loss = 0.52 (23.3 examples/sec; 1.290 sec/batch)\n",
      "2019-05-21 18:00:58.883342: step 70280, loss = 0.53 (23.4 examples/sec; 1.280 sec/batch)\n",
      "2019-05-21 18:01:11.397714: step 70290, loss = 0.61 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-05-21 18:01:24.230168: step 70300, loss = 0.59 (22.3 examples/sec; 1.346 sec/batch)\n",
      "2019-05-21 18:01:39.203306: step 70310, loss = 0.56 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 18:01:51.846913: step 70320, loss = 0.60 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 18:02:04.459035: step 70330, loss = 0.54 (23.3 examples/sec; 1.286 sec/batch)\n",
      "2019-05-21 18:02:17.084663: step 70340, loss = 0.56 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 18:02:29.655889: step 70350, loss = 0.55 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 18:02:42.414765: step 70360, loss = 0.61 (22.2 examples/sec; 1.349 sec/batch)\n",
      "2019-05-21 18:02:54.985444: step 70370, loss = 0.64 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-05-21 18:03:07.562368: step 70380, loss = 0.63 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 18:03:20.247163: step 70390, loss = 0.53 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 18:03:32.823951: step 70400, loss = 0.53 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 18:03:47.583436: step 70410, loss = 0.57 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 18:04:00.271863: step 70420, loss = 0.57 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-05-21 18:04:12.880329: step 70430, loss = 0.63 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 18:04:25.438063: step 70440, loss = 0.58 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 18:04:38.066853: step 70450, loss = 0.55 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 18:04:50.756820: step 70460, loss = 0.58 (23.5 examples/sec; 1.275 sec/batch)\n",
      "2019-05-21 18:05:03.341312: step 70470, loss = 0.55 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 18:05:15.901315: step 70480, loss = 0.57 (23.6 examples/sec; 1.269 sec/batch)\n",
      "2019-05-21 18:05:28.687025: step 70490, loss = 0.55 (22.5 examples/sec; 1.334 sec/batch)\n",
      "2019-05-21 18:05:41.314856: step 70500, loss = 0.63 (23.1 examples/sec; 1.296 sec/batch)\n",
      "2019-05-21 18:05:55.746607: step 70510, loss = 0.58 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 18:06:08.436221: step 70520, loss = 0.70 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-05-21 18:06:21.146592: step 70530, loss = 0.59 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 18:06:33.805466: step 70540, loss = 0.59 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 18:06:46.505089: step 70550, loss = 0.57 (22.5 examples/sec; 1.331 sec/batch)\n",
      "2019-05-21 18:06:59.071429: step 70560, loss = 0.58 (23.2 examples/sec; 1.295 sec/batch)\n",
      "2019-05-21 18:07:11.804535: step 70570, loss = 0.52 (23.4 examples/sec; 1.280 sec/batch)\n",
      "2019-05-21 18:07:24.353683: step 70580, loss = 0.53 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 18:07:36.804305: step 70590, loss = 0.57 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 18:07:49.252723: step 70600, loss = 0.49 (23.9 examples/sec; 1.258 sec/batch)\n",
      "2019-05-21 18:08:03.603035: step 70610, loss = 0.59 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 18:08:15.957322: step 70620, loss = 0.59 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 18:08:28.310492: step 70630, loss = 0.60 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 18:08:40.737818: step 70640, loss = 0.55 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 18:08:53.109961: step 70650, loss = 0.57 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 18:09:05.584568: step 70660, loss = 0.64 (23.8 examples/sec; 1.258 sec/batch)\n",
      "2019-05-21 18:09:18.073320: step 70670, loss = 0.55 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 18:09:30.556714: step 70680, loss = 0.62 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 18:09:42.991517: step 70690, loss = 0.56 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 18:09:55.475700: step 70700, loss = 0.62 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-05-21 18:10:09.890569: step 70710, loss = 0.66 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 18:10:22.289419: step 70720, loss = 0.64 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 18:10:34.621428: step 70730, loss = 0.57 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 18:10:47.046021: step 70740, loss = 0.61 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 18:10:59.459718: step 70750, loss = 0.58 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 18:11:11.828951: step 70760, loss = 0.60 (24.1 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 18:11:24.284625: step 70770, loss = 0.54 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 18:11:36.718299: step 70780, loss = 0.55 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 18:11:49.151455: step 70790, loss = 0.61 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 18:12:01.586298: step 70800, loss = 0.52 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 18:12:16.199642: step 70810, loss = 0.56 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 18:12:28.576003: step 70820, loss = 0.62 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 18:12:40.870518: step 70830, loss = 0.62 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 18:12:53.263421: step 70840, loss = 0.60 (24.2 examples/sec; 1.242 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 18:13:05.679072: step 70850, loss = 0.53 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 18:13:18.134399: step 70860, loss = 0.61 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 18:13:30.545907: step 70870, loss = 0.59 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 18:13:42.961789: step 70880, loss = 0.56 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 18:13:55.300560: step 70890, loss = 0.52 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 18:14:07.655140: step 70900, loss = 0.61 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 18:14:21.858257: step 70910, loss = 0.55 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 18:14:34.279426: step 70920, loss = 0.67 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 18:14:46.714320: step 70930, loss = 0.59 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 18:14:59.120505: step 70940, loss = 0.59 (23.7 examples/sec; 1.266 sec/batch)\n",
      "2019-05-21 18:15:11.511355: step 70950, loss = 0.56 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 18:15:23.904475: step 70960, loss = 0.56 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 18:15:36.361015: step 70970, loss = 0.56 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 18:15:48.835431: step 70980, loss = 0.54 (23.7 examples/sec; 1.268 sec/batch)\n",
      "2019-05-21 18:16:01.218164: step 70990, loss = 0.58 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 18:16:13.646394: step 71000, loss = 0.53 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 18:16:27.895967: step 71010, loss = 0.61 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 18:16:40.332206: step 71020, loss = 0.54 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 18:16:52.755464: step 71030, loss = 0.58 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-05-21 18:17:05.175284: step 71040, loss = 0.60 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 18:17:17.604671: step 71050, loss = 0.57 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 18:17:30.030883: step 71060, loss = 0.59 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 18:17:42.479738: step 71070, loss = 0.69 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-05-21 18:17:54.735847: step 71080, loss = 0.60 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 18:18:07.153952: step 71090, loss = 0.62 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 18:18:19.580293: step 71100, loss = 0.62 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 18:18:34.239331: step 71110, loss = 0.59 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 18:18:46.694749: step 71120, loss = 0.56 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 18:18:59.162619: step 71130, loss = 0.55 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 18:19:11.551576: step 71140, loss = 0.56 (23.8 examples/sec; 1.258 sec/batch)\n",
      "2019-05-21 18:19:24.005004: step 71150, loss = 0.64 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 18:19:36.368106: step 71160, loss = 0.58 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 18:19:48.752920: step 71170, loss = 0.63 (23.8 examples/sec; 1.263 sec/batch)\n",
      "2019-05-21 18:20:01.188825: step 71180, loss = 0.56 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 18:20:13.633947: step 71190, loss = 0.61 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 18:20:26.085671: step 71200, loss = 0.60 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 18:20:40.513832: step 71210, loss = 0.59 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 18:20:53.000886: step 71220, loss = 0.55 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-05-21 18:21:05.386761: step 71230, loss = 0.62 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 18:21:17.806507: step 71240, loss = 0.57 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 18:21:30.190456: step 71250, loss = 0.59 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 18:21:42.649992: step 71260, loss = 0.60 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 18:21:55.092483: step 71270, loss = 0.56 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 18:22:07.504124: step 71280, loss = 0.64 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 18:22:19.920477: step 71290, loss = 0.52 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 18:22:32.383391: step 71300, loss = 0.52 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 18:22:46.724739: step 71310, loss = 0.60 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 18:22:59.100116: step 71320, loss = 0.65 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 18:23:11.460002: step 71330, loss = 0.59 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 18:23:23.847299: step 71340, loss = 0.58 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 18:23:36.157396: step 71350, loss = 0.51 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 18:23:48.195246: step 71360, loss = 0.63 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 18:24:00.210237: step 71370, loss = 0.54 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 18:24:12.167909: step 71380, loss = 0.62 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 18:24:24.154455: step 71390, loss = 0.56 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 18:24:36.163021: step 71400, loss = 0.54 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 18:24:50.537697: step 71410, loss = 0.56 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 18:25:02.542101: step 71420, loss = 0.58 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 18:25:14.512553: step 71430, loss = 0.58 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 18:25:26.515236: step 71440, loss = 0.55 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 18:25:38.508118: step 71450, loss = 0.61 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 18:25:50.471283: step 71460, loss = 0.67 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 18:26:02.358125: step 71470, loss = 0.63 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 18:26:14.238213: step 71480, loss = 0.57 (25.5 examples/sec; 1.178 sec/batch)\n",
      "2019-05-21 18:26:26.214792: step 71490, loss = 0.58 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 18:26:38.661842: step 71500, loss = 0.63 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 18:26:53.247129: step 71510, loss = 0.56 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 18:27:05.584384: step 71520, loss = 0.60 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 18:27:18.035458: step 71530, loss = 0.58 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 18:27:30.437221: step 71540, loss = 0.62 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 18:27:42.792124: step 71550, loss = 0.61 (23.8 examples/sec; 1.258 sec/batch)\n",
      "2019-05-21 18:27:55.214095: step 71560, loss = 0.61 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 18:28:07.582568: step 71570, loss = 0.55 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 18:28:19.881888: step 71580, loss = 0.62 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 18:28:32.315125: step 71590, loss = 0.65 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 18:28:44.718364: step 71600, loss = 0.60 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 18:28:59.288456: step 71610, loss = 0.59 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 18:29:11.687779: step 71620, loss = 0.62 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 18:29:24.043601: step 71630, loss = 0.53 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 18:29:36.524617: step 71640, loss = 0.68 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 18:29:48.952072: step 71650, loss = 0.65 (23.9 examples/sec; 1.258 sec/batch)\n",
      "2019-05-21 18:30:01.352632: step 71660, loss = 0.60 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 18:30:13.786061: step 71670, loss = 0.62 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 18:30:26.233360: step 71680, loss = 0.55 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 18:30:38.669665: step 71690, loss = 0.56 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 18:30:51.079975: step 71700, loss = 0.63 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 18:31:05.429699: step 71710, loss = 0.63 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 18:31:17.790441: step 71720, loss = 0.53 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 18:31:30.205303: step 71730, loss = 0.54 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 18:31:42.667052: step 71740, loss = 0.58 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 18:31:55.095433: step 71750, loss = 0.54 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 18:32:07.527157: step 71760, loss = 0.54 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 18:32:19.939204: step 71770, loss = 0.59 (24.1 examples/sec; 1.243 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 18:32:32.401848: step 71780, loss = 0.55 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 18:32:44.873669: step 71790, loss = 0.55 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 18:32:57.225333: step 71800, loss = 0.59 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 18:33:11.928683: step 71810, loss = 0.53 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 18:33:24.243895: step 71820, loss = 0.57 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 18:33:36.605193: step 71830, loss = 0.58 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 18:33:49.010681: step 71840, loss = 0.53 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 18:34:01.400310: step 71850, loss = 0.59 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 18:34:13.822234: step 71860, loss = 0.53 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 18:34:26.179733: step 71870, loss = 0.60 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-05-21 18:34:38.583557: step 71880, loss = 0.56 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 18:34:51.003555: step 71890, loss = 0.65 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 18:35:03.323434: step 71900, loss = 0.62 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 18:35:17.529664: step 71910, loss = 0.59 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 18:35:29.909147: step 71920, loss = 0.60 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 18:35:42.350093: step 71930, loss = 0.54 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 18:35:54.796461: step 71940, loss = 0.60 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 18:36:07.199448: step 71950, loss = 0.58 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 18:36:19.652760: step 71960, loss = 0.62 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 18:36:32.036337: step 71970, loss = 0.58 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 18:36:44.387106: step 71980, loss = 0.60 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 18:36:56.842735: step 71990, loss = 0.53 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 18:37:09.271839: step 72000, loss = 0.55 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 18:37:23.557382: step 72010, loss = 0.66 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 18:37:35.990507: step 72020, loss = 0.65 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 18:37:48.477706: step 72030, loss = 0.56 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-05-21 18:38:00.880556: step 72040, loss = 0.56 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 18:38:13.367117: step 72050, loss = 0.55 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 18:38:25.797127: step 72060, loss = 0.54 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 18:38:38.091157: step 72070, loss = 0.61 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 18:38:50.543930: step 72080, loss = 0.63 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 18:39:02.968689: step 72090, loss = 0.61 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 18:39:15.353942: step 72100, loss = 0.60 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 18:39:29.607178: step 72110, loss = 0.59 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 18:39:42.081169: step 72120, loss = 0.53 (23.5 examples/sec; 1.275 sec/batch)\n",
      "2019-05-21 18:39:54.508384: step 72130, loss = 0.66 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 18:40:06.859611: step 72140, loss = 0.63 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 18:40:19.195175: step 72150, loss = 0.55 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 18:40:31.625697: step 72160, loss = 0.68 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 18:40:44.029276: step 72170, loss = 0.64 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 18:40:56.500520: step 72180, loss = 0.51 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 18:41:08.864385: step 72190, loss = 0.69 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 18:41:21.238817: step 72200, loss = 0.61 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 18:41:35.888154: step 72210, loss = 0.57 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 18:41:48.238131: step 72220, loss = 0.68 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 18:42:00.718281: step 72230, loss = 0.61 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 18:42:13.162253: step 72240, loss = 0.56 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 18:42:25.620477: step 72250, loss = 0.61 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 18:42:38.000174: step 72260, loss = 0.60 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 18:42:50.447281: step 72270, loss = 0.59 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 18:43:02.865204: step 72280, loss = 0.67 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 18:43:15.334002: step 72290, loss = 0.63 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-05-21 18:43:27.738775: step 72300, loss = 0.55 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 18:43:42.039698: step 72310, loss = 0.64 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 18:43:54.312200: step 72320, loss = 0.61 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 18:44:06.735061: step 72330, loss = 0.52 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 18:44:19.122827: step 72340, loss = 0.60 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 18:44:31.450616: step 72350, loss = 0.55 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 18:44:43.896189: step 72360, loss = 0.65 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 18:44:56.345597: step 72370, loss = 0.57 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 18:45:08.729620: step 72380, loss = 0.58 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 18:45:21.209637: step 72390, loss = 0.63 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 18:45:33.630039: step 72400, loss = 0.51 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 18:45:48.008689: step 72410, loss = 0.56 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 18:46:00.433064: step 72420, loss = 0.51 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 18:46:12.819249: step 72430, loss = 0.56 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 18:46:25.221144: step 72440, loss = 0.58 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 18:46:37.616027: step 72450, loss = 0.62 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 18:46:50.057820: step 72460, loss = 0.58 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 18:47:02.463043: step 72470, loss = 0.60 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 18:47:14.883073: step 72480, loss = 0.59 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 18:47:27.260115: step 72490, loss = 0.55 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 18:47:39.608395: step 72500, loss = 0.49 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 18:47:53.817829: step 72510, loss = 0.58 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 18:48:06.214927: step 72520, loss = 0.58 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 18:48:18.643216: step 72530, loss = 0.53 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 18:48:31.089611: step 72540, loss = 0.58 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 18:48:43.463479: step 72550, loss = 0.60 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 18:48:55.753827: step 72560, loss = 0.53 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 18:49:08.208950: step 72570, loss = 0.64 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 18:49:20.657423: step 72580, loss = 0.57 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-05-21 18:49:33.058364: step 72590, loss = 0.54 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 18:49:45.399029: step 72600, loss = 0.55 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 18:49:59.699086: step 72610, loss = 0.64 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 18:50:12.089131: step 72620, loss = 0.61 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 18:50:24.507562: step 72630, loss = 0.68 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 18:50:36.929753: step 72640, loss = 0.54 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 18:50:49.256871: step 72650, loss = 0.61 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 18:51:01.688338: step 72660, loss = 0.55 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 18:51:14.112727: step 72670, loss = 0.55 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 18:51:26.569086: step 72680, loss = 0.52 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 18:51:39.000850: step 72690, loss = 0.52 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 18:51:51.388034: step 72700, loss = 0.59 (24.5 examples/sec; 1.223 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 18:52:05.681088: step 72710, loss = 0.55 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 18:52:18.088319: step 72720, loss = 0.58 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 18:52:30.522565: step 72730, loss = 0.56 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 18:52:42.984160: step 72740, loss = 0.58 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 18:52:55.459713: step 72750, loss = 0.54 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 18:53:07.895612: step 72760, loss = 0.53 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 18:53:20.347258: step 72770, loss = 0.57 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 18:53:32.785091: step 72780, loss = 0.65 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 18:53:45.253754: step 72790, loss = 0.59 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 18:53:57.724459: step 72800, loss = 0.57 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 18:54:11.959474: step 72810, loss = 0.58 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 18:54:24.343669: step 72820, loss = 0.61 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 18:54:36.843052: step 72830, loss = 0.53 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 18:54:49.192294: step 72840, loss = 0.55 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 18:55:01.673677: step 72850, loss = 0.65 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 18:55:14.062128: step 72860, loss = 0.59 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 18:55:26.482656: step 72870, loss = 0.51 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 18:55:38.847984: step 72880, loss = 0.63 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 18:55:51.276185: step 72890, loss = 0.52 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 18:56:03.696953: step 72900, loss = 0.57 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 18:56:18.296798: step 72910, loss = 0.46 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 18:56:30.702797: step 72920, loss = 0.61 (23.6 examples/sec; 1.273 sec/batch)\n",
      "2019-05-21 18:56:43.062868: step 72930, loss = 0.59 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 18:56:55.491048: step 72940, loss = 0.60 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 18:57:07.919570: step 72950, loss = 0.56 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 18:57:20.373434: step 72960, loss = 0.58 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 18:57:32.791088: step 72970, loss = 0.52 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 18:57:45.200937: step 72980, loss = 0.65 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 18:57:57.647015: step 72990, loss = 0.53 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 18:58:10.111192: step 73000, loss = 0.53 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 18:58:24.399994: step 73010, loss = 0.55 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 18:58:36.741235: step 73020, loss = 0.57 (23.7 examples/sec; 1.266 sec/batch)\n",
      "2019-05-21 18:58:49.119001: step 73030, loss = 0.58 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 18:59:01.547525: step 73040, loss = 0.57 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 18:59:13.963596: step 73050, loss = 0.49 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 18:59:26.297871: step 73060, loss = 0.64 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 18:59:38.716907: step 73070, loss = 0.57 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 18:59:51.026055: step 73080, loss = 0.56 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 19:00:03.583767: step 73090, loss = 0.52 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 19:00:16.021927: step 73100, loss = 0.58 (23.4 examples/sec; 1.283 sec/batch)\n",
      "2019-05-21 19:00:30.488028: step 73110, loss = 0.57 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 19:00:43.024469: step 73120, loss = 0.55 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 19:00:55.553387: step 73130, loss = 0.55 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 19:01:08.050636: step 73140, loss = 0.60 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 19:01:20.562262: step 73150, loss = 0.60 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 19:01:33.111662: step 73160, loss = 0.54 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 19:01:45.597314: step 73170, loss = 0.63 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 19:01:58.178112: step 73180, loss = 0.59 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 19:02:10.662199: step 73190, loss = 0.54 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 19:02:23.034604: step 73200, loss = 0.58 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 19:02:37.680656: step 73210, loss = 0.61 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 19:02:50.050474: step 73220, loss = 0.62 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 19:03:02.424962: step 73230, loss = 0.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 19:03:14.872207: step 73240, loss = 0.66 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 19:03:27.330376: step 73250, loss = 0.61 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 19:03:39.775394: step 73260, loss = 0.56 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 19:03:52.222632: step 73270, loss = 0.54 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 19:04:04.711268: step 73280, loss = 0.50 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 19:04:17.116241: step 73290, loss = 0.58 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 19:04:29.444836: step 73300, loss = 0.58 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 19:04:44.091526: step 73310, loss = 0.52 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 19:04:56.408802: step 73320, loss = 0.56 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 19:05:08.903060: step 73330, loss = 0.54 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 19:05:21.388178: step 73340, loss = 0.60 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 19:05:33.748885: step 73350, loss = 0.53 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 19:05:46.180968: step 73360, loss = 0.54 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 19:05:58.603085: step 73370, loss = 0.59 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 19:06:11.036664: step 73380, loss = 0.57 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 19:06:23.544048: step 73390, loss = 0.58 (23.9 examples/sec; 1.258 sec/batch)\n",
      "2019-05-21 19:06:36.037135: step 73400, loss = 0.52 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 19:06:50.425048: step 73410, loss = 0.49 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 19:07:02.831851: step 73420, loss = 0.57 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 19:07:15.302804: step 73430, loss = 0.58 (23.5 examples/sec; 1.277 sec/batch)\n",
      "2019-05-21 19:07:27.789001: step 73440, loss = 0.58 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 19:07:40.250867: step 73450, loss = 0.52 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 19:07:52.716337: step 73460, loss = 0.59 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 19:08:05.172256: step 73470, loss = 0.57 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 19:08:17.619354: step 73480, loss = 0.61 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 19:08:30.118394: step 73490, loss = 0.63 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 19:08:42.482031: step 73500, loss = 0.61 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 19:08:56.755029: step 73510, loss = 0.57 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 19:09:09.206016: step 73520, loss = 0.57 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 19:09:21.665366: step 73530, loss = 0.66 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 19:09:34.029998: step 73540, loss = 0.56 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 19:09:46.364587: step 73550, loss = 0.50 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 19:09:58.806886: step 73560, loss = 0.61 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 19:10:11.220119: step 73570, loss = 0.55 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 19:10:23.678589: step 73580, loss = 0.53 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 19:10:36.131257: step 73590, loss = 0.50 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 19:10:48.607188: step 73600, loss = 0.60 (23.8 examples/sec; 1.258 sec/batch)\n",
      "2019-05-21 19:11:03.346371: step 73610, loss = 0.59 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 19:11:15.826038: step 73620, loss = 0.54 (24.0 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 19:11:28.312615: step 73630, loss = 0.63 (24.3 examples/sec; 1.234 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 19:11:40.729020: step 73640, loss = 0.57 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 19:11:53.201232: step 73650, loss = 0.63 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 19:12:05.720773: step 73660, loss = 0.61 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 19:12:18.276814: step 73670, loss = 0.53 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-05-21 19:12:30.750392: step 73680, loss = 0.50 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 19:12:43.190927: step 73690, loss = 0.58 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 19:12:55.742659: step 73700, loss = 0.61 (23.7 examples/sec; 1.268 sec/batch)\n",
      "2019-05-21 19:13:10.219880: step 73710, loss = 0.53 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 19:13:22.743503: step 73720, loss = 0.56 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 19:13:35.264836: step 73730, loss = 0.65 (23.7 examples/sec; 1.268 sec/batch)\n",
      "2019-05-21 19:13:47.737303: step 73740, loss = 0.56 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 19:14:00.303928: step 73750, loss = 0.59 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 19:14:12.787086: step 73760, loss = 0.69 (23.5 examples/sec; 1.276 sec/batch)\n",
      "2019-05-21 19:14:25.278721: step 73770, loss = 0.55 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 19:14:37.701583: step 73780, loss = 0.65 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 19:14:50.049393: step 73790, loss = 0.60 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 19:15:02.474308: step 73800, loss = 0.70 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 19:15:16.889989: step 73810, loss = 0.55 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 19:15:29.329289: step 73820, loss = 0.58 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 19:15:41.724290: step 73830, loss = 0.61 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 19:15:54.240190: step 73840, loss = 0.53 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 19:16:06.763661: step 73850, loss = 0.59 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 19:16:19.211302: step 73860, loss = 0.59 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 19:16:31.667505: step 73870, loss = 0.52 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 19:16:44.158676: step 73880, loss = 0.61 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 19:16:56.636859: step 73890, loss = 0.54 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-05-21 19:17:09.154569: step 73900, loss = 0.56 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 19:17:23.589252: step 73910, loss = 0.64 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 19:17:36.101077: step 73920, loss = 0.59 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 19:17:48.663203: step 73930, loss = 0.55 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 19:18:01.214729: step 73940, loss = 0.57 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 19:18:13.782581: step 73950, loss = 0.53 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-05-21 19:18:26.301645: step 73960, loss = 0.63 (23.5 examples/sec; 1.276 sec/batch)\n",
      "2019-05-21 19:18:38.842407: step 73970, loss = 0.57 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 19:18:51.367317: step 73980, loss = 0.63 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 19:19:03.887050: step 73990, loss = 0.57 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 19:19:16.429036: step 74000, loss = 0.59 (23.7 examples/sec; 1.266 sec/batch)\n",
      "2019-05-21 19:19:30.873343: step 74010, loss = 0.64 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 19:19:43.378412: step 74020, loss = 0.60 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 19:19:55.794602: step 74030, loss = 0.52 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 19:20:08.107807: step 74040, loss = 0.61 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 19:20:20.475043: step 74050, loss = 0.63 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 19:20:32.781599: step 74060, loss = 0.67 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 19:20:45.165449: step 74070, loss = 0.54 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 19:20:57.691958: step 74080, loss = 0.65 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 19:21:10.265576: step 74090, loss = 0.58 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 19:21:22.720459: step 74100, loss = 0.54 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 19:21:37.285963: step 74110, loss = 0.61 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 19:21:49.729538: step 74120, loss = 0.66 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 19:22:02.113372: step 74130, loss = 0.56 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 19:22:14.606806: step 74140, loss = 0.56 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-05-21 19:22:27.155411: step 74150, loss = 0.52 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 19:22:39.712365: step 74160, loss = 0.55 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 19:22:52.224583: step 74170, loss = 0.59 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 19:23:04.717823: step 74180, loss = 0.59 (23.8 examples/sec; 1.258 sec/batch)\n",
      "2019-05-21 19:23:17.242438: step 74190, loss = 0.48 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 19:23:29.786423: step 74200, loss = 0.55 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 19:23:44.164369: step 74210, loss = 0.54 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 19:23:56.685784: step 74220, loss = 0.59 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 19:24:09.208793: step 74230, loss = 0.59 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-05-21 19:24:21.750996: step 74240, loss = 0.53 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 19:24:34.269839: step 74250, loss = 0.63 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 19:24:46.850309: step 74260, loss = 0.61 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 19:24:59.413152: step 74270, loss = 0.58 (23.5 examples/sec; 1.279 sec/batch)\n",
      "2019-05-21 19:25:11.859738: step 74280, loss = 0.58 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 19:25:24.255797: step 74290, loss = 0.50 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 19:25:36.766254: step 74300, loss = 0.56 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 19:25:51.235744: step 74310, loss = 0.58 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 19:26:03.723406: step 74320, loss = 0.63 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 19:26:16.117735: step 74330, loss = 0.56 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 19:26:28.366902: step 74340, loss = 0.56 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 19:26:40.793830: step 74350, loss = 0.54 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 19:26:53.143289: step 74360, loss = 0.63 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 19:27:05.569403: step 74370, loss = 0.53 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 19:27:17.848522: step 74380, loss = 0.57 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 19:27:30.278657: step 74390, loss = 0.57 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 19:27:42.763191: step 74400, loss = 0.58 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 19:27:56.989332: step 74410, loss = 0.63 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 19:28:09.387331: step 74420, loss = 0.59 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 19:28:21.796064: step 74430, loss = 0.50 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 19:28:34.282574: step 74440, loss = 0.54 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 19:28:46.683740: step 74450, loss = 0.64 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 19:28:59.150818: step 74460, loss = 0.59 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 19:29:11.622012: step 74470, loss = 0.52 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 19:29:24.071042: step 74480, loss = 0.65 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 19:29:36.504832: step 74490, loss = 0.66 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 19:29:48.931430: step 74500, loss = 0.54 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 19:30:03.363360: step 74510, loss = 0.57 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 19:30:15.724076: step 74520, loss = 0.61 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 19:30:28.051716: step 74530, loss = 0.57 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 19:30:40.488785: step 74540, loss = 0.56 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 19:30:52.874517: step 74550, loss = 0.63 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 19:31:05.292910: step 74560, loss = 0.55 (23.9 examples/sec; 1.255 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 19:31:17.745848: step 74570, loss = 0.57 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 19:31:30.206370: step 74580, loss = 0.62 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 19:31:42.584132: step 74590, loss = 0.53 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 19:31:55.023080: step 74600, loss = 0.62 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 19:32:09.446280: step 74610, loss = 0.52 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 19:32:21.858852: step 74620, loss = 0.57 (23.8 examples/sec; 1.263 sec/batch)\n",
      "2019-05-21 19:32:34.305286: step 74630, loss = 0.52 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 19:32:46.727823: step 74640, loss = 0.62 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 19:32:59.175347: step 74650, loss = 0.52 (23.7 examples/sec; 1.266 sec/batch)\n",
      "2019-05-21 19:33:11.733685: step 74660, loss = 0.62 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 19:33:24.230193: step 74670, loss = 0.60 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 19:33:36.778428: step 74680, loss = 0.58 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 19:33:49.269623: step 74690, loss = 0.64 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 19:34:01.704029: step 74700, loss = 0.54 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 19:34:16.304892: step 74710, loss = 0.62 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 19:34:28.736144: step 74720, loss = 0.62 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 19:34:41.145077: step 74730, loss = 0.55 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 19:34:53.558007: step 74740, loss = 0.54 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 19:35:06.013408: step 74750, loss = 0.55 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 19:35:18.436953: step 74760, loss = 0.61 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 19:35:30.808076: step 74770, loss = 0.56 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 19:35:43.180588: step 74780, loss = 0.52 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 19:35:55.620809: step 74790, loss = 0.58 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 19:36:08.016889: step 74800, loss = 0.59 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 19:36:22.794928: step 74810, loss = 0.58 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 19:36:35.196374: step 74820, loss = 0.56 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 19:36:47.703409: step 74830, loss = 0.52 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 19:37:00.188321: step 74840, loss = 0.64 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 19:37:12.647588: step 74850, loss = 0.55 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 19:37:24.976634: step 74860, loss = 0.59 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 19:37:37.415104: step 74870, loss = 0.54 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 19:37:49.892749: step 74880, loss = 0.63 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 19:38:02.299947: step 74890, loss = 0.55 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 19:38:14.760738: step 74900, loss = 0.62 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 19:38:29.294607: step 74910, loss = 0.54 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 19:38:41.665466: step 74920, loss = 0.58 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 19:38:54.218780: step 74930, loss = 0.61 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 19:39:06.748403: step 74940, loss = 0.64 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 19:39:19.369115: step 74950, loss = 0.56 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 19:39:31.877018: step 74960, loss = 0.54 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 19:39:44.464662: step 74970, loss = 0.62 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 19:39:57.039698: step 74980, loss = 0.56 (23.8 examples/sec; 1.263 sec/batch)\n",
      "2019-05-21 19:40:09.530496: step 74990, loss = 0.62 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 19:40:22.096866: step 75000, loss = 0.60 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 19:40:39.879750: step 75010, loss = 0.58 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 19:40:52.322774: step 75020, loss = 0.51 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 19:41:04.783123: step 75030, loss = 0.67 (23.7 examples/sec; 1.268 sec/batch)\n",
      "2019-05-21 19:41:17.362241: step 75040, loss = 0.54 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 19:41:29.911936: step 75050, loss = 0.58 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 19:41:42.493989: step 75060, loss = 0.64 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 19:41:55.099032: step 75070, loss = 0.56 (23.5 examples/sec; 1.275 sec/batch)\n",
      "2019-05-21 19:42:07.647412: step 75080, loss = 0.54 (23.8 examples/sec; 1.258 sec/batch)\n",
      "2019-05-21 19:42:20.255123: step 75090, loss = 0.57 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 19:42:32.739144: step 75100, loss = 0.71 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 19:42:47.167016: step 75110, loss = 0.53 (24.1 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 19:42:59.690998: step 75120, loss = 0.62 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 19:43:12.215963: step 75130, loss = 0.66 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 19:43:24.672713: step 75140, loss = 0.59 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 19:43:37.254136: step 75150, loss = 0.60 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 19:43:49.826196: step 75160, loss = 0.57 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 19:44:02.338567: step 75170, loss = 0.62 (23.6 examples/sec; 1.272 sec/batch)\n",
      "2019-05-21 19:44:14.894846: step 75180, loss = 0.62 (23.7 examples/sec; 1.266 sec/batch)\n",
      "2019-05-21 19:44:27.403533: step 75190, loss = 0.56 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 19:44:39.943285: step 75200, loss = 0.66 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 19:44:54.758705: step 75210, loss = 0.68 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 19:45:07.369460: step 75220, loss = 0.65 (23.5 examples/sec; 1.279 sec/batch)\n",
      "2019-05-21 19:45:19.849311: step 75230, loss = 0.57 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 19:45:32.379802: step 75240, loss = 0.58 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 19:45:44.805950: step 75250, loss = 0.53 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 19:45:57.132003: step 75260, loss = 0.52 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 19:46:09.692494: step 75270, loss = 0.64 (23.7 examples/sec; 1.266 sec/batch)\n",
      "2019-05-21 19:46:22.279788: step 75280, loss = 0.67 (23.7 examples/sec; 1.266 sec/batch)\n",
      "2019-05-21 19:46:34.850652: step 75290, loss = 0.51 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 19:46:47.379598: step 75300, loss = 0.57 (23.2 examples/sec; 1.293 sec/batch)\n",
      "2019-05-21 19:47:01.963639: step 75310, loss = 0.60 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 19:47:14.447905: step 75320, loss = 0.57 (23.6 examples/sec; 1.271 sec/batch)\n",
      "2019-05-21 19:47:27.005443: step 75330, loss = 0.55 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 19:47:39.569690: step 75340, loss = 0.56 (24.1 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 19:47:52.174634: step 75350, loss = 0.52 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 19:48:04.681045: step 75360, loss = 0.58 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 19:48:17.197060: step 75370, loss = 0.58 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 19:48:29.755021: step 75380, loss = 0.62 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 19:48:42.303064: step 75390, loss = 0.57 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 19:48:54.824214: step 75400, loss = 0.55 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-05-21 19:49:09.343321: step 75410, loss = 0.53 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 19:49:21.892864: step 75420, loss = 0.57 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-05-21 19:49:34.301658: step 75430, loss = 0.60 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 19:49:46.815898: step 75440, loss = 0.53 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 19:49:59.374539: step 75450, loss = 0.58 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-05-21 19:50:11.862896: step 75460, loss = 0.56 (23.5 examples/sec; 1.275 sec/batch)\n",
      "2019-05-21 19:50:24.402670: step 75470, loss = 0.61 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 19:50:36.909337: step 75480, loss = 0.55 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 19:50:49.299796: step 75490, loss = 0.57 (23.8 examples/sec; 1.262 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 19:51:01.731663: step 75500, loss = 0.60 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 19:51:16.291144: step 75510, loss = 0.58 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 19:51:28.787917: step 75520, loss = 0.51 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 19:51:41.343624: step 75530, loss = 0.51 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 19:51:53.861256: step 75540, loss = 0.56 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 19:52:06.648279: step 75550, loss = 0.55 (23.6 examples/sec; 1.269 sec/batch)\n",
      "2019-05-21 19:52:19.104742: step 75560, loss = 0.62 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 19:52:31.496829: step 75570, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 19:52:43.996999: step 75580, loss = 0.58 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 19:52:56.533348: step 75590, loss = 0.61 (23.3 examples/sec; 1.288 sec/batch)\n",
      "2019-05-21 19:53:09.114695: step 75600, loss = 0.58 (23.5 examples/sec; 1.274 sec/batch)\n",
      "2019-05-21 19:53:23.499293: step 75610, loss = 0.53 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 19:53:36.006092: step 75620, loss = 0.54 (23.7 examples/sec; 1.266 sec/batch)\n",
      "2019-05-21 19:53:48.523084: step 75630, loss = 0.57 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 19:54:01.061109: step 75640, loss = 0.52 (23.6 examples/sec; 1.269 sec/batch)\n",
      "2019-05-21 19:54:13.600838: step 75650, loss = 0.54 (23.5 examples/sec; 1.279 sec/batch)\n",
      "2019-05-21 19:54:26.015105: step 75660, loss = 0.68 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 19:54:38.396146: step 75670, loss = 0.63 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 19:54:50.857147: step 75680, loss = 0.59 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 19:55:03.251684: step 75690, loss = 0.64 (23.9 examples/sec; 1.258 sec/batch)\n",
      "2019-05-21 19:55:15.773826: step 75700, loss = 0.62 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 19:55:30.084233: step 75710, loss = 0.60 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 19:55:42.494129: step 75720, loss = 0.59 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 19:55:54.889876: step 75730, loss = 0.57 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 19:56:07.313864: step 75740, loss = 0.63 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 19:56:19.599122: step 75750, loss = 0.57 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 19:56:32.127115: step 75760, loss = 0.56 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 19:56:44.633700: step 75770, loss = 0.58 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 19:56:57.000734: step 75780, loss = 0.63 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 19:57:09.276727: step 75790, loss = 0.61 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 19:57:21.557914: step 75800, loss = 0.56 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 19:57:36.072355: step 75810, loss = 0.58 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 19:57:48.412482: step 75820, loss = 0.62 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 19:58:00.846411: step 75830, loss = 0.51 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 19:58:13.267822: step 75840, loss = 0.54 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 19:58:25.649361: step 75850, loss = 0.55 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 19:58:38.052132: step 75860, loss = 0.58 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 19:58:50.454892: step 75870, loss = 0.58 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 19:59:02.867511: step 75880, loss = 0.55 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 19:59:15.333329: step 75890, loss = 0.63 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 19:59:27.744101: step 75900, loss = 0.50 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 19:59:41.995213: step 75910, loss = 0.61 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 19:59:54.453298: step 75920, loss = 0.57 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 20:00:06.926595: step 75930, loss = 0.62 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 20:00:19.322326: step 75940, loss = 0.52 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 20:00:31.763213: step 75950, loss = 0.59 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 20:00:44.281987: step 75960, loss = 0.61 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 20:00:56.671907: step 75970, loss = 0.56 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 20:01:09.121299: step 75980, loss = 0.55 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 20:01:21.487606: step 75990, loss = 0.56 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 20:01:33.807581: step 76000, loss = 0.54 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 20:01:48.274537: step 76010, loss = 0.56 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 20:02:00.730941: step 76020, loss = 0.57 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 20:02:13.224541: step 76030, loss = 0.58 (23.8 examples/sec; 1.258 sec/batch)\n",
      "2019-05-21 20:02:25.639857: step 76040, loss = 0.57 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 20:02:38.051891: step 76050, loss = 0.55 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 20:02:50.546692: step 76060, loss = 0.67 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 20:03:03.012992: step 76070, loss = 0.63 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 20:03:15.471214: step 76080, loss = 0.51 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 20:03:28.034524: step 76090, loss = 0.65 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 20:03:40.596834: step 76100, loss = 0.67 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 20:03:54.996385: step 76110, loss = 0.53 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 20:04:07.548305: step 76120, loss = 0.63 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 20:04:20.080841: step 76130, loss = 0.57 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 20:04:32.586973: step 76140, loss = 0.54 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 20:04:45.092149: step 76150, loss = 0.54 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 20:04:57.646216: step 76160, loss = 0.59 (23.5 examples/sec; 1.275 sec/batch)\n",
      "2019-05-21 20:05:10.145132: step 76170, loss = 0.56 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 20:05:22.683647: step 76180, loss = 0.59 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 20:05:35.264389: step 76190, loss = 0.56 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 20:05:47.802339: step 76200, loss = 0.57 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 20:06:02.236258: step 76210, loss = 0.60 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 20:06:14.721183: step 76220, loss = 0.60 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 20:06:27.251184: step 76230, loss = 0.71 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 20:06:39.587719: step 76240, loss = 0.54 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 20:06:52.148543: step 76250, loss = 0.65 (23.6 examples/sec; 1.271 sec/batch)\n",
      "2019-05-21 20:07:04.617966: step 76260, loss = 0.53 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 20:07:17.102528: step 76270, loss = 0.60 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 20:07:29.640647: step 76280, loss = 0.57 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 20:07:42.192074: step 76290, loss = 0.53 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 20:07:54.776227: step 76300, loss = 0.55 (23.8 examples/sec; 1.263 sec/batch)\n",
      "2019-05-21 20:08:09.681813: step 76310, loss = 0.62 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-05-21 20:08:22.215207: step 76320, loss = 0.53 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 20:08:34.737234: step 76330, loss = 0.60 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 20:08:47.260975: step 76340, loss = 0.60 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 20:08:59.806747: step 76350, loss = 0.61 (23.4 examples/sec; 1.281 sec/batch)\n",
      "2019-05-21 20:09:12.372923: step 76360, loss = 0.54 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 20:09:24.948351: step 76370, loss = 0.62 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 20:09:37.598636: step 76380, loss = 0.50 (23.7 examples/sec; 1.268 sec/batch)\n",
      "2019-05-21 20:09:50.208662: step 76390, loss = 0.58 (23.4 examples/sec; 1.280 sec/batch)\n",
      "2019-05-21 20:10:02.802904: step 76400, loss = 0.66 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 20:10:17.811607: step 76410, loss = 0.58 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 20:10:30.344580: step 76420, loss = 0.61 (23.9 examples/sec; 1.254 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 20:10:42.852644: step 76430, loss = 0.51 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 20:10:55.435463: step 76440, loss = 0.53 (23.4 examples/sec; 1.284 sec/batch)\n",
      "2019-05-21 20:11:07.902636: step 76450, loss = 0.53 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 20:11:20.341234: step 76460, loss = 0.53 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 20:11:32.886388: step 76470, loss = 0.58 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 20:11:45.410849: step 76480, loss = 0.56 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-05-21 20:11:57.823674: step 76490, loss = 0.67 (23.7 examples/sec; 1.268 sec/batch)\n",
      "2019-05-21 20:12:10.395798: step 76500, loss = 0.61 (23.7 examples/sec; 1.266 sec/batch)\n",
      "2019-05-21 20:12:24.762636: step 76510, loss = 0.58 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 20:12:37.282130: step 76520, loss = 0.51 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 20:12:49.851239: step 76530, loss = 0.61 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 20:13:02.418489: step 76540, loss = 0.67 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 20:13:14.939586: step 76550, loss = 0.61 (23.8 examples/sec; 1.258 sec/batch)\n",
      "2019-05-21 20:13:27.504998: step 76560, loss = 0.59 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 20:13:40.106073: step 76570, loss = 0.52 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 20:13:52.653764: step 76580, loss = 0.63 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 20:14:05.207627: step 76590, loss = 0.52 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 20:14:17.811516: step 76600, loss = 0.60 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 20:14:32.209077: step 76610, loss = 0.62 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 20:14:44.734151: step 76620, loss = 0.61 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 20:14:57.202711: step 76630, loss = 0.60 (23.7 examples/sec; 1.263 sec/batch)\n",
      "2019-05-21 20:15:09.792455: step 76640, loss = 0.57 (23.6 examples/sec; 1.269 sec/batch)\n",
      "2019-05-21 20:15:22.280380: step 76650, loss = 0.59 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 20:15:34.864909: step 76660, loss = 0.55 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 20:15:47.436991: step 76670, loss = 0.56 (24.1 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 20:15:59.984303: step 76680, loss = 0.55 (23.9 examples/sec; 1.258 sec/batch)\n",
      "2019-05-21 20:16:12.462767: step 76690, loss = 0.57 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 20:16:24.977385: step 76700, loss = 0.53 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 20:16:39.776755: step 76710, loss = 0.58 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 20:16:52.317214: step 76720, loss = 0.59 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 20:17:04.716132: step 76730, loss = 0.48 (23.5 examples/sec; 1.277 sec/batch)\n",
      "2019-05-21 20:17:17.199861: step 76740, loss = 0.57 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 20:17:29.769434: step 76750, loss = 0.55 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 20:17:42.314539: step 76760, loss = 0.54 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 20:17:54.854163: step 76770, loss = 0.60 (23.8 examples/sec; 1.263 sec/batch)\n",
      "2019-05-21 20:18:07.390481: step 76780, loss = 0.65 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 20:18:19.866492: step 76790, loss = 0.58 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 20:18:32.420831: step 76800, loss = 0.56 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 20:18:47.152909: step 76810, loss = 0.51 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 20:18:59.676527: step 76820, loss = 0.60 (23.6 examples/sec; 1.271 sec/batch)\n",
      "2019-05-21 20:19:12.209777: step 76830, loss = 0.57 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 20:19:24.642013: step 76840, loss = 0.57 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 20:19:37.183454: step 76850, loss = 0.60 (24.1 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 20:19:49.686235: step 76860, loss = 0.54 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-05-21 20:20:02.188049: step 76870, loss = 0.59 (23.6 examples/sec; 1.269 sec/batch)\n",
      "2019-05-21 20:20:14.743237: step 76880, loss = 0.55 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 20:20:27.265154: step 76890, loss = 0.50 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 20:20:39.771209: step 76900, loss = 0.56 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 20:20:54.353872: step 76910, loss = 0.59 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 20:21:06.886806: step 76920, loss = 0.54 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 20:21:19.364451: step 76930, loss = 0.63 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 20:21:31.779542: step 76940, loss = 0.49 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 20:21:44.324477: step 76950, loss = 0.55 (23.7 examples/sec; 1.266 sec/batch)\n",
      "2019-05-21 20:21:56.849669: step 76960, loss = 0.57 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-05-21 20:22:09.265960: step 76970, loss = 0.54 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 20:22:21.721120: step 76980, loss = 0.56 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 20:22:34.255571: step 76990, loss = 0.49 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 20:22:46.817920: step 77000, loss = 0.50 (23.7 examples/sec; 1.268 sec/batch)\n",
      "2019-05-21 20:23:01.574948: step 77010, loss = 0.59 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 20:23:14.060199: step 77020, loss = 0.57 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-05-21 20:23:26.618043: step 77030, loss = 0.57 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 20:23:39.161390: step 77040, loss = 0.56 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 20:23:51.686569: step 77050, loss = 0.56 (23.8 examples/sec; 1.263 sec/batch)\n",
      "2019-05-21 20:24:04.227844: step 77060, loss = 0.58 (23.5 examples/sec; 1.276 sec/batch)\n",
      "2019-05-21 20:24:16.801750: step 77070, loss = 0.58 (23.7 examples/sec; 1.268 sec/batch)\n",
      "2019-05-21 20:24:29.412405: step 77080, loss = 0.58 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 20:24:41.924412: step 77090, loss = 0.65 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 20:24:54.485635: step 77100, loss = 0.55 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 20:25:08.886984: step 77110, loss = 0.56 (23.7 examples/sec; 1.268 sec/batch)\n",
      "2019-05-21 20:25:21.297087: step 77120, loss = 0.56 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 20:25:33.821106: step 77130, loss = 0.65 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 20:25:46.264219: step 77140, loss = 0.54 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 20:25:58.764337: step 77150, loss = 0.56 (23.7 examples/sec; 1.266 sec/batch)\n",
      "2019-05-21 20:26:11.328188: step 77160, loss = 0.68 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 20:26:23.850254: step 77170, loss = 0.56 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 20:26:36.279409: step 77180, loss = 0.58 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 20:26:48.882248: step 77190, loss = 0.55 (23.6 examples/sec; 1.269 sec/batch)\n",
      "2019-05-21 20:27:01.491151: step 77200, loss = 0.54 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 20:27:16.132922: step 77210, loss = 0.51 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 20:27:28.512015: step 77220, loss = 0.59 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 20:27:41.003873: step 77230, loss = 0.62 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 20:27:53.548565: step 77240, loss = 0.66 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 20:28:06.037640: step 77250, loss = 0.54 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 20:28:18.428878: step 77260, loss = 0.56 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 20:28:30.782784: step 77270, loss = 0.55 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 20:28:43.223372: step 77280, loss = 0.58 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 20:28:55.744185: step 77290, loss = 0.58 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 20:29:08.229501: step 77300, loss = 0.53 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 20:29:22.783919: step 77310, loss = 0.56 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 20:29:35.289833: step 77320, loss = 0.54 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 20:29:47.696476: step 77330, loss = 0.56 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 20:30:00.156500: step 77340, loss = 0.52 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 20:30:12.591851: step 77350, loss = 0.62 (23.7 examples/sec; 1.264 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 20:30:25.109602: step 77360, loss = 0.56 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 20:30:37.520641: step 77370, loss = 0.54 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 20:30:49.837516: step 77380, loss = 0.52 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 20:31:02.129012: step 77390, loss = 0.53 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 20:31:14.413211: step 77400, loss = 0.56 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 20:31:29.062216: step 77410, loss = 0.63 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 20:31:41.342061: step 77420, loss = 0.59 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 20:31:53.704539: step 77430, loss = 0.54 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 20:32:06.145440: step 77440, loss = 0.56 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 20:32:18.584211: step 77450, loss = 0.58 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 20:32:30.900699: step 77460, loss = 0.61 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 20:32:43.320661: step 77470, loss = 0.61 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 20:32:55.803748: step 77480, loss = 0.51 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 20:33:08.271989: step 77490, loss = 0.53 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 20:33:20.681066: step 77500, loss = 0.56 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 20:33:35.001586: step 77510, loss = 0.52 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 20:33:47.468105: step 77520, loss = 0.59 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 20:34:00.009168: step 77530, loss = 0.51 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 20:34:12.342263: step 77540, loss = 0.53 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 20:34:24.792134: step 77550, loss = 0.57 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 20:34:37.171240: step 77560, loss = 0.59 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 20:34:49.582071: step 77570, loss = 0.57 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 20:35:02.011340: step 77580, loss = 0.50 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 20:35:14.419236: step 77590, loss = 0.57 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 20:35:26.836812: step 77600, loss = 0.57 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 20:35:41.345223: step 77610, loss = 0.52 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 20:35:53.834104: step 77620, loss = 0.59 (24.0 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 20:36:06.232443: step 77630, loss = 0.62 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 20:36:18.673922: step 77640, loss = 0.54 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 20:36:31.073231: step 77650, loss = 0.60 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 20:36:43.471696: step 77660, loss = 0.63 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 20:36:55.876519: step 77670, loss = 0.61 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 20:37:08.311890: step 77680, loss = 0.55 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 20:37:20.797210: step 77690, loss = 0.57 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 20:37:33.323223: step 77700, loss = 0.57 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 20:37:47.895727: step 77710, loss = 0.58 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 20:38:00.424330: step 77720, loss = 0.56 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 20:38:12.923638: step 77730, loss = 0.59 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 20:38:25.465418: step 77740, loss = 0.61 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 20:38:38.024555: step 77750, loss = 0.54 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 20:38:50.533109: step 77760, loss = 0.61 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 20:39:03.035409: step 77770, loss = 0.56 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 20:39:15.534391: step 77780, loss = 0.57 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 20:39:28.096465: step 77790, loss = 0.56 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 20:39:40.646254: step 77800, loss = 0.62 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 20:39:55.046432: step 77810, loss = 0.49 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 20:40:07.526226: step 77820, loss = 0.52 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 20:40:20.089162: step 77830, loss = 0.53 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 20:40:32.579099: step 77840, loss = 0.59 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 20:40:45.154174: step 77850, loss = 0.55 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 20:40:57.731236: step 77860, loss = 0.51 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 20:41:10.271445: step 77870, loss = 0.65 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 20:41:22.815120: step 77880, loss = 0.55 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 20:41:35.298677: step 77890, loss = 0.53 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 20:41:47.705757: step 77900, loss = 0.61 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 20:42:02.198720: step 77910, loss = 0.57 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 20:42:14.696224: step 77920, loss = 0.54 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 20:42:27.222708: step 77930, loss = 0.54 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 20:42:39.800025: step 77940, loss = 0.54 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 20:42:52.207170: step 77950, loss = 0.58 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 20:43:04.548317: step 77960, loss = 0.48 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 20:43:16.927357: step 77970, loss = 0.56 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 20:43:29.257225: step 77980, loss = 0.54 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 20:43:41.692290: step 77990, loss = 0.55 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 20:43:54.097133: step 78000, loss = 0.61 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 20:44:08.580984: step 78010, loss = 0.51 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-05-21 20:44:21.012784: step 78020, loss = 0.58 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 20:44:33.516696: step 78030, loss = 0.56 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 20:44:45.940871: step 78040, loss = 0.54 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 20:44:58.420420: step 78050, loss = 0.55 (23.6 examples/sec; 1.272 sec/batch)\n",
      "2019-05-21 20:45:10.881848: step 78060, loss = 0.55 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 20:45:23.324084: step 78070, loss = 0.62 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 20:45:35.800259: step 78080, loss = 0.46 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 20:45:48.299738: step 78090, loss = 0.54 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 20:46:00.760313: step 78100, loss = 0.52 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 20:46:15.154807: step 78110, loss = 0.63 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 20:46:27.619992: step 78120, loss = 0.63 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 20:46:40.090830: step 78130, loss = 0.54 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 20:46:52.551983: step 78140, loss = 0.61 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 20:47:04.977794: step 78150, loss = 0.69 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 20:47:17.481153: step 78160, loss = 0.50 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 20:47:29.995255: step 78170, loss = 0.59 (23.6 examples/sec; 1.269 sec/batch)\n",
      "2019-05-21 20:47:42.481741: step 78180, loss = 0.59 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 20:47:54.929973: step 78190, loss = 0.59 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 20:48:07.275125: step 78200, loss = 0.60 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 20:48:21.842642: step 78210, loss = 0.64 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 20:48:34.346567: step 78220, loss = 0.54 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 20:48:46.829203: step 78230, loss = 0.62 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 20:48:59.313831: step 78240, loss = 0.54 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 20:49:11.750086: step 78250, loss = 0.52 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 20:49:24.259108: step 78260, loss = 0.58 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 20:49:36.727728: step 78270, loss = 0.73 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 20:49:49.197309: step 78280, loss = 0.55 (24.0 examples/sec; 1.249 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 20:50:01.709979: step 78290, loss = 0.57 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 20:50:14.265924: step 78300, loss = 0.66 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 20:50:28.930149: step 78310, loss = 0.54 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 20:50:41.454080: step 78320, loss = 0.56 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 20:50:53.975822: step 78330, loss = 0.59 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-05-21 20:51:06.590001: step 78340, loss = 0.60 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 20:51:19.211724: step 78350, loss = 0.58 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 20:51:31.796881: step 78360, loss = 0.63 (23.8 examples/sec; 1.258 sec/batch)\n",
      "2019-05-21 20:51:44.350609: step 78370, loss = 0.64 (23.7 examples/sec; 1.268 sec/batch)\n",
      "2019-05-21 20:51:56.771644: step 78380, loss = 0.66 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 20:52:09.359659: step 78390, loss = 0.62 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 20:52:21.904742: step 78400, loss = 0.52 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 20:52:36.439198: step 78410, loss = 0.56 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 20:52:49.026547: step 78420, loss = 0.56 (23.5 examples/sec; 1.277 sec/batch)\n",
      "2019-05-21 20:53:01.585377: step 78430, loss = 0.54 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 20:53:13.987159: step 78440, loss = 0.53 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 20:53:26.517805: step 78450, loss = 0.56 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 20:53:39.083098: step 78460, loss = 0.56 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 20:53:51.717960: step 78470, loss = 0.55 (23.3 examples/sec; 1.287 sec/batch)\n",
      "2019-05-21 20:54:04.274053: step 78480, loss = 0.62 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 20:54:16.884166: step 78490, loss = 0.64 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 20:54:29.468424: step 78500, loss = 0.59 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-05-21 20:54:43.960496: step 78510, loss = 0.53 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 20:54:56.485206: step 78520, loss = 0.61 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 20:55:09.056985: step 78530, loss = 0.59 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 20:55:21.639255: step 78540, loss = 0.63 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 20:55:34.243065: step 78550, loss = 0.59 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 20:55:46.870608: step 78560, loss = 0.50 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 20:55:59.479673: step 78570, loss = 0.54 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-05-21 20:56:12.060566: step 78580, loss = 0.58 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 20:56:24.563091: step 78590, loss = 0.55 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 20:56:37.182335: step 78600, loss = 0.60 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 20:56:51.768968: step 78610, loss = 0.56 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 20:57:04.167710: step 78620, loss = 0.53 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 20:57:16.701816: step 78630, loss = 0.54 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 20:57:29.132395: step 78640, loss = 0.52 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 20:57:41.679656: step 78650, loss = 0.57 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 20:57:54.200097: step 78660, loss = 0.57 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 20:58:06.700809: step 78670, loss = 0.57 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 20:58:19.209584: step 78680, loss = 0.55 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 20:58:31.617250: step 78690, loss = 0.53 (23.6 examples/sec; 1.269 sec/batch)\n",
      "2019-05-21 20:58:44.217918: step 78700, loss = 0.65 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-05-21 20:58:58.931210: step 78710, loss = 0.59 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 20:59:11.469030: step 78720, loss = 0.65 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 20:59:24.059919: step 78730, loss = 0.60 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 20:59:36.658433: step 78740, loss = 0.65 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 20:59:49.217129: step 78750, loss = 0.56 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 21:00:01.748941: step 78760, loss = 0.58 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 21:00:14.252287: step 78770, loss = 0.54 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 21:00:26.787586: step 78780, loss = 0.66 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-05-21 21:00:39.324978: step 78790, loss = 0.63 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 21:00:51.909674: step 78800, loss = 0.63 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 21:01:06.575635: step 78810, loss = 0.64 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 21:01:19.127715: step 78820, loss = 0.58 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 21:01:31.732004: step 78830, loss = 0.58 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 21:01:44.285543: step 78840, loss = 0.56 (23.6 examples/sec; 1.269 sec/batch)\n",
      "2019-05-21 21:01:56.818159: step 78850, loss = 0.57 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 21:02:09.222461: step 78860, loss = 0.58 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 21:02:21.766719: step 78870, loss = 0.55 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 21:02:34.359694: step 78880, loss = 0.63 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 21:02:46.950584: step 78890, loss = 0.56 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-05-21 21:02:59.502180: step 78900, loss = 0.53 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 21:03:14.228641: step 78910, loss = 0.48 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 21:03:26.715160: step 78920, loss = 0.58 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 21:03:39.036706: step 78930, loss = 0.62 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 21:03:51.641760: step 78940, loss = 0.61 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 21:04:04.235825: step 78950, loss = 0.58 (23.6 examples/sec; 1.274 sec/batch)\n",
      "2019-05-21 21:04:16.801891: step 78960, loss = 0.63 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 21:04:29.349968: step 78970, loss = 0.57 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 21:04:41.959886: step 78980, loss = 0.71 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-05-21 21:04:54.484304: step 78990, loss = 0.57 (24.0 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 21:05:07.120210: step 79000, loss = 0.55 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-05-21 21:05:21.658209: step 79010, loss = 0.62 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 21:05:34.188278: step 79020, loss = 0.63 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 21:05:46.704538: step 79030, loss = 0.57 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 21:05:59.236599: step 79040, loss = 0.56 (23.9 examples/sec; 1.258 sec/batch)\n",
      "2019-05-21 21:06:11.800062: step 79050, loss = 0.49 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 21:06:24.390595: step 79060, loss = 0.59 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 21:06:36.875966: step 79070, loss = 0.60 (23.6 examples/sec; 1.273 sec/batch)\n",
      "2019-05-21 21:06:49.411184: step 79080, loss = 0.63 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 21:07:01.863569: step 79090, loss = 0.72 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 21:07:14.237842: step 79100, loss = 0.56 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 21:07:28.530751: step 79110, loss = 0.55 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-21 21:07:40.991203: step 79120, loss = 0.54 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 21:07:53.501535: step 79130, loss = 0.65 (23.9 examples/sec; 1.256 sec/batch)\n",
      "2019-05-21 21:08:06.078862: step 79140, loss = 0.54 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 21:08:18.593359: step 79150, loss = 0.59 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 21:08:31.149933: step 79160, loss = 0.53 (23.5 examples/sec; 1.275 sec/batch)\n",
      "2019-05-21 21:08:43.643085: step 79170, loss = 0.57 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 21:08:56.096985: step 79180, loss = 0.64 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 21:09:08.675427: step 79190, loss = 0.58 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 21:09:21.148825: step 79200, loss = 0.53 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 21:09:35.614799: step 79210, loss = 0.67 (24.3 examples/sec; 1.235 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 21:09:48.205801: step 79220, loss = 0.64 (23.4 examples/sec; 1.285 sec/batch)\n",
      "2019-05-21 21:10:00.685748: step 79230, loss = 0.51 (23.7 examples/sec; 1.265 sec/batch)\n",
      "2019-05-21 21:10:13.200506: step 79240, loss = 0.48 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 21:10:25.779024: step 79250, loss = 0.55 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 21:10:38.353702: step 79260, loss = 0.68 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 21:10:50.921905: step 79270, loss = 0.55 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 21:11:03.467322: step 79280, loss = 0.57 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 21:11:15.981332: step 79290, loss = 0.58 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 21:11:28.577035: step 79300, loss = 0.57 (23.6 examples/sec; 1.269 sec/batch)\n",
      "2019-05-21 21:11:43.342817: step 79310, loss = 0.57 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 21:11:55.849755: step 79320, loss = 0.58 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 21:12:08.398859: step 79330, loss = 0.51 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 21:12:20.866869: step 79340, loss = 0.55 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 21:12:33.362083: step 79350, loss = 0.55 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 21:12:45.930383: step 79360, loss = 0.54 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-21 21:12:58.451963: step 79370, loss = 0.54 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 21:13:11.003158: step 79380, loss = 0.53 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 21:13:23.446612: step 79390, loss = 0.57 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 21:13:35.975204: step 79400, loss = 0.62 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 21:13:50.390181: step 79410, loss = 0.59 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 21:14:02.795242: step 79420, loss = 0.55 (23.5 examples/sec; 1.279 sec/batch)\n",
      "2019-05-21 21:14:15.352581: step 79430, loss = 0.57 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 21:14:27.984379: step 79440, loss = 0.58 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 21:14:40.597202: step 79450, loss = 0.50 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 21:14:53.196211: step 79460, loss = 0.57 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 21:15:05.819836: step 79470, loss = 0.58 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 21:15:18.397376: step 79480, loss = 0.57 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 21:15:30.988510: step 79490, loss = 0.65 (23.6 examples/sec; 1.269 sec/batch)\n",
      "2019-05-21 21:15:43.591247: step 79500, loss = 0.55 (23.6 examples/sec; 1.273 sec/batch)\n",
      "2019-05-21 21:15:58.242624: step 79510, loss = 0.66 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 21:16:10.712584: step 79520, loss = 0.58 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 21:16:23.295087: step 79530, loss = 0.54 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 21:16:35.826930: step 79540, loss = 0.51 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 21:16:48.417390: step 79550, loss = 0.56 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-05-21 21:17:01.023261: step 79560, loss = 0.57 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-05-21 21:17:13.676990: step 79570, loss = 0.68 (24.1 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 21:17:26.164051: step 79580, loss = 0.54 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 21:17:38.747348: step 79590, loss = 0.56 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 21:17:51.312470: step 79600, loss = 0.51 (23.6 examples/sec; 1.272 sec/batch)\n",
      "2019-05-21 21:18:05.823474: step 79610, loss = 0.57 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 21:18:18.436203: step 79620, loss = 0.58 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 21:18:31.040081: step 79630, loss = 0.55 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 21:18:43.586541: step 79640, loss = 0.52 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 21:18:56.169775: step 79650, loss = 0.59 (23.6 examples/sec; 1.269 sec/batch)\n",
      "2019-05-21 21:19:08.556462: step 79660, loss = 0.56 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 21:19:21.061724: step 79670, loss = 0.54 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-21 21:19:33.654720: step 79680, loss = 0.60 (23.7 examples/sec; 1.268 sec/batch)\n",
      "2019-05-21 21:19:46.246619: step 79690, loss = 0.54 (23.7 examples/sec; 1.263 sec/batch)\n",
      "2019-05-21 21:19:58.846271: step 79700, loss = 0.58 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 21:20:13.380380: step 79710, loss = 0.58 (23.9 examples/sec; 1.253 sec/batch)\n",
      "2019-05-21 21:20:25.966418: step 79720, loss = 0.60 (23.7 examples/sec; 1.264 sec/batch)\n",
      "2019-05-21 21:20:38.558799: step 79730, loss = 0.62 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 21:20:51.169952: step 79740, loss = 0.56 (23.6 examples/sec; 1.269 sec/batch)\n",
      "2019-05-21 21:21:03.778381: step 79750, loss = 0.51 (23.4 examples/sec; 1.280 sec/batch)\n",
      "2019-05-21 21:21:16.356275: step 79760, loss = 0.61 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 21:21:28.923469: step 79770, loss = 0.61 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 21:21:41.508108: step 79780, loss = 0.59 (23.7 examples/sec; 1.263 sec/batch)\n",
      "2019-05-21 21:21:54.081351: step 79790, loss = 0.59 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 21:22:06.657222: step 79800, loss = 0.52 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 21:22:21.667825: step 79810, loss = 0.61 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 21:22:34.066515: step 79820, loss = 0.58 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-21 21:22:46.668400: step 79830, loss = 0.60 (23.7 examples/sec; 1.268 sec/batch)\n",
      "2019-05-21 21:22:59.222106: step 79840, loss = 0.62 (23.8 examples/sec; 1.258 sec/batch)\n",
      "2019-05-21 21:23:11.792788: step 79850, loss = 0.58 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 21:23:24.397845: step 79860, loss = 0.62 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-21 21:23:36.936407: step 79870, loss = 0.61 (23.8 examples/sec; 1.258 sec/batch)\n",
      "2019-05-21 21:23:49.500237: step 79880, loss = 0.59 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 21:24:02.048405: step 79890, loss = 0.60 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 21:24:14.675475: step 79900, loss = 0.50 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 21:24:28.888876: step 79910, loss = 0.66 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 21:24:41.370224: step 79920, loss = 0.59 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 21:24:53.872708: step 79930, loss = 0.53 (24.0 examples/sec; 1.252 sec/batch)\n",
      "2019-05-21 21:25:06.487871: step 79940, loss = 0.61 (23.7 examples/sec; 1.267 sec/batch)\n",
      "2019-05-21 21:25:19.037619: step 79950, loss = 0.60 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 21:25:31.561821: step 79960, loss = 0.58 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 21:25:44.036302: step 79970, loss = 0.49 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 21:25:56.426726: step 79980, loss = 0.51 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 21:26:08.857822: step 79990, loss = 0.54 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 21:26:21.275001: step 80000, loss = 0.59 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 21:26:38.962352: step 80010, loss = 0.58 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 21:26:51.429907: step 80020, loss = 0.58 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 21:27:03.777327: step 80030, loss = 0.51 (24.1 examples/sec; 1.242 sec/batch)\n",
      "2019-05-21 21:27:16.273125: step 80040, loss = 0.60 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 21:27:28.776094: step 80050, loss = 0.57 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 21:27:41.251781: step 80060, loss = 0.54 (23.5 examples/sec; 1.274 sec/batch)\n",
      "2019-05-21 21:27:53.793041: step 80070, loss = 0.60 (23.6 examples/sec; 1.272 sec/batch)\n",
      "2019-05-21 21:28:06.296774: step 80080, loss = 0.69 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-21 21:28:18.818827: step 80090, loss = 0.54 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 21:28:31.384890: step 80100, loss = 0.54 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-05-21 21:28:45.883022: step 80110, loss = 0.53 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-21 21:28:58.439116: step 80120, loss = 0.57 (23.5 examples/sec; 1.277 sec/batch)\n",
      "2019-05-21 21:29:10.967620: step 80130, loss = 0.61 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 21:29:23.528278: step 80140, loss = 0.61 (24.6 examples/sec; 1.220 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 21:29:36.023988: step 80150, loss = 0.56 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 21:29:48.570355: step 80160, loss = 0.54 (23.4 examples/sec; 1.283 sec/batch)\n",
      "2019-05-21 21:30:01.102322: step 80170, loss = 0.54 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 21:30:13.547538: step 80180, loss = 0.57 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 21:30:26.058120: step 80190, loss = 0.62 (23.7 examples/sec; 1.263 sec/batch)\n",
      "2019-05-21 21:30:38.551398: step 80200, loss = 0.62 (23.5 examples/sec; 1.278 sec/batch)\n",
      "2019-05-21 21:30:53.190512: step 80210, loss = 0.60 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 21:31:05.704435: step 80220, loss = 0.67 (23.6 examples/sec; 1.274 sec/batch)\n",
      "2019-05-21 21:31:18.276716: step 80230, loss = 0.55 (23.9 examples/sec; 1.254 sec/batch)\n",
      "2019-05-21 21:31:30.796183: step 80240, loss = 0.59 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-21 21:31:43.245806: step 80250, loss = 0.60 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 21:31:55.734959: step 80260, loss = 0.57 (23.9 examples/sec; 1.258 sec/batch)\n",
      "2019-05-21 21:32:08.240327: step 80270, loss = 0.60 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 21:32:20.771402: step 80280, loss = 0.57 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 21:32:33.334300: step 80290, loss = 0.57 (23.5 examples/sec; 1.279 sec/batch)\n",
      "2019-05-21 21:32:45.730889: step 80300, loss = 0.51 (23.8 examples/sec; 1.260 sec/batch)\n",
      "2019-05-21 21:33:00.102700: step 80310, loss = 0.60 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 21:33:12.643661: step 80320, loss = 0.56 (23.7 examples/sec; 1.266 sec/batch)\n",
      "2019-05-21 21:33:25.172510: step 80330, loss = 0.46 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 21:33:37.652663: step 80340, loss = 0.62 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 21:33:50.197285: step 80350, loss = 0.57 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 21:34:02.756240: step 80360, loss = 0.59 (23.5 examples/sec; 1.278 sec/batch)\n",
      "2019-05-21 21:34:15.327971: step 80370, loss = 0.61 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 21:34:27.891678: step 80380, loss = 0.54 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-21 21:34:40.300871: step 80390, loss = 0.55 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-21 21:34:52.811909: step 80400, loss = 0.60 (23.5 examples/sec; 1.276 sec/batch)\n",
      "2019-05-21 21:35:07.386680: step 80410, loss = 0.50 (23.9 examples/sec; 1.255 sec/batch)\n",
      "2019-05-21 21:35:20.001902: step 80420, loss = 0.49 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 21:35:32.572200: step 80430, loss = 0.54 (24.0 examples/sec; 1.248 sec/batch)\n",
      "2019-05-21 21:35:45.136191: step 80440, loss = 0.55 (23.7 examples/sec; 1.266 sec/batch)\n",
      "2019-05-21 21:35:57.688988: step 80450, loss = 0.52 (23.8 examples/sec; 1.259 sec/batch)\n",
      "2019-05-21 21:36:10.276525: step 80460, loss = 0.53 (23.8 examples/sec; 1.263 sec/batch)\n",
      "2019-05-21 21:36:22.846708: step 80470, loss = 0.56 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 21:36:35.471524: step 80480, loss = 0.60 (23.6 examples/sec; 1.271 sec/batch)\n",
      "2019-05-21 21:36:48.068981: step 80490, loss = 0.59 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-05-21 21:37:00.586461: step 80500, loss = 0.62 (23.6 examples/sec; 1.271 sec/batch)\n",
      "2019-05-21 21:37:15.065590: step 80510, loss = 0.51 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 21:37:27.545580: step 80520, loss = 0.54 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 21:37:40.089353: step 80530, loss = 0.52 (23.5 examples/sec; 1.276 sec/batch)\n",
      "2019-05-21 21:37:52.507515: step 80540, loss = 0.57 (23.6 examples/sec; 1.270 sec/batch)\n",
      "2019-05-21 21:38:04.929363: step 80550, loss = 0.57 (23.8 examples/sec; 1.261 sec/batch)\n",
      "2019-05-21 21:38:17.402196: step 80560, loss = 0.61 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 21:38:29.941517: step 80570, loss = 0.56 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 21:38:42.425230: step 80580, loss = 0.56 (23.5 examples/sec; 1.276 sec/batch)\n",
      "2019-05-21 21:38:54.910126: step 80590, loss = 0.54 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-21 21:39:07.526237: step 80600, loss = 0.58 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-21 21:39:22.079202: step 80610, loss = 0.56 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 21:39:34.621668: step 80620, loss = 0.54 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 21:39:47.122546: step 80630, loss = 0.62 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 21:39:59.543093: step 80640, loss = 0.58 (23.8 examples/sec; 1.263 sec/batch)\n",
      "2019-05-21 21:40:12.036669: step 80650, loss = 0.67 (24.0 examples/sec; 1.251 sec/batch)\n",
      "2019-05-21 21:40:24.618604: step 80660, loss = 0.61 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-21 21:40:37.168615: step 80670, loss = 0.53 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-21 21:40:49.761438: step 80680, loss = 0.56 (23.6 examples/sec; 1.272 sec/batch)\n",
      "2019-05-21 21:41:02.231034: step 80690, loss = 0.56 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-21 21:41:14.788357: step 80700, loss = 0.52 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 21:41:29.102114: step 80710, loss = 0.51 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 21:41:41.658206: step 80720, loss = 0.59 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-21 21:41:54.202599: step 80730, loss = 0.50 (23.9 examples/sec; 1.257 sec/batch)\n",
      "2019-05-21 21:42:06.608960: step 80740, loss = 0.62 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 21:42:18.771075: step 80750, loss = 0.62 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 21:42:30.959806: step 80760, loss = 0.62 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 21:42:43.098112: step 80770, loss = 0.56 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 21:42:55.135073: step 80780, loss = 0.48 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 21:43:07.290948: step 80790, loss = 0.61 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 21:43:19.420670: step 80800, loss = 0.60 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 21:43:33.604444: step 80810, loss = 0.60 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 21:43:45.798513: step 80820, loss = 0.62 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 21:43:57.922857: step 80830, loss = 0.54 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 21:44:10.082125: step 80840, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 21:44:22.228292: step 80850, loss = 0.52 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 21:44:34.275132: step 80860, loss = 0.58 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 21:44:46.435167: step 80870, loss = 0.63 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 21:44:58.553334: step 80880, loss = 0.53 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 21:45:10.637781: step 80890, loss = 0.57 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 21:45:22.770363: step 80900, loss = 0.58 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 21:45:37.288966: step 80910, loss = 0.57 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 21:45:49.464328: step 80920, loss = 0.62 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-21 21:46:01.614786: step 80930, loss = 0.50 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 21:46:13.746892: step 80940, loss = 0.53 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 21:46:25.955732: step 80950, loss = 0.54 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 21:46:38.172472: step 80960, loss = 0.62 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 21:46:50.382564: step 80970, loss = 0.64 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 21:47:02.555428: step 80980, loss = 0.60 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 21:47:14.731431: step 80990, loss = 0.55 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 21:47:26.917150: step 81000, loss = 0.56 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-21 21:47:41.425152: step 81010, loss = 0.69 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 21:47:53.513960: step 81020, loss = 0.56 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 21:48:05.553250: step 81030, loss = 0.52 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 21:48:17.712741: step 81040, loss = 0.53 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 21:48:29.756900: step 81050, loss = 0.49 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 21:48:41.867912: step 81060, loss = 0.64 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 21:48:54.089524: step 81070, loss = 0.48 (24.4 examples/sec; 1.232 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 21:49:06.310794: step 81080, loss = 0.59 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 21:49:18.535227: step 81090, loss = 0.57 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 21:49:30.665406: step 81100, loss = 0.54 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 21:49:44.718187: step 81110, loss = 0.54 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 21:49:56.790295: step 81120, loss = 0.55 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 21:50:08.875345: step 81130, loss = 0.50 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 21:50:20.954474: step 81140, loss = 0.55 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 21:50:33.052738: step 81150, loss = 0.51 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 21:50:45.241983: step 81160, loss = 0.57 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 21:50:57.391932: step 81170, loss = 0.53 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 21:51:09.543470: step 81180, loss = 0.58 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 21:51:21.618661: step 81190, loss = 0.56 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 21:51:33.667076: step 81200, loss = 0.65 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 21:51:47.755143: step 81210, loss = 0.54 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 21:51:59.994677: step 81220, loss = 0.55 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 21:52:12.173497: step 81230, loss = 0.54 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 21:52:24.323463: step 81240, loss = 0.53 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 21:52:36.454129: step 81250, loss = 0.57 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 21:52:48.498338: step 81260, loss = 0.59 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 21:53:00.576494: step 81270, loss = 0.54 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 21:53:12.651248: step 81280, loss = 0.61 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 21:53:24.830207: step 81290, loss = 0.55 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 21:53:36.963205: step 81300, loss = 0.58 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-21 21:53:51.438856: step 81310, loss = 0.56 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 21:54:03.600025: step 81320, loss = 0.55 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 21:54:15.745298: step 81330, loss = 0.60 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 21:54:27.965774: step 81340, loss = 0.57 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 21:54:40.108546: step 81350, loss = 0.58 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 21:54:52.295660: step 81360, loss = 0.55 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 21:55:04.413012: step 81370, loss = 0.65 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 21:55:16.467526: step 81380, loss = 0.53 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 21:55:28.482674: step 81390, loss = 0.55 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 21:55:40.593676: step 81400, loss = 0.58 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 21:55:54.884948: step 81410, loss = 0.62 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 21:56:06.976712: step 81420, loss = 0.55 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 21:56:19.100627: step 81430, loss = 0.56 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 21:56:31.217155: step 81440, loss = 0.52 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 21:56:43.399216: step 81450, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 21:56:55.586436: step 81460, loss = 0.50 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 21:57:07.766927: step 81470, loss = 0.57 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 21:57:19.932570: step 81480, loss = 0.57 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 21:57:32.123801: step 81490, loss = 0.52 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 21:57:44.321510: step 81500, loss = 0.55 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-21 21:57:58.491414: step 81510, loss = 0.52 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 21:58:10.543641: step 81520, loss = 0.54 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 21:58:22.655575: step 81530, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 21:58:34.748149: step 81540, loss = 0.59 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 21:58:46.907019: step 81550, loss = 0.56 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 21:58:58.984877: step 81560, loss = 0.57 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 21:59:11.144331: step 81570, loss = 0.55 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 21:59:23.298381: step 81580, loss = 0.67 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 21:59:35.376477: step 81590, loss = 0.53 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 21:59:47.460988: step 81600, loss = 0.63 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 22:00:01.530003: step 81610, loss = 0.52 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 22:00:13.584434: step 81620, loss = 0.66 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 22:00:25.624652: step 81630, loss = 0.53 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 22:00:37.644928: step 81640, loss = 0.61 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 22:00:49.717386: step 81650, loss = 0.64 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 22:01:01.810140: step 81660, loss = 0.59 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-21 22:01:13.854290: step 81670, loss = 0.56 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 22:01:25.978848: step 81680, loss = 0.63 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 22:01:38.152382: step 81690, loss = 0.62 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 22:01:50.290940: step 81700, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 22:02:04.497389: step 81710, loss = 0.53 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 22:02:16.622236: step 81720, loss = 0.62 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-21 22:02:28.768667: step 81730, loss = 0.60 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 22:02:40.884606: step 81740, loss = 0.61 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 22:02:53.052306: step 81750, loss = 0.57 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 22:03:05.186010: step 81760, loss = 0.57 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-21 22:03:17.225468: step 81770, loss = 0.60 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 22:03:29.366508: step 81780, loss = 0.53 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 22:03:41.542763: step 81790, loss = 0.60 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 22:03:53.660202: step 81800, loss = 0.59 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 22:04:07.864335: step 81810, loss = 0.53 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 22:04:19.959797: step 81820, loss = 0.52 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 22:04:32.064365: step 81830, loss = 0.54 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 22:04:44.175608: step 81840, loss = 0.51 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 22:04:56.308718: step 81850, loss = 0.60 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-21 22:05:08.476922: step 81860, loss = 0.50 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 22:05:20.644280: step 81870, loss = 0.59 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 22:05:32.823248: step 81880, loss = 0.58 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 22:05:44.889885: step 81890, loss = 0.58 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 22:05:57.003406: step 81900, loss = 0.57 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 22:06:11.182082: step 81910, loss = 0.57 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 22:06:23.314409: step 81920, loss = 0.57 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 22:06:35.474903: step 81930, loss = 0.57 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 22:06:47.575374: step 81940, loss = 0.60 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 22:06:59.689029: step 81950, loss = 0.54 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-21 22:07:11.822233: step 81960, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 22:07:23.920255: step 81970, loss = 0.64 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 22:07:36.056318: step 81980, loss = 0.59 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 22:07:48.163263: step 81990, loss = 0.57 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 22:08:00.240038: step 82000, loss = 0.61 (24.9 examples/sec; 1.204 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 22:08:14.281642: step 82010, loss = 0.60 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 22:08:26.306033: step 82020, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 22:08:38.429729: step 82030, loss = 0.55 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 22:08:50.547967: step 82040, loss = 0.54 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 22:09:02.604603: step 82050, loss = 0.60 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 22:09:14.678404: step 82060, loss = 0.58 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 22:09:26.792875: step 82070, loss = 0.50 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 22:09:38.912052: step 82080, loss = 0.58 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 22:09:51.009673: step 82090, loss = 0.56 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 22:10:03.081915: step 82100, loss = 0.55 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 22:10:17.086888: step 82110, loss = 0.59 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 22:10:29.182449: step 82120, loss = 0.61 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 22:10:41.296435: step 82130, loss = 0.59 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 22:10:53.268021: step 82140, loss = 0.57 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 22:11:05.298072: step 82150, loss = 0.58 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 22:11:17.405889: step 82160, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 22:11:29.563103: step 82170, loss = 0.60 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 22:11:41.666674: step 82180, loss = 0.53 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 22:11:53.768997: step 82190, loss = 0.58 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 22:12:05.837345: step 82200, loss = 0.61 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-21 22:12:20.157912: step 82210, loss = 0.51 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 22:12:32.224586: step 82220, loss = 0.62 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 22:12:44.265572: step 82230, loss = 0.58 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 22:12:56.326479: step 82240, loss = 0.64 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-21 22:13:08.404401: step 82250, loss = 0.48 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 22:13:20.499303: step 82260, loss = 0.54 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 22:13:32.524883: step 82270, loss = 0.60 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 22:13:44.608408: step 82280, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 22:13:56.735864: step 82290, loss = 0.57 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 22:14:08.858810: step 82300, loss = 0.56 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 22:14:23.281635: step 82310, loss = 0.59 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 22:14:35.400140: step 82320, loss = 0.56 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 22:14:47.448310: step 82330, loss = 0.59 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 22:14:59.569950: step 82340, loss = 0.55 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 22:15:11.679650: step 82350, loss = 0.62 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 22:15:23.732154: step 82360, loss = 0.49 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 22:15:35.881441: step 82370, loss = 0.61 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 22:15:47.976256: step 82380, loss = 0.67 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-21 22:16:00.077292: step 82390, loss = 0.54 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 22:16:12.071611: step 82400, loss = 0.55 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 22:16:26.158956: step 82410, loss = 0.62 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 22:16:38.223650: step 82420, loss = 0.63 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 22:16:50.343738: step 82430, loss = 0.54 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 22:17:02.422714: step 82440, loss = 0.55 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 22:17:14.513938: step 82450, loss = 0.57 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 22:17:26.548723: step 82460, loss = 0.54 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 22:17:38.601369: step 82470, loss = 0.59 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 22:17:50.624118: step 82480, loss = 0.59 (25.4 examples/sec; 1.179 sec/batch)\n",
      "2019-05-21 22:18:02.652352: step 82490, loss = 0.53 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 22:18:14.707764: step 82500, loss = 0.58 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 22:18:28.646499: step 82510, loss = 0.56 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 22:18:40.655219: step 82520, loss = 0.57 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 22:18:52.659557: step 82530, loss = 0.64 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 22:19:04.754518: step 82540, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 22:19:16.765457: step 82550, loss = 0.58 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 22:19:28.780434: step 82560, loss = 0.62 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 22:19:40.766068: step 82570, loss = 0.55 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 22:19:52.818325: step 82580, loss = 0.50 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 22:20:04.775064: step 82590, loss = 0.57 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 22:20:16.825706: step 82600, loss = 0.62 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 22:20:31.346639: step 82610, loss = 0.53 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 22:20:43.323138: step 82620, loss = 0.48 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 22:20:55.211238: step 82630, loss = 0.56 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 22:21:07.102109: step 82640, loss = 0.55 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-21 22:21:18.989806: step 82650, loss = 0.63 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 22:21:30.969399: step 82660, loss = 0.60 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 22:21:42.875555: step 82670, loss = 0.56 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 22:21:54.788478: step 82680, loss = 0.67 (25.5 examples/sec; 1.175 sec/batch)\n",
      "2019-05-21 22:22:06.719931: step 82690, loss = 0.57 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 22:22:18.614842: step 82700, loss = 0.59 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 22:22:32.925551: step 82710, loss = 0.53 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 22:22:44.852407: step 82720, loss = 0.56 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 22:22:56.759567: step 82730, loss = 0.63 (25.5 examples/sec; 1.179 sec/batch)\n",
      "2019-05-21 22:23:08.677931: step 82740, loss = 0.49 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 22:23:20.567111: step 82750, loss = 0.52 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 22:23:32.560968: step 82760, loss = 0.63 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 22:23:44.471141: step 82770, loss = 0.50 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 22:23:56.411672: step 82780, loss = 0.57 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 22:24:08.365005: step 82790, loss = 0.48 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 22:24:20.259677: step 82800, loss = 0.62 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 22:24:34.159152: step 82810, loss = 0.52 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 22:24:46.133074: step 82820, loss = 0.57 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 22:24:58.081719: step 82830, loss = 0.57 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-21 22:25:10.015609: step 82840, loss = 0.57 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 22:25:21.952155: step 82850, loss = 0.62 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 22:25:33.882343: step 82860, loss = 0.61 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 22:25:45.844233: step 82870, loss = 0.61 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 22:25:57.768583: step 82880, loss = 0.57 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 22:26:09.699182: step 82890, loss = 0.48 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 22:26:21.617618: step 82900, loss = 0.57 (25.5 examples/sec; 1.178 sec/batch)\n",
      "2019-05-21 22:26:35.665988: step 82910, loss = 0.56 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 22:26:47.594499: step 82920, loss = 0.65 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 22:26:59.528983: step 82930, loss = 0.61 (25.3 examples/sec; 1.188 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 22:27:11.477306: step 82940, loss = 0.62 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 22:27:23.489404: step 82950, loss = 0.54 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 22:27:35.481664: step 82960, loss = 0.65 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 22:27:47.491155: step 82970, loss = 0.60 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 22:27:59.437624: step 82980, loss = 0.57 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 22:28:11.322506: step 82990, loss = 0.56 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 22:28:23.277726: step 83000, loss = 0.56 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 22:28:37.297956: step 83010, loss = 0.49 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 22:28:49.198642: step 83020, loss = 0.60 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 22:29:01.157740: step 83030, loss = 0.63 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 22:29:13.116357: step 83040, loss = 0.57 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 22:29:25.077123: step 83050, loss = 0.51 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 22:29:37.050089: step 83060, loss = 0.61 (25.4 examples/sec; 1.179 sec/batch)\n",
      "2019-05-21 22:29:48.995456: step 83070, loss = 0.58 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 22:30:00.928025: step 83080, loss = 0.57 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 22:30:12.846745: step 83090, loss = 0.51 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-21 22:30:24.769523: step 83100, loss = 0.55 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 22:30:39.077996: step 83110, loss = 0.70 (25.5 examples/sec; 1.176 sec/batch)\n",
      "2019-05-21 22:30:51.061112: step 83120, loss = 0.53 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 22:31:03.007512: step 83130, loss = 0.60 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 22:31:14.882357: step 83140, loss = 0.67 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 22:31:26.808215: step 83150, loss = 0.53 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 22:31:38.687965: step 83160, loss = 0.59 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 22:31:50.615070: step 83170, loss = 0.54 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 22:32:02.509219: step 83180, loss = 0.64 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 22:32:14.422835: step 83190, loss = 0.57 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 22:32:26.335163: step 83200, loss = 0.54 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 22:32:40.459448: step 83210, loss = 0.53 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-21 22:32:52.449958: step 83220, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 22:33:04.408663: step 83230, loss = 0.55 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 22:33:16.339530: step 83240, loss = 0.49 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 22:33:28.227360: step 83250, loss = 0.58 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 22:33:40.196203: step 83260, loss = 0.51 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 22:33:52.116822: step 83270, loss = 0.58 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 22:34:04.077887: step 83280, loss = 0.56 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 22:34:16.028562: step 83290, loss = 0.48 (25.5 examples/sec; 1.178 sec/batch)\n",
      "2019-05-21 22:34:27.957182: step 83300, loss = 0.49 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 22:34:41.837237: step 83310, loss = 0.49 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 22:34:53.759413: step 83320, loss = 0.58 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 22:35:05.711541: step 83330, loss = 0.56 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 22:35:17.582654: step 83340, loss = 0.60 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 22:35:29.510990: step 83350, loss = 0.58 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 22:35:41.425433: step 83360, loss = 0.57 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 22:35:53.402935: step 83370, loss = 0.56 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 22:36:05.326992: step 83380, loss = 0.64 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 22:36:17.333381: step 83390, loss = 0.53 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 22:36:29.276627: step 83400, loss = 0.59 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 22:36:43.232900: step 83410, loss = 0.64 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 22:36:55.184490: step 83420, loss = 0.55 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 22:37:07.134794: step 83430, loss = 0.61 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 22:37:19.098023: step 83440, loss = 0.54 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 22:37:31.042440: step 83450, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 22:37:42.978525: step 83460, loss = 0.54 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 22:37:54.891261: step 83470, loss = 0.56 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 22:38:06.840227: step 83480, loss = 0.51 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 22:38:18.724494: step 83490, loss = 0.60 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 22:38:30.639798: step 83500, loss = 0.51 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 22:38:44.383408: step 83510, loss = 0.54 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 22:38:56.335335: step 83520, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 22:39:08.224304: step 83530, loss = 0.62 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 22:39:20.138076: step 83540, loss = 0.55 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 22:39:32.025567: step 83550, loss = 0.60 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 22:39:43.953418: step 83560, loss = 0.58 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 22:39:55.869415: step 83570, loss = 0.57 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 22:40:07.789804: step 83580, loss = 0.64 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 22:40:19.741845: step 83590, loss = 0.57 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 22:40:31.669023: step 83600, loss = 0.56 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 22:40:46.009518: step 83610, loss = 0.57 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 22:40:57.993546: step 83620, loss = 0.53 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 22:41:09.928426: step 83630, loss = 0.50 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 22:41:21.883474: step 83640, loss = 0.59 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 22:41:33.781964: step 83650, loss = 0.60 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 22:41:45.700287: step 83660, loss = 0.52 (25.5 examples/sec; 1.178 sec/batch)\n",
      "2019-05-21 22:41:57.604560: step 83670, loss = 0.59 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 22:42:09.550872: step 83680, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 22:42:21.430260: step 83690, loss = 0.54 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 22:42:33.370612: step 83700, loss = 0.57 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 22:42:47.650085: step 83710, loss = 0.63 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 22:42:59.612556: step 83720, loss = 0.60 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 22:43:11.546162: step 83730, loss = 0.54 (25.5 examples/sec; 1.174 sec/batch)\n",
      "2019-05-21 22:43:23.539487: step 83740, loss = 0.54 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 22:43:35.533119: step 83750, loss = 0.59 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 22:43:47.539455: step 83760, loss = 0.53 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 22:43:59.470939: step 83770, loss = 0.57 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 22:44:11.475644: step 83780, loss = 0.59 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 22:44:23.472863: step 83790, loss = 0.59 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 22:44:35.477628: step 83800, loss = 0.57 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 22:44:49.424734: step 83810, loss = 0.61 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 22:45:01.452433: step 83820, loss = 0.56 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 22:45:13.455551: step 83830, loss = 0.55 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 22:45:25.456713: step 83840, loss = 0.49 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 22:45:37.454187: step 83850, loss = 0.61 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 22:45:49.390887: step 83860, loss = 0.69 (25.3 examples/sec; 1.185 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 22:46:01.413166: step 83870, loss = 0.57 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 22:46:13.484942: step 83880, loss = 0.57 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 22:46:25.525349: step 83890, loss = 0.55 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 22:46:37.593276: step 83900, loss = 0.58 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 22:46:51.637902: step 83910, loss = 0.59 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 22:47:03.578964: step 83920, loss = 0.55 (25.4 examples/sec; 1.179 sec/batch)\n",
      "2019-05-21 22:47:15.533024: step 83930, loss = 0.55 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 22:47:27.524924: step 83940, loss = 0.57 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 22:47:39.564709: step 83950, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 22:47:51.564474: step 83960, loss = 0.55 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 22:48:03.556396: step 83970, loss = 0.60 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 22:48:15.520809: step 83980, loss = 0.57 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 22:48:27.496137: step 83990, loss = 0.56 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 22:48:39.438952: step 84000, loss = 0.53 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 22:48:53.560664: step 84010, loss = 0.56 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 22:49:05.558878: step 84020, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 22:49:17.575684: step 84030, loss = 0.56 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 22:49:29.571108: step 84040, loss = 0.62 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 22:49:41.557323: step 84050, loss = 0.60 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 22:49:53.555093: step 84060, loss = 0.53 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 22:50:05.567606: step 84070, loss = 0.50 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 22:50:17.613456: step 84080, loss = 0.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 22:50:29.622574: step 84090, loss = 0.52 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 22:50:41.621016: step 84100, loss = 0.60 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 22:50:55.529727: step 84110, loss = 0.55 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 22:51:07.558824: step 84120, loss = 0.57 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 22:51:19.611106: step 84130, loss = 0.51 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 22:51:31.621998: step 84140, loss = 0.58 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 22:51:43.693595: step 84150, loss = 0.49 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 22:51:55.717462: step 84160, loss = 0.57 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 22:52:07.746467: step 84170, loss = 0.51 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 22:52:19.707429: step 84180, loss = 0.65 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 22:52:31.691346: step 84190, loss = 0.56 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 22:52:43.759502: step 84200, loss = 0.55 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 22:52:57.742161: step 84210, loss = 0.58 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 22:53:09.729250: step 84220, loss = 0.55 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 22:53:21.774989: step 84230, loss = 0.53 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 22:53:33.826237: step 84240, loss = 0.60 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 22:53:45.846746: step 84250, loss = 0.49 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 22:53:57.866288: step 84260, loss = 0.62 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 22:54:09.860233: step 84270, loss = 0.59 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 22:54:21.843844: step 84280, loss = 0.62 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 22:54:33.844273: step 84290, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 22:54:45.852831: step 84300, loss = 0.57 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 22:54:59.829433: step 84310, loss = 0.58 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 22:55:11.840460: step 84320, loss = 0.53 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 22:55:23.883450: step 84330, loss = 0.60 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 22:55:35.869938: step 84340, loss = 0.52 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 22:55:47.822176: step 84350, loss = 0.55 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 22:55:59.820929: step 84360, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 22:56:11.781938: step 84370, loss = 0.51 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 22:56:23.727922: step 84380, loss = 0.57 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 22:56:35.678385: step 84390, loss = 0.58 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 22:56:47.609604: step 84400, loss = 0.61 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 22:57:01.704478: step 84410, loss = 0.59 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 22:57:13.569113: step 84420, loss = 0.53 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 22:58:01.274567: step 84460, loss = 0.51 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 22:58:13.246600: step 84470, loss = 0.53 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 22:58:25.204756: step 84480, loss = 0.58 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 22:58:37.157206: step 84490, loss = 0.65 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 22:58:49.066798: step 84500, loss = 0.63 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 22:59:02.967119: step 84510, loss = 0.53 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 22:59:14.885180: step 84520, loss = 0.56 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-21 22:59:26.846693: step 84530, loss = 0.54 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 22:59:38.803050: step 84540, loss = 0.59 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 22:59:50.764239: step 84550, loss = 0.55 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 23:00:02.719728: step 84560, loss = 0.50 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 23:00:14.677796: step 84570, loss = 0.60 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 23:00:26.584459: step 84580, loss = 0.58 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 23:00:38.515148: step 84590, loss = 0.56 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 23:00:50.532446: step 84600, loss = 0.57 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 23:01:04.685143: step 84610, loss = 0.57 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 23:01:16.697625: step 84620, loss = 0.58 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 23:01:28.689727: step 84630, loss = 0.64 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 23:01:40.735972: step 84640, loss = 0.55 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 23:01:52.728641: step 84650, loss = 0.54 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 23:02:04.729261: step 84660, loss = 0.58 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 23:02:16.725584: step 84670, loss = 0.57 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 23:02:28.729944: step 84680, loss = 0.50 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 23:02:40.684501: step 84690, loss = 0.48 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 23:02:52.650736: step 84700, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 23:03:06.598870: step 84710, loss = 0.61 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 23:03:18.635378: step 84720, loss = 0.58 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 23:03:30.665656: step 84730, loss = 0.61 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 23:03:42.610715: step 84740, loss = 0.55 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 23:03:54.600160: step 84750, loss = 0.53 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 23:04:06.557655: step 84760, loss = 0.59 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 23:04:18.571659: step 84770, loss = 0.64 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 23:04:30.581538: step 84780, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 23:04:42.599987: step 84790, loss = 0.61 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 23:04:54.607767: step 84800, loss = 0.60 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 23:05:08.937941: step 84810, loss = 0.60 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 23:05:21.027319: step 84820, loss = 0.63 (24.9 examples/sec; 1.205 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 23:05:33.081283: step 84830, loss = 0.59 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 23:05:45.099737: step 84840, loss = 0.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 23:05:57.085308: step 84850, loss = 0.51 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 23:06:09.113494: step 84860, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 23:06:21.097586: step 84870, loss = 0.60 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 23:06:33.100703: step 84880, loss = 0.56 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 23:06:45.118482: step 84890, loss = 0.52 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 23:06:57.118992: step 84900, loss = 0.54 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 23:07:11.454485: step 84910, loss = 0.47 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 23:07:23.423257: step 84920, loss = 0.58 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 23:07:35.426912: step 84930, loss = 0.58 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 23:07:47.392024: step 84940, loss = 0.51 (25.5 examples/sec; 1.178 sec/batch)\n",
      "2019-05-21 23:07:59.346270: step 84950, loss = 0.58 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 23:08:11.377957: step 84960, loss = 0.62 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 23:08:23.369665: step 84970, loss = 0.58 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 23:08:35.339766: step 84980, loss = 0.64 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 23:08:47.301635: step 84990, loss = 0.61 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 23:08:59.216419: step 85000, loss = 0.58 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 23:09:16.599287: step 85010, loss = 0.62 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-21 23:09:28.509161: step 85020, loss = 0.56 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 23:09:40.463276: step 85030, loss = 0.51 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 23:09:52.433037: step 85040, loss = 0.59 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 23:10:04.388577: step 85050, loss = 0.56 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 23:10:16.313771: step 85060, loss = 0.53 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 23:10:28.270393: step 85070, loss = 0.49 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 23:10:40.280660: step 85080, loss = 0.46 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 23:10:52.257282: step 85090, loss = 0.57 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 23:11:04.218292: step 85100, loss = 0.64 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 23:11:18.407439: step 85110, loss = 0.58 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 23:11:30.395504: step 85120, loss = 0.63 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 23:11:42.361999: step 85130, loss = 0.56 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 23:11:54.351872: step 85140, loss = 0.45 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 23:12:06.346455: step 85150, loss = 0.55 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 23:12:18.254054: step 85160, loss = 0.58 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 23:12:30.209060: step 85170, loss = 0.55 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 23:12:42.141896: step 85180, loss = 0.60 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 23:12:54.092960: step 85190, loss = 0.54 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 23:13:05.967250: step 85200, loss = 0.58 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 23:13:20.015849: step 85210, loss = 0.64 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 23:13:32.000008: step 85220, loss = 0.56 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 23:13:43.897675: step 85230, loss = 0.63 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 23:13:55.893461: step 85240, loss = 0.59 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 23:14:07.802137: step 85250, loss = 0.60 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 23:14:19.712134: step 85260, loss = 0.59 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 23:14:31.596823: step 85270, loss = 0.56 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 23:14:43.498160: step 85280, loss = 0.54 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 23:14:55.382112: step 85290, loss = 0.63 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 23:15:07.328933: step 85300, loss = 0.56 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 23:15:21.162819: step 85310, loss = 0.55 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 23:15:33.091969: step 85320, loss = 0.54 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 23:15:45.037465: step 85330, loss = 0.50 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 23:15:56.940075: step 85340, loss = 0.50 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 23:16:08.853867: step 85350, loss = 0.63 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 23:16:20.784687: step 85360, loss = 0.53 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 23:16:32.722906: step 85370, loss = 0.50 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 23:16:44.660212: step 85380, loss = 0.56 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 23:16:56.610693: step 85390, loss = 0.58 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 23:17:08.553562: step 85400, loss = 0.62 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 23:17:22.299574: step 85410, loss = 0.51 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-21 23:17:34.190260: step 85420, loss = 0.56 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 23:17:46.142504: step 85430, loss = 0.50 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 23:17:58.093502: step 85440, loss = 0.48 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 23:18:10.053117: step 85450, loss = 0.55 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 23:18:21.979291: step 85460, loss = 0.55 (25.5 examples/sec; 1.177 sec/batch)\n",
      "2019-05-21 23:18:33.930452: step 85470, loss = 0.55 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 23:18:45.862898: step 85480, loss = 0.55 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 23:18:57.798654: step 85490, loss = 0.47 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 23:19:09.680045: step 85500, loss = 0.56 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 23:19:23.454930: step 85510, loss = 0.57 (25.5 examples/sec; 1.177 sec/batch)\n",
      "2019-05-21 23:19:35.386872: step 85520, loss = 0.53 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 23:19:47.306188: step 85530, loss = 0.55 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 23:19:59.238976: step 85540, loss = 0.49 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 23:20:11.204633: step 85550, loss = 0.55 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 23:20:23.130343: step 85560, loss = 0.60 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 23:20:35.087190: step 85570, loss = 0.51 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 23:20:47.050616: step 85580, loss = 0.57 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 23:20:58.990766: step 85590, loss = 0.54 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 23:21:10.959890: step 85600, loss = 0.55 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 23:21:24.815147: step 85610, loss = 0.62 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 23:21:36.766658: step 85620, loss = 0.58 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 23:21:48.741304: step 85630, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 23:22:00.698512: step 85640, loss = 0.51 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 23:22:12.649222: step 85650, loss = 0.59 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 23:22:24.635267: step 85660, loss = 0.58 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 23:22:36.618350: step 85670, loss = 0.53 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 23:22:48.592718: step 85680, loss = 0.58 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 23:23:00.558942: step 85690, loss = 0.57 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 23:23:12.562557: step 85700, loss = 0.59 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 23:23:26.885516: step 85710, loss = 0.57 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 23:23:38.960925: step 85720, loss = 0.55 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 23:23:50.965603: step 85730, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 23:24:02.948076: step 85740, loss = 0.61 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 23:24:14.971407: step 85750, loss = 0.55 (25.1 examples/sec; 1.193 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 23:24:26.944494: step 85760, loss = 0.55 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 23:24:38.946687: step 85770, loss = 0.53 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 23:24:50.927448: step 85780, loss = 0.56 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 23:25:03.011249: step 85790, loss = 0.52 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 23:25:15.034706: step 85800, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 23:25:29.515118: step 85810, loss = 0.46 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 23:25:41.578250: step 85820, loss = 0.60 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 23:25:53.613156: step 85830, loss = 0.68 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 23:26:05.607384: step 85840, loss = 0.60 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 23:26:17.621856: step 85850, loss = 0.60 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 23:26:29.686391: step 85860, loss = 0.55 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 23:26:41.735645: step 85870, loss = 0.58 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 23:26:53.787467: step 85880, loss = 0.60 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 23:27:05.850711: step 85890, loss = 0.56 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 23:27:17.830921: step 85900, loss = 0.54 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 23:27:31.987754: step 85910, loss = 0.59 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 23:27:43.947301: step 85920, loss = 0.60 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 23:27:56.036849: step 85930, loss = 0.53 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 23:28:08.098092: step 85940, loss = 0.56 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 23:28:20.146790: step 85950, loss = 0.56 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 23:28:32.204656: step 85960, loss = 0.53 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 23:28:44.168907: step 85970, loss = 0.54 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 23:28:56.204836: step 85980, loss = 0.56 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 23:29:08.258033: step 85990, loss = 0.62 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 23:29:20.323740: step 86000, loss = 0.58 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 23:29:34.395104: step 86010, loss = 0.60 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 23:29:46.397705: step 86020, loss = 0.61 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 23:29:58.349171: step 86030, loss = 0.57 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 23:30:10.404551: step 86040, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 23:30:22.517424: step 86050, loss = 0.58 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 23:30:34.561641: step 86060, loss = 0.58 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 23:30:46.573821: step 86070, loss = 0.54 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 23:30:58.573350: step 86080, loss = 0.55 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 23:31:10.635130: step 86090, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 23:31:22.700455: step 86100, loss = 0.59 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 23:31:36.664195: step 86110, loss = 0.65 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 23:31:48.714363: step 86120, loss = 0.59 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 23:32:00.802951: step 86130, loss = 0.57 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 23:32:12.864840: step 86140, loss = 0.57 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 23:32:24.893542: step 86150, loss = 0.56 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 23:32:36.917556: step 86160, loss = 0.50 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 23:32:48.925857: step 86170, loss = 0.59 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 23:33:00.966959: step 86180, loss = 0.47 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-21 23:33:12.994568: step 86190, loss = 0.63 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 23:33:25.040657: step 86200, loss = 0.52 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 23:33:39.127127: step 86210, loss = 0.56 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 23:33:51.043013: step 86220, loss = 0.63 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 23:34:03.048375: step 86230, loss = 0.53 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 23:34:15.041304: step 86240, loss = 0.56 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 23:34:27.050622: step 86250, loss = 0.52 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 23:34:39.058299: step 86260, loss = 0.63 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 23:34:51.142656: step 86270, loss = 0.60 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-21 23:35:03.158324: step 86280, loss = 0.58 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 23:35:15.198406: step 86290, loss = 0.59 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 23:35:27.184059: step 86300, loss = 0.57 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 23:35:41.498028: step 86310, loss = 0.53 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 23:35:53.511816: step 86320, loss = 0.56 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 23:36:05.536632: step 86330, loss = 0.59 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 23:36:17.614928: step 86340, loss = 0.58 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 23:36:29.670093: step 86350, loss = 0.53 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 23:36:41.711334: step 86360, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 23:36:53.775307: step 86370, loss = 0.59 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 23:37:05.806685: step 86380, loss = 0.56 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-21 23:37:17.840231: step 86390, loss = 0.54 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 23:37:29.869522: step 86400, loss = 0.60 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 23:37:43.929838: step 86410, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 23:37:55.942765: step 86420, loss = 0.52 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 23:38:08.007243: step 86430, loss = 0.64 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-21 23:38:20.030221: step 86440, loss = 0.57 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 23:38:32.054626: step 86450, loss = 0.44 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 23:38:44.139252: step 86460, loss = 0.56 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-21 23:38:56.125494: step 86470, loss = 0.52 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 23:39:08.131347: step 86480, loss = 0.53 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 23:39:20.149457: step 86490, loss = 0.53 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-21 23:39:32.148356: step 86500, loss = 0.61 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 23:39:46.307284: step 86510, loss = 0.51 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 23:39:58.301630: step 86520, loss = 0.66 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 23:40:10.280169: step 86530, loss = 0.60 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 23:40:22.330641: step 86540, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 23:40:34.333216: step 86550, loss = 0.58 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 23:40:46.340044: step 86560, loss = 0.56 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 23:40:58.346896: step 86570, loss = 0.55 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 23:41:10.338139: step 86580, loss = 0.52 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 23:41:22.362815: step 86590, loss = 0.60 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-21 23:41:34.406336: step 86600, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-21 23:41:48.709097: step 86610, loss = 0.62 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 23:42:00.715730: step 86620, loss = 0.66 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 23:42:12.748367: step 86630, loss = 0.50 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 23:42:24.677458: step 86640, loss = 0.63 (25.5 examples/sec; 1.178 sec/batch)\n",
      "2019-05-21 23:42:36.639112: step 86650, loss = 0.57 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-21 23:42:48.661964: step 86660, loss = 0.60 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 23:43:00.708050: step 86670, loss = 0.55 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 23:43:12.765245: step 86680, loss = 0.52 (25.3 examples/sec; 1.185 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 23:43:24.823954: step 86690, loss = 0.55 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-21 23:43:36.843331: step 86700, loss = 0.53 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 23:43:50.706641: step 86710, loss = 0.58 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-21 23:44:02.685041: step 86720, loss = 0.57 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-21 23:44:14.675142: step 86730, loss = 0.48 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-21 23:44:26.692198: step 86740, loss = 0.61 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 23:44:38.698188: step 86750, loss = 0.54 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 23:44:51.118001: step 86760, loss = 0.55 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 23:45:03.133917: step 86770, loss = 0.55 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 23:45:15.078482: step 86780, loss = 0.56 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 23:45:27.577147: step 86790, loss = 0.59 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-21 23:45:39.546592: step 86800, loss = 0.60 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 23:45:53.419255: step 86810, loss = 0.56 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 23:46:05.343102: step 86820, loss = 0.53 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-21 23:46:17.299793: step 86830, loss = 0.54 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-21 23:46:29.245749: step 86840, loss = 0.53 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 23:46:41.151266: step 86850, loss = 0.60 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 23:46:53.028128: step 86860, loss = 0.52 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-21 23:47:04.940598: step 86870, loss = 0.51 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-21 23:47:16.909002: step 86880, loss = 0.57 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 23:47:28.918677: step 86890, loss = 0.58 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-21 23:47:40.916964: step 86900, loss = 0.66 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 23:47:55.221351: step 86910, loss = 0.53 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 23:48:07.218877: step 86920, loss = 0.58 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 23:48:19.250587: step 86930, loss = 0.59 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 23:48:31.294267: step 86940, loss = 0.53 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-21 23:48:43.320296: step 86950, loss = 0.56 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 23:48:55.326144: step 86960, loss = 0.52 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 23:49:07.378157: step 86970, loss = 0.58 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 23:49:19.375825: step 86980, loss = 0.52 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 23:49:31.393912: step 86990, loss = 0.53 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-21 23:49:43.426691: step 87000, loss = 0.53 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 23:49:57.670128: step 87010, loss = 0.57 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 23:50:09.671243: step 87020, loss = 0.61 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 23:50:21.642503: step 87030, loss = 0.60 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 23:50:33.651410: step 87040, loss = 0.59 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-21 23:50:45.641625: step 87050, loss = 0.58 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-21 23:50:57.598171: step 87060, loss = 0.66 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 23:51:09.523142: step 87070, loss = 0.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-21 23:51:21.499873: step 87080, loss = 0.54 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 23:51:33.477440: step 87090, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 23:51:45.497539: step 87100, loss = 0.54 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 23:51:59.720931: step 87110, loss = 0.63 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 23:52:11.734303: step 87120, loss = 0.63 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 23:52:23.753096: step 87130, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 23:52:35.768911: step 87140, loss = 0.58 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-21 23:52:47.742723: step 87150, loss = 0.54 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 23:52:59.780601: step 87160, loss = 0.58 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 23:53:11.753239: step 87170, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-21 23:53:23.780702: step 87180, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 23:53:35.834621: step 87190, loss = 0.55 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 23:53:47.840552: step 87200, loss = 0.57 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 23:54:02.039182: step 87210, loss = 0.58 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 23:54:14.036772: step 87220, loss = 0.57 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 23:54:25.997368: step 87230, loss = 0.56 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 23:54:37.993643: step 87240, loss = 0.58 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 23:54:50.011436: step 87250, loss = 0.58 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-21 23:55:01.998431: step 87260, loss = 0.60 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-21 23:55:13.961714: step 87270, loss = 0.55 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-21 23:55:25.897943: step 87280, loss = 0.50 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-21 23:55:37.870200: step 87290, loss = 0.65 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 23:55:49.896845: step 87300, loss = 0.61 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-21 23:56:03.772454: step 87310, loss = 0.56 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-21 23:56:15.810969: step 87320, loss = 0.64 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 23:56:27.746536: step 87330, loss = 0.50 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 23:56:39.655183: step 87340, loss = 0.55 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 23:56:51.621275: step 87350, loss = 0.55 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 23:57:03.545975: step 87360, loss = 0.54 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 23:57:15.472362: step 87370, loss = 0.56 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-21 23:57:27.424450: step 87380, loss = 0.63 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-21 23:57:39.378350: step 87390, loss = 0.64 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 23:57:51.307522: step 87400, loss = 0.59 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-21 23:58:05.818543: step 87410, loss = 0.58 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 23:58:17.752314: step 87420, loss = 0.58 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-21 23:58:29.743709: step 87430, loss = 0.61 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-21 23:58:41.727982: step 87440, loss = 0.48 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-21 23:58:53.675884: step 87450, loss = 0.61 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-21 23:59:05.652516: step 87460, loss = 0.78 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-21 23:59:17.599429: step 87470, loss = 0.56 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-21 23:59:29.558970: step 87480, loss = 0.65 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-21 23:59:41.578329: step 87490, loss = 0.60 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-21 23:59:53.549085: step 87500, loss = 0.57 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 00:00:07.395964: step 87510, loss = 0.56 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 00:00:19.378845: step 87520, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 00:00:31.342651: step 87530, loss = 0.58 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 00:00:43.318632: step 87540, loss = 0.63 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 00:00:55.279006: step 87550, loss = 0.58 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 00:01:07.278987: step 87560, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 00:01:19.277096: step 87570, loss = 0.58 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 00:01:31.307477: step 87580, loss = 0.56 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 00:01:43.340723: step 87590, loss = 0.53 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 00:01:55.310068: step 87600, loss = 0.52 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 00:02:09.153028: step 87610, loss = 0.50 (25.4 examples/sec; 1.182 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 00:02:21.088965: step 87620, loss = 0.54 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 00:02:33.093879: step 87630, loss = 0.58 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 00:02:45.122275: step 87640, loss = 0.52 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 00:02:57.071436: step 87650, loss = 0.48 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 00:03:09.128395: step 87660, loss = 0.63 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 00:03:21.166570: step 87670, loss = 0.63 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 00:03:33.228931: step 87680, loss = 0.53 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 00:03:45.259176: step 87690, loss = 0.55 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 00:03:57.238806: step 87700, loss = 0.61 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 00:04:11.489208: step 87710, loss = 0.59 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 00:04:23.475456: step 87720, loss = 0.58 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 00:04:35.482295: step 87730, loss = 0.58 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 00:04:47.463873: step 87740, loss = 0.54 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 00:04:59.389950: step 87750, loss = 0.54 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 00:05:11.409728: step 87760, loss = 0.54 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 00:05:23.454001: step 87770, loss = 0.47 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 00:05:35.441966: step 87780, loss = 0.48 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 00:05:47.416834: step 87790, loss = 0.57 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 00:05:59.411559: step 87800, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 00:06:13.647577: step 87810, loss = 0.59 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-22 00:06:25.536691: step 87820, loss = 0.57 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-22 00:06:37.493197: step 87830, loss = 0.57 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 00:06:49.506656: step 87840, loss = 0.56 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 00:07:01.497283: step 87850, loss = 0.57 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 00:07:13.510488: step 87860, loss = 0.51 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 00:07:25.441266: step 87870, loss = 0.48 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 00:07:37.487205: step 87880, loss = 0.49 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 00:07:49.494367: step 87890, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 00:08:01.555236: step 87900, loss = 0.60 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 00:08:15.558894: step 87910, loss = 0.59 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-22 00:08:27.524367: step 87920, loss = 0.60 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 00:08:39.533969: step 87930, loss = 0.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 00:08:51.547530: step 87940, loss = 0.57 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 00:09:03.555374: step 87950, loss = 0.57 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 00:09:15.549814: step 87960, loss = 0.56 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 00:09:27.576654: step 87970, loss = 0.59 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 00:09:39.552462: step 87980, loss = 0.53 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 00:09:51.511295: step 87990, loss = 0.54 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 00:10:03.462579: step 88000, loss = 0.51 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 00:10:17.822338: step 88010, loss = 0.57 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 00:10:29.858887: step 88020, loss = 0.55 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 00:10:41.800112: step 88030, loss = 0.54 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 00:10:53.809166: step 88040, loss = 0.53 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 00:11:05.882898: step 88050, loss = 0.66 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 00:11:17.940857: step 88060, loss = 0.52 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 00:11:30.036847: step 88070, loss = 0.60 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 00:11:42.107485: step 88080, loss = 0.48 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 00:11:54.146086: step 88090, loss = 0.59 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-22 00:12:06.203436: step 88100, loss = 0.59 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 00:12:20.137389: step 88110, loss = 0.58 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 00:12:32.154416: step 88120, loss = 0.55 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 00:12:44.191645: step 88130, loss = 0.53 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 00:12:56.172313: step 88140, loss = 0.60 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 00:13:08.150397: step 88150, loss = 0.51 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 00:13:20.125534: step 88160, loss = 0.58 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 00:13:32.105599: step 88170, loss = 0.56 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 00:13:44.092000: step 88180, loss = 0.59 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 00:13:56.075319: step 88190, loss = 0.59 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-22 00:14:08.019615: step 88200, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 00:14:21.862634: step 88210, loss = 0.51 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 00:14:33.858296: step 88220, loss = 0.53 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 00:14:45.849211: step 88230, loss = 0.55 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 00:14:57.840493: step 88240, loss = 0.58 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 00:15:09.833178: step 88250, loss = 0.53 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 00:15:21.823153: step 88260, loss = 0.49 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 00:15:33.835204: step 88270, loss = 0.55 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 00:15:45.782753: step 88280, loss = 0.54 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 00:15:57.788275: step 88290, loss = 0.57 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 00:16:09.783138: step 88300, loss = 0.57 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 00:16:23.636119: step 88310, loss = 0.55 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 00:16:35.648904: step 88320, loss = 0.61 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 00:16:47.628704: step 88330, loss = 0.60 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-22 00:16:59.651410: step 88340, loss = 0.55 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 00:17:11.668267: step 88350, loss = 0.49 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 00:17:23.650820: step 88360, loss = 0.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 00:17:35.631833: step 88370, loss = 0.53 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 00:17:47.640220: step 88380, loss = 0.66 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 00:17:59.690006: step 88390, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 00:18:11.730311: step 88400, loss = 0.59 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 00:18:25.615494: step 88410, loss = 0.62 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 00:18:37.616820: step 88420, loss = 0.57 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 00:18:49.723502: step 88430, loss = 0.55 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 00:19:01.760107: step 88440, loss = 0.55 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 00:19:13.818200: step 88450, loss = 0.66 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 00:19:25.855903: step 88460, loss = 0.52 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 00:19:37.897347: step 88470, loss = 0.54 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-22 00:19:49.970514: step 88480, loss = 0.53 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 00:20:01.977210: step 88490, loss = 0.54 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 00:20:13.996768: step 88500, loss = 0.53 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 00:20:27.844301: step 88510, loss = 0.49 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 00:20:39.887449: step 88520, loss = 0.56 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 00:20:51.858736: step 88530, loss = 0.49 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 00:21:03.893425: step 88540, loss = 0.56 (24.8 examples/sec; 1.209 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 00:21:15.988058: step 88550, loss = 0.54 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 00:21:28.020869: step 88560, loss = 0.57 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 00:21:40.107639: step 88570, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 00:21:52.190771: step 88580, loss = 0.68 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 00:22:04.205441: step 88590, loss = 0.59 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 00:22:16.308724: step 88600, loss = 0.55 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 00:22:30.711411: step 88610, loss = 0.55 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 00:22:42.749700: step 88620, loss = 0.57 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 00:22:54.761839: step 88630, loss = 0.60 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 00:23:06.843348: step 88640, loss = 0.56 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 00:23:18.886000: step 88650, loss = 0.50 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 00:23:30.902210: step 88660, loss = 0.59 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 00:23:42.928223: step 88670, loss = 0.62 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 00:23:54.966099: step 88680, loss = 0.54 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 00:24:07.056601: step 88690, loss = 0.51 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 00:24:19.121576: step 88700, loss = 0.49 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 00:24:33.487400: step 88710, loss = 0.52 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 00:24:45.529925: step 88720, loss = 0.62 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 00:24:57.575087: step 88730, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 00:25:09.586498: step 88740, loss = 0.62 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 00:25:21.580879: step 88750, loss = 0.63 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 00:25:33.555161: step 88760, loss = 0.58 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 00:25:45.529736: step 88770, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 00:25:57.543664: step 88780, loss = 0.58 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 00:26:09.584273: step 88790, loss = 0.50 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 00:26:21.677163: step 88800, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 00:26:35.598906: step 88810, loss = 0.54 (25.5 examples/sec; 1.178 sec/batch)\n",
      "2019-05-22 00:26:47.678443: step 88820, loss = 0.54 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 00:26:59.706406: step 88830, loss = 0.51 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 00:27:11.757309: step 88840, loss = 0.71 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 00:27:23.798601: step 88850, loss = 0.61 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 00:27:35.868481: step 88860, loss = 0.55 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 00:27:47.900032: step 88870, loss = 0.62 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 00:27:59.945788: step 88880, loss = 0.52 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 00:28:11.950688: step 88890, loss = 0.55 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 00:28:23.941325: step 88900, loss = 0.56 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 00:28:37.878661: step 88910, loss = 0.59 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 00:28:49.917960: step 88920, loss = 0.54 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 00:29:01.960757: step 88930, loss = 0.60 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 00:29:14.054248: step 88940, loss = 0.47 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 00:29:26.098545: step 88950, loss = 0.59 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 00:29:38.167322: step 88960, loss = 0.46 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 00:29:50.190886: step 88970, loss = 0.61 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 00:30:02.218220: step 88980, loss = 0.51 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 00:30:14.238259: step 88990, loss = 0.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 00:30:26.289130: step 89000, loss = 0.66 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 00:30:40.427833: step 89010, loss = 0.58 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 00:30:52.379403: step 89020, loss = 0.60 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 00:31:04.392000: step 89030, loss = 0.54 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 00:31:16.411152: step 89040, loss = 0.52 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 00:31:28.483916: step 89050, loss = 0.64 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 00:31:40.566150: step 89060, loss = 0.59 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 00:31:52.663369: step 89070, loss = 0.54 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 00:32:04.783339: step 89080, loss = 0.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 00:32:16.867504: step 89090, loss = 0.62 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 00:32:28.905146: step 89100, loss = 0.55 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 00:32:42.773147: step 89110, loss = 0.49 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 00:32:54.799910: step 89120, loss = 0.59 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 00:33:06.909439: step 89130, loss = 0.55 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 00:33:18.965839: step 89140, loss = 0.56 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 00:33:31.051317: step 89150, loss = 0.55 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 00:33:43.141909: step 89160, loss = 0.56 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 00:33:55.188195: step 89170, loss = 0.62 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 00:34:07.284389: step 89180, loss = 0.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 00:34:19.365961: step 89190, loss = 0.55 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 00:34:31.440215: step 89200, loss = 0.56 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 00:34:45.461592: step 89210, loss = 0.55 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 00:34:57.559724: step 89220, loss = 0.50 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 00:35:09.582242: step 89230, loss = 0.56 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 00:35:21.636365: step 89240, loss = 0.51 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 00:35:33.684668: step 89250, loss = 0.61 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 00:35:45.784303: step 89260, loss = 0.53 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 00:35:57.743441: step 89270, loss = 0.51 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 00:36:09.662844: step 89280, loss = 0.55 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 00:36:21.675000: step 89290, loss = 0.53 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 00:36:33.731316: step 89300, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 00:36:47.707211: step 89310, loss = 0.57 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 00:36:59.766281: step 89320, loss = 0.59 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 00:37:11.836672: step 89330, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 00:37:23.843939: step 89340, loss = 0.55 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 00:37:35.826638: step 89350, loss = 0.51 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 00:37:47.874038: step 89360, loss = 0.56 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 00:37:59.907445: step 89370, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 00:38:11.985888: step 89380, loss = 0.59 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 00:38:24.001369: step 89390, loss = 0.62 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 00:38:36.017318: step 89400, loss = 0.51 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 00:38:50.377533: step 89410, loss = 0.55 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 00:39:02.403753: step 89420, loss = 0.62 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 00:39:14.449604: step 89430, loss = 0.63 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 00:39:26.465669: step 89440, loss = 0.53 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 00:39:38.482541: step 89450, loss = 0.60 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 00:39:50.506958: step 89460, loss = 0.59 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 00:40:02.543903: step 89470, loss = 0.56 (25.0 examples/sec; 1.202 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 00:40:14.617160: step 89480, loss = 0.57 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 00:40:26.670780: step 89490, loss = 0.52 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 00:40:38.711207: step 89500, loss = 0.59 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 00:40:52.665425: step 89510, loss = 0.52 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 00:41:04.638396: step 89520, loss = 0.58 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 00:41:16.567317: step 89530, loss = 0.52 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 00:41:28.614549: step 89540, loss = 0.53 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 00:41:40.657000: step 89550, loss = 0.54 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 00:41:52.707931: step 89560, loss = 0.49 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 00:42:04.738419: step 89570, loss = 0.59 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 00:42:16.764161: step 89580, loss = 0.56 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 00:42:28.854333: step 89590, loss = 0.52 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 00:42:40.919650: step 89600, loss = 0.50 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 00:42:54.796912: step 89610, loss = 0.55 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 00:43:06.855371: step 89620, loss = 0.55 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 00:43:18.867718: step 89630, loss = 0.47 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 00:43:30.874358: step 89640, loss = 0.51 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 00:43:42.882804: step 89650, loss = 0.54 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 00:43:54.919466: step 89660, loss = 0.48 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 00:44:06.961310: step 89670, loss = 0.62 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 00:44:18.951409: step 89680, loss = 0.55 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 00:44:31.005018: step 89690, loss = 0.58 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 00:44:43.010855: step 89700, loss = 0.59 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 00:44:56.940959: step 89710, loss = 0.52 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 00:45:08.971260: step 89720, loss = 0.57 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 00:45:20.988999: step 89730, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 00:45:33.007669: step 89740, loss = 0.61 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 00:45:45.007875: step 89750, loss = 0.53 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 00:45:57.018478: step 89760, loss = 0.53 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 00:46:09.100645: step 89770, loss = 0.62 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 00:46:21.094223: step 89780, loss = 0.55 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 00:46:33.151094: step 89790, loss = 0.59 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 00:46:45.224063: step 89800, loss = 0.52 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 00:46:59.158285: step 89810, loss = 0.57 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 00:47:11.280819: step 89820, loss = 0.49 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 00:47:23.397884: step 89830, loss = 0.64 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 00:47:35.515467: step 89840, loss = 0.55 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 00:47:47.595862: step 89850, loss = 0.56 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 00:47:59.645242: step 89860, loss = 0.54 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 00:48:11.721290: step 89870, loss = 0.55 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 00:48:23.775191: step 89880, loss = 0.56 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 00:48:35.875138: step 89890, loss = 0.51 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 00:48:47.952983: step 89900, loss = 0.57 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 00:49:02.072845: step 89910, loss = 0.62 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 00:49:14.141888: step 89920, loss = 0.53 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 00:49:26.186575: step 89930, loss = 0.61 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 00:49:38.230545: step 89940, loss = 0.63 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 00:49:50.282403: step 89950, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 00:50:02.319197: step 89960, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 00:50:14.372701: step 89970, loss = 0.55 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 00:50:26.407164: step 89980, loss = 0.58 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 00:50:38.496951: step 89990, loss = 0.50 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 00:50:50.618621: step 90000, loss = 0.55 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 00:51:08.094141: step 90010, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 00:51:20.172211: step 90020, loss = 0.57 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 00:51:32.092951: step 90030, loss = 0.55 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 00:51:44.107928: step 90040, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 00:51:56.131346: step 90050, loss = 0.61 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 00:52:08.217161: step 90060, loss = 0.59 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 00:52:20.271857: step 90070, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 00:52:32.316049: step 90080, loss = 0.60 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 00:52:44.377949: step 90090, loss = 0.62 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 00:52:56.443570: step 90100, loss = 0.53 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 00:53:10.397651: step 90110, loss = 0.51 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 00:53:22.453516: step 90120, loss = 0.51 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 00:53:34.489424: step 90130, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 00:53:46.554143: step 90140, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 00:53:58.644484: step 90150, loss = 0.56 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 00:54:10.687301: step 90160, loss = 0.51 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 00:54:22.726449: step 90170, loss = 0.60 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 00:54:34.821709: step 90180, loss = 0.60 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 00:54:46.882167: step 90190, loss = 0.65 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 00:54:58.936254: step 90200, loss = 0.60 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 00:55:13.462148: step 90210, loss = 0.53 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 00:55:25.569742: step 90220, loss = 0.53 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 00:55:37.652943: step 90230, loss = 0.57 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 00:55:49.711075: step 90240, loss = 0.57 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 00:56:01.799428: step 90250, loss = 0.53 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 00:56:13.883438: step 90260, loss = 0.56 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 00:56:25.972844: step 90270, loss = 0.47 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 00:56:37.919250: step 90280, loss = 0.46 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 00:56:49.956434: step 90290, loss = 0.53 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 00:57:02.042029: step 90300, loss = 0.48 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 00:57:16.205955: step 90310, loss = 0.54 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 00:57:28.269382: step 90320, loss = 0.62 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 00:57:40.347195: step 90330, loss = 0.60 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 00:57:52.404821: step 90340, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 00:58:04.477318: step 90350, loss = 0.51 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 00:58:16.555616: step 90360, loss = 0.53 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 00:58:28.617500: step 90370, loss = 0.59 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 00:58:40.727861: step 90380, loss = 0.61 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 00:58:52.788409: step 90390, loss = 0.60 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 00:59:04.846281: step 90400, loss = 0.55 (25.0 examples/sec; 1.202 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 00:59:18.966699: step 90410, loss = 0.45 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 00:59:31.077509: step 90420, loss = 0.59 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 00:59:43.190419: step 90430, loss = 0.57 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 00:59:55.257384: step 90440, loss = 0.46 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 01:00:07.347298: step 90450, loss = 0.54 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 01:00:19.426511: step 90460, loss = 0.57 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 01:00:31.475180: step 90470, loss = 0.60 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 01:00:43.566340: step 90480, loss = 0.56 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 01:00:56.077897: step 90490, loss = 0.60 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 01:01:08.122577: step 90500, loss = 0.61 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 01:01:22.118499: step 90510, loss = 0.63 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 01:01:34.156740: step 90520, loss = 0.60 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 01:01:46.097045: step 90530, loss = 0.52 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 01:01:58.087325: step 90540, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 01:02:10.107829: step 90550, loss = 0.52 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 01:02:22.159554: step 90560, loss = 0.59 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 01:02:34.148005: step 90570, loss = 0.56 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 01:02:46.210901: step 90580, loss = 0.56 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 01:02:58.308103: step 90590, loss = 0.56 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 01:03:10.347283: step 90600, loss = 0.52 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 01:03:24.291498: step 90610, loss = 0.52 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 01:03:36.325968: step 90620, loss = 0.58 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 01:03:48.318619: step 90630, loss = 0.60 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 01:04:00.399135: step 90640, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 01:04:12.447801: step 90650, loss = 0.61 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 01:04:24.519945: step 90660, loss = 0.50 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 01:04:36.536741: step 90670, loss = 0.59 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 01:04:48.641384: step 90680, loss = 0.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 01:05:00.675876: step 90690, loss = 0.58 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 01:05:12.718315: step 90700, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 01:05:27.201095: step 90710, loss = 0.50 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 01:05:39.259956: step 90720, loss = 0.57 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 01:05:51.319090: step 90730, loss = 0.49 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 01:06:03.401490: step 90740, loss = 0.48 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 01:06:15.496201: step 90750, loss = 0.56 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 01:06:27.531878: step 90760, loss = 0.57 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 01:06:39.582940: step 90770, loss = 0.55 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 01:06:51.558332: step 90780, loss = 0.53 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 01:07:03.533331: step 90790, loss = 0.54 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 01:07:15.560926: step 90800, loss = 0.60 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 01:07:29.574248: step 90810, loss = 0.56 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 01:07:41.645127: step 90820, loss = 0.61 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 01:07:53.668852: step 90830, loss = 0.51 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 01:08:05.724510: step 90840, loss = 0.53 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 01:08:17.802563: step 90850, loss = 0.57 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 01:08:29.900224: step 90860, loss = 0.63 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 01:08:41.932775: step 90870, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 01:08:54.022699: step 90880, loss = 0.55 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 01:09:06.075471: step 90890, loss = 0.59 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 01:09:18.140733: step 90900, loss = 0.63 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 01:09:31.994913: step 90910, loss = 0.51 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 01:09:44.055912: step 90920, loss = 0.52 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 01:09:56.077047: step 90930, loss = 0.57 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 01:10:08.089438: step 90940, loss = 0.57 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 01:10:20.148219: step 90950, loss = 0.55 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 01:10:32.266568: step 90960, loss = 0.59 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 01:10:44.323008: step 90970, loss = 0.55 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 01:10:56.407281: step 90980, loss = 0.57 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 01:11:08.467368: step 90990, loss = 0.58 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 01:11:20.527935: step 91000, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 01:11:34.956648: step 91010, loss = 0.53 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 01:11:46.961352: step 91020, loss = 0.56 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-22 01:11:59.028439: step 91030, loss = 0.57 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 01:12:11.014515: step 91040, loss = 0.50 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 01:12:22.988890: step 91050, loss = 0.54 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 01:12:35.050585: step 91060, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 01:12:47.122162: step 91070, loss = 0.63 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 01:12:59.198380: step 91080, loss = 0.58 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 01:13:11.274499: step 91090, loss = 0.59 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 01:13:23.315782: step 91100, loss = 0.56 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 01:13:37.253941: step 91110, loss = 0.52 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 01:13:49.339128: step 91120, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 01:14:01.389429: step 91130, loss = 0.63 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 01:14:13.423235: step 91140, loss = 0.58 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 01:14:25.549998: step 91150, loss = 0.72 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 01:14:37.594954: step 91160, loss = 0.58 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 01:14:49.713204: step 91170, loss = 0.55 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 01:15:01.861675: step 91180, loss = 0.59 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 01:15:14.014322: step 91190, loss = 0.57 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 01:15:26.175420: step 91200, loss = 0.52 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 01:15:40.256187: step 91210, loss = 0.57 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 01:15:52.435436: step 91220, loss = 0.53 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 01:16:04.637035: step 91230, loss = 0.63 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 01:16:16.821793: step 91240, loss = 0.55 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 01:16:29.036891: step 91250, loss = 0.55 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 01:16:41.242150: step 91260, loss = 0.56 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 01:16:53.373558: step 91270, loss = 0.54 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 01:17:05.510520: step 91280, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 01:17:17.655702: step 91290, loss = 0.65 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 01:17:29.710252: step 91300, loss = 0.57 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 01:17:43.810180: step 91310, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 01:17:55.999374: step 91320, loss = 0.55 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 01:18:08.200361: step 91330, loss = 0.55 (24.6 examples/sec; 1.218 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 01:18:20.392966: step 91340, loss = 0.60 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 01:18:32.541550: step 91350, loss = 0.61 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 01:18:44.730366: step 91360, loss = 0.53 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 01:18:56.910621: step 91370, loss = 0.55 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 01:19:09.067561: step 91380, loss = 0.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 01:19:21.210590: step 91390, loss = 0.58 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 01:19:33.374934: step 91400, loss = 0.58 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 01:19:47.915804: step 91410, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 01:20:00.077961: step 91420, loss = 0.54 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 01:20:12.225109: step 91430, loss = 0.60 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 01:20:24.395364: step 91440, loss = 0.56 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 01:20:36.570035: step 91450, loss = 0.54 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 01:20:48.742597: step 91460, loss = 0.54 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 01:21:00.929153: step 91470, loss = 0.53 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 01:21:13.090510: step 91480, loss = 0.55 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 01:21:25.276752: step 91490, loss = 0.55 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 01:21:37.448714: step 91500, loss = 0.50 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 01:21:51.708471: step 91510, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 01:22:03.834389: step 91520, loss = 0.56 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 01:22:16.056923: step 91530, loss = 0.65 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 01:22:28.184508: step 91540, loss = 0.63 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 01:22:40.219945: step 91550, loss = 0.57 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 01:22:52.423922: step 91560, loss = 0.60 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 01:23:04.558151: step 91570, loss = 0.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 01:23:16.710260: step 91580, loss = 0.55 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 01:23:28.927519: step 91590, loss = 0.54 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-22 01:23:41.105154: step 91600, loss = 0.49 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 01:23:55.262054: step 91610, loss = 0.47 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 01:24:07.415613: step 91620, loss = 0.57 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 01:24:19.554562: step 91630, loss = 0.59 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 01:24:31.595623: step 91640, loss = 0.58 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 01:24:43.694743: step 91650, loss = 0.60 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 01:24:55.828313: step 91660, loss = 0.56 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 01:25:07.914232: step 91670, loss = 0.53 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 01:25:20.001800: step 91680, loss = 0.61 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 01:25:32.084987: step 91690, loss = 0.67 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 01:25:44.149461: step 91700, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 01:25:58.289452: step 91710, loss = 0.51 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 01:26:10.407999: step 91720, loss = 0.49 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 01:26:22.439972: step 91730, loss = 0.50 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 01:26:34.456204: step 91740, loss = 0.45 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 01:26:46.459601: step 91750, loss = 0.62 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 01:26:58.507989: step 91760, loss = 0.59 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 01:27:10.535910: step 91770, loss = 0.54 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 01:27:22.631096: step 91780, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 01:27:34.732067: step 91790, loss = 0.60 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 01:27:46.716062: step 91800, loss = 0.62 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 01:28:00.690464: step 91810, loss = 0.55 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 01:28:12.728351: step 91820, loss = 0.48 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 01:28:24.808365: step 91830, loss = 0.62 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 01:28:36.817505: step 91840, loss = 0.60 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 01:28:48.804383: step 91850, loss = 0.54 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 01:29:00.793158: step 91860, loss = 0.51 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 01:29:12.870643: step 91870, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 01:29:24.941085: step 91880, loss = 0.63 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 01:29:36.978545: step 91890, loss = 0.53 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 01:29:48.999169: step 91900, loss = 0.54 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 01:30:03.413575: step 91910, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 01:30:15.459335: step 91920, loss = 0.60 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 01:30:27.590612: step 91930, loss = 0.52 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 01:30:39.695168: step 91940, loss = 0.60 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 01:30:51.719048: step 91950, loss = 0.59 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 01:31:03.736448: step 91960, loss = 0.52 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 01:31:15.749955: step 91970, loss = 0.54 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 01:31:27.728039: step 91980, loss = 0.52 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 01:31:39.707398: step 91990, loss = 0.52 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 01:31:51.744187: step 92000, loss = 0.46 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 01:32:05.780624: step 92010, loss = 0.59 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 01:32:17.759221: step 92020, loss = 0.64 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 01:32:29.730780: step 92030, loss = 0.60 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 01:32:41.671244: step 92040, loss = 0.57 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 01:32:53.620649: step 92050, loss = 0.56 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 01:33:05.594189: step 92060, loss = 0.63 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 01:33:17.605976: step 92070, loss = 0.56 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 01:33:29.540852: step 92080, loss = 0.54 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 01:33:41.492085: step 92090, loss = 0.54 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 01:33:53.472872: step 92100, loss = 0.51 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 01:34:07.253975: step 92110, loss = 0.57 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 01:34:19.189886: step 92120, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 01:34:31.129966: step 92130, loss = 0.52 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 01:34:43.085053: step 92140, loss = 0.48 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 01:34:55.014859: step 92150, loss = 0.62 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 01:35:07.011838: step 92160, loss = 0.56 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 01:35:19.026528: step 92170, loss = 0.55 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 01:35:31.144709: step 92180, loss = 0.56 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 01:35:43.310762: step 92190, loss = 0.55 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 01:35:55.427204: step 92200, loss = 0.54 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 01:36:09.521226: step 92210, loss = 0.51 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 01:36:21.657632: step 92220, loss = 0.63 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 01:36:33.754599: step 92230, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 01:36:45.795821: step 92240, loss = 0.66 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 01:36:57.823672: step 92250, loss = 0.53 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 01:37:09.909662: step 92260, loss = 0.59 (24.9 examples/sec; 1.203 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 01:37:21.861707: step 92270, loss = 0.61 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 01:37:33.870377: step 92280, loss = 0.50 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 01:37:45.888681: step 92290, loss = 0.59 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 01:37:57.930921: step 92300, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 01:38:11.790268: step 92310, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 01:38:23.775216: step 92320, loss = 0.47 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 01:38:35.767088: step 92330, loss = 0.51 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-22 01:38:47.736721: step 92340, loss = 0.58 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 01:38:59.773610: step 92350, loss = 0.61 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 01:39:11.815241: step 92360, loss = 0.59 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 01:39:23.837159: step 92370, loss = 0.52 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 01:39:35.891251: step 92380, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 01:39:47.932031: step 92390, loss = 0.62 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 01:40:00.030620: step 92400, loss = 0.62 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 01:40:14.205451: step 92410, loss = 0.49 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 01:40:26.280085: step 92420, loss = 0.53 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 01:40:38.331203: step 92430, loss = 0.58 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 01:40:50.457767: step 92440, loss = 0.57 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 01:41:02.544657: step 92450, loss = 0.50 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 01:41:14.653779: step 92460, loss = 0.54 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 01:41:26.848422: step 92470, loss = 0.61 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 01:41:39.056270: step 92480, loss = 0.60 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 01:41:51.252991: step 92490, loss = 0.67 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 01:42:03.434771: step 92500, loss = 0.61 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 01:42:17.563726: step 92510, loss = 0.56 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 01:42:29.696028: step 92520, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 01:42:41.879266: step 92530, loss = 0.64 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 01:42:54.043331: step 92540, loss = 0.56 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 01:43:06.280720: step 92550, loss = 0.53 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 01:43:18.400632: step 92560, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 01:43:30.555820: step 92570, loss = 0.56 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 01:43:42.796719: step 92580, loss = 0.57 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 01:43:54.974178: step 92590, loss = 0.54 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 01:44:07.137535: step 92600, loss = 0.60 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 01:44:21.165303: step 92610, loss = 0.62 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 01:44:33.290422: step 92620, loss = 0.57 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 01:44:45.455435: step 92630, loss = 0.52 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 01:44:57.656873: step 92640, loss = 0.54 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 01:45:09.882722: step 92650, loss = 0.60 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 01:45:22.066446: step 92660, loss = 0.55 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 01:45:34.291012: step 92670, loss = 0.49 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 01:45:46.448825: step 92680, loss = 0.59 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 01:45:58.625690: step 92690, loss = 0.60 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 01:46:10.796544: step 92700, loss = 0.60 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 01:46:25.012450: step 92710, loss = 0.60 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 01:46:37.200407: step 92720, loss = 0.58 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 01:46:49.394665: step 92730, loss = 0.54 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 01:47:01.640611: step 92740, loss = 0.57 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 01:47:13.807856: step 92750, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 01:47:25.973200: step 92760, loss = 0.56 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 01:47:38.079977: step 92770, loss = 0.63 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 01:47:50.250282: step 92780, loss = 0.60 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 01:48:02.409790: step 92790, loss = 0.60 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 01:48:14.564887: step 92800, loss = 0.54 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 01:48:28.590868: step 92810, loss = 0.54 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 01:48:40.722202: step 92820, loss = 0.56 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 01:48:52.930054: step 92830, loss = 0.51 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 01:49:05.136328: step 92840, loss = 0.61 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 01:49:17.302059: step 92850, loss = 0.51 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 01:49:29.513504: step 92860, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 01:49:41.663756: step 92870, loss = 0.51 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 01:49:53.820163: step 92880, loss = 0.54 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 01:50:05.962272: step 92890, loss = 0.48 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 01:50:18.057021: step 92900, loss = 0.52 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 01:50:32.044170: step 92910, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 01:50:44.175016: step 92920, loss = 0.57 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 01:50:56.281540: step 92930, loss = 0.59 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 01:51:08.405872: step 92940, loss = 0.56 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 01:51:20.487494: step 92950, loss = 0.48 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 01:51:32.504362: step 92960, loss = 0.54 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 01:51:44.581013: step 92970, loss = 0.55 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 01:51:56.656083: step 92980, loss = 0.53 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 01:52:08.769014: step 92990, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 01:52:20.887954: step 93000, loss = 0.49 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 01:52:35.359098: step 93010, loss = 0.52 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 01:52:47.615820: step 93020, loss = 0.51 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 01:52:59.787559: step 93030, loss = 0.61 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 01:53:11.855215: step 93040, loss = 0.55 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 01:53:24.015605: step 93050, loss = 0.51 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 01:53:36.133345: step 93060, loss = 0.61 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 01:53:48.243218: step 93070, loss = 0.51 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 01:54:00.374258: step 93080, loss = 0.68 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 01:54:12.560670: step 93090, loss = 0.57 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 01:54:24.731828: step 93100, loss = 0.50 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 01:54:38.876709: step 93110, loss = 0.51 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 01:54:51.048159: step 93120, loss = 0.54 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 01:55:03.205502: step 93130, loss = 0.46 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 01:55:15.379055: step 93140, loss = 0.63 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 01:55:27.552044: step 93150, loss = 0.62 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 01:55:39.731502: step 93160, loss = 0.51 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 01:55:51.865915: step 93170, loss = 0.52 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 01:56:04.100806: step 93180, loss = 0.60 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 01:56:16.262058: step 93190, loss = 0.60 (24.5 examples/sec; 1.223 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 01:56:28.370675: step 93200, loss = 0.50 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 01:56:42.324277: step 93210, loss = 0.52 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 01:56:54.454344: step 93220, loss = 0.55 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 01:57:06.769825: step 93230, loss = 0.57 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 01:57:19.030543: step 93240, loss = 0.52 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 01:57:31.340432: step 93250, loss = 0.63 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 01:57:43.393011: step 93260, loss = 0.51 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 01:57:55.633764: step 93270, loss = 0.46 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 01:58:07.699457: step 93280, loss = 0.60 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 01:58:19.822241: step 93290, loss = 0.63 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 01:58:32.054000: step 93300, loss = 0.51 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 01:58:46.324698: step 93310, loss = 0.52 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 01:58:58.354165: step 93320, loss = 0.59 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 01:59:10.554762: step 93330, loss = 0.50 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 01:59:22.905070: step 93340, loss = 0.59 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-05-22 01:59:35.180386: step 93350, loss = 0.58 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 01:59:47.331655: step 93360, loss = 0.54 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 01:59:59.559015: step 93370, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 02:00:11.679751: step 93380, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 02:00:23.885124: step 93390, loss = 0.56 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 02:00:36.102518: step 93400, loss = 0.54 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 02:00:50.079919: step 93410, loss = 0.55 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 02:01:02.175002: step 93420, loss = 0.53 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 02:01:14.290246: step 93430, loss = 0.55 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 02:01:26.616328: step 93440, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 02:01:38.924367: step 93450, loss = 0.57 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 02:01:51.247217: step 93460, loss = 0.47 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 02:02:03.563485: step 93470, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 02:02:15.665462: step 93480, loss = 0.55 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 02:02:27.890909: step 93490, loss = 0.63 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 02:02:39.985983: step 93500, loss = 0.58 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 02:02:53.997198: step 93510, loss = 0.54 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 02:03:06.140020: step 93520, loss = 0.53 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 02:03:18.368521: step 93530, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 02:03:30.671402: step 93540, loss = 0.54 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 02:03:42.839719: step 93550, loss = 0.61 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 02:03:54.901973: step 93560, loss = 0.52 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 02:04:06.937894: step 93570, loss = 0.51 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 02:04:19.107301: step 93580, loss = 0.65 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 02:04:31.229211: step 93590, loss = 0.60 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 02:04:43.523276: step 93600, loss = 0.55 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 02:04:58.058457: step 93610, loss = 0.51 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 02:05:10.108351: step 93620, loss = 0.51 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 02:05:22.403299: step 93630, loss = 0.54 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 02:05:34.560614: step 93640, loss = 0.61 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 02:05:46.899260: step 93650, loss = 0.54 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-05-22 02:05:59.039990: step 93660, loss = 0.54 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 02:06:11.150413: step 93670, loss = 0.58 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 02:06:23.369292: step 93680, loss = 0.55 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 02:06:35.496217: step 93690, loss = 0.55 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 02:06:47.619998: step 93700, loss = 0.65 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 02:07:01.595359: step 93710, loss = 0.55 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 02:07:13.662745: step 93720, loss = 0.49 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 02:07:25.893366: step 93730, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 02:07:37.968147: step 93740, loss = 0.47 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 02:07:50.201357: step 93750, loss = 0.56 (25.6 examples/sec; 1.173 sec/batch)\n",
      "2019-05-22 02:08:02.581243: step 93760, loss = 0.49 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 02:08:14.647707: step 93770, loss = 0.58 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 02:08:26.784434: step 93780, loss = 0.46 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 02:08:38.892815: step 93790, loss = 0.56 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 02:08:51.002506: step 93800, loss = 0.51 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 02:09:05.245381: step 93810, loss = 0.53 (23.5 examples/sec; 1.278 sec/batch)\n",
      "2019-05-22 02:09:17.196514: step 93820, loss = 0.60 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 02:09:29.467867: step 93830, loss = 0.52 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 02:09:41.661776: step 93840, loss = 0.59 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 02:09:53.890650: step 93850, loss = 0.56 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 02:10:06.135158: step 93860, loss = 0.66 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 02:10:18.441552: step 93870, loss = 0.59 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 02:10:30.570559: step 93880, loss = 0.55 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 02:10:42.848691: step 93890, loss = 0.54 (23.1 examples/sec; 1.298 sec/batch)\n",
      "2019-05-22 02:10:54.980830: step 93900, loss = 0.54 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 02:11:09.386722: step 93910, loss = 0.62 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 02:11:21.514191: step 93920, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 02:11:33.671361: step 93930, loss = 0.62 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 02:11:46.133296: step 93940, loss = 0.61 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 02:11:58.300005: step 93950, loss = 0.55 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 02:12:10.470777: step 93960, loss = 0.59 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-22 02:12:22.659586: step 93970, loss = 0.56 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 02:12:34.952920: step 93980, loss = 0.55 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 02:12:47.151860: step 93990, loss = 0.54 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 02:12:59.351783: step 94000, loss = 0.58 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 02:13:14.004849: step 94010, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 02:13:26.140392: step 94020, loss = 0.55 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 02:13:38.412306: step 94030, loss = 0.57 (23.2 examples/sec; 1.295 sec/batch)\n",
      "2019-05-22 02:13:50.647766: step 94040, loss = 0.63 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 02:14:03.041176: step 94050, loss = 0.53 (23.0 examples/sec; 1.306 sec/batch)\n",
      "2019-05-22 02:14:15.425835: step 94060, loss = 0.51 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 02:14:27.598885: step 94070, loss = 0.56 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 02:14:39.802797: step 94080, loss = 0.58 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 02:14:51.999988: step 94090, loss = 0.48 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 02:15:04.195784: step 94100, loss = 0.64 (23.4 examples/sec; 1.284 sec/batch)\n",
      "2019-05-22 02:15:18.682318: step 94110, loss = 0.54 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 02:15:30.831085: step 94120, loss = 0.53 (24.5 examples/sec; 1.224 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 02:15:42.974404: step 94130, loss = 0.51 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 02:15:55.291519: step 94140, loss = 0.63 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 02:16:07.484130: step 94150, loss = 0.61 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 02:16:19.661303: step 94160, loss = 0.55 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 02:16:31.895008: step 94170, loss = 0.51 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-05-22 02:16:44.033003: step 94180, loss = 0.53 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 02:16:56.224167: step 94190, loss = 0.55 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 02:17:08.338658: step 94200, loss = 0.56 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 02:17:22.637913: step 94210, loss = 0.57 (23.3 examples/sec; 1.288 sec/batch)\n",
      "2019-05-22 02:17:34.841340: step 94220, loss = 0.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 02:17:47.269834: step 94230, loss = 0.56 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 02:17:59.392813: step 94240, loss = 0.52 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 02:18:11.563573: step 94250, loss = 0.63 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 02:18:23.873022: step 94260, loss = 0.56 (23.1 examples/sec; 1.297 sec/batch)\n",
      "2019-05-22 02:18:36.063728: step 94270, loss = 0.55 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 02:18:48.218341: step 94280, loss = 0.66 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 02:19:00.458031: step 94290, loss = 0.54 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 02:19:12.662443: step 94300, loss = 0.55 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-05-22 02:19:27.108128: step 94310, loss = 0.57 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 02:19:39.332109: step 94320, loss = 0.53 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 02:19:51.538754: step 94330, loss = 0.53 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-05-22 02:20:03.609837: step 94340, loss = 0.61 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 02:20:15.727734: step 94350, loss = 0.59 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 02:20:27.838459: step 94360, loss = 0.51 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 02:20:40.104945: step 94370, loss = 0.52 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-22 02:20:52.380456: step 94380, loss = 0.50 (22.7 examples/sec; 1.322 sec/batch)\n",
      "2019-05-22 02:21:04.508295: step 94390, loss = 0.52 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 02:21:16.729466: step 94400, loss = 0.48 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-05-22 02:21:31.107489: step 94410, loss = 0.56 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 02:21:43.348013: step 94420, loss = 0.47 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-05-22 02:21:55.640486: step 94430, loss = 0.43 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 02:22:07.752812: step 94440, loss = 0.61 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 02:22:19.974776: step 94450, loss = 0.54 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 02:22:32.338373: step 94460, loss = 0.57 (23.2 examples/sec; 1.294 sec/batch)\n",
      "2019-05-22 02:22:44.515237: step 94470, loss = 0.50 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 02:22:56.706032: step 94480, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 02:23:08.730562: step 94490, loss = 0.52 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 02:23:21.028328: step 94500, loss = 0.52 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 02:23:35.535548: step 94510, loss = 0.55 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 02:23:47.694753: step 94520, loss = 0.53 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-22 02:23:59.943326: step 94530, loss = 0.59 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 02:24:12.076822: step 94540, loss = 0.55 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 02:24:24.349785: step 94550, loss = 0.50 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 02:24:36.540860: step 94560, loss = 0.51 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-05-22 02:24:48.596787: step 94570, loss = 0.54 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 02:25:00.798557: step 94580, loss = 0.54 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 02:25:13.062875: step 94590, loss = 0.63 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 02:25:25.332035: step 94600, loss = 0.54 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 02:25:39.921197: step 94610, loss = 0.59 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 02:25:51.994933: step 94620, loss = 0.57 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 02:26:04.163705: step 94630, loss = 0.53 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-05-22 02:26:16.352534: step 94640, loss = 0.54 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 02:26:28.447117: step 94650, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 02:26:40.640975: step 94660, loss = 0.52 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 02:26:52.757307: step 94670, loss = 0.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 02:27:04.846085: step 94680, loss = 0.60 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 02:27:17.058601: step 94690, loss = 0.53 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 02:27:29.153202: step 94700, loss = 0.51 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 02:27:43.460168: step 94710, loss = 0.54 (23.3 examples/sec; 1.288 sec/batch)\n",
      "2019-05-22 02:27:55.625438: step 94720, loss = 0.54 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 02:28:07.672901: step 94730, loss = 0.48 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 02:28:19.884969: step 94740, loss = 0.55 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 02:28:31.967174: step 94750, loss = 0.53 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 02:28:44.202957: step 94760, loss = 0.55 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-05-22 02:28:56.323022: step 94770, loss = 0.58 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 02:29:08.323079: step 94780, loss = 0.45 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 02:29:20.528890: step 94790, loss = 0.55 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 02:29:32.589333: step 94800, loss = 0.54 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 02:29:47.047771: step 94810, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 02:29:59.022178: step 94820, loss = 0.53 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 02:30:11.203642: step 94830, loss = 0.55 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 02:30:23.406731: step 94840, loss = 0.60 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 02:30:35.524821: step 94850, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 02:30:47.742754: step 94860, loss = 0.60 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 02:30:59.986177: step 94870, loss = 0.52 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 02:31:12.217735: step 94880, loss = 0.57 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 02:31:24.492771: step 94890, loss = 0.64 (22.5 examples/sec; 1.332 sec/batch)\n",
      "2019-05-22 02:31:36.746369: step 94900, loss = 0.54 (23.2 examples/sec; 1.295 sec/batch)\n",
      "2019-05-22 02:31:51.341213: step 94910, loss = 0.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 02:32:03.419017: step 94920, loss = 0.62 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 02:32:15.576826: step 94930, loss = 0.54 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 02:32:27.706854: step 94940, loss = 0.49 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 02:32:39.953350: step 94950, loss = 0.50 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 02:32:52.190335: step 94960, loss = 0.48 (25.5 examples/sec; 1.178 sec/batch)\n",
      "2019-05-22 02:33:04.356445: step 94970, loss = 0.57 (23.1 examples/sec; 1.298 sec/batch)\n",
      "2019-05-22 02:33:16.554676: step 94980, loss = 0.59 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 02:33:28.652203: step 94990, loss = 0.59 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 02:33:40.712024: step 95000, loss = 0.47 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 02:33:58.215739: step 95010, loss = 0.55 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 02:34:10.384199: step 95020, loss = 0.57 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 02:34:22.464476: step 95030, loss = 0.59 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 02:34:34.622167: step 95040, loss = 0.58 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 02:34:46.890064: step 95050, loss = 0.56 (25.0 examples/sec; 1.200 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 02:34:59.120069: step 95060, loss = 0.63 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 02:35:11.305830: step 95070, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 02:35:23.429823: step 95080, loss = 0.55 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 02:35:35.586550: step 95090, loss = 0.56 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 02:35:47.805068: step 95100, loss = 0.58 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 02:36:02.068754: step 95110, loss = 0.50 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 02:36:14.299198: step 95120, loss = 0.49 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 02:36:26.606702: step 95130, loss = 0.54 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 02:36:38.684756: step 95140, loss = 0.55 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 02:36:50.736724: step 95150, loss = 0.55 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 02:37:03.041372: step 95160, loss = 0.67 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 02:37:15.240296: step 95170, loss = 0.66 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 02:37:27.436198: step 95180, loss = 0.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 02:37:39.751998: step 95190, loss = 0.61 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 02:37:51.806220: step 95200, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 02:38:05.751949: step 95210, loss = 0.52 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 02:38:17.795859: step 95220, loss = 0.54 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 02:38:29.890447: step 95230, loss = 0.64 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 02:38:42.005081: step 95240, loss = 0.56 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 02:38:54.219545: step 95250, loss = 0.50 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 02:39:06.432703: step 95260, loss = 0.54 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 02:39:18.607826: step 95270, loss = 0.61 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 02:39:30.814699: step 95280, loss = 0.52 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 02:39:42.910053: step 95290, loss = 0.55 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 02:39:55.122372: step 95300, loss = 0.55 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 02:40:08.966129: step 95310, loss = 0.53 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 02:40:21.069814: step 95320, loss = 0.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 02:40:33.172290: step 95330, loss = 0.56 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 02:40:45.390666: step 95340, loss = 0.67 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 02:40:57.478020: step 95350, loss = 0.59 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 02:41:09.544161: step 95360, loss = 0.59 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 02:41:21.765804: step 95370, loss = 0.48 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 02:41:33.844942: step 95380, loss = 0.49 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 02:41:46.008355: step 95390, loss = 0.58 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 02:41:58.113927: step 95400, loss = 0.57 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 02:42:12.130641: step 95410, loss = 0.67 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 02:42:24.229797: step 95420, loss = 0.56 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 02:42:36.321740: step 95430, loss = 0.46 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 02:42:48.590965: step 95440, loss = 0.50 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 02:43:00.715123: step 95450, loss = 0.57 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 02:43:12.819196: step 95460, loss = 0.47 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 02:43:25.380893: step 95470, loss = 0.54 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 02:43:37.458296: step 95480, loss = 0.51 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 02:43:49.654387: step 95490, loss = 0.59 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 02:44:01.865103: step 95500, loss = 0.56 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 02:44:16.130765: step 95510, loss = 0.50 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 02:44:28.266984: step 95520, loss = 0.55 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 02:44:40.301903: step 95530, loss = 0.57 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 02:44:52.546297: step 95540, loss = 0.49 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 02:45:04.850390: step 95550, loss = 0.50 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 02:45:17.063363: step 95560, loss = 0.47 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 02:45:29.020258: step 95570, loss = 0.55 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 02:45:41.250669: step 95580, loss = 0.55 (23.2 examples/sec; 1.295 sec/batch)\n",
      "2019-05-22 02:45:53.460567: step 95590, loss = 0.50 (23.3 examples/sec; 1.289 sec/batch)\n",
      "2019-05-22 02:46:05.798661: step 95600, loss = 0.56 (21.7 examples/sec; 1.385 sec/batch)\n",
      "2019-05-22 02:46:19.617212: step 95610, loss = 0.59 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 02:46:31.685708: step 95620, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 02:46:43.983457: step 95630, loss = 0.53 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-22 02:46:56.235946: step 95640, loss = 0.52 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-05-22 02:47:08.268774: step 95650, loss = 0.59 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 02:47:20.422172: step 95660, loss = 0.61 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 02:47:32.574839: step 95670, loss = 0.50 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 02:47:44.727840: step 95680, loss = 0.49 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 02:47:56.806208: step 95690, loss = 0.56 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 02:48:08.910782: step 95700, loss = 0.49 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 02:48:23.003605: step 95710, loss = 0.56 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 02:48:35.104763: step 95720, loss = 0.52 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 02:48:47.181326: step 95730, loss = 0.55 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 02:48:59.532305: step 95740, loss = 0.52 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-05-22 02:49:11.741019: step 95750, loss = 0.50 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 02:49:23.804628: step 95760, loss = 0.58 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 02:49:35.967270: step 95770, loss = 0.49 (23.2 examples/sec; 1.291 sec/batch)\n",
      "2019-05-22 02:49:48.245063: step 95780, loss = 0.56 (23.1 examples/sec; 1.299 sec/batch)\n",
      "2019-05-22 02:50:00.272315: step 95790, loss = 0.57 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 02:50:12.434643: step 95800, loss = 0.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 02:50:26.487716: step 95810, loss = 0.59 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 02:50:38.401374: step 95820, loss = 0.49 (25.5 examples/sec; 1.178 sec/batch)\n",
      "2019-05-22 02:50:50.674411: step 95830, loss = 0.55 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 02:51:02.845327: step 95840, loss = 0.47 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 02:51:15.050092: step 95850, loss = 0.50 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 02:51:27.258505: step 95860, loss = 0.49 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 02:51:39.419543: step 95870, loss = 0.67 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 02:51:51.562448: step 95880, loss = 0.61 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 02:52:03.618124: step 95890, loss = 0.53 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 02:52:15.973932: step 95900, loss = 0.50 (23.5 examples/sec; 1.279 sec/batch)\n",
      "2019-05-22 02:52:29.987757: step 95910, loss = 0.51 (23.1 examples/sec; 1.296 sec/batch)\n",
      "2019-05-22 02:52:42.124861: step 95920, loss = 0.51 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 02:52:54.310745: step 95930, loss = 0.55 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 02:53:06.490023: step 95940, loss = 0.59 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 02:53:18.764411: step 95950, loss = 0.51 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 02:53:31.005174: step 95960, loss = 0.49 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 02:53:43.146015: step 95970, loss = 0.55 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 02:53:55.422021: step 95980, loss = 0.52 (24.5 examples/sec; 1.224 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 02:54:07.551991: step 95990, loss = 0.55 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 02:54:19.676337: step 96000, loss = 0.50 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 02:54:33.758993: step 96010, loss = 0.47 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 02:54:45.929045: step 96020, loss = 0.55 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 02:54:58.126907: step 96030, loss = 0.60 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 02:55:10.287605: step 96040, loss = 0.51 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 02:55:22.463995: step 96050, loss = 0.48 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 02:55:34.689372: step 96060, loss = 0.56 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 02:55:46.712435: step 96070, loss = 0.42 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 02:55:58.979509: step 96080, loss = 0.57 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 02:56:11.145539: step 96090, loss = 0.53 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 02:56:23.322105: step 96100, loss = 0.58 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 02:56:37.672982: step 96110, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 02:56:49.857954: step 96120, loss = 0.59 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 02:57:02.004231: step 96130, loss = 0.57 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 02:57:14.247743: step 96140, loss = 0.41 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 02:57:26.450310: step 96150, loss = 0.51 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 02:57:38.574126: step 96160, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 02:57:50.849899: step 96170, loss = 0.55 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 02:58:03.153578: step 96180, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 02:58:15.513542: step 96190, loss = 0.48 (23.2 examples/sec; 1.292 sec/batch)\n",
      "2019-05-22 02:58:27.855110: step 96200, loss = 0.55 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 02:58:42.214878: step 96210, loss = 0.47 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 02:58:54.296048: step 96220, loss = 0.61 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 02:59:06.523574: step 96230, loss = 0.57 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 02:59:18.786999: step 96240, loss = 0.50 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 02:59:31.002729: step 96250, loss = 0.51 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 02:59:43.224426: step 96260, loss = 0.54 (23.0 examples/sec; 1.305 sec/batch)\n",
      "2019-05-22 02:59:55.449093: step 96270, loss = 0.54 (23.5 examples/sec; 1.279 sec/batch)\n",
      "2019-05-22 03:00:07.754758: step 96280, loss = 0.61 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 03:00:19.969656: step 96290, loss = 0.61 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 03:00:32.064253: step 96300, loss = 0.53 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 03:00:46.437686: step 96310, loss = 0.54 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-05-22 03:00:58.544947: step 96320, loss = 0.49 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 03:01:10.706735: step 96330, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 03:01:22.844112: step 96340, loss = 0.51 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 03:01:35.114966: step 96350, loss = 0.59 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 03:01:47.382813: step 96360, loss = 0.59 (23.2 examples/sec; 1.293 sec/batch)\n",
      "2019-05-22 03:01:59.551681: step 96370, loss = 0.61 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 03:02:11.672607: step 96380, loss = 0.60 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 03:02:24.092736: step 96390, loss = 0.48 (22.7 examples/sec; 1.324 sec/batch)\n",
      "2019-05-22 03:02:36.320891: step 96400, loss = 0.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 03:02:50.774594: step 96410, loss = 0.53 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 03:03:03.154804: step 96420, loss = 0.54 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-05-22 03:03:15.430815: step 96430, loss = 0.51 (23.2 examples/sec; 1.295 sec/batch)\n",
      "2019-05-22 03:03:27.633175: step 96440, loss = 0.53 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 03:03:40.186236: step 96450, loss = 0.62 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 03:03:52.396302: step 96460, loss = 0.55 (23.6 examples/sec; 1.269 sec/batch)\n",
      "2019-05-22 03:04:04.471831: step 96470, loss = 0.56 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 03:04:16.712637: step 96480, loss = 0.55 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 03:04:28.831700: step 96490, loss = 0.48 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 03:04:40.978699: step 96500, loss = 0.61 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 03:04:55.478905: step 96510, loss = 0.56 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 03:05:07.573741: step 96520, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 03:05:19.714232: step 96530, loss = 0.53 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 03:05:31.845959: step 96540, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 03:05:44.118376: step 96550, loss = 0.55 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 03:05:56.315231: step 96560, loss = 0.52 (22.9 examples/sec; 1.307 sec/batch)\n",
      "2019-05-22 03:06:08.416101: step 96570, loss = 0.59 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-22 03:06:20.517082: step 96580, loss = 0.56 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 03:06:33.024873: step 96590, loss = 0.55 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 03:06:45.411517: step 96600, loss = 0.49 (23.1 examples/sec; 1.296 sec/batch)\n",
      "2019-05-22 03:06:59.815384: step 96610, loss = 0.44 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 03:07:11.934997: step 96620, loss = 0.48 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 03:07:24.152055: step 96630, loss = 0.56 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 03:07:36.394342: step 96640, loss = 0.49 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 03:07:48.500635: step 96650, loss = 0.57 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 03:08:00.668846: step 96660, loss = 0.50 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 03:08:12.889248: step 96670, loss = 0.65 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 03:08:25.116560: step 96680, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 03:08:37.360865: step 96690, loss = 0.56 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 03:08:49.453005: step 96700, loss = 0.53 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 03:09:03.846288: step 96710, loss = 0.59 (23.2 examples/sec; 1.293 sec/batch)\n",
      "2019-05-22 03:09:15.989566: step 96720, loss = 0.58 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 03:09:28.215553: step 96730, loss = 0.51 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 03:09:40.439478: step 96740, loss = 0.62 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 03:09:52.567942: step 96750, loss = 0.59 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 03:10:04.614851: step 96760, loss = 0.57 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 03:10:16.724027: step 96770, loss = 0.53 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 03:10:28.826247: step 96780, loss = 0.62 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 03:10:40.993215: step 96790, loss = 0.54 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 03:10:53.097762: step 96800, loss = 0.50 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-22 03:11:07.205820: step 96810, loss = 0.62 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 03:11:19.284073: step 96820, loss = 0.47 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 03:11:31.346116: step 96830, loss = 0.53 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 03:11:43.711385: step 96840, loss = 0.54 (23.1 examples/sec; 1.300 sec/batch)\n",
      "2019-05-22 03:11:55.943292: step 96850, loss = 0.59 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 03:12:08.080405: step 96860, loss = 0.53 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 03:12:20.256256: step 96870, loss = 0.55 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 03:12:32.453225: step 96880, loss = 0.54 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 03:12:44.566249: step 96890, loss = 0.46 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 03:12:56.730869: step 96900, loss = 0.52 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 03:13:10.682302: step 96910, loss = 0.61 (25.0 examples/sec; 1.200 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 03:13:22.735872: step 96920, loss = 0.55 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 03:13:34.836752: step 96930, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 03:13:46.995153: step 96940, loss = 0.51 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 03:13:59.070530: step 96950, loss = 0.49 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 03:14:11.164599: step 96960, loss = 0.49 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 03:14:23.576041: step 96970, loss = 0.51 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-05-22 03:14:35.641597: step 96980, loss = 0.59 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 03:14:47.745762: step 96990, loss = 0.48 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 03:15:00.017771: step 97000, loss = 0.57 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 03:15:14.166325: step 97010, loss = 0.56 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 03:15:26.267755: step 97020, loss = 0.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 03:15:38.491317: step 97030, loss = 0.57 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 03:15:50.866128: step 97040, loss = 0.52 (23.3 examples/sec; 1.289 sec/batch)\n",
      "2019-05-22 03:16:03.076546: step 97050, loss = 0.48 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 03:16:15.136198: step 97060, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 03:16:27.370574: step 97070, loss = 0.49 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 03:16:39.394538: step 97080, loss = 0.55 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 03:16:51.531095: step 97090, loss = 0.52 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 03:17:03.758621: step 97100, loss = 0.56 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 03:17:17.889517: step 97110, loss = 0.46 (22.9 examples/sec; 1.307 sec/batch)\n",
      "2019-05-22 03:17:30.042766: step 97120, loss = 0.48 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 03:17:42.134489: step 97130, loss = 0.58 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 03:17:54.458951: step 97140, loss = 0.56 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 03:18:06.815253: step 97150, loss = 0.62 (23.2 examples/sec; 1.291 sec/batch)\n",
      "2019-05-22 03:18:19.120160: step 97160, loss = 0.53 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 03:18:31.348589: step 97170, loss = 0.53 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 03:18:43.398645: step 97180, loss = 0.51 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 03:18:55.569170: step 97190, loss = 0.59 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 03:19:07.718762: step 97200, loss = 0.51 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 03:19:22.170170: step 97210, loss = 0.56 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 03:19:34.238304: step 97220, loss = 0.54 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 03:19:46.556592: step 97230, loss = 0.57 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 03:19:58.767839: step 97240, loss = 0.60 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 03:20:10.969316: step 97250, loss = 0.56 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 03:20:23.303749: step 97260, loss = 0.64 (23.1 examples/sec; 1.300 sec/batch)\n",
      "2019-05-22 03:20:35.507998: step 97270, loss = 0.55 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 03:20:47.816967: step 97280, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 03:21:00.044194: step 97290, loss = 0.47 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 03:21:12.320384: step 97300, loss = 0.53 (23.1 examples/sec; 1.297 sec/batch)\n",
      "2019-05-22 03:21:26.678589: step 97310, loss = 0.59 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 03:21:38.706743: step 97320, loss = 0.52 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 03:21:50.954529: step 97330, loss = 0.50 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-05-22 03:22:03.362755: step 97340, loss = 0.61 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 03:22:15.411075: step 97350, loss = 0.53 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 03:22:27.573940: step 97360, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 03:22:39.810483: step 97370, loss = 0.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 03:22:52.125456: step 97380, loss = 0.47 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-05-22 03:23:04.255367: step 97390, loss = 0.48 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 03:23:16.354597: step 97400, loss = 0.44 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 03:23:30.373615: step 97410, loss = 0.56 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 03:23:42.614824: step 97420, loss = 0.59 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 03:23:54.703273: step 97430, loss = 0.55 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-22 03:24:06.858756: step 97440, loss = 0.57 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 03:24:19.232482: step 97450, loss = 0.63 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 03:24:31.309431: step 97460, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 03:24:43.659790: step 97470, loss = 0.57 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 03:24:56.015015: step 97480, loss = 0.52 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 03:25:08.330101: step 97490, loss = 0.54 (23.2 examples/sec; 1.295 sec/batch)\n",
      "2019-05-22 03:25:20.662538: step 97500, loss = 0.58 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 03:25:35.096806: step 97510, loss = 0.55 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 03:25:47.328689: step 97520, loss = 0.52 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 03:25:59.455989: step 97530, loss = 0.55 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 03:26:11.688975: step 97540, loss = 0.56 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 03:26:23.984344: step 97550, loss = 0.54 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 03:26:36.302651: step 97560, loss = 0.55 (23.0 examples/sec; 1.307 sec/batch)\n",
      "2019-05-22 03:26:48.335219: step 97570, loss = 0.55 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 03:27:00.631997: step 97580, loss = 0.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 03:27:12.969793: step 97590, loss = 0.52 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 03:27:25.149613: step 97600, loss = 0.58 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 03:27:39.189988: step 97610, loss = 0.56 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 03:27:51.329268: step 97620, loss = 0.54 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 03:28:03.423783: step 97630, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 03:28:15.628766: step 97640, loss = 0.50 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 03:28:27.701888: step 97650, loss = 0.55 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 03:28:39.893957: step 97660, loss = 0.55 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 03:28:52.124095: step 97670, loss = 0.48 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 03:29:04.296442: step 97680, loss = 0.70 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 03:29:16.762261: step 97690, loss = 0.54 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 03:29:29.052393: step 97700, loss = 0.51 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 03:29:43.103189: step 97710, loss = 0.55 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 03:29:55.326322: step 97720, loss = 0.54 (22.7 examples/sec; 1.321 sec/batch)\n",
      "2019-05-22 03:30:07.538077: step 97730, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 03:30:19.882226: step 97740, loss = 0.56 (23.3 examples/sec; 1.290 sec/batch)\n",
      "2019-05-22 03:30:32.109243: step 97750, loss = 0.56 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 03:30:44.341446: step 97760, loss = 0.46 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 03:30:56.606795: step 97770, loss = 0.52 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 03:31:08.844407: step 97780, loss = 0.52 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 03:31:21.097053: step 97790, loss = 0.56 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 03:31:33.369458: step 97800, loss = 0.56 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 03:31:47.656604: step 97810, loss = 0.60 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 03:31:59.702551: step 97820, loss = 0.50 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 03:32:11.825862: step 97830, loss = 0.53 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 03:32:24.168307: step 97840, loss = 0.57 (24.6 examples/sec; 1.219 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 03:32:36.584474: step 97850, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 03:32:48.868681: step 97860, loss = 0.54 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-05-22 03:33:01.181748: step 97870, loss = 0.55 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 03:33:13.490927: step 97880, loss = 0.54 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 03:33:25.738125: step 97890, loss = 0.50 (23.2 examples/sec; 1.295 sec/batch)\n",
      "2019-05-22 03:33:38.056419: step 97900, loss = 0.59 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 03:33:52.168434: step 97910, loss = 0.64 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 03:34:04.397675: step 97920, loss = 0.60 (22.9 examples/sec; 1.310 sec/batch)\n",
      "2019-05-22 03:34:16.776989: step 97930, loss = 0.53 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 03:34:28.884083: step 97940, loss = 0.57 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 03:34:41.121734: step 97950, loss = 0.63 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 03:34:53.337006: step 97960, loss = 0.54 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 03:35:05.650703: step 97970, loss = 0.55 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 03:35:17.971655: step 97980, loss = 0.55 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 03:35:30.176756: step 97990, loss = 0.47 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 03:35:42.486349: step 98000, loss = 0.59 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 03:35:57.039471: step 98010, loss = 0.42 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 03:36:09.265659: step 98020, loss = 0.61 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 03:36:21.465748: step 98030, loss = 0.59 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 03:36:33.596350: step 98040, loss = 0.57 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 03:36:45.944726: step 98050, loss = 0.61 (23.1 examples/sec; 1.298 sec/batch)\n",
      "2019-05-22 03:36:58.162873: step 98060, loss = 0.58 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 03:37:10.359810: step 98070, loss = 0.54 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 03:37:22.476091: step 98080, loss = 0.57 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-05-22 03:37:34.526671: step 98090, loss = 0.59 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 03:37:46.838745: step 98100, loss = 0.50 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 03:38:00.936716: step 98110, loss = 0.62 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 03:38:13.132714: step 98120, loss = 0.55 (23.2 examples/sec; 1.292 sec/batch)\n",
      "2019-05-22 03:38:25.346607: step 98130, loss = 0.55 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 03:38:37.706864: step 98140, loss = 0.47 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 03:38:50.205697: step 98150, loss = 0.51 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 03:39:02.284970: step 98160, loss = 0.54 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 03:39:14.576948: step 98170, loss = 0.51 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 03:39:26.765985: step 98180, loss = 0.56 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 03:39:38.747564: step 98190, loss = 0.57 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 03:39:51.016280: step 98200, loss = 0.48 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 03:40:05.201904: step 98210, loss = 0.55 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 03:40:17.385762: step 98220, loss = 0.56 (23.1 examples/sec; 1.299 sec/batch)\n",
      "2019-05-22 03:40:29.440113: step 98230, loss = 0.56 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 03:40:41.401280: step 98240, loss = 0.53 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 03:40:53.433840: step 98250, loss = 0.51 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 03:41:05.685364: step 98260, loss = 0.57 (23.4 examples/sec; 1.285 sec/batch)\n",
      "2019-05-22 03:41:17.686827: step 98270, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 03:41:29.653420: step 98280, loss = 0.53 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 03:41:41.803584: step 98290, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 03:41:53.810615: step 98300, loss = 0.52 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 03:42:07.702817: step 98310, loss = 0.50 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-22 03:42:19.778022: step 98320, loss = 0.54 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-22 03:42:31.871007: step 98330, loss = 0.52 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-05-22 03:42:43.832106: step 98340, loss = 0.55 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 03:42:56.077028: step 98350, loss = 0.54 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 03:43:08.501881: step 98360, loss = 0.50 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 03:43:20.590606: step 98370, loss = 0.53 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 03:43:32.700091: step 98380, loss = 0.57 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 03:43:44.634269: step 98390, loss = 0.50 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 03:43:56.651612: step 98400, loss = 0.50 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 03:44:11.194823: step 98410, loss = 0.50 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 03:44:23.469852: step 98420, loss = 0.65 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 03:44:35.529623: step 98430, loss = 0.51 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 03:44:47.640120: step 98440, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 03:44:59.574087: step 98450, loss = 0.65 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 03:45:11.638737: step 98460, loss = 0.52 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 03:45:23.619011: step 98470, loss = 0.51 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 03:45:35.747114: step 98480, loss = 0.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 03:45:48.202175: step 98490, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 03:46:00.316495: step 98500, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 03:46:14.469783: step 98510, loss = 0.57 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-22 03:46:26.628683: step 98520, loss = 0.58 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 03:46:38.766014: step 98530, loss = 0.59 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 03:46:51.045830: step 98540, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 03:47:03.438329: step 98550, loss = 0.65 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 03:47:15.756593: step 98560, loss = 0.53 (22.8 examples/sec; 1.318 sec/batch)\n",
      "2019-05-22 03:47:28.025068: step 98570, loss = 0.56 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 03:47:40.068516: step 98580, loss = 0.59 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 03:47:52.342019: step 98590, loss = 0.52 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 03:48:04.527628: step 98600, loss = 0.44 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 03:48:18.777148: step 98610, loss = 0.58 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 03:48:31.112046: step 98620, loss = 0.55 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 03:48:43.244111: step 98630, loss = 0.54 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 03:48:55.545897: step 98640, loss = 0.50 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 03:49:07.865386: step 98650, loss = 0.59 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 03:49:20.040095: step 98660, loss = 0.54 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 03:49:32.198287: step 98670, loss = 0.58 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 03:49:44.285942: step 98680, loss = 0.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 03:49:56.384096: step 98690, loss = 0.48 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 03:50:08.679710: step 98700, loss = 0.59 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 03:50:22.866327: step 98710, loss = 0.54 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 03:50:35.240283: step 98720, loss = 0.49 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 03:50:47.464731: step 98730, loss = 0.56 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 03:50:59.596326: step 98740, loss = 0.53 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 03:51:11.941463: step 98750, loss = 0.54 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 03:51:24.286800: step 98760, loss = 0.55 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 03:51:36.492739: step 98770, loss = 0.55 (24.5 examples/sec; 1.224 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 03:51:48.773881: step 98780, loss = 0.51 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 03:52:01.006849: step 98790, loss = 0.51 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 03:52:13.211472: step 98800, loss = 0.56 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 03:52:27.175229: step 98810, loss = 0.54 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 03:52:39.566290: step 98820, loss = 0.48 (23.0 examples/sec; 1.303 sec/batch)\n",
      "2019-05-22 03:52:51.511215: step 98830, loss = 0.54 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 03:53:03.604847: step 98840, loss = 0.55 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 03:53:15.677995: step 98850, loss = 0.54 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 03:53:27.765592: step 98860, loss = 0.55 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 03:53:39.725499: step 98870, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 03:53:51.776874: step 98880, loss = 0.49 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 03:54:03.759197: step 98890, loss = 0.60 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-22 03:54:15.791714: step 98900, loss = 0.62 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 03:54:30.132441: step 98910, loss = 0.61 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 03:54:42.172451: step 98920, loss = 0.55 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 03:54:54.192049: step 98930, loss = 0.54 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 03:55:06.380460: step 98940, loss = 0.58 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 03:55:18.525108: step 98950, loss = 0.55 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 03:55:30.914845: step 98960, loss = 0.55 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 03:55:42.989025: step 98970, loss = 0.49 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 03:55:55.144223: step 98980, loss = 0.53 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 03:56:07.222344: step 98990, loss = 0.54 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 03:56:19.225340: step 99000, loss = 0.52 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 03:56:33.184311: step 99010, loss = 0.53 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 03:56:45.265805: step 99020, loss = 0.49 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 03:56:57.345980: step 99030, loss = 0.49 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 03:57:09.578122: step 99040, loss = 0.51 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 03:57:21.644593: step 99050, loss = 0.55 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 03:57:33.677632: step 99060, loss = 0.54 (23.8 examples/sec; 1.262 sec/batch)\n",
      "2019-05-22 03:57:45.681799: step 99070, loss = 0.55 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 03:57:57.913122: step 99080, loss = 0.53 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 03:58:09.945701: step 99090, loss = 0.56 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 03:58:22.085119: step 99100, loss = 0.54 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 03:58:36.105015: step 99110, loss = 0.57 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 03:58:48.282449: step 99120, loss = 0.58 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 03:59:00.377629: step 99130, loss = 0.57 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 03:59:12.401347: step 99140, loss = 0.57 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 03:59:24.449672: step 99150, loss = 0.59 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 03:59:36.497670: step 99160, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 03:59:48.601577: step 99170, loss = 0.57 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 04:00:00.648981: step 99180, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 04:00:12.706216: step 99190, loss = 0.60 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 04:00:24.773790: step 99200, loss = 0.52 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 04:00:38.754848: step 99210, loss = 0.52 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 04:00:50.800222: step 99220, loss = 0.51 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 04:01:02.897827: step 99230, loss = 0.57 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 04:01:14.968474: step 99240, loss = 0.61 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 04:01:27.007852: step 99250, loss = 0.52 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 04:01:39.035433: step 99260, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 04:01:51.111802: step 99270, loss = 0.56 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 04:02:03.142241: step 99280, loss = 0.55 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 04:02:15.131116: step 99290, loss = 0.55 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 04:02:27.179814: step 99300, loss = 0.53 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 04:02:41.628309: step 99310, loss = 0.50 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 04:02:53.651965: step 99320, loss = 0.57 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 04:03:05.667812: step 99330, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 04:03:17.641369: step 99340, loss = 0.52 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 04:03:29.673200: step 99350, loss = 0.51 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 04:03:41.744411: step 99360, loss = 0.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 04:03:53.758632: step 99370, loss = 0.58 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 04:04:05.777861: step 99380, loss = 0.55 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 04:04:17.849181: step 99390, loss = 0.56 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 04:04:29.921178: step 99400, loss = 0.51 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 04:04:44.012138: step 99410, loss = 0.55 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 04:04:56.056280: step 99420, loss = 0.54 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 04:05:08.054455: step 99430, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 04:05:20.073944: step 99440, loss = 0.52 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 04:05:32.099888: step 99450, loss = 0.63 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 04:05:44.202472: step 99460, loss = 0.48 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 04:05:56.305373: step 99470, loss = 0.59 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 04:06:08.463446: step 99480, loss = 0.59 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 04:06:20.583254: step 99490, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 04:06:32.708978: step 99500, loss = 0.62 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 04:06:47.138057: step 99510, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 04:06:59.160122: step 99520, loss = 0.53 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 04:07:11.444330: step 99530, loss = 0.53 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 04:07:23.841978: step 99540, loss = 0.56 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 04:07:36.219957: step 99550, loss = 0.56 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 04:07:48.481523: step 99560, loss = 0.58 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 04:08:00.728913: step 99570, loss = 0.53 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 04:08:13.135163: step 99580, loss = 0.58 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 04:08:25.248346: step 99590, loss = 0.53 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 04:08:37.578984: step 99600, loss = 0.50 (23.2 examples/sec; 1.294 sec/batch)\n",
      "2019-05-22 04:08:51.503693: step 99610, loss = 0.62 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 04:09:04.020058: step 99620, loss = 0.51 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-05-22 04:09:16.438577: step 99630, loss = 0.49 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 04:09:28.703942: step 99640, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 04:09:41.060481: step 99650, loss = 0.55 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 04:09:53.292395: step 99660, loss = 0.60 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 04:10:05.476297: step 99670, loss = 0.55 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 04:10:17.574950: step 99680, loss = 0.48 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 04:10:29.865168: step 99690, loss = 0.52 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 04:10:42.167628: step 99700, loss = 0.54 (24.9 examples/sec; 1.207 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 04:10:56.361211: step 99710, loss = 0.61 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 04:11:08.486007: step 99720, loss = 0.47 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 04:11:20.750329: step 99730, loss = 0.53 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 04:11:33.028564: step 99740, loss = 0.56 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 04:11:45.180243: step 99750, loss = 0.50 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 04:11:57.471491: step 99760, loss = 0.59 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 04:12:09.781191: step 99770, loss = 0.60 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 04:12:21.962253: step 99780, loss = 0.53 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 04:12:34.316875: step 99790, loss = 0.53 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 04:12:46.707612: step 99800, loss = 0.53 (23.1 examples/sec; 1.296 sec/batch)\n",
      "2019-05-22 04:13:00.869014: step 99810, loss = 0.52 (23.3 examples/sec; 1.289 sec/batch)\n",
      "2019-05-22 04:13:13.048539: step 99820, loss = 0.54 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 04:13:25.133777: step 99830, loss = 0.47 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 04:13:37.434853: step 99840, loss = 0.53 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 04:13:49.704598: step 99850, loss = 0.57 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 04:14:02.117720: step 99860, loss = 0.48 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 04:14:14.374478: step 99870, loss = 0.44 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 04:14:26.506645: step 99880, loss = 0.64 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 04:14:38.640636: step 99890, loss = 0.60 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 04:14:50.894892: step 99900, loss = 0.55 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 04:15:05.095184: step 99910, loss = 0.59 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 04:15:17.261646: step 99920, loss = 0.52 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 04:15:29.427824: step 99930, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 04:15:41.751579: step 99940, loss = 0.56 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 04:15:53.843356: step 99950, loss = 0.51 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 04:16:06.006326: step 99960, loss = 0.46 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 04:16:18.333615: step 99970, loss = 0.48 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 04:16:30.560499: step 99980, loss = 0.52 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 04:16:42.780883: step 99990, loss = 0.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 04:16:54.925910: step 100000, loss = 0.54 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-22 04:17:12.537604: step 100010, loss = 0.52 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 04:17:24.874702: step 100020, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 04:17:37.122290: step 100030, loss = 0.57 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 04:17:49.337526: step 100040, loss = 0.51 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 04:18:01.610776: step 100050, loss = 0.51 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 04:18:14.076815: step 100060, loss = 0.44 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 04:18:26.392999: step 100070, loss = 0.47 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 04:18:38.525353: step 100080, loss = 0.53 (25.5 examples/sec; 1.179 sec/batch)\n",
      "2019-05-22 04:18:50.665633: step 100090, loss = 0.49 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 04:19:03.209887: step 100100, loss = 0.51 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-22 04:19:17.711343: step 100110, loss = 0.51 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 04:19:29.893625: step 100120, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 04:19:42.036206: step 100130, loss = 0.60 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 04:19:54.199198: step 100140, loss = 0.49 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 04:20:06.483807: step 100150, loss = 0.54 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 04:20:18.610526: step 100160, loss = 0.58 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 04:20:30.793568: step 100170, loss = 0.58 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 04:20:42.941687: step 100180, loss = 0.55 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 04:20:55.252909: step 100190, loss = 0.59 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 04:21:07.495627: step 100200, loss = 0.48 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 04:21:21.447482: step 100210, loss = 0.51 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 04:21:33.635507: step 100220, loss = 0.54 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 04:21:45.765614: step 100230, loss = 0.45 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 04:21:57.863651: step 100240, loss = 0.52 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 04:22:10.014430: step 100250, loss = 0.54 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 04:22:22.344349: step 100260, loss = 0.49 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 04:22:34.526662: step 100270, loss = 0.54 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 04:22:46.763474: step 100280, loss = 0.50 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 04:22:59.135062: step 100290, loss = 0.57 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 04:23:11.333311: step 100300, loss = 0.48 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 04:23:25.438915: step 100310, loss = 0.53 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 04:23:37.627477: step 100320, loss = 0.55 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 04:23:49.811748: step 100330, loss = 0.58 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 04:24:01.982883: step 100340, loss = 0.62 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 04:24:14.096857: step 100350, loss = 0.54 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 04:24:26.385011: step 100360, loss = 0.47 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 04:24:38.491004: step 100370, loss = 0.59 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 04:24:50.690619: step 100380, loss = 0.58 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 04:25:02.818548: step 100390, loss = 0.59 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 04:25:15.156504: step 100400, loss = 0.57 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 04:25:29.666901: step 100410, loss = 0.64 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 04:25:41.970416: step 100420, loss = 0.54 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 04:25:54.226517: step 100430, loss = 0.58 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 04:26:06.461264: step 100440, loss = 0.59 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 04:26:18.600796: step 100450, loss = 0.50 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 04:26:30.820185: step 100460, loss = 0.50 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 04:26:42.981731: step 100470, loss = 0.47 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 04:26:55.091272: step 100480, loss = 0.55 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 04:27:07.478295: step 100490, loss = 0.52 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 04:27:19.721016: step 100500, loss = 0.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 04:27:33.998022: step 100510, loss = 0.58 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 04:27:46.362720: step 100520, loss = 0.52 (23.2 examples/sec; 1.292 sec/batch)\n",
      "2019-05-22 04:27:58.635318: step 100530, loss = 0.56 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 04:28:11.062387: step 100540, loss = 0.61 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 04:28:23.369784: step 100550, loss = 0.47 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 04:28:35.627456: step 100560, loss = 0.56 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 04:28:47.721831: step 100570, loss = 0.63 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 04:28:59.821916: step 100580, loss = 0.56 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 04:29:11.987067: step 100590, loss = 0.59 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 04:29:24.268193: step 100600, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 04:29:38.233261: step 100610, loss = 0.54 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 04:29:50.372296: step 100620, loss = 0.53 (24.9 examples/sec; 1.207 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 04:30:02.479240: step 100630, loss = 0.54 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 04:30:14.756601: step 100640, loss = 0.51 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 04:30:26.925365: step 100650, loss = 0.53 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 04:30:39.218746: step 100660, loss = 0.57 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 04:30:51.456633: step 100670, loss = 0.55 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 04:31:03.840198: step 100680, loss = 0.57 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 04:31:16.007121: step 100690, loss = 0.52 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-05-22 04:31:28.488262: step 100700, loss = 0.53 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 04:31:42.917347: step 100710, loss = 0.56 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 04:31:55.016914: step 100720, loss = 0.58 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 04:32:07.460376: step 100730, loss = 0.64 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 04:32:19.805650: step 100740, loss = 0.51 (22.8 examples/sec; 1.315 sec/batch)\n",
      "2019-05-22 04:32:32.054316: step 100750, loss = 0.58 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 04:32:44.246205: step 100760, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 04:32:56.393283: step 100770, loss = 0.48 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 04:33:08.611774: step 100780, loss = 0.57 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 04:33:20.847814: step 100790, loss = 0.57 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 04:33:33.086934: step 100800, loss = 0.52 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 04:33:47.450880: step 100810, loss = 0.61 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 04:33:59.633540: step 100820, loss = 0.49 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-05-22 04:34:11.766448: step 100830, loss = 0.59 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-22 04:34:23.918521: step 100840, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 04:34:36.075579: step 100850, loss = 0.57 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 04:34:48.255920: step 100860, loss = 0.60 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 04:35:00.501907: step 100870, loss = 0.59 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 04:35:12.820676: step 100880, loss = 0.53 (23.1 examples/sec; 1.299 sec/batch)\n",
      "2019-05-22 04:35:25.071379: step 100890, loss = 0.52 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 04:35:37.227195: step 100900, loss = 0.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 04:35:51.354780: step 100910, loss = 0.58 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 04:36:03.532960: step 100920, loss = 0.60 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 04:36:15.658477: step 100930, loss = 0.52 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 04:36:27.961593: step 100940, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 04:36:40.142537: step 100950, loss = 0.57 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 04:36:52.419989: step 100960, loss = 0.55 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 04:37:04.747894: step 100970, loss = 0.53 (23.2 examples/sec; 1.294 sec/batch)\n",
      "2019-05-22 04:37:16.909453: step 100980, loss = 0.47 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 04:37:29.185323: step 100990, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 04:37:41.414184: step 101000, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 04:37:56.202979: step 101010, loss = 0.55 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-05-22 04:38:08.306268: step 101020, loss = 0.47 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 04:38:20.461942: step 101030, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 04:38:32.766722: step 101040, loss = 0.60 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 04:38:45.025790: step 101050, loss = 0.54 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 04:38:57.205840: step 101060, loss = 0.58 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 04:39:09.522072: step 101070, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 04:39:21.630466: step 101080, loss = 0.54 (25.5 examples/sec; 1.176 sec/batch)\n",
      "2019-05-22 04:39:33.671784: step 101090, loss = 0.54 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 04:39:45.812008: step 101100, loss = 0.57 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 04:40:00.136837: step 101110, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 04:40:12.237513: step 101120, loss = 0.54 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 04:40:24.212249: step 101130, loss = 0.56 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 04:40:36.387870: step 101140, loss = 0.48 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 04:40:48.379871: step 101150, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 04:41:00.468934: step 101160, loss = 0.55 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 04:41:12.531638: step 101170, loss = 0.54 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 04:41:24.693434: step 101180, loss = 0.53 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 04:41:36.974335: step 101190, loss = 0.59 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 04:41:49.166710: step 101200, loss = 0.59 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 04:42:03.150208: step 101210, loss = 0.54 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 04:42:15.235914: step 101220, loss = 0.52 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 04:42:27.450146: step 101230, loss = 0.56 (23.3 examples/sec; 1.286 sec/batch)\n",
      "2019-05-22 04:42:39.752831: step 101240, loss = 0.53 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 04:42:51.810810: step 101250, loss = 0.56 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 04:43:04.065875: step 101260, loss = 0.54 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 04:43:16.358128: step 101270, loss = 0.55 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 04:43:28.492044: step 101280, loss = 0.50 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 04:43:40.845647: step 101290, loss = 0.57 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 04:43:53.046078: step 101300, loss = 0.50 (23.4 examples/sec; 1.280 sec/batch)\n",
      "2019-05-22 04:44:07.188577: step 101310, loss = 0.47 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 04:44:19.394280: step 101320, loss = 0.57 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 04:44:31.646149: step 101330, loss = 0.57 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 04:44:43.794854: step 101340, loss = 0.53 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 04:44:55.928313: step 101350, loss = 0.48 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 04:45:08.293989: step 101360, loss = 0.54 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 04:45:20.499119: step 101370, loss = 0.55 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 04:45:32.578893: step 101380, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 04:45:44.829722: step 101390, loss = 0.50 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-22 04:45:56.970601: step 101400, loss = 0.55 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 04:46:11.052362: step 101410, loss = 0.53 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 04:46:23.106359: step 101420, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 04:46:35.326868: step 101430, loss = 0.48 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 04:46:47.578065: step 101440, loss = 0.51 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 04:46:59.839244: step 101450, loss = 0.59 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 04:47:12.164840: step 101460, loss = 0.54 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 04:47:24.355061: step 101470, loss = 0.60 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 04:47:36.443333: step 101480, loss = 0.47 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 04:47:48.580493: step 101490, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 04:48:00.711218: step 101500, loss = 0.58 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 04:48:14.960156: step 101510, loss = 0.60 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 04:48:27.095837: step 101520, loss = 0.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 04:48:39.263218: step 101530, loss = 0.59 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 04:48:51.543855: step 101540, loss = 0.48 (24.6 examples/sec; 1.217 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 04:49:03.693439: step 101550, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 04:49:15.764548: step 101560, loss = 0.58 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 04:49:28.154654: step 101570, loss = 0.52 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 04:49:40.319659: step 101580, loss = 0.57 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 04:49:52.479620: step 101590, loss = 0.58 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 04:50:04.728712: step 101600, loss = 0.52 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 04:50:19.095945: step 101610, loss = 0.47 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 04:50:31.273100: step 101620, loss = 0.54 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 04:50:43.616278: step 101630, loss = 0.58 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 04:50:55.722159: step 101640, loss = 0.45 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 04:51:07.867523: step 101650, loss = 0.53 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 04:51:20.151244: step 101660, loss = 0.59 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 04:51:32.281738: step 101670, loss = 0.58 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 04:51:44.513864: step 101680, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 04:51:56.635297: step 101690, loss = 0.53 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 04:52:08.994808: step 101700, loss = 0.49 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 04:52:23.396416: step 101710, loss = 0.51 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-22 04:52:35.564341: step 101720, loss = 0.53 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 04:52:47.682320: step 101730, loss = 0.53 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 04:52:59.967571: step 101740, loss = 0.50 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 04:53:12.138986: step 101750, loss = 0.57 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 04:53:24.341773: step 101760, loss = 0.49 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 04:53:36.403625: step 101770, loss = 0.54 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 04:53:48.532365: step 101780, loss = 0.44 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 04:54:00.764518: step 101790, loss = 0.54 (23.1 examples/sec; 1.299 sec/batch)\n",
      "2019-05-22 04:54:12.970981: step 101800, loss = 0.47 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 04:54:26.924230: step 101810, loss = 0.56 (23.4 examples/sec; 1.282 sec/batch)\n",
      "2019-05-22 04:54:39.139211: step 101820, loss = 0.54 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 04:54:51.260922: step 101830, loss = 0.54 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 04:55:03.364651: step 101840, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 04:55:15.582757: step 101850, loss = 0.52 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 04:55:27.697020: step 101860, loss = 0.62 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 04:55:39.932700: step 101870, loss = 0.57 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 04:55:52.115580: step 101880, loss = 0.53 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 04:56:04.333703: step 101890, loss = 0.45 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 04:56:16.566189: step 101900, loss = 0.52 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 04:56:30.785244: step 101910, loss = 0.43 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 04:56:43.066743: step 101920, loss = 0.49 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-05-22 04:56:55.314900: step 101930, loss = 0.50 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 04:57:07.633121: step 101940, loss = 0.47 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 04:57:20.065909: step 101950, loss = 0.50 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 04:57:32.379504: step 101960, loss = 0.51 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 04:57:44.630141: step 101970, loss = 0.47 (23.1 examples/sec; 1.300 sec/batch)\n",
      "2019-05-22 04:57:56.763580: step 101980, loss = 0.52 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 04:58:08.974136: step 101990, loss = 0.56 (23.3 examples/sec; 1.285 sec/batch)\n",
      "2019-05-22 04:58:21.338129: step 102000, loss = 0.60 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 04:58:35.537046: step 102010, loss = 0.53 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 04:58:47.566608: step 102020, loss = 0.60 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 04:58:59.828716: step 102030, loss = 0.51 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 04:59:11.955748: step 102040, loss = 0.55 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 04:59:24.153408: step 102050, loss = 0.51 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 04:59:36.289993: step 102060, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 04:59:48.484651: step 102070, loss = 0.50 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 05:00:00.628493: step 102080, loss = 0.53 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 05:00:12.679575: step 102090, loss = 0.54 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 05:00:24.907032: step 102100, loss = 0.55 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 05:00:39.238999: step 102110, loss = 0.50 (23.2 examples/sec; 1.292 sec/batch)\n",
      "2019-05-22 05:00:51.293894: step 102120, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 05:01:03.505165: step 102130, loss = 0.57 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 05:01:15.702573: step 102140, loss = 0.51 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 05:01:27.919736: step 102150, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 05:01:40.036545: step 102160, loss = 0.49 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 05:01:52.182421: step 102170, loss = 0.55 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 05:02:04.406813: step 102180, loss = 0.58 (23.2 examples/sec; 1.292 sec/batch)\n",
      "2019-05-22 05:02:16.580539: step 102190, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 05:02:28.789217: step 102200, loss = 0.57 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 05:02:43.344681: step 102210, loss = 0.59 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 05:02:55.442809: step 102220, loss = 0.48 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 05:03:07.598922: step 102230, loss = 0.50 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 05:03:19.669875: step 102240, loss = 0.57 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 05:03:31.822708: step 102250, loss = 0.53 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 05:03:43.957291: step 102260, loss = 0.49 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 05:03:56.203872: step 102270, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 05:04:08.315508: step 102280, loss = 0.54 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 05:04:20.424708: step 102290, loss = 0.45 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 05:04:32.655714: step 102300, loss = 0.55 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 05:04:46.662575: step 102310, loss = 0.58 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 05:04:58.737252: step 102320, loss = 0.64 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 05:05:10.873851: step 102330, loss = 0.57 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 05:05:22.867558: step 102340, loss = 0.52 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-22 05:05:34.969220: step 102350, loss = 0.50 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 05:05:47.172660: step 102360, loss = 0.56 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 05:05:59.550412: step 102370, loss = 0.53 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-05-22 05:06:11.577940: step 102380, loss = 0.51 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 05:06:23.748048: step 102390, loss = 0.55 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 05:06:35.988531: step 102400, loss = 0.60 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 05:06:50.125293: step 102410, loss = 0.43 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-22 05:07:02.221204: step 102420, loss = 0.54 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 05:07:14.536331: step 102430, loss = 0.59 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-22 05:07:26.676535: step 102440, loss = 0.53 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 05:07:38.887407: step 102450, loss = 0.55 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 05:07:51.207910: step 102460, loss = 0.51 (23.0 examples/sec; 1.306 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 05:08:03.278714: step 102470, loss = 0.57 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 05:08:15.506550: step 102480, loss = 0.60 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 05:08:27.728976: step 102490, loss = 0.52 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 05:08:39.890852: step 102500, loss = 0.55 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 05:08:54.427345: step 102510, loss = 0.50 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 05:09:06.646487: step 102520, loss = 0.53 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 05:09:18.985835: step 102530, loss = 0.54 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 05:09:31.160032: step 102540, loss = 0.49 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 05:09:43.306299: step 102550, loss = 0.57 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 05:09:55.564909: step 102560, loss = 0.55 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 05:10:07.835057: step 102570, loss = 0.54 (23.3 examples/sec; 1.288 sec/batch)\n",
      "2019-05-22 05:10:20.183837: step 102580, loss = 0.55 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 05:10:32.273051: step 102590, loss = 0.59 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 05:10:44.421652: step 102600, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 05:10:58.623322: step 102610, loss = 0.55 (23.4 examples/sec; 1.285 sec/batch)\n",
      "2019-05-22 05:11:10.834602: step 102620, loss = 0.61 (22.7 examples/sec; 1.323 sec/batch)\n",
      "2019-05-22 05:11:23.049956: step 102630, loss = 0.58 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 05:11:35.280548: step 102640, loss = 0.55 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 05:11:47.576581: step 102650, loss = 0.67 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 05:11:59.808669: step 102660, loss = 0.48 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 05:12:12.093164: step 102670, loss = 0.47 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 05:12:24.419748: step 102680, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 05:12:36.740071: step 102690, loss = 0.57 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 05:12:48.985216: step 102700, loss = 0.57 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 05:13:03.066733: step 102710, loss = 0.55 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 05:13:15.197371: step 102720, loss = 0.48 (23.0 examples/sec; 1.307 sec/batch)\n",
      "2019-05-22 05:13:27.348297: step 102730, loss = 0.56 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 05:13:39.801270: step 102740, loss = 0.53 (22.8 examples/sec; 1.313 sec/batch)\n",
      "2019-05-22 05:13:52.010366: step 102750, loss = 0.54 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 05:14:04.232769: step 102760, loss = 0.52 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 05:14:16.398460: step 102770, loss = 0.59 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 05:14:28.686495: step 102780, loss = 0.53 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 05:14:40.864788: step 102790, loss = 0.46 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 05:14:53.034465: step 102800, loss = 0.55 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 05:15:07.309305: step 102810, loss = 0.55 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 05:15:19.524977: step 102820, loss = 0.46 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 05:15:31.794624: step 102830, loss = 0.55 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 05:15:43.784421: step 102840, loss = 0.53 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 05:15:56.113106: step 102850, loss = 0.52 (22.8 examples/sec; 1.316 sec/batch)\n",
      "2019-05-22 05:16:08.258639: step 102860, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 05:16:20.503126: step 102870, loss = 0.51 (23.5 examples/sec; 1.278 sec/batch)\n",
      "2019-05-22 05:16:32.732318: step 102880, loss = 0.53 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-22 05:16:44.915510: step 102890, loss = 0.51 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 05:16:57.157290: step 102900, loss = 0.55 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 05:17:11.153449: step 102910, loss = 0.58 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 05:17:23.378451: step 102920, loss = 0.52 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 05:17:35.675886: step 102930, loss = 0.55 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 05:17:47.830833: step 102940, loss = 0.54 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 05:18:00.050248: step 102950, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 05:18:12.408118: step 102960, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 05:18:24.718031: step 102970, loss = 0.57 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 05:18:36.838067: step 102980, loss = 0.45 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 05:18:49.208886: step 102990, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 05:19:01.483003: step 103000, loss = 0.57 (22.9 examples/sec; 1.308 sec/batch)\n",
      "2019-05-22 05:19:16.092582: step 103010, loss = 0.50 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 05:19:28.351728: step 103020, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 05:19:40.707818: step 103030, loss = 0.65 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 05:19:52.940375: step 103040, loss = 0.54 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 05:20:05.231479: step 103050, loss = 0.57 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 05:20:17.544322: step 103060, loss = 0.45 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 05:20:29.791276: step 103070, loss = 0.50 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 05:20:41.912223: step 103080, loss = 0.58 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 05:20:53.964444: step 103090, loss = 0.49 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 05:21:06.219106: step 103100, loss = 0.48 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 05:21:20.268258: step 103110, loss = 0.56 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 05:21:32.560424: step 103120, loss = 0.53 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 05:21:44.936151: step 103130, loss = 0.65 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 05:21:56.999042: step 103140, loss = 0.56 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 05:22:09.269188: step 103150, loss = 0.51 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 05:22:21.530239: step 103160, loss = 0.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 05:22:33.658780: step 103170, loss = 0.42 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 05:22:45.818667: step 103180, loss = 0.50 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-22 05:22:58.088598: step 103190, loss = 0.64 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 05:23:10.385772: step 103200, loss = 0.54 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 05:23:24.557845: step 103210, loss = 0.53 (23.4 examples/sec; 1.280 sec/batch)\n",
      "2019-05-22 05:23:36.659561: step 103220, loss = 0.58 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 05:23:49.007160: step 103230, loss = 0.59 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 05:24:01.296060: step 103240, loss = 0.52 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 05:24:13.613363: step 103250, loss = 0.47 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 05:24:25.976467: step 103260, loss = 0.52 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 05:24:38.051242: step 103270, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 05:24:50.384962: step 103280, loss = 0.59 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 05:25:02.568950: step 103290, loss = 0.46 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 05:25:14.822878: step 103300, loss = 0.55 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 05:25:29.047845: step 103310, loss = 0.58 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 05:25:41.370737: step 103320, loss = 0.57 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-05-22 05:25:53.625709: step 103330, loss = 0.45 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 05:26:05.676184: step 103340, loss = 0.48 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 05:26:17.637024: step 103350, loss = 0.52 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 05:26:29.663874: step 103360, loss = 0.51 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 05:26:41.648427: step 103370, loss = 0.49 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 05:26:53.696016: step 103380, loss = 0.46 (24.9 examples/sec; 1.204 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 05:27:05.794773: step 103390, loss = 0.57 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 05:27:17.912699: step 103400, loss = 0.53 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 05:27:32.053153: step 103410, loss = 0.56 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 05:27:44.221140: step 103420, loss = 0.50 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 05:27:56.437535: step 103430, loss = 0.64 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 05:28:08.664409: step 103440, loss = 0.63 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 05:28:20.865937: step 103450, loss = 0.53 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 05:28:33.035450: step 103460, loss = 0.49 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 05:28:45.266722: step 103470, loss = 0.59 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 05:28:57.486943: step 103480, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 05:29:09.683571: step 103490, loss = 0.52 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 05:29:21.914509: step 103500, loss = 0.46 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 05:29:36.137367: step 103510, loss = 0.63 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 05:29:48.369160: step 103520, loss = 0.53 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 05:30:00.642384: step 103530, loss = 0.45 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 05:30:12.844022: step 103540, loss = 0.49 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 05:30:25.026970: step 103550, loss = 0.46 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 05:30:37.225247: step 103560, loss = 0.50 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 05:30:49.426400: step 103570, loss = 0.58 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 05:31:01.678120: step 103580, loss = 0.50 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 05:31:13.723505: step 103590, loss = 0.49 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 05:31:25.943328: step 103600, loss = 0.56 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 05:31:40.415453: step 103610, loss = 0.55 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 05:31:52.615788: step 103620, loss = 0.49 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 05:32:04.859649: step 103630, loss = 0.51 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 05:32:17.054276: step 103640, loss = 0.49 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 05:32:29.261191: step 103650, loss = 0.54 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 05:32:41.383809: step 103660, loss = 0.51 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 05:32:53.637256: step 103670, loss = 0.49 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 05:33:05.875981: step 103680, loss = 0.41 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 05:33:18.135939: step 103690, loss = 0.61 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 05:33:30.309063: step 103700, loss = 0.58 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 05:33:44.341932: step 103710, loss = 0.49 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 05:33:56.507199: step 103720, loss = 0.45 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 05:34:08.571073: step 103730, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 05:34:20.630427: step 103740, loss = 0.49 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 05:34:32.717119: step 103750, loss = 0.67 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 05:34:44.792134: step 103760, loss = 0.52 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 05:34:56.847608: step 103770, loss = 0.56 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 05:35:08.998906: step 103780, loss = 0.55 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 05:35:21.074263: step 103790, loss = 0.59 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 05:35:33.209642: step 103800, loss = 0.48 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 05:35:47.562389: step 103810, loss = 0.57 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 05:35:59.672143: step 103820, loss = 0.53 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 05:36:11.828350: step 103830, loss = 0.63 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 05:36:23.914104: step 103840, loss = 0.49 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 05:36:36.059251: step 103850, loss = 0.51 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 05:36:48.219957: step 103860, loss = 0.60 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 05:37:00.450659: step 103870, loss = 0.56 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-22 05:37:12.712676: step 103880, loss = 0.52 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 05:37:24.933499: step 103890, loss = 0.52 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 05:37:37.141761: step 103900, loss = 0.55 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 05:37:51.239576: step 103910, loss = 0.55 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 05:38:03.470200: step 103920, loss = 0.46 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 05:38:15.683081: step 103930, loss = 0.46 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 05:38:27.849455: step 103940, loss = 0.61 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 05:38:40.037842: step 103950, loss = 0.57 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 05:38:52.167580: step 103960, loss = 0.50 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 05:39:04.213187: step 103970, loss = 0.60 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 05:39:16.346543: step 103980, loss = 0.54 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 05:39:28.594385: step 103990, loss = 0.49 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 05:39:40.811989: step 104000, loss = 0.53 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 05:39:55.066743: step 104010, loss = 0.51 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 05:40:07.280962: step 104020, loss = 0.58 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 05:40:19.578013: step 104030, loss = 0.66 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 05:40:31.822545: step 104040, loss = 0.54 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 05:40:44.046008: step 104050, loss = 0.57 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 05:40:56.241345: step 104060, loss = 0.53 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 05:41:08.494578: step 104070, loss = 0.53 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 05:41:20.725553: step 104080, loss = 0.53 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 05:41:32.834610: step 104090, loss = 0.45 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 05:41:45.064070: step 104100, loss = 0.60 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-22 05:41:59.222470: step 104110, loss = 0.52 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 05:42:11.449728: step 104120, loss = 0.47 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 05:42:23.620295: step 104130, loss = 0.53 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 05:42:35.848959: step 104140, loss = 0.56 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 05:42:48.069553: step 104150, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 05:43:00.279218: step 104160, loss = 0.48 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 05:43:12.524004: step 104170, loss = 0.53 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 05:43:24.636706: step 104180, loss = 0.61 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 05:43:36.803793: step 104190, loss = 0.46 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 05:43:48.973696: step 104200, loss = 0.50 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 05:44:03.085894: step 104210, loss = 0.59 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 05:44:15.293143: step 104220, loss = 0.55 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 05:44:27.506954: step 104230, loss = 0.52 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 05:44:39.712277: step 104240, loss = 0.52 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 05:44:51.952595: step 104250, loss = 0.58 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 05:45:04.146490: step 104260, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 05:45:16.333627: step 104270, loss = 0.58 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 05:45:28.546820: step 104280, loss = 0.54 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 05:45:40.757428: step 104290, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 05:45:52.976882: step 104300, loss = 0.53 (24.3 examples/sec; 1.233 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 05:46:07.112936: step 104310, loss = 0.49 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 05:46:19.357278: step 104320, loss = 0.51 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 05:46:31.517866: step 104330, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 05:46:43.618244: step 104340, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 05:46:55.740972: step 104350, loss = 0.60 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 05:47:07.892657: step 104360, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 05:47:20.112087: step 104370, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 05:47:32.334128: step 104380, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 05:47:44.571545: step 104390, loss = 0.48 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 05:47:56.787905: step 104400, loss = 0.49 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 05:48:11.436851: step 104410, loss = 0.53 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 05:48:23.647679: step 104420, loss = 0.52 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 05:48:35.840005: step 104430, loss = 0.62 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 05:48:48.034545: step 104440, loss = 0.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 05:49:00.224960: step 104450, loss = 0.59 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 05:49:12.437999: step 104460, loss = 0.59 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 05:49:24.675519: step 104470, loss = 0.56 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 05:49:36.861106: step 104480, loss = 0.63 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 05:49:49.067005: step 104490, loss = 0.56 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 05:50:01.253734: step 104500, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 05:50:15.406152: step 104510, loss = 0.51 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 05:50:27.626211: step 104520, loss = 0.50 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 05:50:39.841334: step 104530, loss = 0.47 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 05:50:52.129856: step 104540, loss = 0.54 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 05:51:04.278442: step 104550, loss = 0.47 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 05:51:16.425525: step 104560, loss = 0.55 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 05:51:28.642858: step 104570, loss = 0.58 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 05:51:40.900933: step 104580, loss = 0.62 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-22 05:51:53.015258: step 104590, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 05:52:05.090497: step 104600, loss = 0.57 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 05:52:19.256128: step 104610, loss = 0.55 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 05:52:31.517370: step 104620, loss = 0.51 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 05:52:43.714004: step 104630, loss = 0.53 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 05:52:55.914286: step 104640, loss = 0.49 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 05:53:08.114566: step 104650, loss = 0.44 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 05:53:20.319049: step 104660, loss = 0.59 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 05:53:32.543587: step 104670, loss = 0.49 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 05:53:44.725308: step 104680, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 05:53:56.899355: step 104690, loss = 0.58 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 05:54:09.101321: step 104700, loss = 0.50 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 05:54:23.256132: step 104710, loss = 0.47 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 05:54:35.474312: step 104720, loss = 0.55 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 05:54:47.663080: step 104730, loss = 0.54 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 05:54:59.849780: step 104740, loss = 0.64 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 05:55:11.985808: step 104750, loss = 0.52 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 05:55:24.190533: step 104760, loss = 0.51 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 05:55:36.387828: step 104770, loss = 0.57 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 05:55:48.566031: step 104780, loss = 0.55 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 05:56:00.766127: step 104790, loss = 0.54 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 05:56:12.973557: step 104800, loss = 0.58 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 05:56:27.093462: step 104810, loss = 0.56 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 05:56:39.307315: step 104820, loss = 0.46 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 05:56:51.521711: step 104830, loss = 0.51 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 05:57:03.643773: step 104840, loss = 0.53 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 05:57:15.701818: step 104850, loss = 0.57 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 05:57:27.880375: step 104860, loss = 0.44 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 05:57:40.123617: step 104870, loss = 0.57 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 05:57:52.347486: step 104880, loss = 0.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 05:58:04.533209: step 104890, loss = 0.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 05:58:16.763216: step 104900, loss = 0.56 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 05:58:31.027217: step 104910, loss = 0.53 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 05:58:43.117184: step 104920, loss = 0.57 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 05:58:55.272620: step 104930, loss = 0.50 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 05:59:07.320787: step 104940, loss = 0.53 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 05:59:19.357494: step 104950, loss = 0.53 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 05:59:31.379363: step 104960, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 05:59:43.431231: step 104970, loss = 0.56 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 05:59:55.503000: step 104980, loss = 0.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 06:00:07.645520: step 104990, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 06:00:19.791981: step 105000, loss = 0.50 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 06:00:37.504119: step 105010, loss = 0.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 06:00:49.539357: step 105020, loss = 0.58 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 06:01:01.600404: step 105030, loss = 0.45 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 06:01:13.696236: step 105040, loss = 0.54 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 06:01:25.840754: step 105050, loss = 0.57 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 06:01:38.006681: step 105060, loss = 0.48 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 06:01:50.143211: step 105070, loss = 0.62 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 06:02:02.312265: step 105080, loss = 0.50 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 06:02:14.433241: step 105090, loss = 0.57 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 06:02:26.607572: step 105100, loss = 0.50 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 06:02:41.090352: step 105110, loss = 0.53 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 06:02:53.314388: step 105120, loss = 0.54 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 06:03:05.544452: step 105130, loss = 0.56 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 06:03:17.650966: step 105140, loss = 0.51 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 06:03:29.771877: step 105150, loss = 0.52 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 06:03:41.852818: step 105160, loss = 0.51 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 06:03:53.901165: step 105170, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 06:04:05.959073: step 105180, loss = 0.55 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 06:04:18.047389: step 105190, loss = 0.53 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 06:04:30.151687: step 105200, loss = 0.53 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 06:04:44.484329: step 105210, loss = 0.54 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 06:04:56.539363: step 105220, loss = 0.50 (24.8 examples/sec; 1.208 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 06:05:08.617177: step 105230, loss = 0.49 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 06:05:20.724660: step 105240, loss = 0.54 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 06:05:32.814699: step 105250, loss = 0.50 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 06:05:44.866984: step 105260, loss = 0.49 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 06:05:56.923956: step 105270, loss = 0.56 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 06:06:09.032080: step 105280, loss = 0.47 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 06:06:21.117450: step 105290, loss = 0.56 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 06:06:33.188930: step 105300, loss = 0.53 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 06:06:47.087237: step 105310, loss = 0.50 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 06:06:59.175442: step 105320, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 06:07:11.197260: step 105330, loss = 0.45 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 06:07:23.145920: step 105340, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 06:07:35.144435: step 105350, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 06:07:47.209235: step 105360, loss = 0.46 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 06:07:59.213186: step 105370, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 06:08:11.307540: step 105380, loss = 0.58 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 06:08:23.426893: step 105390, loss = 0.54 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 06:08:35.518663: step 105400, loss = 0.49 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 06:08:49.795209: step 105410, loss = 0.46 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 06:09:01.824963: step 105420, loss = 0.51 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 06:09:13.910163: step 105430, loss = 0.49 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 06:09:25.985116: step 105440, loss = 0.46 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 06:09:38.021988: step 105450, loss = 0.57 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 06:09:50.047393: step 105460, loss = 0.60 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 06:10:02.094275: step 105470, loss = 0.56 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 06:10:14.222219: step 105480, loss = 0.59 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 06:10:26.282198: step 105490, loss = 0.52 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 06:10:38.349988: step 105500, loss = 0.53 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 06:10:52.365719: step 105510, loss = 0.53 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 06:11:04.450095: step 105520, loss = 0.57 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 06:11:16.520748: step 105530, loss = 0.56 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 06:11:28.604953: step 105540, loss = 0.52 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 06:11:40.700007: step 105550, loss = 0.46 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 06:11:52.815619: step 105560, loss = 0.48 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 06:12:04.894510: step 105570, loss = 0.58 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 06:12:17.054373: step 105580, loss = 0.59 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 06:12:29.124664: step 105590, loss = 0.52 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 06:12:41.122148: step 105600, loss = 0.57 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 06:12:55.150751: step 105610, loss = 0.56 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 06:13:07.202593: step 105620, loss = 0.55 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 06:13:19.306093: step 105630, loss = 0.57 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 06:13:31.368777: step 105640, loss = 0.49 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 06:13:43.418429: step 105650, loss = 0.55 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 06:13:55.553424: step 105660, loss = 0.59 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 06:14:07.620368: step 105670, loss = 0.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 06:14:19.658614: step 105680, loss = 0.51 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 06:14:31.708287: step 105690, loss = 0.52 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 06:14:43.782908: step 105700, loss = 0.52 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 06:14:57.768548: step 105710, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 06:15:09.913176: step 105720, loss = 0.60 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 06:15:22.012137: step 105730, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 06:15:34.105692: step 105740, loss = 0.60 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 06:15:46.158548: step 105750, loss = 0.56 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 06:15:58.275081: step 105760, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 06:16:10.333252: step 105770, loss = 0.59 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 06:16:22.470055: step 105780, loss = 0.50 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 06:16:34.600221: step 105790, loss = 0.57 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 06:16:46.725389: step 105800, loss = 0.49 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 06:17:01.093724: step 105810, loss = 0.60 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 06:17:13.160620: step 105820, loss = 0.55 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 06:17:25.234017: step 105830, loss = 0.51 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 06:17:37.286935: step 105840, loss = 0.56 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 06:17:49.245910: step 105850, loss = 0.57 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 06:18:01.307180: step 105860, loss = 0.54 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 06:18:13.373552: step 105870, loss = 0.56 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 06:18:25.460818: step 105880, loss = 0.47 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 06:18:37.588695: step 105890, loss = 0.48 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 06:18:49.637154: step 105900, loss = 0.55 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 06:19:04.120933: step 105910, loss = 0.47 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 06:19:16.190374: step 105920, loss = 0.49 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 06:19:28.280990: step 105930, loss = 0.54 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 06:19:40.389227: step 105940, loss = 0.54 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 06:19:52.459218: step 105950, loss = 0.56 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 06:20:04.497901: step 105960, loss = 0.53 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 06:20:16.602589: step 105970, loss = 0.49 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 06:20:28.669688: step 105980, loss = 0.51 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 06:20:40.790835: step 105990, loss = 0.57 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 06:20:52.887848: step 106000, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 06:21:07.278800: step 106010, loss = 0.58 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 06:21:19.352321: step 106020, loss = 0.44 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 06:21:31.435321: step 106030, loss = 0.53 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 06:21:43.554127: step 106040, loss = 0.52 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 06:21:55.661990: step 106050, loss = 0.53 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 06:22:07.724049: step 106060, loss = 0.46 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 06:22:19.840531: step 106070, loss = 0.54 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 06:22:31.928730: step 106080, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 06:22:43.903464: step 106090, loss = 0.45 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 06:22:55.943293: step 106100, loss = 0.60 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-22 06:23:09.947055: step 106110, loss = 0.52 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 06:23:22.041589: step 106120, loss = 0.51 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 06:23:34.181904: step 106130, loss = 0.50 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 06:23:46.268849: step 106140, loss = 0.58 (24.6 examples/sec; 1.218 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 06:23:58.374915: step 106150, loss = 0.52 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 06:24:10.400436: step 106160, loss = 0.58 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 06:24:22.470202: step 106170, loss = 0.54 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 06:24:34.454589: step 106180, loss = 0.52 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 06:24:46.496328: step 106190, loss = 0.55 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 06:24:58.536986: step 106200, loss = 0.52 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 06:25:12.543380: step 106210, loss = 0.51 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 06:25:24.581699: step 106220, loss = 0.58 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 06:25:36.583294: step 106230, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 06:25:48.628132: step 106240, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 06:26:00.667170: step 106250, loss = 0.55 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 06:26:12.696596: step 106260, loss = 0.51 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 06:26:24.760708: step 106270, loss = 0.55 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 06:26:36.829649: step 106280, loss = 0.48 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 06:26:48.901175: step 106290, loss = 0.60 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 06:27:00.910424: step 106300, loss = 0.50 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 06:27:14.866792: step 106310, loss = 0.58 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 06:27:26.911900: step 106320, loss = 0.58 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 06:27:38.976175: step 106330, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 06:27:50.961908: step 106340, loss = 0.42 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 06:28:02.986173: step 106350, loss = 0.59 (25.5 examples/sec; 1.177 sec/batch)\n",
      "2019-05-22 06:28:14.936971: step 106360, loss = 0.46 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 06:28:26.964044: step 106370, loss = 0.58 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 06:28:39.007889: step 106380, loss = 0.60 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 06:28:51.096740: step 106390, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 06:29:03.159061: step 106400, loss = 0.53 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 06:29:17.178233: step 106410, loss = 0.57 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 06:29:29.230812: step 106420, loss = 0.55 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 06:29:41.248314: step 106430, loss = 0.53 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 06:29:53.308203: step 106440, loss = 0.58 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 06:30:05.341614: step 106450, loss = 0.54 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 06:30:17.359332: step 106460, loss = 0.49 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 06:30:29.408162: step 106470, loss = 0.53 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 06:30:41.453925: step 106480, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 06:30:53.523039: step 106490, loss = 0.58 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 06:31:05.570854: step 106500, loss = 0.46 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 06:31:19.547327: step 106510, loss = 0.48 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 06:31:31.567301: step 106520, loss = 0.56 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 06:31:43.587206: step 106530, loss = 0.55 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 06:31:55.681086: step 106540, loss = 0.58 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 06:32:07.701217: step 106550, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 06:32:19.744490: step 106560, loss = 0.52 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 06:32:31.774888: step 106570, loss = 0.50 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 06:32:43.789428: step 106580, loss = 0.54 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 06:32:55.771056: step 106590, loss = 0.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 06:33:07.794322: step 106600, loss = 0.52 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 06:33:21.723652: step 106610, loss = 0.57 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 06:33:33.761018: step 106620, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 06:33:45.810601: step 106630, loss = 0.59 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 06:33:57.833698: step 106640, loss = 0.60 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 06:34:09.863339: step 106650, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 06:34:21.904605: step 106660, loss = 0.58 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 06:34:33.968975: step 106670, loss = 0.52 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 06:34:45.995797: step 106680, loss = 0.58 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 06:34:58.071906: step 106690, loss = 0.54 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 06:35:10.043680: step 106700, loss = 0.49 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 06:35:24.068969: step 106710, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 06:35:36.130263: step 106720, loss = 0.52 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 06:35:48.149599: step 106730, loss = 0.49 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 06:36:00.167282: step 106740, loss = 0.61 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 06:36:12.241989: step 106750, loss = 0.53 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 06:36:24.356478: step 106760, loss = 0.51 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 06:36:36.522309: step 106770, loss = 0.55 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 06:36:48.645234: step 106780, loss = 0.53 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 06:37:00.890124: step 106790, loss = 0.49 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 06:37:13.067379: step 106800, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 06:37:27.233593: step 106810, loss = 0.55 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 06:37:39.373165: step 106820, loss = 0.54 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 06:37:51.497327: step 106830, loss = 0.50 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-22 06:38:03.564785: step 106840, loss = 0.47 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 06:38:15.743092: step 106850, loss = 0.58 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 06:38:27.849235: step 106860, loss = 0.52 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 06:38:40.016557: step 106870, loss = 0.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 06:38:52.212994: step 106880, loss = 0.56 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 06:39:04.420641: step 106890, loss = 0.52 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 06:39:16.620315: step 106900, loss = 0.56 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 06:39:30.717310: step 106910, loss = 0.51 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 06:39:42.909740: step 106920, loss = 0.52 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 06:39:55.177708: step 106930, loss = 0.48 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 06:40:07.359646: step 106940, loss = 0.47 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 06:40:19.462331: step 106950, loss = 0.51 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 06:40:31.529200: step 106960, loss = 0.50 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 06:40:43.644903: step 106970, loss = 0.51 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 06:40:55.776653: step 106980, loss = 0.55 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 06:41:07.946828: step 106990, loss = 0.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 06:41:20.078676: step 107000, loss = 0.57 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 06:41:34.203380: step 107010, loss = 0.54 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 06:41:46.301028: step 107020, loss = 0.47 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 06:41:58.456976: step 107030, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 06:42:10.611101: step 107040, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 06:42:22.758382: step 107050, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 06:42:34.870095: step 107060, loss = 0.69 (24.9 examples/sec; 1.204 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 06:42:46.930728: step 107070, loss = 0.57 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 06:42:59.020097: step 107080, loss = 0.52 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 06:43:11.100164: step 107090, loss = 0.54 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 06:43:23.182581: step 107100, loss = 0.49 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 06:43:37.086274: step 107110, loss = 0.50 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-22 06:43:49.160052: step 107120, loss = 0.62 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 06:44:01.295006: step 107130, loss = 0.59 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 06:44:13.402990: step 107140, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 06:44:25.569872: step 107150, loss = 0.47 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 06:44:37.699985: step 107160, loss = 0.47 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 06:44:49.784256: step 107170, loss = 0.54 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 06:45:01.822038: step 107180, loss = 0.52 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 06:45:13.894490: step 107190, loss = 0.47 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 06:45:25.996525: step 107200, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 06:45:40.296268: step 107210, loss = 0.50 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 06:45:52.354197: step 107220, loss = 0.54 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 06:46:04.396321: step 107230, loss = 0.55 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 06:46:16.435249: step 107240, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 06:46:28.488385: step 107250, loss = 0.50 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 06:46:40.559051: step 107260, loss = 0.57 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 06:46:52.614520: step 107270, loss = 0.48 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 06:47:04.675989: step 107280, loss = 0.52 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 06:47:16.647876: step 107290, loss = 0.51 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 06:47:28.662270: step 107300, loss = 0.64 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 06:47:43.027110: step 107310, loss = 0.46 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 06:47:55.083542: step 107320, loss = 0.60 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 06:48:07.060111: step 107330, loss = 0.57 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 06:48:19.081127: step 107340, loss = 0.65 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 06:48:31.167072: step 107350, loss = 0.50 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 06:48:43.269432: step 107360, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 06:48:55.220632: step 107370, loss = 0.59 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 06:49:07.314867: step 107380, loss = 0.61 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 06:49:19.480406: step 107390, loss = 0.57 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 06:49:31.684058: step 107400, loss = 0.53 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 06:49:45.793762: step 107410, loss = 0.51 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 06:49:57.986587: step 107420, loss = 0.53 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 06:50:10.183239: step 107430, loss = 0.51 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 06:50:22.382847: step 107440, loss = 0.60 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 06:50:34.582509: step 107450, loss = 0.51 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 06:50:46.770614: step 107460, loss = 0.53 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 06:50:58.987942: step 107470, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 06:51:11.231954: step 107480, loss = 0.55 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 06:51:23.456733: step 107490, loss = 0.56 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 06:51:35.663099: step 107500, loss = 0.49 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 06:51:49.836021: step 107510, loss = 0.53 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 06:52:01.973840: step 107520, loss = 0.56 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 06:52:14.152088: step 107530, loss = 0.51 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 06:52:26.421198: step 107540, loss = 0.53 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 06:52:38.679207: step 107550, loss = 0.57 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 06:52:50.891361: step 107560, loss = 0.54 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 06:53:03.178881: step 107570, loss = 0.58 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 06:53:15.364801: step 107580, loss = 0.54 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 06:53:27.625526: step 107590, loss = 0.59 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 06:53:39.861242: step 107600, loss = 0.58 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-22 06:53:54.005561: step 107610, loss = 0.51 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 06:54:06.061164: step 107620, loss = 0.46 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 06:54:18.278962: step 107630, loss = 0.53 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 06:54:30.501120: step 107640, loss = 0.49 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 06:54:42.782020: step 107650, loss = 0.57 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 06:54:55.028431: step 107660, loss = 0.51 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 06:55:07.290681: step 107670, loss = 0.51 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 06:55:19.574483: step 107680, loss = 0.51 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 06:55:31.832126: step 107690, loss = 0.54 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 06:55:44.064587: step 107700, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 06:55:58.253825: step 107710, loss = 0.49 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 06:56:10.344521: step 107720, loss = 0.56 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 06:56:22.465858: step 107730, loss = 0.50 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 06:56:34.588038: step 107740, loss = 0.48 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 06:56:46.792783: step 107750, loss = 0.54 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 06:56:59.002515: step 107760, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 06:57:11.067319: step 107770, loss = 0.49 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 06:57:23.172494: step 107780, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 06:57:35.314676: step 107790, loss = 0.47 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 06:57:47.521724: step 107800, loss = 0.55 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 06:58:01.717364: step 107810, loss = 0.58 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 06:58:13.861636: step 107820, loss = 0.50 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 06:58:25.923875: step 107830, loss = 0.55 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 06:58:38.014670: step 107840, loss = 0.48 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 06:58:50.107136: step 107850, loss = 0.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 06:59:02.131699: step 107860, loss = 0.52 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 06:59:14.181388: step 107870, loss = 0.53 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 06:59:26.158159: step 107880, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 06:59:38.291680: step 107890, loss = 0.54 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 06:59:50.352067: step 107900, loss = 0.55 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 07:00:04.298731: step 107910, loss = 0.51 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 07:00:16.386373: step 107920, loss = 0.53 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 07:00:28.471403: step 107930, loss = 0.56 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 07:00:40.535828: step 107940, loss = 0.57 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 07:00:52.581930: step 107950, loss = 0.55 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 07:01:04.615098: step 107960, loss = 0.56 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 07:01:16.637278: step 107970, loss = 0.51 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 07:01:28.666976: step 107980, loss = 0.50 (24.9 examples/sec; 1.206 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 07:01:40.730149: step 107990, loss = 0.43 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 07:01:52.844310: step 108000, loss = 0.56 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 07:02:07.008242: step 108010, loss = 0.55 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 07:02:19.079570: step 108020, loss = 0.52 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 07:02:31.156211: step 108030, loss = 0.45 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 07:02:43.212927: step 108040, loss = 0.52 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 07:02:55.350508: step 108050, loss = 0.53 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 07:03:07.516221: step 108060, loss = 0.57 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 07:03:19.708205: step 108070, loss = 0.57 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 07:03:31.873364: step 108080, loss = 0.50 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 07:03:43.984603: step 108090, loss = 0.49 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 07:03:56.234974: step 108100, loss = 0.61 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-22 07:04:10.747795: step 108110, loss = 0.54 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 07:04:22.966721: step 108120, loss = 0.57 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 07:04:35.159160: step 108130, loss = 0.47 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 07:04:47.378857: step 108140, loss = 0.56 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 07:04:59.640702: step 108150, loss = 0.58 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 07:05:11.843587: step 108160, loss = 0.51 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 07:05:24.031265: step 108170, loss = 0.44 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 07:05:36.224421: step 108180, loss = 0.46 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 07:05:48.415945: step 108190, loss = 0.42 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 07:06:00.603389: step 108200, loss = 0.49 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 07:06:15.251629: step 108210, loss = 0.47 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 07:06:27.435355: step 108220, loss = 0.50 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 07:06:39.599445: step 108230, loss = 0.58 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 07:06:51.748937: step 108240, loss = 0.46 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 07:07:03.959892: step 108250, loss = 0.49 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 07:07:16.169137: step 108260, loss = 0.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 07:07:28.418413: step 108270, loss = 0.53 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 07:07:40.649031: step 108280, loss = 0.56 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 07:07:52.915064: step 108290, loss = 0.52 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 07:08:05.141562: step 108300, loss = 0.50 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 07:08:19.433515: step 108310, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 07:08:31.564050: step 108320, loss = 0.57 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 07:08:43.798244: step 108330, loss = 0.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 07:08:56.018854: step 108340, loss = 0.53 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 07:09:08.250893: step 108350, loss = 0.49 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 07:09:20.523693: step 108360, loss = 0.50 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 07:09:32.662960: step 108370, loss = 0.53 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 07:09:44.776169: step 108380, loss = 0.56 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 07:09:57.036170: step 108390, loss = 0.53 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 07:10:09.242859: step 108400, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 07:10:23.512614: step 108410, loss = 0.50 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 07:10:35.745042: step 108420, loss = 0.51 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 07:10:47.951917: step 108430, loss = 0.45 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 07:11:00.120897: step 108440, loss = 0.45 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 07:11:12.325128: step 108450, loss = 0.48 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 07:11:24.579603: step 108460, loss = 0.47 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 07:11:36.838128: step 108470, loss = 0.49 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 07:11:49.025590: step 108480, loss = 0.57 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 07:12:01.278531: step 108490, loss = 0.59 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 07:12:13.448826: step 108500, loss = 0.56 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 07:12:27.958410: step 108510, loss = 0.46 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 07:12:40.237685: step 108520, loss = 0.54 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 07:12:52.482480: step 108530, loss = 0.49 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 07:13:04.722314: step 108540, loss = 0.51 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-22 07:13:16.910845: step 108550, loss = 0.50 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 07:13:29.093745: step 108560, loss = 0.60 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 07:13:41.271013: step 108570, loss = 0.53 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 07:13:53.491530: step 108580, loss = 0.47 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 07:14:05.721929: step 108590, loss = 0.62 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 07:14:17.933399: step 108600, loss = 0.51 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 07:14:32.060821: step 108610, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 07:14:44.193301: step 108620, loss = 0.57 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 07:14:56.376036: step 108630, loss = 0.49 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 07:15:08.591169: step 108640, loss = 0.48 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 07:15:20.801448: step 108650, loss = 0.52 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 07:15:32.994546: step 108660, loss = 0.53 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 07:15:45.229351: step 108670, loss = 0.54 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 07:15:57.439660: step 108680, loss = 0.57 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 07:16:09.682602: step 108690, loss = 0.51 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 07:16:21.880107: step 108700, loss = 0.50 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 07:16:36.020091: step 108710, loss = 0.51 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 07:16:48.255455: step 108720, loss = 0.52 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 07:17:00.476039: step 108730, loss = 0.57 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 07:17:12.695922: step 108740, loss = 0.57 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 07:17:24.929824: step 108750, loss = 0.54 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 07:17:37.153365: step 108760, loss = 0.52 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 07:17:49.387878: step 108770, loss = 0.49 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 07:18:01.555335: step 108780, loss = 0.45 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 07:18:13.749654: step 108790, loss = 0.53 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 07:18:25.977964: step 108800, loss = 0.60 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 07:18:40.274278: step 108810, loss = 0.50 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 07:18:52.483851: step 108820, loss = 0.47 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 07:19:04.700840: step 108830, loss = 0.63 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 07:19:16.891556: step 108840, loss = 0.53 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 07:19:29.142030: step 108850, loss = 0.50 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 07:19:41.363286: step 108860, loss = 0.57 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 07:19:53.543911: step 108870, loss = 0.64 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 07:20:05.605228: step 108880, loss = 0.57 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 07:20:17.823493: step 108890, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 07:20:30.030295: step 108900, loss = 0.48 (24.3 examples/sec; 1.232 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 07:20:44.211135: step 108910, loss = 0.48 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 07:20:56.440844: step 108920, loss = 0.57 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-22 07:21:08.620310: step 108930, loss = 0.58 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 07:21:20.872912: step 108940, loss = 0.50 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 07:21:33.121989: step 108950, loss = 0.55 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 07:21:45.364327: step 108960, loss = 0.53 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 07:21:57.616066: step 108970, loss = 0.51 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 07:22:09.834607: step 108980, loss = 0.53 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 07:22:22.038363: step 108990, loss = 0.58 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 07:22:34.209206: step 109000, loss = 0.56 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 07:22:48.537860: step 109010, loss = 0.50 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 07:23:00.738825: step 109020, loss = 0.57 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 07:23:12.940918: step 109030, loss = 0.54 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 07:23:25.141399: step 109040, loss = 0.62 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 07:23:37.307174: step 109050, loss = 0.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 07:23:49.411659: step 109060, loss = 0.49 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 07:24:01.630127: step 109070, loss = 0.64 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 07:24:13.858960: step 109080, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 07:24:26.059857: step 109090, loss = 0.60 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 07:24:38.207925: step 109100, loss = 0.47 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 07:24:52.355067: step 109110, loss = 0.57 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 07:25:04.462148: step 109120, loss = 0.56 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 07:25:16.583135: step 109130, loss = 0.50 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 07:25:28.770108: step 109140, loss = 0.43 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 07:25:40.900017: step 109150, loss = 0.53 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 07:25:53.108139: step 109160, loss = 0.53 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 07:26:05.329045: step 109170, loss = 0.55 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 07:26:17.521070: step 109180, loss = 0.49 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 07:26:29.782456: step 109190, loss = 0.54 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 07:26:41.941273: step 109200, loss = 0.51 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 07:26:56.105037: step 109210, loss = 0.53 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 07:27:08.290453: step 109220, loss = 0.47 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 07:27:20.545926: step 109230, loss = 0.59 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 07:27:32.730498: step 109240, loss = 0.57 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 07:27:44.972557: step 109250, loss = 0.49 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 07:27:57.191278: step 109260, loss = 0.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 07:28:09.427075: step 109270, loss = 0.56 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 07:28:21.675211: step 109280, loss = 0.52 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 07:28:33.860401: step 109290, loss = 0.52 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 07:28:46.061790: step 109300, loss = 0.56 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 07:29:00.366202: step 109310, loss = 0.45 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 07:29:12.525300: step 109320, loss = 0.50 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 07:29:24.711971: step 109330, loss = 0.54 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 07:29:36.874859: step 109340, loss = 0.56 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 07:29:48.938471: step 109350, loss = 0.46 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 07:30:00.948835: step 109360, loss = 0.51 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 07:30:12.987326: step 109370, loss = 0.55 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 07:30:25.002560: step 109380, loss = 0.54 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 07:30:37.118592: step 109390, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 07:30:49.195073: step 109400, loss = 0.45 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 07:31:03.662252: step 109410, loss = 0.52 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 07:31:15.737243: step 109420, loss = 0.55 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 07:31:27.837903: step 109430, loss = 0.50 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-22 07:31:39.944152: step 109440, loss = 0.54 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 07:31:52.103702: step 109450, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 07:32:04.278812: step 109460, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 07:32:16.478542: step 109470, loss = 0.53 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 07:32:28.666704: step 109480, loss = 0.57 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-22 07:32:40.834674: step 109490, loss = 0.52 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 07:32:53.039349: step 109500, loss = 0.53 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 07:33:07.129235: step 109510, loss = 0.50 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 07:33:19.366072: step 109520, loss = 0.50 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 07:33:31.628041: step 109530, loss = 0.50 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-22 07:33:43.898593: step 109540, loss = 0.53 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 07:33:56.093324: step 109550, loss = 0.54 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 07:34:08.296267: step 109560, loss = 0.52 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 07:34:20.555639: step 109570, loss = 0.54 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 07:34:32.800065: step 109580, loss = 0.51 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 07:34:45.044462: step 109590, loss = 0.59 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 07:34:57.233803: step 109600, loss = 0.49 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 07:35:11.378014: step 109610, loss = 0.55 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 07:35:23.571239: step 109620, loss = 0.51 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 07:35:35.630269: step 109630, loss = 0.58 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 07:35:47.838911: step 109640, loss = 0.54 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 07:36:00.028670: step 109650, loss = 0.53 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 07:36:12.317710: step 109660, loss = 0.54 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 07:36:24.548420: step 109670, loss = 0.55 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 07:36:36.772050: step 109680, loss = 0.54 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 07:36:48.947045: step 109690, loss = 0.51 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 07:37:01.148058: step 109700, loss = 0.49 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 07:37:15.704932: step 109710, loss = 0.56 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 07:37:27.823973: step 109720, loss = 0.57 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 07:37:39.950795: step 109730, loss = 0.46 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 07:37:52.081849: step 109740, loss = 0.53 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 07:38:04.237432: step 109750, loss = 0.45 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 07:38:16.315183: step 109760, loss = 0.48 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 07:38:28.390727: step 109770, loss = 0.61 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 07:38:40.485972: step 109780, loss = 0.47 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 07:38:52.613607: step 109790, loss = 0.46 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 07:39:04.709454: step 109800, loss = 0.46 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 07:39:18.950799: step 109810, loss = 0.63 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 07:39:31.124681: step 109820, loss = 0.62 (25.0 examples/sec; 1.202 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 07:39:43.270361: step 109830, loss = 0.56 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 07:39:55.530675: step 109840, loss = 0.61 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 07:40:07.652524: step 109850, loss = 0.46 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 07:40:19.787874: step 109860, loss = 0.46 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 07:40:31.940679: step 109870, loss = 0.51 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 07:40:44.023768: step 109880, loss = 0.54 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 07:40:56.213457: step 109890, loss = 0.54 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 07:41:08.285867: step 109900, loss = 0.60 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 07:41:22.505963: step 109910, loss = 0.49 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 07:41:34.748702: step 109920, loss = 0.51 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-22 07:41:46.827561: step 109930, loss = 0.53 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 07:41:59.015015: step 109940, loss = 0.47 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 07:42:11.166359: step 109950, loss = 0.56 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 07:42:23.293257: step 109960, loss = 0.54 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 07:42:35.479877: step 109970, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 07:42:47.716371: step 109980, loss = 0.56 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 07:42:59.926549: step 109990, loss = 0.53 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 07:43:12.167388: step 110000, loss = 0.49 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 07:43:29.762195: step 110010, loss = 0.46 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 07:43:41.993553: step 110020, loss = 0.57 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 07:43:54.171367: step 110030, loss = 0.51 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 07:44:06.425995: step 110040, loss = 0.54 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 07:44:18.581637: step 110050, loss = 0.50 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 07:44:30.756621: step 110060, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 07:44:42.977462: step 110070, loss = 0.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 07:44:55.200865: step 110080, loss = 0.51 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 07:45:07.382559: step 110090, loss = 0.60 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 07:45:19.641609: step 110100, loss = 0.53 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 07:45:34.021230: step 110110, loss = 0.56 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 07:45:46.220751: step 110120, loss = 0.53 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 07:45:58.300191: step 110130, loss = 0.52 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 07:46:10.545130: step 110140, loss = 0.64 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 07:46:22.708767: step 110150, loss = 0.56 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 07:46:34.940105: step 110160, loss = 0.53 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 07:46:47.195369: step 110170, loss = 0.49 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 07:46:59.399489: step 110180, loss = 0.49 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 07:47:11.567534: step 110190, loss = 0.53 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 07:47:23.781438: step 110200, loss = 0.54 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 07:47:38.359073: step 110210, loss = 0.58 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 07:47:50.610323: step 110220, loss = 0.55 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 07:48:02.835062: step 110230, loss = 0.57 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 07:48:15.099389: step 110240, loss = 0.49 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 07:48:27.311803: step 110250, loss = 0.65 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 07:48:39.517166: step 110260, loss = 0.45 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 07:48:51.765635: step 110270, loss = 0.52 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 07:49:04.006962: step 110280, loss = 0.46 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 07:49:16.177333: step 110290, loss = 0.54 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 07:49:28.400868: step 110300, loss = 0.52 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 07:49:42.599073: step 110310, loss = 0.51 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 07:49:54.837544: step 110320, loss = 0.52 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 07:50:07.070260: step 110330, loss = 0.54 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 07:50:19.312718: step 110340, loss = 0.46 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 07:50:31.553503: step 110350, loss = 0.56 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 07:50:43.770538: step 110360, loss = 0.62 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 07:50:55.990053: step 110370, loss = 0.68 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 07:51:08.041551: step 110380, loss = 0.48 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 07:51:20.233874: step 110390, loss = 0.53 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 07:51:32.492797: step 110400, loss = 0.49 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 07:51:46.600071: step 110410, loss = 0.56 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 07:51:58.808236: step 110420, loss = 0.49 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 07:52:10.970817: step 110430, loss = 0.53 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 07:52:23.169806: step 110440, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 07:52:35.397463: step 110450, loss = 0.45 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 07:52:47.635215: step 110460, loss = 0.46 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 07:52:59.860390: step 110470, loss = 0.51 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 07:53:12.050834: step 110480, loss = 0.44 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 07:53:24.251181: step 110490, loss = 0.54 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 07:53:36.486301: step 110500, loss = 0.47 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 07:53:50.672646: step 110510, loss = 0.61 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 07:54:02.861426: step 110520, loss = 0.60 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 07:54:15.106258: step 110530, loss = 0.55 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 07:54:27.273340: step 110540, loss = 0.52 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 07:54:39.501435: step 110550, loss = 0.49 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 07:54:51.687883: step 110560, loss = 0.59 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 07:55:03.922156: step 110570, loss = 0.53 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 07:55:16.197145: step 110580, loss = 0.59 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 07:55:28.416720: step 110590, loss = 0.45 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 07:55:40.671081: step 110600, loss = 0.54 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 07:55:54.901551: step 110610, loss = 0.58 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 07:56:06.998439: step 110620, loss = 0.51 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 07:56:19.100846: step 110630, loss = 0.49 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 07:56:31.295202: step 110640, loss = 0.64 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 07:56:43.549852: step 110650, loss = 0.53 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 07:56:55.771406: step 110660, loss = 0.51 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 07:57:07.951529: step 110670, loss = 0.50 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 07:57:20.182003: step 110680, loss = 0.48 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 07:57:32.374980: step 110690, loss = 0.51 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 07:57:44.570863: step 110700, loss = 0.50 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 07:57:58.729164: step 110710, loss = 0.57 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 07:58:10.925501: step 110720, loss = 0.44 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 07:58:23.117819: step 110730, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 07:58:35.330964: step 110740, loss = 0.53 (24.7 examples/sec; 1.212 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 07:58:47.531347: step 110750, loss = 0.56 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-22 07:58:59.759992: step 110760, loss = 0.53 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 07:59:11.958473: step 110770, loss = 0.51 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 07:59:24.173730: step 110780, loss = 0.55 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 07:59:36.368123: step 110790, loss = 0.53 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 07:59:48.594569: step 110800, loss = 0.47 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 08:00:02.681432: step 110810, loss = 0.52 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 08:00:14.891810: step 110820, loss = 0.49 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 08:00:27.147870: step 110830, loss = 0.57 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 08:00:39.367288: step 110840, loss = 0.42 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 08:00:51.566335: step 110850, loss = 0.49 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 08:01:03.795277: step 110860, loss = 0.55 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 08:01:15.957462: step 110870, loss = 0.53 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 08:01:28.008844: step 110880, loss = 0.54 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 08:01:40.262426: step 110890, loss = 0.56 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 08:01:52.451657: step 110900, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 08:02:06.474901: step 110910, loss = 0.58 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 08:02:18.707021: step 110920, loss = 0.53 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 08:02:30.916723: step 110930, loss = 0.49 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 08:02:43.106190: step 110940, loss = 0.46 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 08:02:55.338942: step 110950, loss = 0.50 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 08:03:07.515644: step 110960, loss = 0.51 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 08:03:19.728133: step 110970, loss = 0.54 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 08:03:31.986550: step 110980, loss = 0.50 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 08:03:44.148135: step 110990, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 08:03:56.359047: step 111000, loss = 0.54 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 08:04:10.585537: step 111010, loss = 0.54 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 08:04:22.767739: step 111020, loss = 0.49 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 08:04:34.926297: step 111030, loss = 0.61 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 08:04:47.145193: step 111040, loss = 0.47 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 08:04:59.365577: step 111050, loss = 0.51 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 08:05:11.589195: step 111060, loss = 0.56 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 08:05:23.836109: step 111070, loss = 0.59 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-22 08:05:36.043301: step 111080, loss = 0.49 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 08:05:48.265900: step 111090, loss = 0.58 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 08:06:00.454974: step 111100, loss = 0.60 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 08:06:14.657613: step 111110, loss = 0.50 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 08:06:26.886547: step 111120, loss = 0.48 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 08:06:38.932116: step 111130, loss = 0.54 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 08:06:51.161966: step 111140, loss = 0.55 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 08:07:03.358025: step 111150, loss = 0.55 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 08:07:15.597734: step 111160, loss = 0.56 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 08:07:27.802131: step 111170, loss = 0.46 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 08:07:39.951846: step 111180, loss = 0.50 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 08:07:52.203396: step 111190, loss = 0.61 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 08:08:04.409232: step 111200, loss = 0.53 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 08:08:18.761697: step 111210, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 08:08:30.931649: step 111220, loss = 0.61 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 08:08:43.174889: step 111230, loss = 0.48 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 08:08:55.356797: step 111240, loss = 0.48 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 08:09:07.561086: step 111250, loss = 0.54 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 08:09:19.750618: step 111260, loss = 0.49 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 08:09:31.930447: step 111270, loss = 0.51 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 08:09:44.097855: step 111280, loss = 0.52 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 08:09:56.313497: step 111290, loss = 0.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 08:10:08.542658: step 111300, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 08:10:22.739677: step 111310, loss = 0.58 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 08:10:34.953312: step 111320, loss = 0.56 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-22 08:10:47.137371: step 111330, loss = 0.65 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 08:10:59.370383: step 111340, loss = 0.59 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 08:11:11.628872: step 111350, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 08:11:23.837558: step 111360, loss = 0.52 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 08:11:36.069161: step 111370, loss = 0.49 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 08:11:48.140617: step 111380, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 08:12:00.291191: step 111390, loss = 0.51 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 08:12:12.509637: step 111400, loss = 0.47 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 08:12:27.178029: step 111410, loss = 0.57 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 08:12:39.378423: step 111420, loss = 0.52 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 08:12:51.557929: step 111430, loss = 0.49 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 08:13:03.765933: step 111440, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 08:13:15.960969: step 111450, loss = 0.46 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 08:13:28.113699: step 111460, loss = 0.57 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 08:13:40.266135: step 111470, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 08:13:52.442791: step 111480, loss = 0.51 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 08:14:04.672498: step 111490, loss = 0.54 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 08:14:16.905059: step 111500, loss = 0.44 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 08:14:31.413311: step 111510, loss = 0.58 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 08:14:43.511924: step 111520, loss = 0.51 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 08:14:55.658224: step 111530, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 08:15:07.848571: step 111540, loss = 0.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 08:15:19.979421: step 111550, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 08:15:32.149411: step 111560, loss = 0.60 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 08:15:44.329833: step 111570, loss = 0.45 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 08:15:56.516935: step 111580, loss = 0.53 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 08:16:08.688901: step 111590, loss = 0.52 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 08:16:20.905588: step 111600, loss = 0.53 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 08:16:35.205075: step 111610, loss = 0.54 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 08:16:47.334347: step 111620, loss = 0.47 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 08:16:59.418255: step 111630, loss = 0.49 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 08:17:11.558740: step 111640, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 08:17:23.752272: step 111650, loss = 0.46 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 08:17:35.943206: step 111660, loss = 0.55 (24.5 examples/sec; 1.222 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 08:17:48.167990: step 111670, loss = 0.49 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 08:18:00.351266: step 111680, loss = 0.48 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 08:18:12.500260: step 111690, loss = 0.58 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 08:18:24.669340: step 111700, loss = 0.58 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 08:18:38.766593: step 111710, loss = 0.45 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 08:18:50.951079: step 111720, loss = 0.52 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 08:19:03.139329: step 111730, loss = 0.50 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 08:19:15.298649: step 111740, loss = 0.53 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 08:19:27.511410: step 111750, loss = 0.54 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 08:19:39.661978: step 111760, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 08:19:51.770618: step 111770, loss = 0.49 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 08:20:03.967118: step 111780, loss = 0.53 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 08:20:16.169768: step 111790, loss = 0.55 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 08:20:28.372838: step 111800, loss = 0.52 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 08:20:42.599788: step 111810, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 08:20:54.779463: step 111820, loss = 0.56 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 08:21:07.004791: step 111830, loss = 0.57 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 08:21:19.186089: step 111840, loss = 0.60 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 08:21:31.381405: step 111850, loss = 0.45 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 08:21:43.562179: step 111860, loss = 0.53 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 08:21:55.716440: step 111870, loss = 0.42 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 08:22:07.788481: step 111880, loss = 0.51 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 08:22:19.924550: step 111890, loss = 0.56 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 08:22:32.187997: step 111900, loss = 0.54 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 08:22:46.351394: step 111910, loss = 0.59 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 08:22:58.501689: step 111920, loss = 0.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 08:23:10.700183: step 111930, loss = 0.57 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 08:23:22.849955: step 111940, loss = 0.59 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 08:23:35.027807: step 111950, loss = 0.54 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 08:23:47.160970: step 111960, loss = 0.54 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 08:23:59.355786: step 111970, loss = 0.47 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 08:24:11.616124: step 111980, loss = 0.53 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 08:24:23.805314: step 111990, loss = 0.45 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 08:24:35.967365: step 112000, loss = 0.43 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 08:24:50.167409: step 112010, loss = 0.53 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 08:25:02.319919: step 112020, loss = 0.51 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 08:25:14.504575: step 112030, loss = 0.51 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 08:25:26.719191: step 112040, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 08:25:38.874482: step 112050, loss = 0.57 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 08:25:51.086302: step 112060, loss = 0.50 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 08:26:03.240815: step 112070, loss = 0.55 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 08:26:15.432212: step 112080, loss = 0.41 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 08:26:27.587857: step 112090, loss = 0.49 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 08:26:39.818609: step 112100, loss = 0.53 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 08:26:54.151537: step 112110, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 08:27:06.343939: step 112120, loss = 0.60 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 08:27:18.370332: step 112130, loss = 0.48 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 08:27:30.550413: step 112140, loss = 0.51 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 08:27:42.777143: step 112150, loss = 0.58 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 08:27:54.961466: step 112160, loss = 0.47 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 08:28:07.100118: step 112170, loss = 0.53 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 08:28:19.343505: step 112180, loss = 0.54 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 08:28:31.534806: step 112190, loss = 0.55 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 08:28:43.700679: step 112200, loss = 0.44 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 08:28:57.770819: step 112210, loss = 0.54 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 08:29:09.907706: step 112220, loss = 0.60 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 08:29:22.075311: step 112230, loss = 0.61 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 08:29:34.242862: step 112240, loss = 0.56 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 08:29:46.433699: step 112250, loss = 0.50 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 08:29:58.601729: step 112260, loss = 0.41 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 08:30:10.737844: step 112270, loss = 0.53 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 08:30:22.923769: step 112280, loss = 0.48 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 08:30:35.138406: step 112290, loss = 0.47 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 08:30:47.336928: step 112300, loss = 0.50 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 08:31:01.453740: step 112310, loss = 0.44 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 08:31:13.667023: step 112320, loss = 0.53 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 08:31:25.879696: step 112330, loss = 0.54 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 08:31:38.073890: step 112340, loss = 0.49 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 08:31:50.249542: step 112350, loss = 0.53 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 08:32:02.441982: step 112360, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 08:32:14.644875: step 112370, loss = 0.57 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 08:32:26.719265: step 112380, loss = 0.54 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 08:32:38.874069: step 112390, loss = 0.45 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 08:32:51.024173: step 112400, loss = 0.47 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 08:33:05.497043: step 112410, loss = 0.52 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 08:33:17.607024: step 112420, loss = 0.50 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 08:33:29.748503: step 112430, loss = 0.50 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 08:33:41.975118: step 112440, loss = 0.54 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 08:33:54.161450: step 112450, loss = 0.56 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 08:34:06.321905: step 112460, loss = 0.56 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 08:34:18.510641: step 112470, loss = 0.47 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 08:34:30.697722: step 112480, loss = 0.50 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 08:34:42.880036: step 112490, loss = 0.53 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 08:34:55.087260: step 112500, loss = 0.46 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 08:35:09.328977: step 112510, loss = 0.59 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 08:35:21.495572: step 112520, loss = 0.59 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 08:35:33.683860: step 112530, loss = 0.51 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 08:35:45.876172: step 112540, loss = 0.48 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 08:35:58.089671: step 112550, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 08:36:10.280460: step 112560, loss = 0.55 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 08:36:22.483638: step 112570, loss = 0.61 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 08:36:34.666123: step 112580, loss = 0.52 (24.3 examples/sec; 1.234 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 08:36:46.839554: step 112590, loss = 0.60 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 08:36:58.988243: step 112600, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 08:37:13.064464: step 112610, loss = 0.59 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 08:37:25.218069: step 112620, loss = 0.53 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 08:37:37.334586: step 112630, loss = 0.50 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 08:37:49.501464: step 112640, loss = 0.48 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 08:38:01.666469: step 112650, loss = 0.53 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 08:38:13.792271: step 112660, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 08:38:25.864114: step 112670, loss = 0.51 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 08:38:37.866959: step 112680, loss = 0.53 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 08:38:49.838201: step 112690, loss = 0.56 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 08:39:01.912174: step 112700, loss = 0.50 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 08:39:15.867262: step 112710, loss = 0.55 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 08:39:27.987529: step 112720, loss = 0.49 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 08:39:40.119891: step 112730, loss = 0.50 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 08:39:52.201759: step 112740, loss = 0.49 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 08:40:04.242304: step 112750, loss = 0.49 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 08:40:16.201327: step 112760, loss = 0.55 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 08:40:28.261331: step 112770, loss = 0.60 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 08:40:40.300229: step 112780, loss = 0.64 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 08:40:52.400245: step 112790, loss = 0.43 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 08:41:04.500143: step 112800, loss = 0.49 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 08:41:18.519890: step 112810, loss = 0.49 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 08:41:30.678575: step 112820, loss = 0.44 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 08:41:42.796636: step 112830, loss = 0.59 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 08:41:54.880768: step 112840, loss = 0.57 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 08:42:07.026909: step 112850, loss = 0.48 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 08:42:19.232535: step 112860, loss = 0.50 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 08:42:31.298879: step 112870, loss = 0.47 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 08:42:43.274002: step 112880, loss = 0.50 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 08:42:55.196252: step 112890, loss = 0.57 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 08:43:07.165145: step 112900, loss = 0.50 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 08:43:21.221667: step 112910, loss = 0.53 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-22 08:43:33.142587: step 112920, loss = 0.44 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 08:43:45.118580: step 112930, loss = 0.50 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 08:43:57.128299: step 112940, loss = 0.46 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 08:44:09.096492: step 112950, loss = 0.47 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-22 08:44:21.037640: step 112960, loss = 0.51 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 08:44:32.988380: step 112970, loss = 0.56 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 08:44:45.003881: step 112980, loss = 0.52 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-22 08:44:56.959926: step 112990, loss = 0.48 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 08:45:08.901054: step 113000, loss = 0.50 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 08:45:23.161380: step 113010, loss = 0.48 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 08:45:35.152520: step 113020, loss = 0.55 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 08:45:47.134019: step 113030, loss = 0.50 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 08:45:59.090755: step 113040, loss = 0.55 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 08:46:11.096776: step 113050, loss = 0.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 08:46:23.057806: step 113060, loss = 0.50 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 08:46:35.007235: step 113070, loss = 0.53 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 08:46:46.915051: step 113080, loss = 0.55 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 08:46:58.849002: step 113090, loss = 0.53 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 08:47:10.859893: step 113100, loss = 0.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 08:47:24.708540: step 113110, loss = 0.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 08:47:36.670595: step 113120, loss = 0.49 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 08:47:48.670533: step 113130, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 08:48:00.610492: step 113140, loss = 0.51 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 08:48:12.613271: step 113150, loss = 0.46 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 08:48:24.569169: step 113160, loss = 0.47 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 08:48:36.519667: step 113170, loss = 0.59 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 08:48:48.518867: step 113180, loss = 0.45 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 08:49:00.457050: step 113190, loss = 0.51 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 08:49:12.444474: step 113200, loss = 0.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 08:49:26.458722: step 113210, loss = 0.54 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 08:49:38.538371: step 113220, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 08:49:50.582877: step 113230, loss = 0.60 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 08:50:02.610651: step 113240, loss = 0.58 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 08:50:14.575627: step 113250, loss = 0.58 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 08:50:26.478599: step 113260, loss = 0.48 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 08:50:38.521042: step 113270, loss = 0.54 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 08:50:50.541314: step 113280, loss = 0.54 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 08:51:02.524293: step 113290, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 08:51:14.507325: step 113300, loss = 0.53 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 08:51:28.457941: step 113310, loss = 0.54 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 08:51:40.409924: step 113320, loss = 0.52 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 08:51:52.401112: step 113330, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 08:52:04.350069: step 113340, loss = 0.61 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 08:52:16.333231: step 113350, loss = 0.49 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 08:52:28.308737: step 113360, loss = 0.60 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 08:52:40.281944: step 113370, loss = 0.50 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 08:52:52.246873: step 113380, loss = 0.61 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 08:53:04.184596: step 113390, loss = 0.53 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-22 08:53:16.092132: step 113400, loss = 0.53 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-22 08:53:29.864164: step 113410, loss = 0.50 (25.3 examples/sec; 1.183 sec/batch)\n",
      "2019-05-22 08:53:41.874372: step 113420, loss = 0.48 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 08:53:53.839393: step 113430, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 08:54:05.787434: step 113440, loss = 0.50 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 08:54:17.741363: step 113450, loss = 0.53 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 08:54:29.752118: step 113460, loss = 0.63 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 08:54:41.784096: step 113470, loss = 0.53 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 08:54:53.918163: step 113480, loss = 0.48 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 08:55:06.004466: step 113490, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 08:55:18.099658: step 113500, loss = 0.53 (24.7 examples/sec; 1.217 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 08:55:32.212234: step 113510, loss = 0.49 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 08:55:44.232773: step 113520, loss = 0.55 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 08:55:56.227239: step 113530, loss = 0.59 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 08:56:08.091494: step 113540, loss = 0.55 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 08:56:20.045170: step 113550, loss = 0.48 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 08:56:31.989975: step 113560, loss = 0.50 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 08:56:43.949178: step 113570, loss = 0.50 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 08:56:55.909658: step 113580, loss = 0.62 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 08:57:07.827281: step 113590, loss = 0.60 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 08:57:19.820550: step 113600, loss = 0.56 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 08:57:33.856572: step 113610, loss = 0.51 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 08:57:45.888101: step 113620, loss = 0.57 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 08:57:57.860547: step 113630, loss = 0.47 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 08:58:09.826963: step 113640, loss = 0.45 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 08:58:21.795321: step 113650, loss = 0.53 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-22 08:58:33.690910: step 113660, loss = 0.51 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 08:58:45.630453: step 113670, loss = 0.51 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 08:58:57.581102: step 113680, loss = 0.55 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-22 08:59:09.530585: step 113690, loss = 0.51 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 08:59:21.486301: step 113700, loss = 0.50 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 08:59:35.483487: step 113710, loss = 0.58 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 08:59:47.487586: step 113720, loss = 0.61 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 08:59:59.454748: step 113730, loss = 0.48 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 09:00:11.407955: step 113740, loss = 0.47 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 09:00:23.372386: step 113750, loss = 0.57 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 09:00:35.288938: step 113760, loss = 0.53 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 09:00:47.204384: step 113770, loss = 0.57 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 09:00:59.205319: step 113780, loss = 0.52 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-22 09:01:11.158288: step 113790, loss = 0.56 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 09:01:23.082589: step 113800, loss = 0.52 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 09:01:36.964392: step 113810, loss = 0.48 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 09:01:48.872554: step 113820, loss = 0.51 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 09:02:00.861501: step 113830, loss = 0.50 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 09:02:12.811528: step 113840, loss = 0.61 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 09:02:24.780005: step 113850, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 09:02:36.769163: step 113860, loss = 0.43 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 09:02:48.868873: step 113870, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 09:03:00.965113: step 113880, loss = 0.57 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 09:03:13.067278: step 113890, loss = 0.55 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 09:03:25.166115: step 113900, loss = 0.51 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 09:03:39.169209: step 113910, loss = 0.42 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 09:03:51.248670: step 113920, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 09:04:03.331068: step 113930, loss = 0.47 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 09:04:15.394138: step 113940, loss = 0.56 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 09:04:27.460220: step 113950, loss = 0.51 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 09:04:39.409676: step 113960, loss = 0.49 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 09:04:51.396063: step 113970, loss = 0.50 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 09:05:03.354802: step 113980, loss = 0.44 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 09:05:15.354582: step 113990, loss = 0.58 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 09:05:27.280947: step 114000, loss = 0.51 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 09:05:41.136249: step 114010, loss = 0.52 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 09:05:53.083904: step 114020, loss = 0.55 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 09:06:05.030117: step 114030, loss = 0.55 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 09:06:17.020111: step 114040, loss = 0.44 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 09:06:28.965002: step 114050, loss = 0.55 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 09:06:40.933606: step 114060, loss = 0.50 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 09:06:52.901542: step 114070, loss = 0.48 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 09:07:04.814880: step 114080, loss = 0.49 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 09:07:16.746562: step 114090, loss = 0.54 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 09:07:28.707058: step 114100, loss = 0.51 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 09:07:42.798983: step 114110, loss = 0.48 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 09:07:54.749703: step 114120, loss = 0.58 (25.5 examples/sec; 1.179 sec/batch)\n",
      "2019-05-22 09:08:06.723177: step 114130, loss = 0.51 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 09:08:18.693112: step 114140, loss = 0.50 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 09:08:30.618641: step 114150, loss = 0.53 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 09:08:42.536891: step 114160, loss = 0.51 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 09:08:54.515728: step 114170, loss = 0.56 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 09:09:06.475946: step 114180, loss = 0.65 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 09:09:18.491607: step 114190, loss = 0.51 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 09:09:30.524766: step 114200, loss = 0.51 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 09:09:44.655663: step 114210, loss = 0.52 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 09:09:56.713643: step 114220, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 09:10:08.723878: step 114230, loss = 0.58 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 09:10:20.749870: step 114240, loss = 0.51 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 09:10:32.814092: step 114250, loss = 0.46 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 09:10:44.794108: step 114260, loss = 0.52 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 09:10:56.857963: step 114270, loss = 0.54 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 09:11:08.891476: step 114280, loss = 0.58 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 09:11:20.937296: step 114290, loss = 0.53 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 09:11:32.953435: step 114300, loss = 0.46 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 09:11:46.965634: step 114310, loss = 0.48 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 09:11:58.984675: step 114320, loss = 0.47 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 09:12:11.033279: step 114330, loss = 0.59 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 09:12:23.056860: step 114340, loss = 0.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 09:12:35.063828: step 114350, loss = 0.53 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 09:12:47.087069: step 114360, loss = 0.47 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 09:12:59.163609: step 114370, loss = 0.54 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 09:13:11.230163: step 114380, loss = 0.56 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 09:13:23.267535: step 114390, loss = 0.45 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 09:13:35.303356: step 114400, loss = 0.44 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 09:13:49.189427: step 114410, loss = 0.47 (25.4 examples/sec; 1.179 sec/batch)\n",
      "2019-05-22 09:14:01.169618: step 114420, loss = 0.49 (24.8 examples/sec; 1.208 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 09:14:13.192049: step 114430, loss = 0.44 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 09:14:25.263094: step 114440, loss = 0.55 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 09:14:37.286486: step 114450, loss = 0.59 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-22 09:14:49.359173: step 114460, loss = 0.53 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 09:15:01.399397: step 114470, loss = 0.54 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 09:15:13.440742: step 114480, loss = 0.64 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 09:15:25.477333: step 114490, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 09:15:37.523469: step 114500, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 09:15:51.505500: step 114510, loss = 0.57 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 09:16:03.497384: step 114520, loss = 0.54 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 09:16:15.373631: step 114530, loss = 0.43 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 09:16:27.381438: step 114540, loss = 0.54 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 09:16:39.415754: step 114550, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 09:16:51.472691: step 114560, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 09:17:03.586288: step 114570, loss = 0.50 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 09:17:15.676274: step 114580, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 09:17:27.874995: step 114590, loss = 0.51 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 09:17:40.017048: step 114600, loss = 0.63 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 09:17:54.529816: step 114610, loss = 0.58 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 09:18:06.679735: step 114620, loss = 0.55 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 09:18:18.832829: step 114630, loss = 0.51 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 09:18:31.035108: step 114640, loss = 0.53 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 09:18:43.224567: step 114650, loss = 0.43 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 09:18:55.382395: step 114660, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 09:19:07.424475: step 114670, loss = 0.52 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 09:19:19.613116: step 114680, loss = 0.49 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 09:19:31.842998: step 114690, loss = 0.52 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 09:19:44.115935: step 114700, loss = 0.53 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 09:19:58.185842: step 114710, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 09:20:10.379924: step 114720, loss = 0.54 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 09:20:22.550735: step 114730, loss = 0.53 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 09:20:34.671939: step 114740, loss = 0.46 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 09:20:46.768545: step 114750, loss = 0.57 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 09:20:58.787406: step 114760, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 09:21:10.831693: step 114770, loss = 0.49 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 09:21:22.950014: step 114780, loss = 0.60 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 09:21:35.011734: step 114790, loss = 0.58 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 09:21:47.176881: step 114800, loss = 0.53 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 09:22:01.651553: step 114810, loss = 0.55 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 09:22:13.746161: step 114820, loss = 0.46 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 09:22:25.825636: step 114830, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 09:22:37.903808: step 114840, loss = 0.53 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 09:22:49.934409: step 114850, loss = 0.56 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 09:23:01.995696: step 114860, loss = 0.61 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 09:23:14.055194: step 114870, loss = 0.54 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 09:23:26.059925: step 114880, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 09:23:37.996113: step 114890, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 09:23:49.981873: step 114900, loss = 0.49 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 09:24:03.895057: step 114910, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 09:24:15.827156: step 114920, loss = 0.47 (25.5 examples/sec; 1.174 sec/batch)\n",
      "2019-05-22 09:24:27.789546: step 114930, loss = 0.50 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 09:24:39.783043: step 114940, loss = 0.50 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 09:24:51.740562: step 114950, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 09:25:03.774096: step 114960, loss = 0.59 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 09:25:15.763174: step 114970, loss = 0.50 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 09:25:27.766587: step 114980, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 09:25:39.809149: step 114990, loss = 0.43 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 09:25:51.818528: step 115000, loss = 0.48 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 09:26:09.486847: step 115010, loss = 0.54 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 09:26:21.535311: step 115020, loss = 0.51 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 09:26:33.492085: step 115030, loss = 0.50 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 09:26:45.504136: step 115040, loss = 0.43 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 09:26:57.507574: step 115050, loss = 0.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 09:27:09.538778: step 115060, loss = 0.56 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 09:27:21.540526: step 115070, loss = 0.58 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 09:27:33.547810: step 115080, loss = 0.56 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 09:27:45.586445: step 115090, loss = 0.41 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 09:27:57.607404: step 115100, loss = 0.52 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 09:28:11.671178: step 115110, loss = 0.56 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 09:28:23.697429: step 115120, loss = 0.51 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 09:28:35.732891: step 115130, loss = 0.44 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 09:28:47.732329: step 115140, loss = 0.50 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 09:28:59.848277: step 115150, loss = 0.48 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 09:29:11.926739: step 115160, loss = 0.44 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 09:29:23.877267: step 115170, loss = 0.50 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 09:29:35.883709: step 115180, loss = 0.50 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 09:29:47.951362: step 115190, loss = 0.51 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 09:30:00.009985: step 115200, loss = 0.55 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 09:30:14.383495: step 115210, loss = 0.59 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 09:30:26.430578: step 115220, loss = 0.61 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 09:30:38.480908: step 115230, loss = 0.53 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 09:30:50.549954: step 115240, loss = 0.54 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 09:31:02.586741: step 115250, loss = 0.51 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 09:31:14.582800: step 115260, loss = 0.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 09:31:26.646352: step 115270, loss = 0.53 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 09:31:38.755448: step 115280, loss = 0.49 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 09:31:50.848362: step 115290, loss = 0.49 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 09:32:02.949135: step 115300, loss = 0.46 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 09:32:17.107710: step 115310, loss = 0.46 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 09:32:29.163080: step 115320, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 09:32:41.260589: step 115330, loss = 0.56 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 09:32:53.334923: step 115340, loss = 0.52 (25.0 examples/sec; 1.201 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 09:33:05.429590: step 115350, loss = 0.47 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 09:33:17.508959: step 115360, loss = 0.54 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 09:33:29.556474: step 115370, loss = 0.49 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 09:33:41.633801: step 115380, loss = 0.49 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 09:33:53.733592: step 115390, loss = 0.58 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 09:34:05.817397: step 115400, loss = 0.51 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 09:34:19.913853: step 115410, loss = 0.48 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 09:34:31.926105: step 115420, loss = 0.56 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 09:34:43.944902: step 115430, loss = 0.50 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 09:34:56.256740: step 115440, loss = 0.52 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-22 09:35:08.540192: step 115450, loss = 0.55 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 09:35:20.653986: step 115460, loss = 0.49 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 09:35:32.741297: step 115470, loss = 0.47 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 09:35:44.743034: step 115480, loss = 0.54 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 09:35:56.737458: step 115490, loss = 0.58 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 09:36:08.777302: step 115500, loss = 0.62 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 09:36:22.972546: step 115510, loss = 0.56 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 09:36:35.017398: step 115520, loss = 0.45 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 09:36:47.070574: step 115530, loss = 0.54 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 09:36:59.067885: step 115540, loss = 0.49 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 09:37:11.147752: step 115550, loss = 0.47 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 09:37:23.173412: step 115560, loss = 0.53 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 09:37:35.169266: step 115570, loss = 0.56 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 09:37:47.214077: step 115580, loss = 0.52 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 09:37:59.195864: step 115590, loss = 0.45 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 09:38:11.218714: step 115600, loss = 0.59 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 09:38:25.524916: step 115610, loss = 0.48 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 09:38:37.553431: step 115620, loss = 0.40 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 09:38:49.558741: step 115630, loss = 0.54 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 09:39:01.633317: step 115640, loss = 0.49 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 09:39:13.674823: step 115650, loss = 0.60 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 09:39:25.737941: step 115660, loss = 0.48 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 09:39:37.762911: step 115670, loss = 0.50 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 09:39:49.682327: step 115680, loss = 0.54 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 09:40:01.727925: step 115690, loss = 0.59 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 09:40:13.750533: step 115700, loss = 0.51 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 09:40:28.034480: step 115710, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 09:40:40.038147: step 115720, loss = 0.60 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 09:40:52.071389: step 115730, loss = 0.49 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 09:41:04.154622: step 115740, loss = 0.43 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 09:41:16.183294: step 115750, loss = 0.43 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 09:41:28.149016: step 115760, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 09:41:40.203661: step 115770, loss = 0.52 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 09:41:52.239107: step 115780, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 09:42:04.302443: step 115790, loss = 0.48 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 09:42:16.384112: step 115800, loss = 0.48 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 09:42:30.280873: step 115810, loss = 0.49 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 09:42:42.292784: step 115820, loss = 0.59 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 09:42:54.376860: step 115830, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 09:43:06.373884: step 115840, loss = 0.59 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 09:43:18.406481: step 115850, loss = 0.55 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 09:43:30.419403: step 115860, loss = 0.47 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 09:43:42.459563: step 115870, loss = 0.49 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 09:43:54.455092: step 115880, loss = 0.55 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 09:44:06.436841: step 115890, loss = 0.48 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 09:45:32.497512: step 115960, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 09:45:44.589178: step 115970, loss = 0.49 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 09:45:56.649224: step 115980, loss = 0.44 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 09:46:08.715962: step 115990, loss = 0.56 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 09:46:20.745063: step 116000, loss = 0.49 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 09:46:34.760565: step 116010, loss = 0.42 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 09:46:46.811067: step 116020, loss = 0.48 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 09:46:58.849011: step 116030, loss = 0.56 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 09:47:10.900763: step 116040, loss = 0.47 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 09:47:22.967213: step 116050, loss = 0.64 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 09:47:35.018705: step 116060, loss = 0.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 09:47:47.178269: step 116070, loss = 0.48 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 09:47:59.420283: step 116080, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 09:48:11.594731: step 116090, loss = 0.47 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 09:48:23.785252: step 116100, loss = 0.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 09:48:38.273830: step 116110, loss = 0.59 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 09:48:50.483247: step 116120, loss = 0.50 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 09:49:02.602691: step 116130, loss = 0.51 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 09:49:14.839443: step 116140, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 09:49:27.034736: step 116150, loss = 0.57 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 09:49:39.157052: step 116160, loss = 0.47 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 09:49:51.367962: step 116170, loss = 0.51 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-22 09:50:03.545607: step 116180, loss = 0.51 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 09:50:15.529387: step 116190, loss = 0.48 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 09:50:27.667312: step 116200, loss = 0.55 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 09:50:41.651120: step 116210, loss = 0.59 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 09:50:53.775033: step 116220, loss = 0.56 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 09:51:05.742054: step 116230, loss = 0.62 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 09:51:17.876492: step 116240, loss = 0.55 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 09:51:29.986262: step 116250, loss = 0.52 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 09:51:41.929362: step 116260, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 09:51:53.916570: step 116270, loss = 0.50 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 09:52:06.039407: step 116280, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 09:52:18.038312: step 116290, loss = 0.58 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 09:52:30.003595: step 116300, loss = 0.48 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 09:52:44.161220: step 116310, loss = 0.54 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 09:52:56.278704: step 116320, loss = 0.54 (25.1 examples/sec; 1.196 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 09:53:08.302379: step 116330, loss = 0.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 09:53:20.388495: step 116340, loss = 0.48 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 09:53:32.440984: step 116350, loss = 0.51 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 09:53:44.452603: step 116360, loss = 0.52 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 09:53:56.514764: step 116370, loss = 0.53 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 09:54:08.634973: step 116380, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 09:54:20.641375: step 116390, loss = 0.45 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 09:54:32.685727: step 116400, loss = 0.58 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 09:54:46.573950: step 116410, loss = 0.53 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 09:54:58.536151: step 116420, loss = 0.51 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 09:55:10.542224: step 116430, loss = 0.47 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 09:55:22.537660: step 116440, loss = 0.52 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 09:55:34.603243: step 116450, loss = 0.55 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 09:55:46.597721: step 116460, loss = 0.57 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 09:55:58.588240: step 116470, loss = 0.44 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 09:56:10.604695: step 116480, loss = 0.60 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 09:56:22.603599: step 116490, loss = 0.59 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 09:56:34.621975: step 116500, loss = 0.48 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 09:56:48.787954: step 116510, loss = 0.51 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 09:57:00.958308: step 116520, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 09:57:13.122864: step 116530, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 09:57:25.237921: step 116540, loss = 0.40 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 09:57:37.335489: step 116550, loss = 0.57 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 09:57:49.556810: step 116560, loss = 0.44 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 09:58:01.770698: step 116570, loss = 0.53 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 09:58:13.861372: step 116580, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 09:58:26.010295: step 116590, loss = 0.47 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 09:58:38.196875: step 116600, loss = 0.47 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 09:58:52.618769: step 116610, loss = 0.55 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 09:59:04.775157: step 116620, loss = 0.53 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 09:59:16.941300: step 116630, loss = 0.47 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 09:59:29.023816: step 116640, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 09:59:41.216458: step 116650, loss = 0.45 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 09:59:53.355681: step 116660, loss = 0.59 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 10:00:05.457085: step 116670, loss = 0.54 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 10:00:17.654640: step 116680, loss = 0.53 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 10:00:29.676792: step 116690, loss = 0.48 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 10:00:41.766974: step 116700, loss = 0.46 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 10:00:56.166623: step 116710, loss = 0.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 10:01:08.348207: step 116720, loss = 0.53 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 10:01:20.459501: step 116730, loss = 0.51 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 10:01:32.664876: step 116740, loss = 0.48 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 10:01:44.746808: step 116750, loss = 0.46 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 10:01:56.880798: step 116760, loss = 0.51 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 10:02:08.896570: step 116770, loss = 0.47 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 10:02:20.984264: step 116780, loss = 0.60 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 10:02:33.033992: step 116790, loss = 0.44 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 10:02:45.102006: step 116800, loss = 0.56 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 10:02:59.140488: step 116810, loss = 0.58 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 10:03:11.130104: step 116820, loss = 0.43 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 10:03:23.144282: step 116830, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 10:03:35.193921: step 116840, loss = 0.53 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 10:03:47.271790: step 116850, loss = 0.54 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 10:03:59.335134: step 116860, loss = 0.56 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 10:04:11.363347: step 116870, loss = 0.54 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 10:04:23.408315: step 116880, loss = 0.58 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 10:04:35.457838: step 116890, loss = 0.52 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 10:04:47.457728: step 116900, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 10:05:01.717810: step 116910, loss = 0.53 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 10:05:13.732613: step 116920, loss = 0.59 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 10:05:25.819937: step 116930, loss = 0.52 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 10:05:37.858893: step 116940, loss = 0.56 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-22 10:05:49.864535: step 116950, loss = 0.50 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 10:06:01.981428: step 116960, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 10:06:14.209587: step 116970, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 10:06:26.377124: step 116980, loss = 0.62 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 10:06:38.501364: step 116990, loss = 0.53 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 10:06:50.627900: step 117000, loss = 0.62 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 10:07:04.643657: step 117010, loss = 0.46 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 10:07:16.676777: step 117020, loss = 0.51 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 10:07:28.739921: step 117030, loss = 0.50 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 10:07:40.737994: step 117040, loss = 0.53 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 10:07:52.728789: step 117050, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 10:08:04.828453: step 117060, loss = 0.57 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 10:08:16.942222: step 117070, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 10:08:29.009503: step 117080, loss = 0.56 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 10:08:41.101886: step 117090, loss = 0.58 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 10:08:53.193898: step 117100, loss = 0.52 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 10:09:07.340937: step 117110, loss = 0.47 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 10:09:19.443660: step 117120, loss = 0.53 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 10:09:31.536643: step 117130, loss = 0.45 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-22 10:09:43.505867: step 117140, loss = 0.44 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 10:09:55.446938: step 117150, loss = 0.47 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 10:10:07.484905: step 117160, loss = 0.54 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 10:10:19.472500: step 117170, loss = 0.54 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-22 10:10:31.426454: step 117180, loss = 0.51 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 10:10:43.439436: step 117190, loss = 0.48 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 10:10:55.347524: step 117200, loss = 0.50 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 10:11:09.309942: step 117210, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 10:11:21.263766: step 117220, loss = 0.51 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 10:11:33.248360: step 117230, loss = 0.47 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 10:11:45.325684: step 117240, loss = 0.48 (24.8 examples/sec; 1.208 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 10:11:57.341743: step 117250, loss = 0.58 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 10:12:09.311979: step 117260, loss = 0.48 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 10:12:21.304093: step 117270, loss = 0.46 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 10:12:33.385887: step 117280, loss = 0.52 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 10:12:45.407389: step 117290, loss = 0.49 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 10:12:57.425020: step 117300, loss = 0.48 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 10:13:11.366207: step 117310, loss = 0.54 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 10:13:23.404719: step 117320, loss = 0.45 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 10:13:35.407886: step 117330, loss = 0.49 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 10:13:47.428970: step 117340, loss = 0.59 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 10:13:59.467385: step 117350, loss = 0.49 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 10:14:11.469755: step 117360, loss = 0.55 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 10:14:23.482631: step 117370, loss = 0.55 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 10:14:35.562616: step 117380, loss = 0.54 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 10:14:47.629563: step 117390, loss = 0.67 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 10:14:59.671748: step 117400, loss = 0.52 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 10:15:13.819048: step 117410, loss = 0.46 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 10:15:25.798420: step 117420, loss = 0.52 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 10:15:37.803107: step 117430, loss = 0.53 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 10:15:49.801988: step 117440, loss = 0.50 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 10:16:01.764351: step 117450, loss = 0.52 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 10:16:13.803509: step 117460, loss = 0.47 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 10:16:25.900829: step 117470, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 10:16:37.932469: step 117480, loss = 0.53 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 10:16:50.111903: step 117490, loss = 0.57 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 10:17:02.181811: step 117500, loss = 0.59 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 10:17:16.242262: step 117510, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 10:17:28.410008: step 117520, loss = 0.55 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 10:17:40.596530: step 117530, loss = 0.55 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 10:17:52.732961: step 117540, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 10:18:04.925628: step 117550, loss = 0.57 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 10:18:17.124162: step 117560, loss = 0.50 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 10:18:29.249507: step 117570, loss = 0.51 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 10:18:41.254954: step 117580, loss = 0.53 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 10:18:53.335248: step 117590, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 10:19:05.376635: step 117600, loss = 0.58 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 10:19:19.341222: step 117610, loss = 0.50 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 10:19:31.391035: step 117620, loss = 0.49 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 10:19:43.455906: step 117630, loss = 0.57 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 10:19:55.438162: step 117640, loss = 0.49 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 10:20:07.361432: step 117650, loss = 0.51 (25.5 examples/sec; 1.176 sec/batch)\n",
      "2019-05-22 10:20:19.289819: step 117660, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 10:20:31.227003: step 117670, loss = 0.49 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 10:20:43.185986: step 117680, loss = 0.61 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 10:20:55.144257: step 117690, loss = 0.52 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 10:21:07.139479: step 117700, loss = 0.55 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 10:21:21.077933: step 117710, loss = 0.46 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 10:21:33.062936: step 117720, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 10:21:44.974683: step 117730, loss = 0.49 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 10:21:56.995111: step 117740, loss = 0.59 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 10:22:08.954316: step 117750, loss = 0.53 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 10:22:20.941396: step 117760, loss = 0.41 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 10:22:32.887454: step 117770, loss = 0.55 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 10:22:44.925710: step 117780, loss = 0.42 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 10:22:56.912057: step 117790, loss = 0.52 (25.6 examples/sec; 1.173 sec/batch)\n",
      "2019-05-22 10:23:08.911608: step 117800, loss = 0.59 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 10:23:23.337521: step 117810, loss = 0.53 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 10:23:35.287720: step 117820, loss = 0.55 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 10:23:47.270912: step 117830, loss = 0.60 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 10:23:59.180791: step 117840, loss = 0.48 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-22 10:24:11.149639: step 117850, loss = 0.47 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 10:24:23.250948: step 117860, loss = 0.41 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 10:24:35.220765: step 117870, loss = 0.62 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 10:24:47.212522: step 117880, loss = 0.48 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 10:24:59.211146: step 117890, loss = 0.46 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 10:25:11.206075: step 117900, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 10:25:25.212179: step 117910, loss = 0.56 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 10:25:37.218332: step 117920, loss = 0.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 10:25:49.208766: step 117930, loss = 0.52 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 10:26:01.237446: step 117940, loss = 0.47 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 10:26:13.280103: step 117950, loss = 0.52 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 10:26:25.222645: step 117960, loss = 0.49 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 10:26:37.216156: step 117970, loss = 0.54 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 10:26:49.260433: step 117980, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 10:27:01.281187: step 117990, loss = 0.48 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 10:27:13.306373: step 118000, loss = 0.50 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 10:27:27.300835: step 118010, loss = 0.51 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 10:27:39.350061: step 118020, loss = 0.53 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 10:27:51.422031: step 118030, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 10:28:03.466155: step 118040, loss = 0.53 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 10:28:15.517207: step 118050, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 10:28:27.490132: step 118060, loss = 0.45 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 10:28:39.587630: step 118070, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 10:28:51.668899: step 118080, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 10:29:03.728191: step 118090, loss = 0.54 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 10:29:15.767322: step 118100, loss = 0.58 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 10:29:29.701629: step 118110, loss = 0.52 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 10:29:41.714110: step 118120, loss = 0.52 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 10:29:53.722219: step 118130, loss = 0.53 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 10:30:05.750783: step 118140, loss = 0.51 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 10:30:17.783656: step 118150, loss = 0.41 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 10:30:29.791446: step 118160, loss = 0.49 (25.2 examples/sec; 1.193 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 10:30:41.859062: step 118170, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 10:30:53.905207: step 118180, loss = 0.62 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 10:31:05.936428: step 118190, loss = 0.60 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 10:31:17.979899: step 118200, loss = 0.59 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 10:31:32.241948: step 118210, loss = 0.49 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 10:31:44.232692: step 118220, loss = 0.46 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 10:31:56.297327: step 118230, loss = 0.51 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 10:32:08.325324: step 118240, loss = 0.51 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 10:32:20.270227: step 118250, loss = 0.48 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 10:32:32.328213: step 118260, loss = 0.58 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 10:32:44.380982: step 118270, loss = 0.47 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 10:32:56.411679: step 118280, loss = 0.48 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 10:33:08.431247: step 118290, loss = 0.50 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 10:33:20.454909: step 118300, loss = 0.44 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 10:33:34.389639: step 118310, loss = 0.47 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 10:33:46.415433: step 118320, loss = 0.48 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 10:33:58.439285: step 118330, loss = 0.58 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 10:34:10.525001: step 118340, loss = 0.49 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 10:34:22.552178: step 118350, loss = 0.56 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 10:34:34.591589: step 118360, loss = 0.48 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 10:34:46.624248: step 118370, loss = 0.64 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 10:34:58.635819: step 118380, loss = 0.47 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 10:35:10.677335: step 118390, loss = 0.65 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 10:35:22.702686: step 118400, loss = 0.53 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 10:35:36.630263: step 118410, loss = 0.49 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 10:35:48.668038: step 118420, loss = 0.50 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 10:36:00.712660: step 118430, loss = 0.54 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 10:36:12.787154: step 118440, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 10:36:24.851308: step 118450, loss = 0.57 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 10:36:36.868454: step 118460, loss = 0.61 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 10:36:48.784984: step 118470, loss = 0.55 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 10:37:00.798925: step 118480, loss = 0.54 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 10:37:12.806756: step 118490, loss = 0.51 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 10:37:24.804678: step 118500, loss = 0.52 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 10:37:38.949535: step 118510, loss = 0.44 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 10:37:50.952545: step 118520, loss = 0.50 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 10:38:02.927722: step 118530, loss = 0.52 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 10:38:14.954573: step 118540, loss = 0.56 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 10:38:26.988089: step 118550, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 10:38:39.015692: step 118560, loss = 0.48 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 10:38:51.068520: step 118570, loss = 0.48 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 10:39:03.104361: step 118580, loss = 0.48 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 10:39:15.130311: step 118590, loss = 0.43 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 10:39:27.191972: step 118600, loss = 0.53 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 10:39:41.246219: step 118610, loss = 0.52 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 10:39:53.229450: step 118620, loss = 0.47 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 10:40:05.272810: step 118630, loss = 0.53 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 10:40:17.302165: step 118640, loss = 0.46 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 10:40:29.367860: step 118650, loss = 0.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 10:40:41.429598: step 118660, loss = 0.51 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 10:40:53.433312: step 118670, loss = 0.50 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 10:41:05.491942: step 118680, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 10:41:17.578219: step 118690, loss = 0.59 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 10:41:29.634675: step 118700, loss = 0.49 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 10:41:43.952431: step 118710, loss = 0.47 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 10:41:55.913286: step 118720, loss = 0.51 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 10:42:07.903118: step 118730, loss = 0.55 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 10:42:19.923846: step 118740, loss = 0.52 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 10:42:31.907717: step 118750, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 10:42:43.940815: step 118760, loss = 0.57 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 10:42:55.984957: step 118770, loss = 0.53 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 10:43:08.037587: step 118780, loss = 0.55 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 10:43:20.094019: step 118790, loss = 0.45 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 10:43:32.149112: step 118800, loss = 0.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 10:43:46.259589: step 118810, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 10:43:58.260334: step 118820, loss = 0.50 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 10:44:10.282782: step 118830, loss = 0.59 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 10:44:22.345656: step 118840, loss = 0.54 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 10:44:34.381864: step 118850, loss = 0.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 10:44:46.404326: step 118860, loss = 0.51 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 10:44:58.425735: step 118870, loss = 0.51 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 10:45:10.498739: step 118880, loss = 0.52 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 10:45:22.471822: step 118890, loss = 0.58 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 10:45:34.470250: step 118900, loss = 0.60 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 10:45:48.328964: step 118910, loss = 0.56 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 10:46:00.363153: step 118920, loss = 0.55 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 10:46:12.405490: step 118930, loss = 0.57 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 10:46:24.450980: step 118940, loss = 0.48 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 10:46:36.445227: step 118950, loss = 0.50 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 10:46:48.462307: step 118960, loss = 0.52 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 10:47:00.513931: step 118970, loss = 0.57 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 10:47:12.445792: step 118980, loss = 0.51 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 10:47:24.374199: step 118990, loss = 0.46 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 10:47:36.355358: step 119000, loss = 0.51 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 10:47:50.162458: step 119010, loss = 0.55 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 10:48:02.153114: step 119020, loss = 0.53 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 10:48:14.133408: step 119030, loss = 0.42 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 10:48:26.102633: step 119040, loss = 0.53 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 10:48:38.113054: step 119050, loss = 0.43 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 10:48:50.070978: step 119060, loss = 0.56 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 10:49:02.048488: step 119070, loss = 0.56 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 10:49:14.072583: step 119080, loss = 0.55 (25.0 examples/sec; 1.202 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 10:49:26.043291: step 119090, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 10:49:37.934180: step 119100, loss = 0.50 (25.7 examples/sec; 1.169 sec/batch)\n",
      "2019-05-22 10:49:51.651703: step 119110, loss = 0.50 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 10:50:03.617565: step 119120, loss = 0.48 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 10:50:15.585397: step 119130, loss = 0.55 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 10:50:27.556856: step 119140, loss = 0.48 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 10:50:39.473439: step 119150, loss = 0.53 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 10:50:51.471200: step 119160, loss = 0.49 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 10:51:03.490998: step 119170, loss = 0.52 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 10:51:15.373498: step 119180, loss = 0.58 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-22 10:51:27.349884: step 119190, loss = 0.58 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 10:51:39.341374: step 119200, loss = 0.50 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 10:51:53.222028: step 119210, loss = 0.52 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 10:52:05.215868: step 119220, loss = 0.45 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 10:52:17.135239: step 119230, loss = 0.55 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 10:52:29.087247: step 119240, loss = 0.56 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 10:52:41.043481: step 119250, loss = 0.45 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 10:52:53.037967: step 119260, loss = 0.56 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 10:53:05.007918: step 119270, loss = 0.55 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 10:53:16.996291: step 119280, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 10:53:29.005083: step 119290, loss = 0.45 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 10:53:40.985572: step 119300, loss = 0.49 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 10:53:54.811521: step 119310, loss = 0.51 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 10:54:06.758062: step 119320, loss = 0.51 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 10:54:18.697796: step 119330, loss = 0.51 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 10:54:30.674495: step 119340, loss = 0.54 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 10:54:42.624983: step 119350, loss = 0.60 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 10:54:54.598300: step 119360, loss = 0.53 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 10:55:06.595151: step 119370, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 10:55:18.554945: step 119380, loss = 0.51 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 10:55:30.560241: step 119390, loss = 0.45 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 10:55:42.535312: step 119400, loss = 0.57 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 10:55:56.416510: step 119410, loss = 0.56 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 10:56:08.389886: step 119420, loss = 0.53 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 10:56:20.322168: step 119430, loss = 0.55 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 10:56:32.336254: step 119440, loss = 0.51 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 10:56:44.380386: step 119450, loss = 0.54 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 10:56:56.403158: step 119460, loss = 0.44 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 10:57:08.438247: step 119470, loss = 0.46 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 10:57:20.481355: step 119480, loss = 0.51 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 10:57:32.408996: step 119490, loss = 0.40 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 10:57:44.379834: step 119500, loss = 0.51 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 10:57:58.280179: step 119510, loss = 0.54 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 10:58:10.276049: step 119520, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 10:58:22.335952: step 119530, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 10:58:34.373634: step 119540, loss = 0.46 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 10:58:46.440792: step 119550, loss = 0.51 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 10:58:58.472551: step 119560, loss = 0.51 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 10:59:10.539607: step 119570, loss = 0.48 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 10:59:22.594925: step 119580, loss = 0.48 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 10:59:34.665835: step 119590, loss = 0.50 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 10:59:46.694968: step 119600, loss = 0.55 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 11:00:00.578134: step 119610, loss = 0.57 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 11:00:12.617139: step 119620, loss = 0.48 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 11:00:24.646392: step 119630, loss = 0.56 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 11:00:36.722342: step 119640, loss = 0.60 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 11:00:48.769717: step 119650, loss = 0.61 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 11:01:00.811511: step 119660, loss = 0.44 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 11:01:12.831390: step 119670, loss = 0.49 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 11:01:24.908101: step 119680, loss = 0.52 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 11:01:36.905694: step 119690, loss = 0.57 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 11:01:48.983870: step 119700, loss = 0.48 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 11:02:03.244886: step 119710, loss = 0.50 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 11:02:15.296575: step 119720, loss = 0.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 11:02:27.283430: step 119730, loss = 0.48 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 11:02:39.201563: step 119740, loss = 0.53 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 11:02:51.154408: step 119750, loss = 0.47 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-22 11:03:03.164487: step 119760, loss = 0.48 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 11:03:15.219916: step 119770, loss = 0.53 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 11:03:27.296685: step 119780, loss = 0.49 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 11:03:39.328255: step 119790, loss = 0.53 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 11:03:51.402705: step 119800, loss = 0.44 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 11:04:05.660873: step 119810, loss = 0.54 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 11:04:17.739468: step 119820, loss = 0.52 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 11:04:29.744376: step 119830, loss = 0.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 11:04:41.764304: step 119840, loss = 0.58 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 11:04:53.793216: step 119850, loss = 0.46 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 11:05:05.820989: step 119860, loss = 0.53 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 11:05:17.844087: step 119870, loss = 0.51 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 11:05:29.872835: step 119880, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 11:05:41.922780: step 119890, loss = 0.52 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 11:05:53.971024: step 119900, loss = 0.47 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 11:06:08.062705: step 119910, loss = 0.45 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 11:06:20.129814: step 119920, loss = 0.42 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 11:06:32.183439: step 119930, loss = 0.56 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 11:06:44.211149: step 119940, loss = 0.53 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 11:06:56.262773: step 119950, loss = 0.54 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 11:07:08.292405: step 119960, loss = 0.56 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 11:07:20.331882: step 119970, loss = 0.55 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 11:07:32.329533: step 119980, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 11:07:44.391542: step 119990, loss = 0.50 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-22 11:07:56.292423: step 120000, loss = 0.44 (25.1 examples/sec; 1.195 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 11:08:13.868420: step 120010, loss = 0.46 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 11:08:25.912064: step 120020, loss = 0.48 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 11:08:37.946982: step 120030, loss = 0.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 11:08:50.002133: step 120040, loss = 0.48 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 11:09:02.013277: step 120050, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 11:09:14.020437: step 120060, loss = 0.51 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 11:09:26.068122: step 120070, loss = 0.44 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 11:09:38.094889: step 120080, loss = 0.53 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 11:09:50.131630: step 120090, loss = 0.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 11:10:02.121371: step 120100, loss = 0.52 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 11:10:16.183914: step 120110, loss = 0.47 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 11:10:28.181787: step 120120, loss = 0.55 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 11:10:40.199662: step 120130, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 11:10:52.267956: step 120140, loss = 0.49 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 11:11:04.279700: step 120150, loss = 0.59 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 11:11:16.338984: step 120160, loss = 0.53 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 11:11:28.361595: step 120170, loss = 0.55 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 11:11:40.359730: step 120180, loss = 0.51 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 11:11:52.396685: step 120190, loss = 0.65 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 11:12:04.482153: step 120200, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 11:12:18.801252: step 120210, loss = 0.42 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 11:12:30.823300: step 120220, loss = 0.57 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 11:12:42.838678: step 120230, loss = 0.55 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 11:12:54.825942: step 120240, loss = 0.44 (25.5 examples/sec; 1.177 sec/batch)\n",
      "2019-05-22 11:13:06.750251: step 120250, loss = 0.49 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 11:13:18.779403: step 120260, loss = 0.49 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 11:13:30.834518: step 120270, loss = 0.49 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 11:13:42.885132: step 120280, loss = 0.51 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 11:13:54.965498: step 120290, loss = 0.55 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 11:14:06.990555: step 120300, loss = 0.53 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 11:14:21.482213: step 120310, loss = 0.54 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 11:14:33.524359: step 120320, loss = 0.43 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 11:14:45.591853: step 120330, loss = 0.59 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 11:14:57.618825: step 120340, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 11:15:09.696229: step 120350, loss = 0.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 11:15:21.735895: step 120360, loss = 0.53 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 11:15:33.765818: step 120370, loss = 0.46 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 11:15:45.812764: step 120380, loss = 0.47 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 11:15:57.848928: step 120390, loss = 0.52 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 11:16:09.892622: step 120400, loss = 0.55 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 11:16:24.277845: step 120410, loss = 0.54 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 11:16:36.272341: step 120420, loss = 0.46 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 11:16:48.329168: step 120430, loss = 0.51 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 11:17:00.348186: step 120440, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 11:17:12.387718: step 120450, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 11:17:24.380378: step 120460, loss = 0.51 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 11:17:36.404586: step 120470, loss = 0.46 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 11:17:48.439722: step 120480, loss = 0.53 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 11:18:00.449118: step 120490, loss = 0.50 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 11:18:12.358583: step 120500, loss = 0.47 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 11:18:26.299015: step 120510, loss = 0.50 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 11:18:38.336465: step 120520, loss = 0.45 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 11:18:50.413267: step 120530, loss = 0.43 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 11:19:02.475108: step 120540, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 11:19:14.517409: step 120550, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 11:19:26.579676: step 120560, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 11:19:38.619898: step 120570, loss = 0.48 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 11:19:50.668733: step 120580, loss = 0.45 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 11:20:02.726532: step 120590, loss = 0.48 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 11:20:14.775420: step 120600, loss = 0.44 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 11:20:28.746511: step 120610, loss = 0.39 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 11:20:40.708237: step 120620, loss = 0.47 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 11:20:52.712832: step 120630, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 11:21:04.753692: step 120640, loss = 0.53 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 11:21:16.754360: step 120650, loss = 0.52 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 11:21:28.802475: step 120660, loss = 0.52 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 11:21:40.857632: step 120670, loss = 0.59 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 11:21:52.912207: step 120680, loss = 0.53 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 11:22:04.946707: step 120690, loss = 0.51 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 11:22:16.996727: step 120700, loss = 0.47 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 11:22:31.272333: step 120710, loss = 0.47 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-22 11:22:43.278456: step 120720, loss = 0.47 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 11:22:55.274314: step 120730, loss = 0.62 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 11:23:07.318714: step 120740, loss = 0.47 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 11:23:19.245628: step 120750, loss = 0.58 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 11:23:31.199024: step 120760, loss = 0.48 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 11:23:43.222828: step 120770, loss = 0.49 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 11:23:55.240741: step 120780, loss = 0.49 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 11:24:07.215729: step 120790, loss = 0.56 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 11:24:19.256542: step 120800, loss = 0.56 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 11:24:33.291070: step 120810, loss = 0.50 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 11:24:45.338233: step 120820, loss = 0.54 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 11:24:57.395309: step 120830, loss = 0.49 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 11:25:09.400126: step 120840, loss = 0.49 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 11:25:21.394257: step 120850, loss = 0.41 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 11:25:33.471843: step 120860, loss = 0.55 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 11:25:45.524860: step 120870, loss = 0.48 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 11:25:57.559661: step 120880, loss = 0.52 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 11:26:09.589870: step 120890, loss = 0.49 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 11:26:21.650987: step 120900, loss = 0.53 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 11:26:35.658413: step 120910, loss = 0.43 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 11:26:47.637774: step 120920, loss = 0.54 (25.0 examples/sec; 1.201 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 11:26:59.673224: step 120930, loss = 0.46 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 11:27:11.692588: step 120940, loss = 0.47 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 11:27:23.727200: step 120950, loss = 0.43 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 11:27:35.716080: step 120960, loss = 0.54 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 11:27:47.756969: step 120970, loss = 0.52 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 11:27:59.787555: step 120980, loss = 0.64 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 11:28:11.818934: step 120990, loss = 0.54 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 11:28:23.780699: step 121000, loss = 0.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 11:28:37.679101: step 121010, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 11:28:49.647377: step 121020, loss = 0.54 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 11:29:01.638679: step 121030, loss = 0.58 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 11:29:13.610046: step 121040, loss = 0.47 (25.5 examples/sec; 1.176 sec/batch)\n",
      "2019-05-22 11:29:25.584448: step 121050, loss = 0.49 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 11:29:37.583864: step 121060, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 11:29:49.582287: step 121070, loss = 0.56 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-22 11:30:01.619262: step 121080, loss = 0.59 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 11:30:13.677829: step 121090, loss = 0.50 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 11:30:25.690559: step 121100, loss = 0.53 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 11:30:39.586154: step 121110, loss = 0.60 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 11:30:51.629265: step 121120, loss = 0.50 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 11:31:03.693112: step 121130, loss = 0.56 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 11:31:15.777420: step 121140, loss = 0.54 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 11:31:27.821770: step 121150, loss = 0.43 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 11:31:39.881007: step 121160, loss = 0.56 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 11:31:51.922724: step 121170, loss = 0.49 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 11:32:03.922358: step 121180, loss = 0.50 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 11:32:15.950557: step 121190, loss = 0.49 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 11:32:27.979625: step 121200, loss = 0.62 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 11:32:41.896935: step 121210, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 11:32:53.920347: step 121220, loss = 0.45 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 11:33:05.900985: step 121230, loss = 0.48 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 11:33:17.939470: step 121240, loss = 0.48 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 11:33:29.928947: step 121250, loss = 0.54 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 11:33:41.945426: step 121260, loss = 0.64 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 11:33:53.932284: step 121270, loss = 0.50 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 11:34:06.067542: step 121280, loss = 0.48 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 11:34:18.109072: step 121290, loss = 0.63 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 11:34:30.170443: step 121300, loss = 0.54 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 11:34:44.434011: step 121310, loss = 0.54 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 11:34:56.434391: step 121320, loss = 0.58 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 11:35:08.404142: step 121330, loss = 0.58 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 11:35:20.464260: step 121340, loss = 0.52 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 11:35:32.497008: step 121350, loss = 0.46 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 11:35:44.578057: step 121360, loss = 0.51 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 11:35:56.633641: step 121370, loss = 0.50 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 11:36:08.693989: step 121380, loss = 0.47 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 11:36:20.733965: step 121390, loss = 0.58 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 11:36:32.789054: step 121400, loss = 0.56 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 11:36:47.105045: step 121410, loss = 0.56 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 11:36:59.115292: step 121420, loss = 0.56 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 11:37:11.137082: step 121430, loss = 0.51 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 11:37:23.083117: step 121440, loss = 0.43 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 11:37:35.106793: step 121450, loss = 0.53 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 11:37:47.201440: step 121460, loss = 0.50 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 11:37:59.274520: step 121470, loss = 0.47 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 11:38:11.325729: step 121480, loss = 0.43 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 11:38:23.449211: step 121490, loss = 0.48 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 11:38:35.392244: step 121500, loss = 0.42 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 11:38:49.452418: step 121510, loss = 0.41 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 11:39:01.386337: step 121520, loss = 0.55 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 11:39:13.416211: step 121530, loss = 0.51 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 11:39:25.462638: step 121540, loss = 0.58 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 11:39:37.475339: step 121550, loss = 0.56 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 11:39:49.521609: step 121560, loss = 0.53 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 11:40:01.551946: step 121570, loss = 0.57 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 11:40:13.522670: step 121580, loss = 0.48 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 11:40:25.540979: step 121590, loss = 0.48 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 11:40:37.566597: step 121600, loss = 0.53 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 11:40:51.748733: step 121610, loss = 0.48 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 11:41:03.806194: step 121620, loss = 0.52 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 11:41:15.858050: step 121630, loss = 0.51 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 11:41:27.847371: step 121640, loss = 0.47 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 11:41:39.875378: step 121650, loss = 0.50 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 11:41:51.947158: step 121660, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 11:42:04.004959: step 121670, loss = 0.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 11:42:16.037848: step 121680, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 11:42:28.089725: step 121690, loss = 0.45 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 11:42:40.052251: step 121700, loss = 0.47 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 11:42:53.940265: step 121710, loss = 0.58 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 11:43:05.940638: step 121720, loss = 0.53 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 11:43:17.962901: step 121730, loss = 0.44 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 11:43:29.979160: step 121740, loss = 0.61 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 11:43:41.966980: step 121750, loss = 0.39 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 11:43:53.995118: step 121760, loss = 0.48 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 11:44:05.913305: step 121770, loss = 0.58 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 11:44:17.941119: step 121780, loss = 0.53 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 11:44:30.030837: step 121790, loss = 0.51 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 11:44:42.053766: step 121800, loss = 0.41 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 11:44:56.155140: step 121810, loss = 0.55 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 11:45:08.172853: step 121820, loss = 0.46 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 11:45:20.164509: step 121830, loss = 0.52 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 11:45:32.107761: step 121840, loss = 0.51 (25.4 examples/sec; 1.183 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 11:45:44.137225: step 121850, loss = 0.59 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 11:45:56.190827: step 121860, loss = 0.53 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 11:46:08.207550: step 121870, loss = 0.57 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 11:46:20.193352: step 121880, loss = 0.55 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 11:46:32.256377: step 121890, loss = 0.44 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 11:46:44.309312: step 121900, loss = 0.49 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 11:46:58.337938: step 121910, loss = 0.44 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 11:47:10.396161: step 121920, loss = 0.51 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 11:47:22.387207: step 121930, loss = 0.57 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 11:47:34.400251: step 121940, loss = 0.54 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 11:47:46.441618: step 121950, loss = 0.51 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 11:47:58.447276: step 121960, loss = 0.47 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 11:48:10.475613: step 121970, loss = 0.51 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 11:48:22.486611: step 121980, loss = 0.51 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 11:48:34.473439: step 121990, loss = 0.49 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 11:48:46.423527: step 122000, loss = 0.55 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 11:49:00.629266: step 122010, loss = 0.50 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 11:49:12.628804: step 122020, loss = 0.49 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 11:49:24.586810: step 122030, loss = 0.58 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 11:49:36.583604: step 122040, loss = 0.56 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 11:49:48.585838: step 122050, loss = 0.48 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 11:50:00.618717: step 122060, loss = 0.43 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 11:50:12.654356: step 122070, loss = 0.55 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 11:50:24.699769: step 122080, loss = 0.47 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 11:50:36.697936: step 122090, loss = 0.51 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 11:50:48.734616: step 122100, loss = 0.59 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 11:51:02.887571: step 122110, loss = 0.52 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 11:51:14.932563: step 122120, loss = 0.51 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 11:51:26.974876: step 122130, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 11:51:38.973850: step 122140, loss = 0.59 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 11:51:51.015727: step 122150, loss = 0.55 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 11:52:03.083856: step 122160, loss = 0.46 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 11:52:15.088943: step 122170, loss = 0.54 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 11:52:27.123515: step 122180, loss = 0.53 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 11:52:39.195994: step 122190, loss = 0.45 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 11:52:51.246077: step 122200, loss = 0.45 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 11:53:05.238645: step 122210, loss = 0.41 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 11:53:17.222466: step 122220, loss = 0.45 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 11:53:29.229849: step 122230, loss = 0.49 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 11:53:41.248875: step 122240, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 11:53:53.204934: step 122250, loss = 0.47 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 11:54:05.217331: step 122260, loss = 0.59 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 11:54:17.242606: step 122270, loss = 0.54 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 11:54:29.198240: step 122280, loss = 0.56 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 11:54:41.197141: step 122290, loss = 0.52 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 11:54:53.183609: step 122300, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 11:55:07.147449: step 122310, loss = 0.51 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 11:55:19.148214: step 122320, loss = 0.54 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 11:55:31.141759: step 122330, loss = 0.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 11:55:43.171819: step 122340, loss = 0.50 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 11:55:55.247821: step 122350, loss = 0.47 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 11:56:07.250698: step 122360, loss = 0.57 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 11:56:19.263168: step 122370, loss = 0.53 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 11:56:31.292403: step 122380, loss = 0.52 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 11:56:43.329215: step 122390, loss = 0.50 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 11:56:55.367438: step 122400, loss = 0.53 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 11:57:09.744757: step 122410, loss = 0.49 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 11:57:21.777152: step 122420, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 11:57:33.774371: step 122430, loss = 0.49 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 11:57:45.795802: step 122440, loss = 0.49 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 11:57:57.842099: step 122450, loss = 0.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 11:58:09.845928: step 122460, loss = 0.40 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 11:58:21.834624: step 122470, loss = 0.64 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 11:58:33.824652: step 122480, loss = 0.44 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 11:58:45.856366: step 122490, loss = 0.48 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 11:58:57.857878: step 122500, loss = 0.53 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 11:59:12.093113: step 122510, loss = 0.59 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 11:59:24.113787: step 122520, loss = 0.57 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 11:59:36.100783: step 122530, loss = 0.53 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 11:59:48.116402: step 122540, loss = 0.46 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 12:00:00.151360: step 122550, loss = 0.48 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 12:00:12.187919: step 122560, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 12:00:24.192632: step 122570, loss = 0.47 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 12:00:36.232279: step 122580, loss = 0.59 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 12:00:48.288640: step 122590, loss = 0.56 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 12:01:00.332346: step 122600, loss = 0.48 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 12:01:14.393157: step 122610, loss = 0.54 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 12:01:26.412377: step 122620, loss = 0.50 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 12:01:38.411217: step 122630, loss = 0.56 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 12:01:50.443164: step 122640, loss = 0.47 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 12:02:02.487792: step 122650, loss = 0.50 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 12:02:14.543861: step 122660, loss = 0.47 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 12:02:26.512889: step 122670, loss = 0.59 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 12:02:38.521077: step 122680, loss = 0.48 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-22 12:02:50.561392: step 122690, loss = 0.48 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 12:03:02.554934: step 122700, loss = 0.54 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 12:03:16.419575: step 122710, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 12:03:28.468537: step 122720, loss = 0.56 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 12:03:40.476092: step 122730, loss = 0.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 12:03:52.510700: step 122740, loss = 0.61 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 12:04:04.457387: step 122750, loss = 0.47 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 12:04:16.462652: step 122760, loss = 0.51 (25.1 examples/sec; 1.196 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 12:04:28.481705: step 122770, loss = 0.46 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 12:04:40.519987: step 122780, loss = 0.51 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 12:04:52.443745: step 122790, loss = 0.47 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 12:05:04.473369: step 122800, loss = 0.55 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 12:05:18.800776: step 122810, loss = 0.64 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 12:05:30.803424: step 122820, loss = 0.53 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 12:05:42.821280: step 122830, loss = 0.49 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 12:05:54.834127: step 122840, loss = 0.55 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 12:06:06.864247: step 122850, loss = 0.54 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 12:06:18.861833: step 122860, loss = 0.52 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 12:06:30.832516: step 122870, loss = 0.45 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 12:06:42.896533: step 122880, loss = 0.48 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 12:06:54.910386: step 122890, loss = 0.49 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 12:07:06.996314: step 122900, loss = 0.49 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 12:07:21.025569: step 122910, loss = 0.52 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 12:07:33.161215: step 122920, loss = 0.46 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 12:07:45.291621: step 122930, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 12:07:57.392653: step 122940, loss = 0.57 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 12:08:09.543204: step 122950, loss = 0.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 12:08:21.647643: step 122960, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 12:08:33.749616: step 122970, loss = 0.49 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 12:08:45.858208: step 122980, loss = 0.55 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 12:08:57.997177: step 122990, loss = 0.46 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 12:09:10.075779: step 123000, loss = 0.55 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 12:09:24.153870: step 123010, loss = 0.50 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 12:09:36.307637: step 123020, loss = 0.49 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 12:09:48.451017: step 123030, loss = 0.46 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 12:10:00.549070: step 123040, loss = 0.49 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 12:10:12.684133: step 123050, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 12:10:24.839797: step 123060, loss = 0.54 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 12:10:36.974126: step 123070, loss = 0.49 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 12:10:49.093382: step 123080, loss = 0.57 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 12:11:01.248919: step 123090, loss = 0.57 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 12:11:13.403439: step 123100, loss = 0.55 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 12:11:27.549441: step 123110, loss = 0.50 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 12:11:39.702714: step 123120, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 12:11:52.159414: step 123130, loss = 0.52 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 12:12:04.246807: step 123140, loss = 0.49 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 12:12:16.415158: step 123150, loss = 0.57 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 12:12:28.577323: step 123160, loss = 0.49 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 12:12:40.705819: step 123170, loss = 0.45 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 12:12:52.849634: step 123180, loss = 0.49 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 12:13:04.972830: step 123190, loss = 0.50 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 12:13:17.133745: step 123200, loss = 0.61 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 12:13:31.563584: step 123210, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 12:13:43.669023: step 123220, loss = 0.54 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 12:13:55.818958: step 123230, loss = 0.62 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 12:14:07.969021: step 123240, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 12:14:20.084460: step 123250, loss = 0.49 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 12:14:32.231742: step 123260, loss = 0.50 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 12:14:44.384880: step 123270, loss = 0.49 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 12:14:56.540680: step 123280, loss = 0.56 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 12:15:08.583133: step 123290, loss = 0.54 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 12:15:20.642240: step 123300, loss = 0.48 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 12:15:35.218511: step 123310, loss = 0.56 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 12:15:47.344744: step 123320, loss = 0.55 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 12:15:59.454350: step 123330, loss = 0.45 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 12:16:11.586295: step 123340, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 12:16:23.687195: step 123350, loss = 0.53 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 12:16:35.766440: step 123360, loss = 0.52 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 12:16:48.119658: step 123370, loss = 0.54 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 12:17:00.112614: step 123380, loss = 0.50 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 12:17:12.117935: step 123390, loss = 0.49 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 12:17:24.160365: step 123400, loss = 0.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 12:17:38.127325: step 123410, loss = 0.48 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 12:17:50.226438: step 123420, loss = 0.56 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 12:18:02.271961: step 123430, loss = 0.46 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 12:18:14.348231: step 123440, loss = 0.53 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 12:18:26.435144: step 123450, loss = 0.51 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 12:18:38.561759: step 123460, loss = 0.47 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 12:18:50.681707: step 123470, loss = 0.46 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 12:19:02.731782: step 123480, loss = 0.55 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 12:19:14.723902: step 123490, loss = 0.53 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 12:19:26.764068: step 123500, loss = 0.51 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 12:19:41.172915: step 123510, loss = 0.52 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 12:19:53.269713: step 123520, loss = 0.45 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 12:20:05.337990: step 123530, loss = 0.48 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 12:20:17.380158: step 123540, loss = 0.48 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 12:20:29.501157: step 123550, loss = 0.59 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 12:20:41.553997: step 123560, loss = 0.49 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 12:20:53.738311: step 123570, loss = 0.57 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 12:21:05.821403: step 123580, loss = 0.58 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 12:21:17.911695: step 123590, loss = 0.46 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 12:21:29.899079: step 123600, loss = 0.49 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 12:21:43.769616: step 123610, loss = 0.49 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 12:21:55.723588: step 123620, loss = 0.49 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 12:22:07.613996: step 123630, loss = 0.48 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 12:22:19.610640: step 123640, loss = 0.49 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 12:22:31.584549: step 123650, loss = 0.47 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 12:22:43.567923: step 123660, loss = 0.58 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 12:22:55.531415: step 123670, loss = 0.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 12:23:07.461645: step 123680, loss = 0.49 (25.0 examples/sec; 1.201 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 12:23:19.494455: step 123690, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 12:23:31.497802: step 123700, loss = 0.47 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 12:23:45.359360: step 123710, loss = 0.39 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 12:23:57.398248: step 123720, loss = 0.42 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 12:24:09.501303: step 123730, loss = 0.43 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 12:24:21.566248: step 123740, loss = 0.52 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 12:24:33.631483: step 123750, loss = 0.43 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 12:24:45.775648: step 123760, loss = 0.58 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 12:24:57.908707: step 123770, loss = 0.51 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 12:25:10.092722: step 123780, loss = 0.57 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 12:25:22.220704: step 123790, loss = 0.47 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 12:25:34.250880: step 123800, loss = 0.56 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 12:25:48.354028: step 123810, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 12:26:00.495525: step 123820, loss = 0.49 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 12:26:12.608010: step 123830, loss = 0.48 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 12:26:24.717898: step 123840, loss = 0.49 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 12:26:36.852594: step 123850, loss = 0.45 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 12:26:48.982748: step 123860, loss = 0.52 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 12:27:01.140732: step 123870, loss = 0.45 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 12:27:13.249031: step 123880, loss = 0.51 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 12:27:25.374144: step 123890, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 12:27:37.487896: step 123900, loss = 0.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 12:27:51.862282: step 123910, loss = 0.51 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 12:28:03.941653: step 123920, loss = 0.56 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 12:28:16.116163: step 123930, loss = 0.56 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 12:28:28.232880: step 123940, loss = 0.49 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 12:28:40.377029: step 123950, loss = 0.49 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 12:28:52.554177: step 123960, loss = 0.46 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 12:29:04.707381: step 123970, loss = 0.50 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 12:29:16.817197: step 123980, loss = 0.46 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 12:29:28.935815: step 123990, loss = 0.47 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 12:29:41.073772: step 124000, loss = 0.55 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 12:29:55.141428: step 124010, loss = 0.51 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 12:30:07.270846: step 124020, loss = 0.53 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 12:30:19.378873: step 124030, loss = 0.55 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 12:30:31.562980: step 124040, loss = 0.50 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 12:30:43.618570: step 124050, loss = 0.55 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 12:30:55.777704: step 124060, loss = 0.60 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 12:31:07.860059: step 124070, loss = 0.56 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 12:31:19.925261: step 124080, loss = 0.56 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 12:31:31.987788: step 124090, loss = 0.53 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 12:31:44.018983: step 124100, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 12:31:57.933746: step 124110, loss = 0.47 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 12:32:09.926719: step 124120, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 12:32:21.894560: step 124130, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 12:32:33.862031: step 124140, loss = 0.52 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 12:32:45.848005: step 124150, loss = 0.55 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 12:32:57.854578: step 124160, loss = 0.54 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 12:33:09.827122: step 124170, loss = 0.52 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 12:33:21.835235: step 124180, loss = 0.53 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 12:33:33.800960: step 124190, loss = 0.61 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 12:33:45.791538: step 124200, loss = 0.45 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 12:33:59.731172: step 124210, loss = 0.45 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 12:34:11.688468: step 124220, loss = 0.51 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 12:34:23.714681: step 124230, loss = 0.53 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 12:34:35.664135: step 124240, loss = 0.56 (25.5 examples/sec; 1.177 sec/batch)\n",
      "2019-05-22 12:34:47.683996: step 124250, loss = 0.53 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 12:34:59.646819: step 124260, loss = 0.55 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 12:35:11.642220: step 124270, loss = 0.51 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 12:35:23.659013: step 124280, loss = 0.40 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 12:35:35.702957: step 124290, loss = 0.54 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 12:35:47.744992: step 124300, loss = 0.50 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 12:36:02.003441: step 124310, loss = 0.54 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 12:36:14.118307: step 124320, loss = 0.56 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 12:36:26.267822: step 124330, loss = 0.53 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 12:36:38.409618: step 124340, loss = 0.48 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 12:36:50.515882: step 124350, loss = 0.63 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 12:37:02.654326: step 124360, loss = 0.55 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 12:37:14.784089: step 124370, loss = 0.58 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 12:37:26.946538: step 124380, loss = 0.49 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 12:37:39.054027: step 124390, loss = 0.52 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 12:37:51.174652: step 124400, loss = 0.58 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 12:38:05.198191: step 124410, loss = 0.45 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 12:38:17.356517: step 124420, loss = 0.62 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 12:38:29.476141: step 124430, loss = 0.52 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 12:38:41.585210: step 124440, loss = 0.52 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 12:38:53.746210: step 124450, loss = 0.53 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 12:39:05.824996: step 124460, loss = 0.48 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 12:39:17.910072: step 124470, loss = 0.44 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 12:39:30.033536: step 124480, loss = 0.50 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 12:39:42.072408: step 124490, loss = 0.42 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 12:39:54.183361: step 124500, loss = 0.44 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 12:40:08.239004: step 124510, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 12:40:20.385786: step 124520, loss = 0.49 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 12:40:32.409566: step 124530, loss = 0.48 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 12:40:44.462841: step 124540, loss = 0.54 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 12:40:56.510897: step 124550, loss = 0.58 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 12:41:08.545953: step 124560, loss = 0.58 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 12:41:20.662505: step 124570, loss = 0.53 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 12:41:32.736379: step 124580, loss = 0.57 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 12:41:44.792550: step 124590, loss = 0.57 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 12:41:56.866878: step 124600, loss = 0.46 (24.9 examples/sec; 1.205 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 12:42:11.323945: step 124610, loss = 0.43 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 12:42:23.385780: step 124620, loss = 0.58 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 12:42:35.487454: step 124630, loss = 0.52 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 12:42:47.598589: step 124640, loss = 0.59 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 12:42:59.651017: step 124650, loss = 0.52 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 12:43:11.854648: step 124660, loss = 0.47 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 12:43:23.936669: step 124670, loss = 0.46 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 12:43:36.104967: step 124680, loss = 0.45 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 12:43:48.277183: step 124690, loss = 0.57 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 12:44:00.407191: step 124700, loss = 0.53 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 12:44:14.660923: step 124710, loss = 0.52 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 12:44:26.807616: step 124720, loss = 0.47 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 12:44:38.948130: step 124730, loss = 0.51 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 12:44:50.989429: step 124740, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 12:45:03.140736: step 124750, loss = 0.48 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 12:45:15.283199: step 124760, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 12:45:27.449523: step 124770, loss = 0.47 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 12:45:39.542424: step 124780, loss = 0.44 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 12:45:51.662170: step 124790, loss = 0.51 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 12:46:03.777157: step 124800, loss = 0.52 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 12:46:17.742025: step 124810, loss = 0.47 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 12:46:29.890112: step 124820, loss = 0.50 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 12:46:42.028390: step 124830, loss = 0.46 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 12:46:54.150551: step 124840, loss = 0.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 12:47:06.230946: step 124850, loss = 0.51 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 12:47:18.361191: step 124860, loss = 0.53 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 12:47:30.496527: step 124870, loss = 0.52 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 12:47:42.663060: step 124880, loss = 0.48 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 12:47:54.808903: step 124890, loss = 0.49 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 12:48:07.007161: step 124900, loss = 0.46 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 12:48:21.127131: step 124910, loss = 0.62 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 12:48:33.300734: step 124920, loss = 0.54 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 12:48:45.429806: step 124930, loss = 0.42 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 12:48:57.566200: step 124940, loss = 0.52 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 12:49:09.725396: step 124950, loss = 0.44 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 12:49:21.906333: step 124960, loss = 0.48 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 12:49:34.041752: step 124970, loss = 0.47 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 12:49:46.177442: step 124980, loss = 0.55 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 12:49:58.231432: step 124990, loss = 0.53 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 12:50:10.391969: step 125000, loss = 0.55 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 12:50:27.961571: step 125010, loss = 0.42 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 12:50:40.125921: step 125020, loss = 0.43 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 12:50:52.213442: step 125030, loss = 0.53 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 12:51:04.397242: step 125040, loss = 0.46 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 12:51:16.549531: step 125050, loss = 0.45 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 12:51:28.560567: step 125060, loss = 0.48 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 12:51:40.677548: step 125070, loss = 0.59 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 12:51:52.796178: step 125080, loss = 0.43 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 12:52:04.939806: step 125090, loss = 0.40 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 12:52:17.084274: step 125100, loss = 0.51 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 12:52:31.118475: step 125110, loss = 0.46 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 12:52:43.291295: step 125120, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 12:52:55.423456: step 125130, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 12:53:07.569673: step 125140, loss = 0.49 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 12:53:19.749435: step 125150, loss = 0.56 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 12:53:31.930184: step 125160, loss = 0.52 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 12:53:44.104998: step 125170, loss = 0.44 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 12:53:56.245392: step 125180, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 12:54:08.378533: step 125190, loss = 0.59 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 12:54:20.558124: step 125200, loss = 0.44 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 12:54:34.707161: step 125210, loss = 0.48 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 12:54:46.844629: step 125220, loss = 0.46 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 12:54:58.954187: step 125230, loss = 0.47 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 12:55:11.134447: step 125240, loss = 0.54 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 12:55:23.282526: step 125250, loss = 0.52 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 12:55:35.451340: step 125260, loss = 0.41 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 12:55:47.621134: step 125270, loss = 0.49 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 12:55:59.702619: step 125280, loss = 0.48 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 12:56:11.859295: step 125290, loss = 0.41 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 12:56:24.017359: step 125300, loss = 0.46 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 12:56:38.317387: step 125310, loss = 0.54 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 12:56:50.426573: step 125320, loss = 0.43 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 12:57:02.559090: step 125330, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 12:57:14.679467: step 125340, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 12:57:26.749517: step 125350, loss = 0.49 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 12:57:38.758836: step 125360, loss = 0.48 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 12:57:50.830620: step 125370, loss = 0.60 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 12:58:02.899982: step 125380, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 12:58:14.984661: step 125390, loss = 0.42 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 12:58:26.966003: step 125400, loss = 0.48 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 12:58:40.885465: step 125410, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 12:58:52.877666: step 125420, loss = 0.47 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 12:59:04.854381: step 125430, loss = 0.39 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 12:59:16.907002: step 125440, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 12:59:28.931236: step 125450, loss = 0.43 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 12:59:41.037314: step 125460, loss = 0.54 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 12:59:53.074053: step 125470, loss = 0.47 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 13:00:05.054390: step 125480, loss = 0.45 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 13:00:17.068640: step 125490, loss = 0.51 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 13:00:29.087742: step 125500, loss = 0.53 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 13:00:43.044548: step 125510, loss = 0.48 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 13:00:55.121169: step 125520, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 13:01:07.157966: step 125530, loss = 0.53 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 13:01:19.155966: step 125540, loss = 0.52 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 13:01:31.200162: step 125550, loss = 0.45 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-22 13:01:43.201929: step 125560, loss = 0.42 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 13:01:55.227645: step 125570, loss = 0.49 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 13:02:07.214890: step 125580, loss = 0.57 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 13:02:19.261978: step 125590, loss = 0.48 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 13:02:31.324802: step 125600, loss = 0.54 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 13:02:45.768597: step 125610, loss = 0.45 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 13:02:57.832726: step 125620, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 13:03:09.860778: step 125630, loss = 0.56 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 13:03:21.934518: step 125640, loss = 0.43 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 13:03:33.989924: step 125650, loss = 0.51 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 13:03:46.066251: step 125660, loss = 0.58 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 13:03:58.074692: step 125670, loss = 0.50 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 13:04:10.131974: step 125680, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 13:04:22.178768: step 125690, loss = 0.55 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 13:04:34.241103: step 125700, loss = 0.54 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 13:04:48.502405: step 125710, loss = 0.60 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 13:05:00.594265: step 125720, loss = 0.55 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 13:05:12.580830: step 125730, loss = 0.51 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 13:05:24.609293: step 125740, loss = 0.48 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 13:05:36.684589: step 125750, loss = 0.43 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 13:05:48.752334: step 125760, loss = 0.60 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 13:06:00.813154: step 125770, loss = 0.49 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 13:06:12.914458: step 125780, loss = 0.55 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 13:06:25.000249: step 125790, loss = 0.51 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 13:06:37.051317: step 125800, loss = 0.47 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 13:06:51.080246: step 125810, loss = 0.56 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 13:07:03.066613: step 125820, loss = 0.60 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 13:07:15.150251: step 125830, loss = 0.53 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 13:07:27.268775: step 125840, loss = 0.55 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 13:07:39.321799: step 125850, loss = 0.47 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 13:07:51.380219: step 125860, loss = 0.49 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 13:08:03.451545: step 125870, loss = 0.44 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 13:08:15.519191: step 125880, loss = 0.45 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 13:08:27.625173: step 125890, loss = 0.48 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 13:08:39.652156: step 125900, loss = 0.51 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 13:08:53.609086: step 125910, loss = 0.60 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 13:09:05.682873: step 125920, loss = 0.48 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 13:09:17.767828: step 125930, loss = 0.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 13:09:29.797056: step 125940, loss = 0.58 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 13:09:41.881335: step 125950, loss = 0.53 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 13:09:53.933965: step 125960, loss = 0.48 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 13:10:05.988317: step 125970, loss = 0.51 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 13:10:18.049136: step 125980, loss = 0.45 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 13:10:30.128437: step 125990, loss = 0.49 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 13:10:42.138529: step 126000, loss = 0.45 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 13:10:56.262360: step 126010, loss = 0.57 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 13:11:08.331099: step 126020, loss = 0.56 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 13:11:20.421110: step 126030, loss = 0.57 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 13:11:32.517789: step 126040, loss = 0.47 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 13:11:44.579283: step 126050, loss = 0.40 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 13:11:56.628197: step 126060, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 13:12:08.631369: step 126070, loss = 0.51 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 13:12:20.638788: step 126080, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 13:12:32.697412: step 126090, loss = 0.49 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 13:12:44.705113: step 126100, loss = 0.58 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 13:12:58.887734: step 126110, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 13:13:10.877346: step 126120, loss = 0.52 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 13:13:22.916679: step 126130, loss = 0.47 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 13:13:34.939818: step 126140, loss = 0.60 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 13:13:46.991378: step 126150, loss = 0.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 13:13:59.050680: step 126160, loss = 0.52 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 13:14:11.116412: step 126170, loss = 0.49 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 13:14:23.154284: step 126180, loss = 0.54 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 13:14:35.186927: step 126190, loss = 0.52 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 13:14:47.244650: step 126200, loss = 0.47 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 13:15:01.453819: step 126210, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 13:15:13.490404: step 126220, loss = 0.56 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 13:15:25.474984: step 126230, loss = 0.46 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 13:15:37.483732: step 126240, loss = 0.49 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 13:15:49.562098: step 126250, loss = 0.52 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 13:16:01.617618: step 126260, loss = 0.59 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 13:16:13.672951: step 126270, loss = 0.39 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 13:16:25.713445: step 126280, loss = 0.57 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 13:16:37.728539: step 126290, loss = 0.50 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 13:16:49.830496: step 126300, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 13:17:03.999919: step 126310, loss = 0.56 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 13:17:16.008265: step 126320, loss = 0.60 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 13:17:28.052976: step 126330, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 13:17:40.088031: step 126340, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 13:17:52.110265: step 126350, loss = 0.45 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 13:18:04.167385: step 126360, loss = 0.54 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 13:18:16.259211: step 126370, loss = 0.49 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 13:18:28.319055: step 126380, loss = 0.54 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 13:18:40.355582: step 126390, loss = 0.47 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 13:18:52.393113: step 126400, loss = 0.44 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 13:19:06.424856: step 126410, loss = 0.52 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 13:19:18.507046: step 126420, loss = 0.56 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 13:19:30.531848: step 126430, loss = 0.46 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 13:19:42.562329: step 126440, loss = 0.52 (24.8 examples/sec; 1.207 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 13:19:54.664571: step 126450, loss = 0.44 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 13:20:06.689380: step 126460, loss = 0.42 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 13:20:18.765450: step 126470, loss = 0.46 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 13:20:30.823992: step 126480, loss = 0.42 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 13:20:42.899747: step 126490, loss = 0.48 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 13:20:54.970443: step 126500, loss = 0.52 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 13:21:09.394620: step 126510, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 13:21:21.459232: step 126520, loss = 0.47 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 13:21:33.531752: step 126530, loss = 0.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 13:21:45.560053: step 126540, loss = 0.56 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 13:21:57.586050: step 126550, loss = 0.44 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 13:22:09.655865: step 126560, loss = 0.45 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 13:22:21.697775: step 126570, loss = 0.44 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 13:22:33.631440: step 126580, loss = 0.62 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 13:22:45.726715: step 126590, loss = 0.49 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 13:22:57.822434: step 126600, loss = 0.43 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 13:23:12.173753: step 126610, loss = 0.53 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 13:23:24.259826: step 126620, loss = 0.58 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 13:23:36.319663: step 126630, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 13:23:48.395132: step 126640, loss = 0.53 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 13:24:00.486983: step 126650, loss = 0.50 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 13:24:12.560993: step 126660, loss = 0.51 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 13:24:24.613869: step 126670, loss = 0.51 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 13:24:36.633848: step 126680, loss = 0.44 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 13:24:48.715479: step 126690, loss = 0.47 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 13:25:00.791152: step 126700, loss = 0.45 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 13:25:14.857026: step 126710, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 13:25:26.894032: step 126720, loss = 0.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 13:25:38.925286: step 126730, loss = 0.44 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 13:25:50.992854: step 126740, loss = 0.50 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 13:26:03.050107: step 126750, loss = 0.56 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 13:26:15.097319: step 126760, loss = 0.44 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 13:26:27.136244: step 126770, loss = 0.46 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 13:26:39.208859: step 126780, loss = 0.50 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 13:26:51.293496: step 126790, loss = 0.52 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 13:27:03.327876: step 126800, loss = 0.49 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 13:27:17.273889: step 126810, loss = 0.58 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 13:27:29.301750: step 126820, loss = 0.47 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 13:27:41.293086: step 126830, loss = 0.50 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 13:27:53.329370: step 126840, loss = 0.62 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 13:28:05.408528: step 126850, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 13:28:17.489921: step 126860, loss = 0.58 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 13:28:29.568095: step 126870, loss = 0.58 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 13:28:41.592154: step 126880, loss = 0.41 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 13:28:53.671103: step 126890, loss = 0.47 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 13:29:05.721793: step 126900, loss = 0.51 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 13:29:19.583211: step 126910, loss = 0.49 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 13:29:31.640637: step 126920, loss = 0.60 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 13:29:43.648883: step 126930, loss = 0.52 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 13:29:55.662464: step 126940, loss = 0.56 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 13:30:07.700474: step 126950, loss = 0.52 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 13:30:19.752589: step 126960, loss = 0.43 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 13:30:31.809047: step 126970, loss = 0.53 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 13:30:43.829426: step 126980, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 13:30:55.878737: step 126990, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 13:31:07.908962: step 127000, loss = 0.51 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 13:31:21.921228: step 127010, loss = 0.49 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 13:31:33.942019: step 127020, loss = 0.50 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 13:31:45.991116: step 127030, loss = 0.57 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 13:31:58.055344: step 127040, loss = 0.59 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 13:32:10.083147: step 127050, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 13:32:22.156342: step 127060, loss = 0.46 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 13:32:34.178551: step 127070, loss = 0.53 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 13:32:46.205615: step 127080, loss = 0.49 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 13:32:58.141035: step 127090, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 13:33:10.141908: step 127100, loss = 0.61 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 13:33:24.075511: step 127110, loss = 0.42 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 13:33:36.106222: step 127120, loss = 0.54 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 13:33:48.163474: step 127130, loss = 0.60 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 13:34:00.189546: step 127140, loss = 0.56 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 13:34:12.236356: step 127150, loss = 0.49 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 13:34:24.303123: step 127160, loss = 0.49 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 13:34:36.363497: step 127170, loss = 0.53 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 13:34:48.422860: step 127180, loss = 0.50 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 13:35:00.440870: step 127190, loss = 0.52 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 13:35:12.485299: step 127200, loss = 0.50 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 13:35:26.747891: step 127210, loss = 0.47 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 13:35:38.771344: step 127220, loss = 0.44 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 13:35:50.778770: step 127230, loss = 0.52 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 13:36:02.805730: step 127240, loss = 0.56 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 13:36:14.800972: step 127250, loss = 0.48 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 13:36:26.837027: step 127260, loss = 0.55 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 13:36:38.891789: step 127270, loss = 0.47 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 13:36:50.933002: step 127280, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 13:37:02.966018: step 127290, loss = 0.57 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 13:37:15.046286: step 127300, loss = 0.45 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 13:37:28.979680: step 127310, loss = 0.51 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 13:37:41.036016: step 127320, loss = 0.51 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 13:37:53.032182: step 127330, loss = 0.48 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 13:38:04.966079: step 127340, loss = 0.55 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 13:38:16.997687: step 127350, loss = 0.58 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 13:38:28.990924: step 127360, loss = 0.56 (25.0 examples/sec; 1.202 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 13:38:41.038628: step 127370, loss = 0.49 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 13:38:53.068922: step 127380, loss = 0.65 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 13:39:05.110433: step 127390, loss = 0.58 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 13:39:17.119845: step 127400, loss = 0.47 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 13:39:31.421359: step 127410, loss = 0.45 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 13:39:43.441426: step 127420, loss = 0.46 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 13:39:55.407252: step 127430, loss = 0.55 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 13:40:07.459919: step 127440, loss = 0.45 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 13:40:19.492118: step 127450, loss = 0.49 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 13:40:31.491228: step 127460, loss = 0.41 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 13:40:43.588177: step 127470, loss = 0.46 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 13:40:55.612259: step 127480, loss = 0.48 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 13:41:07.698455: step 127490, loss = 0.46 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 13:41:19.768060: step 127500, loss = 0.47 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 13:41:34.119001: step 127510, loss = 0.61 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 13:41:46.214261: step 127520, loss = 0.54 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 13:41:58.367485: step 127530, loss = 0.58 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 13:42:10.410396: step 127540, loss = 0.50 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 13:42:22.452210: step 127550, loss = 0.53 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 13:42:34.505106: step 127560, loss = 0.57 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 13:42:46.610447: step 127570, loss = 0.49 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 13:42:58.695555: step 127580, loss = 0.57 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 13:43:10.713155: step 127590, loss = 0.50 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-22 13:43:22.718566: step 127600, loss = 0.46 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 13:43:36.957307: step 127610, loss = 0.49 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 13:43:49.079582: step 127620, loss = 0.48 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 13:44:01.218647: step 127630, loss = 0.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 13:44:13.311761: step 127640, loss = 0.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 13:44:25.432469: step 127650, loss = 0.49 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 13:44:37.525820: step 127660, loss = 0.56 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 13:44:49.606754: step 127670, loss = 0.54 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 13:45:01.697197: step 127680, loss = 0.46 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 13:45:13.784542: step 127690, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 13:45:25.929199: step 127700, loss = 0.52 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 13:45:39.976211: step 127710, loss = 0.56 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 13:45:52.067017: step 127720, loss = 0.47 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 13:46:04.108837: step 127730, loss = 0.49 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 13:46:16.184991: step 127740, loss = 0.64 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 13:46:28.280137: step 127750, loss = 0.61 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 13:46:40.304764: step 127760, loss = 0.48 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 13:46:52.413245: step 127770, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 13:47:04.494429: step 127780, loss = 0.49 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 13:47:16.588303: step 127790, loss = 0.53 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 13:47:28.697185: step 127800, loss = 0.45 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 13:47:42.805129: step 127810, loss = 0.51 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 13:47:54.961075: step 127820, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 13:48:07.052359: step 127830, loss = 0.54 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 13:48:19.067715: step 127840, loss = 0.52 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 13:48:31.069629: step 127850, loss = 0.47 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 13:48:43.044909: step 127860, loss = 0.60 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-22 13:48:55.033834: step 127870, loss = 0.44 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 13:49:07.043672: step 127880, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 13:49:19.080632: step 127890, loss = 0.45 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 13:49:31.101256: step 127900, loss = 0.52 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 13:49:45.130328: step 127910, loss = 0.43 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 13:49:57.156352: step 127920, loss = 0.51 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 13:50:09.269575: step 127930, loss = 0.50 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 13:50:21.369217: step 127940, loss = 0.50 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 13:50:33.446095: step 127950, loss = 0.50 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 13:50:45.490425: step 127960, loss = 0.53 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 13:50:57.452704: step 127970, loss = 0.50 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-22 13:51:09.457515: step 127980, loss = 0.50 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 13:51:21.477632: step 127990, loss = 0.55 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 13:51:33.492984: step 128000, loss = 0.53 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 13:51:47.624175: step 128010, loss = 0.57 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 13:51:59.608613: step 128020, loss = 0.51 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 13:52:11.641560: step 128030, loss = 0.46 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 13:52:23.609679: step 128040, loss = 0.52 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 13:52:35.631445: step 128050, loss = 0.49 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 13:52:47.719506: step 128060, loss = 0.45 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 13:52:59.732829: step 128070, loss = 0.52 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 13:53:11.707903: step 128080, loss = 0.51 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 13:53:23.732131: step 128090, loss = 0.40 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 13:53:35.682506: step 128100, loss = 0.46 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 13:53:49.625018: step 128110, loss = 0.44 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 13:54:01.626112: step 128120, loss = 0.55 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 13:54:13.576353: step 128130, loss = 0.52 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 13:54:25.536271: step 128140, loss = 0.39 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 13:54:37.595418: step 128150, loss = 0.53 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 13:54:49.588353: step 128160, loss = 0.45 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 13:55:01.637183: step 128170, loss = 0.50 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 13:55:13.691886: step 128180, loss = 0.54 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 13:55:25.673801: step 128190, loss = 0.46 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 13:55:37.672495: step 128200, loss = 0.64 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 13:55:51.800700: step 128210, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 13:56:03.795075: step 128220, loss = 0.58 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 13:56:15.734811: step 128230, loss = 0.45 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 13:56:27.773127: step 128240, loss = 0.57 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 13:56:39.814860: step 128250, loss = 0.47 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 13:56:51.830119: step 128260, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 13:57:03.823660: step 128270, loss = 0.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 13:57:15.872704: step 128280, loss = 0.48 (24.9 examples/sec; 1.204 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 13:57:27.957583: step 128290, loss = 0.41 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 13:57:40.111671: step 128300, loss = 0.53 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 13:57:54.529809: step 128310, loss = 0.54 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 13:58:06.616750: step 128320, loss = 0.50 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 13:58:18.680998: step 128330, loss = 0.48 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 13:58:30.736197: step 128340, loss = 0.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 13:58:42.731813: step 128350, loss = 0.45 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 13:58:54.771580: step 128360, loss = 0.45 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 13:59:06.902948: step 128370, loss = 0.58 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 13:59:19.037220: step 128380, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 13:59:31.164131: step 128390, loss = 0.51 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 13:59:43.261205: step 128400, loss = 0.50 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 13:59:57.299006: step 128410, loss = 0.49 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 14:00:09.398996: step 128420, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 14:00:21.485151: step 128430, loss = 0.46 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 14:00:33.588866: step 128440, loss = 0.46 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 14:00:45.703325: step 128450, loss = 0.50 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 14:00:57.740875: step 128460, loss = 0.54 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 14:01:09.830409: step 128470, loss = 0.51 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 14:01:21.919200: step 128480, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 14:01:33.997558: step 128490, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 14:01:46.044677: step 128500, loss = 0.49 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 14:02:00.454521: step 128510, loss = 0.53 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 14:02:12.513181: step 128520, loss = 0.48 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 14:02:24.599147: step 128530, loss = 0.56 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 14:02:36.627555: step 128540, loss = 0.53 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 14:02:48.633235: step 128550, loss = 0.45 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 14:03:00.614685: step 128560, loss = 0.58 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 14:03:12.632441: step 128570, loss = 0.50 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-22 14:03:24.678800: step 128580, loss = 0.53 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 14:03:36.680302: step 128590, loss = 0.49 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 14:03:48.687840: step 128600, loss = 0.59 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 14:04:02.587926: step 128610, loss = 0.56 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 14:04:14.597661: step 128620, loss = 0.49 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 14:04:26.616692: step 128630, loss = 0.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 14:04:38.588434: step 128640, loss = 0.51 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 14:04:50.623067: step 128650, loss = 0.43 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 14:05:02.654474: step 128660, loss = 0.53 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-22 14:05:14.653977: step 128670, loss = 0.51 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 14:05:26.634849: step 128680, loss = 0.52 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 14:05:38.627425: step 128690, loss = 0.50 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 14:05:50.673524: step 128700, loss = 0.58 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 14:06:04.665992: step 128710, loss = 0.53 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 14:06:16.676732: step 128720, loss = 0.47 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 14:06:28.669328: step 128730, loss = 0.57 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 14:06:40.693160: step 128740, loss = 0.48 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 14:06:52.716086: step 128750, loss = 0.46 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 14:07:04.740437: step 128760, loss = 0.48 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 14:07:16.778738: step 128770, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 14:07:28.775764: step 128780, loss = 0.48 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 14:07:40.770291: step 128790, loss = 0.46 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 14:07:52.778265: step 128800, loss = 0.46 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 14:08:06.997652: step 128810, loss = 0.48 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 14:08:19.015603: step 128820, loss = 0.48 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 14:08:31.071294: step 128830, loss = 0.54 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 14:08:43.137421: step 128840, loss = 0.56 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 14:08:55.292566: step 128850, loss = 0.52 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 14:09:07.323513: step 128860, loss = 0.46 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 14:09:19.455814: step 128870, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 14:09:31.569240: step 128880, loss = 0.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 14:09:43.693755: step 128890, loss = 0.53 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 14:09:55.823449: step 128900, loss = 0.44 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 14:10:09.947003: step 128910, loss = 0.43 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 14:10:22.061267: step 128920, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 14:10:34.160385: step 128930, loss = 0.47 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 14:10:46.254128: step 128940, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 14:10:58.347317: step 128950, loss = 0.46 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 14:11:10.508894: step 128960, loss = 0.53 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 14:11:22.559391: step 128970, loss = 0.49 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 14:11:34.637367: step 128980, loss = 0.51 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 14:11:46.736022: step 128990, loss = 0.53 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 14:11:58.834965: step 129000, loss = 0.53 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 14:12:12.846014: step 129010, loss = 0.44 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 14:12:24.984880: step 129020, loss = 0.48 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 14:12:37.082824: step 129030, loss = 0.54 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 14:12:49.138994: step 129040, loss = 0.47 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 14:13:01.252719: step 129050, loss = 0.47 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 14:13:13.354762: step 129060, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 14:13:25.456468: step 129070, loss = 0.48 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 14:13:37.560737: step 129080, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 14:13:49.659572: step 129090, loss = 0.47 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 14:14:01.778689: step 129100, loss = 0.52 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 14:14:15.999563: step 129110, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 14:14:28.049492: step 129120, loss = 0.53 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 14:14:40.160366: step 129130, loss = 0.52 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 14:14:52.250252: step 129140, loss = 0.50 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 14:15:04.323417: step 129150, loss = 0.44 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-22 14:15:16.409144: step 129160, loss = 0.46 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 14:15:28.497611: step 129170, loss = 0.53 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 14:15:40.566496: step 129180, loss = 0.46 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 14:15:52.666333: step 129190, loss = 0.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 14:16:04.731005: step 129200, loss = 0.51 (24.9 examples/sec; 1.206 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 14:16:19.133565: step 129210, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 14:16:31.140754: step 129220, loss = 0.46 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 14:16:43.202951: step 129230, loss = 0.55 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 14:16:55.348880: step 129240, loss = 0.47 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 14:17:07.430413: step 129250, loss = 0.61 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 14:17:19.521380: step 129260, loss = 0.60 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 14:17:31.635459: step 129270, loss = 0.48 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 14:17:43.738870: step 129280, loss = 0.43 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 14:17:55.848378: step 129290, loss = 0.46 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 14:18:07.912545: step 129300, loss = 0.40 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 14:18:22.038302: step 129310, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 14:18:34.169410: step 129320, loss = 0.43 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 14:18:46.254905: step 129330, loss = 0.54 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 14:18:58.337244: step 129340, loss = 0.52 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 14:19:10.441324: step 129350, loss = 0.50 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 14:19:22.484916: step 129360, loss = 0.54 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 14:19:34.507582: step 129370, loss = 0.56 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 14:19:46.577670: step 129380, loss = 0.52 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 14:19:58.641731: step 129390, loss = 0.58 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 14:20:10.674609: step 129400, loss = 0.51 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 14:20:24.611046: step 129410, loss = 0.54 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 14:20:36.670632: step 129420, loss = 0.50 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 14:20:48.734157: step 129430, loss = 0.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 14:21:00.775486: step 129440, loss = 0.42 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 14:21:12.834370: step 129450, loss = 0.53 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 14:21:24.945849: step 129460, loss = 0.47 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 14:21:37.006359: step 129470, loss = 0.44 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 14:21:49.106864: step 129480, loss = 0.50 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 14:22:01.156082: step 129490, loss = 0.57 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 14:22:13.191838: step 129500, loss = 0.52 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 14:22:27.185779: step 129510, loss = 0.46 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 14:22:39.249335: step 129520, loss = 0.46 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 14:22:51.309088: step 129530, loss = 0.49 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 14:23:03.330186: step 129540, loss = 0.56 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 14:23:15.417677: step 129550, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 14:23:27.533261: step 129560, loss = 0.60 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 14:23:39.626006: step 129570, loss = 0.47 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 14:23:51.712924: step 129580, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 14:24:03.798370: step 129590, loss = 0.45 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 14:24:15.846537: step 129600, loss = 0.49 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 14:24:29.914107: step 129610, loss = 0.47 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 14:24:41.882251: step 129620, loss = 0.49 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 14:24:54.005759: step 129630, loss = 0.46 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 14:25:06.083114: step 129640, loss = 0.45 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 14:25:18.204711: step 129650, loss = 0.51 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 14:25:30.296261: step 129660, loss = 0.54 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 14:25:42.349878: step 129670, loss = 0.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 14:25:54.442372: step 129680, loss = 0.47 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 14:26:06.550209: step 129690, loss = 0.49 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 14:26:18.613426: step 129700, loss = 0.48 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 14:26:32.589163: step 129710, loss = 0.44 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 14:26:44.645461: step 129720, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 14:26:56.718955: step 129730, loss = 0.54 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 14:27:08.756547: step 129740, loss = 0.48 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 14:27:20.821422: step 129750, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 14:27:32.888211: step 129760, loss = 0.52 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 14:27:44.926326: step 129770, loss = 0.46 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 14:27:57.053112: step 129780, loss = 0.65 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 14:28:09.147317: step 129790, loss = 0.49 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 14:28:21.205912: step 129800, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 14:28:35.274271: step 129810, loss = 0.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 14:28:47.343583: step 129820, loss = 0.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 14:28:59.449910: step 129830, loss = 0.54 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 14:29:11.579799: step 129840, loss = 0.46 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 14:29:23.710277: step 129850, loss = 0.45 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 14:29:35.832869: step 129860, loss = 0.59 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 14:29:47.790433: step 129870, loss = 0.59 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 14:29:59.807415: step 129880, loss = 0.53 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 14:30:11.889550: step 129890, loss = 0.47 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 14:30:23.951030: step 129900, loss = 0.41 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 14:30:38.298962: step 129910, loss = 0.45 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 14:30:50.351432: step 129920, loss = 0.45 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 14:31:02.376884: step 129930, loss = 0.52 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 14:31:14.445634: step 129940, loss = 0.44 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 14:31:26.477159: step 129950, loss = 0.50 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 14:31:38.511761: step 129960, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 14:31:50.473620: step 129970, loss = 0.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 14:32:02.547236: step 129980, loss = 0.51 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 14:32:14.559601: step 129990, loss = 0.46 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 14:32:26.616264: step 130000, loss = 0.62 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 14:32:44.883251: step 130010, loss = 0.55 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 14:32:56.979097: step 130020, loss = 0.52 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 14:33:09.030337: step 130030, loss = 0.51 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 14:33:21.132569: step 130040, loss = 0.50 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 14:33:33.208787: step 130050, loss = 0.47 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 14:33:45.221706: step 130060, loss = 0.46 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 14:33:57.305228: step 130070, loss = 0.46 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 14:34:09.411730: step 130080, loss = 0.55 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 14:34:21.511873: step 130090, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 14:34:33.601418: step 130100, loss = 0.50 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 14:34:48.020277: step 130110, loss = 0.53 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 14:35:00.032141: step 130120, loss = 0.52 (25.1 examples/sec; 1.195 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 14:35:12.155556: step 130130, loss = 0.55 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 14:35:24.274582: step 130140, loss = 0.52 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 14:35:36.389255: step 130150, loss = 0.46 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 14:35:48.474407: step 130160, loss = 0.51 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 14:36:00.580255: step 130170, loss = 0.45 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 14:36:12.703019: step 130180, loss = 0.53 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 14:36:24.753945: step 130190, loss = 0.45 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 14:36:36.846973: step 130200, loss = 0.45 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 14:36:50.782649: step 130210, loss = 0.57 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 14:37:02.842617: step 130220, loss = 0.54 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 14:37:14.920333: step 130230, loss = 0.52 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 14:37:27.040335: step 130240, loss = 0.56 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 14:37:39.116547: step 130250, loss = 0.61 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 14:37:51.234353: step 130260, loss = 0.47 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 14:38:03.342431: step 130270, loss = 0.50 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 14:38:15.408626: step 130280, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 14:38:27.500838: step 130290, loss = 0.54 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 14:38:39.609710: step 130300, loss = 0.51 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 14:38:53.662301: step 130310, loss = 0.63 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 14:39:05.806192: step 130320, loss = 0.51 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 14:39:17.921240: step 130330, loss = 0.56 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 14:39:30.016990: step 130340, loss = 0.50 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 14:39:42.103237: step 130350, loss = 0.55 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 14:39:54.185340: step 130360, loss = 0.57 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 14:40:06.196506: step 130370, loss = 0.47 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 14:40:18.210541: step 130380, loss = 0.59 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 14:40:30.347400: step 130390, loss = 0.54 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 14:40:42.460270: step 130400, loss = 0.45 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 14:40:56.526101: step 130410, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 14:41:08.647138: step 130420, loss = 0.45 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 14:41:20.780864: step 130430, loss = 0.47 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 14:41:32.828988: step 130440, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 14:41:44.901148: step 130450, loss = 0.51 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 14:41:56.984916: step 130460, loss = 0.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 14:42:09.052499: step 130470, loss = 0.59 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 14:42:21.173013: step 130480, loss = 0.45 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 14:42:33.263021: step 130490, loss = 0.50 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 14:42:45.322008: step 130500, loss = 0.46 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 14:42:59.468491: step 130510, loss = 0.49 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 14:43:11.569897: step 130520, loss = 0.57 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 14:43:23.673386: step 130530, loss = 0.48 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 14:43:35.702352: step 130540, loss = 0.49 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 14:43:47.751292: step 130550, loss = 0.55 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 14:43:59.855396: step 130560, loss = 0.43 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 14:44:11.932116: step 130570, loss = 0.46 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 14:44:24.027564: step 130580, loss = 0.46 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 14:44:36.091731: step 130590, loss = 0.47 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 14:44:48.184020: step 130600, loss = 0.52 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 14:45:02.366419: step 130610, loss = 0.51 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 14:45:14.455565: step 130620, loss = 0.50 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 14:45:26.435718: step 130630, loss = 0.54 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 14:45:38.555747: step 130640, loss = 0.47 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 14:45:50.655653: step 130650, loss = 0.46 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 14:46:02.769287: step 130660, loss = 0.59 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 14:46:14.861359: step 130670, loss = 0.56 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 14:46:26.955354: step 130680, loss = 0.46 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 14:46:39.029532: step 130690, loss = 0.57 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 14:46:51.109825: step 130700, loss = 0.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 14:47:05.066381: step 130710, loss = 0.49 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 14:47:17.135006: step 130720, loss = 0.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 14:47:29.224003: step 130730, loss = 0.48 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 14:47:41.276770: step 130740, loss = 0.48 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 14:47:53.373216: step 130750, loss = 0.51 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 14:48:05.438397: step 130760, loss = 0.47 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 14:48:17.501971: step 130770, loss = 0.45 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 14:48:29.651596: step 130780, loss = 0.57 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 14:48:41.744476: step 130790, loss = 0.54 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 14:48:53.831333: step 130800, loss = 0.48 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 14:49:07.893884: step 130810, loss = 0.57 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 14:49:19.992175: step 130820, loss = 0.46 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 14:49:32.102889: step 130830, loss = 0.45 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 14:49:44.160701: step 130840, loss = 0.42 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 14:49:56.261195: step 130850, loss = 0.56 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 14:50:08.333992: step 130860, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 14:50:20.425023: step 130870, loss = 0.43 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 14:50:32.449141: step 130880, loss = 0.51 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 14:50:44.498880: step 130890, loss = 0.56 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 14:50:56.530875: step 130900, loss = 0.53 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 14:51:10.501828: step 130910, loss = 0.56 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 14:51:22.594409: step 130920, loss = 0.57 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 14:51:34.691205: step 130930, loss = 0.49 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 14:51:46.749374: step 130940, loss = 0.54 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 14:51:58.811431: step 130950, loss = 0.48 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 14:52:10.800221: step 130960, loss = 0.51 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 14:52:22.867163: step 130970, loss = 0.49 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 14:52:34.924007: step 130980, loss = 0.47 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 14:52:46.975493: step 130990, loss = 0.50 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 14:52:59.046956: step 131000, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 14:53:13.089027: step 131010, loss = 0.44 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 14:53:25.153860: step 131020, loss = 0.46 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 14:53:37.187552: step 131030, loss = 0.48 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 14:53:49.245584: step 131040, loss = 0.51 (24.7 examples/sec; 1.213 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 14:54:01.312815: step 131050, loss = 0.52 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 14:54:13.390762: step 131060, loss = 0.58 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 14:54:25.447603: step 131070, loss = 0.46 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 14:54:37.475019: step 131080, loss = 0.48 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 14:54:49.527385: step 131090, loss = 0.54 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 14:55:01.547140: step 131100, loss = 0.48 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 14:55:15.735830: step 131110, loss = 0.51 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 14:55:27.726384: step 131120, loss = 0.55 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 14:55:39.689348: step 131130, loss = 0.54 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 14:55:51.624494: step 131140, loss = 0.49 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 14:56:03.678592: step 131150, loss = 0.55 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 14:56:15.728403: step 131160, loss = 0.54 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 14:56:27.844164: step 131170, loss = 0.48 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 14:56:39.955801: step 131180, loss = 0.43 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 14:56:52.017635: step 131190, loss = 0.44 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 14:57:04.098390: step 131200, loss = 0.54 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 14:57:17.984949: step 131210, loss = 0.41 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 14:57:30.035959: step 131220, loss = 0.49 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 14:57:42.106562: step 131230, loss = 0.44 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 14:57:54.252487: step 131240, loss = 0.52 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 14:58:06.324588: step 131250, loss = 0.49 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 14:58:18.430820: step 131260, loss = 0.50 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 14:58:30.505914: step 131270, loss = 0.49 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 14:58:42.628521: step 131280, loss = 0.50 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 14:58:54.748967: step 131290, loss = 0.52 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 14:59:06.908538: step 131300, loss = 0.55 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 14:59:21.101809: step 131310, loss = 0.46 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 14:59:33.141391: step 131320, loss = 0.43 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 14:59:45.286859: step 131330, loss = 0.58 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 14:59:57.442126: step 131340, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 15:00:09.625743: step 131350, loss = 0.58 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 15:00:21.801000: step 131360, loss = 0.55 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 15:00:33.865450: step 131370, loss = 0.44 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 15:00:45.929363: step 131380, loss = 0.51 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 15:00:58.008072: step 131390, loss = 0.44 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 15:01:10.018065: step 131400, loss = 0.52 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 15:01:24.294582: step 131410, loss = 0.56 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 15:01:36.387338: step 131420, loss = 0.50 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 15:01:48.536565: step 131430, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 15:02:00.629504: step 131440, loss = 0.51 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 15:02:12.744566: step 131450, loss = 0.49 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 15:02:24.795809: step 131460, loss = 0.61 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 15:02:36.930225: step 131470, loss = 0.50 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 15:02:49.088464: step 131480, loss = 0.56 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 15:03:01.234265: step 131490, loss = 0.50 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 15:03:13.257977: step 131500, loss = 0.56 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 15:03:27.119499: step 131510, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 15:03:39.093420: step 131520, loss = 0.50 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 15:03:51.110389: step 131530, loss = 0.59 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 15:04:03.117561: step 131540, loss = 0.50 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 15:04:15.116607: step 131550, loss = 0.52 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 15:04:27.108354: step 131560, loss = 0.55 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 15:04:39.174907: step 131570, loss = 0.47 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 15:04:51.182099: step 131580, loss = 0.52 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 15:05:03.193326: step 131590, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 15:05:15.214917: step 131600, loss = 0.51 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 15:05:29.220132: step 131610, loss = 0.44 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 15:05:41.166911: step 131620, loss = 0.55 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 15:05:53.081387: step 131630, loss = 0.47 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 15:06:05.100421: step 131640, loss = 0.58 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 15:06:17.135966: step 131650, loss = 0.55 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 15:06:29.102721: step 131660, loss = 0.43 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 15:06:41.101244: step 131670, loss = 0.48 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 15:06:53.159278: step 131680, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 15:07:05.169118: step 131690, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 15:07:17.161897: step 131700, loss = 0.57 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 15:07:31.079008: step 131710, loss = 0.50 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 15:07:43.066449: step 131720, loss = 0.52 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 15:07:55.069270: step 131730, loss = 0.54 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 15:08:07.091971: step 131740, loss = 0.47 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 15:08:19.032989: step 131750, loss = 0.51 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 15:08:30.990173: step 131760, loss = 0.58 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 15:08:42.914478: step 131770, loss = 0.61 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 15:08:54.840337: step 131780, loss = 0.54 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 15:09:06.744124: step 131790, loss = 0.51 (25.5 examples/sec; 1.177 sec/batch)\n",
      "2019-05-22 15:09:18.658392: step 131800, loss = 0.61 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 15:09:32.780152: step 131810, loss = 0.48 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 15:09:44.768031: step 131820, loss = 0.47 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 15:09:56.778733: step 131830, loss = 0.55 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 15:10:08.775911: step 131840, loss = 0.52 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 15:10:20.760633: step 131850, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 15:10:32.752728: step 131860, loss = 0.47 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 15:10:44.731836: step 131870, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 15:10:56.766110: step 131880, loss = 0.54 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 15:11:08.754187: step 131890, loss = 0.50 (25.5 examples/sec; 1.176 sec/batch)\n",
      "2019-05-22 15:11:20.653628: step 131900, loss = 0.58 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 15:11:34.678319: step 131910, loss = 0.49 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 15:11:46.723463: step 131920, loss = 0.51 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 15:11:58.688546: step 131930, loss = 0.56 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 15:12:10.658747: step 131940, loss = 0.49 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 15:12:22.678125: step 131950, loss = 0.44 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 15:12:34.642937: step 131960, loss = 0.46 (25.0 examples/sec; 1.198 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 15:12:46.732944: step 131970, loss = 0.45 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 15:12:58.823461: step 131980, loss = 0.55 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 15:13:10.858691: step 131990, loss = 0.49 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 15:13:22.873434: step 132000, loss = 0.53 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 15:13:37.013939: step 132010, loss = 0.53 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 15:13:49.099474: step 132020, loss = 0.53 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 15:14:01.199000: step 132030, loss = 0.54 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 15:14:13.287061: step 132040, loss = 0.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 15:14:25.310975: step 132050, loss = 0.49 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 15:14:37.419369: step 132060, loss = 0.47 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 15:14:49.443377: step 132070, loss = 0.51 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 15:15:01.439008: step 132080, loss = 0.57 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 15:15:13.483206: step 132090, loss = 0.54 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 15:15:25.447015: step 132100, loss = 0.46 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 15:15:39.392262: step 132110, loss = 0.51 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 15:15:51.403459: step 132120, loss = 0.61 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 15:16:03.403351: step 132130, loss = 0.44 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 15:16:15.442071: step 132140, loss = 0.54 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 15:16:27.477714: step 132150, loss = 0.48 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 15:16:39.507349: step 132160, loss = 0.54 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 15:16:51.531836: step 132170, loss = 0.43 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 15:17:03.560238: step 132180, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 15:17:15.623025: step 132190, loss = 0.49 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 15:17:27.648796: step 132200, loss = 0.53 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 15:17:41.869752: step 132210, loss = 0.52 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 15:17:53.935056: step 132220, loss = 0.64 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 15:18:05.945370: step 132230, loss = 0.51 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 15:18:18.005570: step 132240, loss = 0.44 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 15:18:30.076937: step 132250, loss = 0.50 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 15:18:42.156370: step 132260, loss = 0.48 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 15:18:54.251830: step 132270, loss = 0.53 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 15:19:06.340438: step 132280, loss = 0.56 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 15:19:18.392115: step 132290, loss = 0.54 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 15:19:30.505022: step 132300, loss = 0.49 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 15:19:44.646672: step 132310, loss = 0.44 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 15:19:56.766871: step 132320, loss = 0.50 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 15:20:08.976415: step 132330, loss = 0.45 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 15:20:21.149248: step 132340, loss = 0.43 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 15:20:33.348347: step 132350, loss = 0.49 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 15:20:45.550407: step 132360, loss = 0.50 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 15:20:57.706745: step 132370, loss = 0.49 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 15:21:09.951149: step 132380, loss = 0.42 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 15:21:22.112827: step 132390, loss = 0.48 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 15:21:34.200874: step 132400, loss = 0.49 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 15:21:48.406974: step 132410, loss = 0.51 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 15:22:00.615701: step 132420, loss = 0.54 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 15:22:12.799523: step 132430, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 15:22:24.990126: step 132440, loss = 0.46 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 15:22:37.125078: step 132450, loss = 0.47 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 15:22:49.242376: step 132460, loss = 0.47 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 15:23:01.391890: step 132470, loss = 0.53 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 15:23:13.604884: step 132480, loss = 0.58 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 15:23:25.809995: step 132490, loss = 0.55 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 15:23:38.015014: step 132500, loss = 0.52 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 15:23:52.095227: step 132510, loss = 0.53 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 15:24:04.288529: step 132520, loss = 0.53 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 15:24:16.470612: step 132530, loss = 0.42 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 15:24:28.653842: step 132540, loss = 0.48 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-22 15:24:40.866646: step 132550, loss = 0.48 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 15:24:53.055063: step 132560, loss = 0.50 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 15:25:05.251583: step 132570, loss = 0.53 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 15:25:17.448106: step 132580, loss = 0.48 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 15:25:29.680569: step 132590, loss = 0.51 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 15:25:41.842468: step 132600, loss = 0.47 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 15:25:56.092738: step 132610, loss = 0.59 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 15:26:08.248934: step 132620, loss = 0.56 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 15:26:20.421948: step 132630, loss = 0.54 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 15:26:32.627256: step 132640, loss = 0.51 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 15:26:44.713677: step 132650, loss = 0.57 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 15:26:56.805416: step 132660, loss = 0.49 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 15:27:08.976651: step 132670, loss = 0.48 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 15:27:21.205954: step 132680, loss = 0.47 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 15:27:33.384764: step 132690, loss = 0.58 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 15:27:45.540740: step 132700, loss = 0.56 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 15:27:59.530264: step 132710, loss = 0.55 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 15:28:11.682751: step 132720, loss = 0.56 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 15:28:23.853816: step 132730, loss = 0.51 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 15:28:36.032423: step 132740, loss = 0.49 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 15:28:48.284857: step 132750, loss = 0.37 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 15:29:00.462795: step 132760, loss = 0.53 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 15:29:12.659319: step 132770, loss = 0.45 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 15:29:24.824495: step 132780, loss = 0.52 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 15:29:37.015141: step 132790, loss = 0.59 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 15:29:49.199456: step 132800, loss = 0.43 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 15:30:03.445582: step 132810, loss = 0.60 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 15:30:15.616795: step 132820, loss = 0.49 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 15:30:27.785429: step 132830, loss = 0.42 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 15:30:39.917775: step 132840, loss = 0.44 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 15:30:52.045025: step 132850, loss = 0.55 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 15:31:04.201019: step 132860, loss = 0.54 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 15:31:16.331553: step 132870, loss = 0.54 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 15:31:28.497694: step 132880, loss = 0.54 (24.8 examples/sec; 1.207 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 15:31:40.580222: step 132890, loss = 0.50 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 15:31:52.634015: step 132900, loss = 0.55 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 15:32:06.707648: step 132910, loss = 0.47 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 15:32:18.788414: step 132920, loss = 0.66 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 15:32:30.885422: step 132930, loss = 0.50 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 15:32:43.020625: step 132940, loss = 0.44 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 15:32:55.060230: step 132950, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 15:33:07.174734: step 132960, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 15:33:19.345732: step 132970, loss = 0.54 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 15:33:31.525894: step 132980, loss = 0.53 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 15:33:43.662005: step 132990, loss = 0.57 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 15:33:55.796540: step 133000, loss = 0.54 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 15:34:10.102821: step 133010, loss = 0.59 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 15:34:22.257941: step 133020, loss = 0.60 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 15:34:34.392551: step 133030, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 15:34:46.533509: step 133040, loss = 0.55 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 15:34:58.727611: step 133050, loss = 0.49 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 15:35:10.843688: step 133060, loss = 0.52 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 15:35:22.984891: step 133070, loss = 0.43 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 15:35:35.120538: step 133080, loss = 0.49 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 15:35:47.213703: step 133090, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 15:35:59.349510: step 133100, loss = 0.47 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 15:36:13.501248: step 133110, loss = 0.54 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 15:36:25.657614: step 133120, loss = 0.50 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 15:36:37.791953: step 133130, loss = 0.48 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 15:36:49.923281: step 133140, loss = 0.46 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 15:37:02.100299: step 133150, loss = 0.54 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 15:37:14.085897: step 133160, loss = 0.61 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 15:37:26.226167: step 133170, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 15:37:38.362869: step 133180, loss = 0.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 15:37:50.517595: step 133190, loss = 0.57 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 15:38:02.583773: step 133200, loss = 0.56 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 15:38:17.150912: step 133210, loss = 0.47 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 15:38:29.310172: step 133220, loss = 0.45 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 15:38:41.440389: step 133230, loss = 0.54 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 15:38:53.616809: step 133240, loss = 0.52 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 15:39:05.730345: step 133250, loss = 0.48 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 15:39:17.843117: step 133260, loss = 0.57 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 15:39:30.009256: step 133270, loss = 0.55 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 15:39:42.139174: step 133280, loss = 0.44 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 15:39:54.241109: step 133290, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 15:40:06.307025: step 133300, loss = 0.48 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 15:40:20.342718: step 133310, loss = 0.51 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 15:40:32.339015: step 133320, loss = 0.50 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 15:40:44.348864: step 133330, loss = 0.43 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 15:40:56.403926: step 133340, loss = 0.48 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 15:41:08.488657: step 133350, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 15:41:20.563395: step 133360, loss = 0.46 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 15:41:32.599935: step 133370, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 15:41:44.657959: step 133380, loss = 0.52 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 15:41:56.755511: step 133390, loss = 0.51 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 15:42:08.886468: step 133400, loss = 0.61 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 15:42:23.029398: step 133410, loss = 0.48 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 15:42:35.128772: step 133420, loss = 0.53 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 15:42:47.303793: step 133430, loss = 0.52 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 15:42:59.432917: step 133440, loss = 0.49 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 15:43:11.489321: step 133450, loss = 0.50 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 15:43:23.609035: step 133460, loss = 0.54 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 15:43:35.785680: step 133470, loss = 0.57 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 15:43:47.932123: step 133480, loss = 0.45 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 15:44:00.078974: step 133490, loss = 0.44 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 15:44:12.240771: step 133500, loss = 0.46 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 15:44:26.275669: step 133510, loss = 0.45 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 15:44:38.364233: step 133520, loss = 0.57 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 15:44:50.444600: step 133530, loss = 0.48 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 15:45:02.743025: step 133540, loss = 0.43 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 15:45:15.016737: step 133550, loss = 0.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 15:45:27.222263: step 133560, loss = 0.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 15:45:39.343585: step 133570, loss = 0.47 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 15:45:51.624345: step 133580, loss = 0.55 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 15:46:04.032314: step 133590, loss = 0.59 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 15:46:16.366504: step 133600, loss = 0.53 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 15:46:30.632635: step 133610, loss = 0.47 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 15:46:43.065875: step 133620, loss = 0.44 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 15:46:55.380499: step 133630, loss = 0.55 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 15:47:07.606299: step 133640, loss = 0.47 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 15:47:19.991620: step 133650, loss = 0.51 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 15:47:31.987102: step 133660, loss = 0.45 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 15:47:44.118459: step 133670, loss = 0.47 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 15:47:56.415170: step 133680, loss = 0.44 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 15:48:08.670298: step 133690, loss = 0.50 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 15:48:20.917195: step 133700, loss = 0.53 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 15:48:35.038898: step 133710, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 15:48:47.199082: step 133720, loss = 0.48 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 15:48:59.569194: step 133730, loss = 0.41 (23.3 examples/sec; 1.289 sec/batch)\n",
      "2019-05-22 15:49:11.817881: step 133740, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 15:49:24.276510: step 133750, loss = 0.48 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 15:49:36.610529: step 133760, loss = 0.46 (23.1 examples/sec; 1.298 sec/batch)\n",
      "2019-05-22 15:49:49.030151: step 133770, loss = 0.47 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 15:50:01.368357: step 133780, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 15:50:13.806483: step 133790, loss = 0.49 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 15:50:26.083452: step 133800, loss = 0.49 (24.4 examples/sec; 1.228 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 15:50:40.257577: step 133810, loss = 0.45 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 15:50:52.668018: step 133820, loss = 0.54 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 15:51:04.925269: step 133830, loss = 0.48 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 15:51:17.198739: step 133840, loss = 0.57 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 15:51:29.548872: step 133850, loss = 0.49 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 15:51:41.882411: step 133860, loss = 0.43 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 15:51:54.143660: step 133870, loss = 0.44 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 15:52:06.612719: step 133880, loss = 0.47 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 15:52:18.932806: step 133890, loss = 0.51 (22.7 examples/sec; 1.319 sec/batch)\n",
      "2019-05-22 15:52:31.231852: step 133900, loss = 0.47 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 15:52:45.421309: step 133910, loss = 0.49 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 15:52:57.677980: step 133920, loss = 0.47 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 15:53:09.941975: step 133930, loss = 0.56 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 15:53:22.296671: step 133940, loss = 0.50 (22.8 examples/sec; 1.314 sec/batch)\n",
      "2019-05-22 15:53:34.618609: step 133950, loss = 0.45 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 15:53:46.917248: step 133960, loss = 0.54 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 15:53:59.191804: step 133970, loss = 0.44 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 15:54:11.446966: step 133980, loss = 0.47 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 15:54:23.770059: step 133990, loss = 0.52 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 15:54:36.174276: step 134000, loss = 0.50 (22.9 examples/sec; 1.313 sec/batch)\n",
      "2019-05-22 15:54:50.598967: step 134010, loss = 0.48 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 15:55:03.011553: step 134020, loss = 0.47 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 15:55:15.350253: step 134030, loss = 0.47 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 15:55:27.565510: step 134040, loss = 0.55 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 15:55:39.935576: step 134050, loss = 0.51 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 15:55:52.282342: step 134060, loss = 0.53 (23.1 examples/sec; 1.299 sec/batch)\n",
      "2019-05-22 15:56:04.613727: step 134070, loss = 0.46 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 15:56:16.962358: step 134080, loss = 0.47 (23.1 examples/sec; 1.301 sec/batch)\n",
      "2019-05-22 15:56:29.429301: step 134090, loss = 0.41 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 15:56:41.761139: step 134100, loss = 0.60 (23.2 examples/sec; 1.295 sec/batch)\n",
      "2019-05-22 15:56:56.123128: step 134110, loss = 0.56 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 15:57:08.386211: step 134120, loss = 0.43 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 15:57:20.508436: step 134130, loss = 0.51 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 15:57:32.640632: step 134140, loss = 0.52 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 15:57:44.730036: step 134150, loss = 0.50 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 15:57:56.626494: step 134160, loss = 0.53 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 15:58:08.719333: step 134170, loss = 0.45 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 15:58:20.938494: step 134180, loss = 0.55 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 15:58:33.036373: step 134190, loss = 0.47 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 15:58:45.271830: step 134200, loss = 0.48 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 15:58:59.368368: step 134210, loss = 0.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 15:59:11.562986: step 134220, loss = 0.46 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 15:59:23.876354: step 134230, loss = 0.45 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 15:59:36.037030: step 134240, loss = 0.40 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 15:59:48.262085: step 134250, loss = 0.50 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 16:00:00.455250: step 134260, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 16:00:12.682018: step 134270, loss = 0.51 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 16:00:24.870129: step 134280, loss = 0.44 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 16:00:37.034541: step 134290, loss = 0.43 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 16:00:49.230582: step 134300, loss = 0.49 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 16:01:03.319700: step 134310, loss = 0.57 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 16:01:15.499191: step 134320, loss = 0.51 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 16:01:27.694418: step 134330, loss = 0.51 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 16:01:39.853049: step 134340, loss = 0.55 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 16:01:52.047459: step 134350, loss = 0.53 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 16:02:04.256362: step 134360, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 16:02:16.445330: step 134370, loss = 0.52 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 16:02:28.631568: step 134380, loss = 0.46 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 16:02:40.818690: step 134390, loss = 0.54 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 16:02:52.966772: step 134400, loss = 0.47 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 16:03:06.922575: step 134410, loss = 0.48 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 16:03:19.120584: step 134420, loss = 0.57 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 16:03:31.222619: step 134430, loss = 0.45 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 16:03:43.414660: step 134440, loss = 0.50 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 16:03:55.595733: step 134450, loss = 0.53 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 16:04:07.816793: step 134460, loss = 0.47 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 16:04:20.050097: step 134470, loss = 0.46 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 16:04:32.265845: step 134480, loss = 0.48 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 16:04:44.463375: step 134490, loss = 0.54 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 16:04:56.659703: step 134500, loss = 0.53 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 16:05:10.766572: step 134510, loss = 0.52 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 16:05:22.894570: step 134520, loss = 0.51 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 16:05:35.124565: step 134530, loss = 0.55 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 16:05:47.345835: step 134540, loss = 0.48 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 16:05:59.478325: step 134550, loss = 0.46 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 16:06:11.635606: step 134560, loss = 0.51 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 16:06:23.803336: step 134570, loss = 0.49 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 16:06:35.971857: step 134580, loss = 0.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 16:06:48.191571: step 134590, loss = 0.46 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 16:07:00.339824: step 134600, loss = 0.56 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 16:07:14.388781: step 134610, loss = 0.55 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 16:07:26.484957: step 134620, loss = 0.57 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 16:07:38.605655: step 134630, loss = 0.44 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 16:07:50.787027: step 134640, loss = 0.45 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 16:08:02.930189: step 134650, loss = 0.47 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 16:08:14.917274: step 134660, loss = 0.43 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 16:08:27.095023: step 134670, loss = 0.57 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 16:08:39.138512: step 134680, loss = 0.53 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 16:08:51.331146: step 134690, loss = 0.52 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 16:09:03.588145: step 134700, loss = 0.60 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 16:09:17.981662: step 134710, loss = 0.45 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 16:09:30.083281: step 134720, loss = 0.58 (24.8 examples/sec; 1.212 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 16:09:42.107146: step 134730, loss = 0.48 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 16:09:54.110404: step 134740, loss = 0.48 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 16:10:06.175085: step 134750, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 16:10:18.205267: step 134760, loss = 0.45 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 16:10:30.363981: step 134770, loss = 0.57 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 16:10:42.489806: step 134780, loss = 0.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 16:10:54.614144: step 134790, loss = 0.51 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 16:11:06.765064: step 134800, loss = 0.55 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 16:11:20.845193: step 134810, loss = 0.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 16:11:33.006538: step 134820, loss = 0.54 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 16:11:45.144205: step 134830, loss = 0.54 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 16:11:57.299700: step 134840, loss = 0.53 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 16:12:09.447283: step 134850, loss = 0.54 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 16:12:21.588554: step 134860, loss = 0.46 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 16:12:33.676734: step 134870, loss = 0.52 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 16:12:45.849801: step 134880, loss = 0.53 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 16:12:58.014212: step 134890, loss = 0.43 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 16:13:10.186863: step 134900, loss = 0.54 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 16:13:24.563959: step 134910, loss = 0.53 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 16:13:36.667821: step 134920, loss = 0.57 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 16:13:48.719378: step 134930, loss = 0.49 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 16:14:00.917817: step 134940, loss = 0.51 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 16:14:13.077148: step 134950, loss = 0.46 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 16:14:25.244209: step 134960, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 16:14:37.422298: step 134970, loss = 0.47 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 16:14:49.575835: step 134980, loss = 0.52 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 16:15:01.716427: step 134990, loss = 0.41 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 16:15:13.817125: step 135000, loss = 0.50 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 16:15:31.398259: step 135010, loss = 0.43 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 16:15:43.596315: step 135020, loss = 0.55 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 16:15:55.666874: step 135030, loss = 0.51 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 16:16:07.813838: step 135040, loss = 0.52 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 16:16:20.013646: step 135050, loss = 0.46 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 16:16:32.183246: step 135060, loss = 0.53 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 16:16:44.303125: step 135070, loss = 0.46 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 16:16:56.435280: step 135080, loss = 0.46 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 16:17:08.534108: step 135090, loss = 0.55 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 16:17:20.682494: step 135100, loss = 0.50 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 16:17:34.845970: step 135110, loss = 0.48 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 16:17:46.981280: step 135120, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 16:17:59.152180: step 135130, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 16:18:11.318349: step 135140, loss = 0.49 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 16:18:23.501779: step 135150, loss = 0.50 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 16:18:35.547717: step 135160, loss = 0.44 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 16:18:47.647710: step 135170, loss = 0.51 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 16:18:59.825115: step 135180, loss = 0.45 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 16:19:12.006710: step 135190, loss = 0.43 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 16:19:24.181165: step 135200, loss = 0.47 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 16:19:38.248411: step 135210, loss = 0.49 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 16:19:50.382574: step 135220, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 16:20:02.587726: step 135230, loss = 0.49 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 16:20:14.766277: step 135240, loss = 0.54 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 16:20:26.915882: step 135250, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 16:20:39.062960: step 135260, loss = 0.49 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 16:20:51.247661: step 135270, loss = 0.47 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 16:21:03.355379: step 135280, loss = 0.54 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 16:21:15.592520: step 135290, loss = 0.42 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 16:21:27.795327: step 135300, loss = 0.53 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 16:21:41.857732: step 135310, loss = 0.43 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 16:21:53.999706: step 135320, loss = 0.56 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 16:22:06.184070: step 135330, loss = 0.48 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 16:22:18.372866: step 135340, loss = 0.47 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 16:22:30.523835: step 135350, loss = 0.43 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 16:22:42.658385: step 135360, loss = 0.46 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 16:22:54.803022: step 135370, loss = 0.50 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 16:23:06.931843: step 135380, loss = 0.51 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 16:23:19.120004: step 135390, loss = 0.49 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 16:23:31.274207: step 135400, loss = 0.54 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 16:23:45.179592: step 135410, loss = 0.51 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 16:23:57.206711: step 135420, loss = 0.54 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 16:24:09.356414: step 135430, loss = 0.44 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 16:24:21.560640: step 135440, loss = 0.54 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 16:24:33.699346: step 135450, loss = 0.54 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 16:24:45.926707: step 135460, loss = 0.53 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 16:24:58.040184: step 135470, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 16:25:10.256116: step 135480, loss = 0.46 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 16:25:22.416918: step 135490, loss = 0.51 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 16:25:34.561777: step 135500, loss = 0.56 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 16:25:48.818201: step 135510, loss = 0.54 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 16:26:00.930114: step 135520, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 16:26:13.030227: step 135530, loss = 0.50 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 16:26:25.196126: step 135540, loss = 0.57 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 16:26:37.305648: step 135550, loss = 0.53 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 16:26:49.535098: step 135560, loss = 0.57 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 16:27:01.708939: step 135570, loss = 0.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 16:27:13.932250: step 135580, loss = 0.47 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 16:27:26.102641: step 135590, loss = 0.51 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 16:27:38.350831: step 135600, loss = 0.48 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 16:27:52.488266: step 135610, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 16:28:04.654237: step 135620, loss = 0.53 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 16:28:16.855204: step 135630, loss = 0.42 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 16:28:29.050355: step 135640, loss = 0.50 (24.6 examples/sec; 1.220 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 16:28:41.251914: step 135650, loss = 0.52 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 16:28:53.335535: step 135660, loss = 0.48 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 16:29:05.389633: step 135670, loss = 0.53 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 16:29:17.560686: step 135680, loss = 0.49 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 16:29:29.713201: step 135690, loss = 0.61 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 16:29:41.902110: step 135700, loss = 0.48 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 16:29:56.012466: step 135710, loss = 0.48 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 16:30:08.200289: step 135720, loss = 0.43 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 16:30:20.389009: step 135730, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 16:30:32.525253: step 135740, loss = 0.57 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 16:30:44.616684: step 135750, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 16:30:56.770527: step 135760, loss = 0.54 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 16:31:08.930230: step 135770, loss = 0.47 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 16:31:21.079325: step 135780, loss = 0.54 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 16:31:33.284733: step 135790, loss = 0.47 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 16:31:45.470588: step 135800, loss = 0.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 16:31:59.664856: step 135810, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 16:32:11.840494: step 135820, loss = 0.47 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 16:32:23.985646: step 135830, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 16:32:36.196804: step 135840, loss = 0.53 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-22 16:32:48.370347: step 135850, loss = 0.44 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 16:33:00.555867: step 135860, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 16:33:12.724353: step 135870, loss = 0.51 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 16:33:24.932391: step 135880, loss = 0.57 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 16:33:37.082553: step 135890, loss = 0.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 16:33:49.247044: step 135900, loss = 0.44 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 16:34:03.190847: step 135910, loss = 0.47 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 16:34:15.261002: step 135920, loss = 0.54 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 16:34:27.421263: step 135930, loss = 0.53 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 16:34:39.605549: step 135940, loss = 0.46 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 16:34:51.754742: step 135950, loss = 0.46 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 16:35:03.895169: step 135960, loss = 0.52 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 16:35:16.072517: step 135970, loss = 0.49 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 16:35:28.212829: step 135980, loss = 0.44 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 16:35:40.324996: step 135990, loss = 0.47 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 16:35:52.421818: step 136000, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 16:36:06.403986: step 136010, loss = 0.53 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 16:36:18.570282: step 136020, loss = 0.51 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 16:36:30.714642: step 136030, loss = 0.48 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 16:36:42.921890: step 136040, loss = 0.45 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 16:36:55.114495: step 136050, loss = 0.56 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 16:37:07.282888: step 136060, loss = 0.45 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 16:37:19.455720: step 136070, loss = 0.48 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 16:37:31.626905: step 136080, loss = 0.49 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 16:37:43.803638: step 136090, loss = 0.51 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 16:37:55.982607: step 136100, loss = 0.48 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 16:38:10.202651: step 136110, loss = 0.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 16:38:22.363152: step 136120, loss = 0.50 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 16:38:34.520915: step 136130, loss = 0.44 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 16:38:46.666898: step 136140, loss = 0.42 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 16:38:58.895601: step 136150, loss = 0.53 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 16:39:10.971873: step 136160, loss = 0.54 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 16:39:22.945771: step 136170, loss = 0.42 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 16:39:35.121222: step 136180, loss = 0.52 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 16:39:47.310273: step 136190, loss = 0.42 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 16:39:59.497320: step 136200, loss = 0.47 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 16:40:13.506357: step 136210, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 16:40:25.709153: step 136220, loss = 0.48 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 16:40:37.876749: step 136230, loss = 0.49 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 16:40:49.984806: step 136240, loss = 0.52 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 16:41:02.172958: step 136250, loss = 0.50 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 16:41:14.355884: step 136260, loss = 0.45 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 16:41:26.510023: step 136270, loss = 0.49 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 16:41:38.634222: step 136280, loss = 0.50 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 16:41:50.790552: step 136290, loss = 0.47 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 16:42:03.017724: step 136300, loss = 0.47 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 16:42:17.056312: step 136310, loss = 0.55 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 16:42:29.231057: step 136320, loss = 0.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 16:42:41.415049: step 136330, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 16:42:53.593925: step 136340, loss = 0.58 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 16:43:05.772126: step 136350, loss = 0.53 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 16:43:17.955136: step 136360, loss = 0.53 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 16:43:30.129773: step 136370, loss = 0.55 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 16:43:42.314325: step 136380, loss = 0.42 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 16:43:54.488757: step 136390, loss = 0.52 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 16:44:06.636602: step 136400, loss = 0.56 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 16:44:20.623818: step 136410, loss = 0.63 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 16:44:32.598788: step 136420, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 16:44:44.791919: step 136430, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 16:44:56.974590: step 136440, loss = 0.54 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 16:45:09.080284: step 136450, loss = 0.51 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 16:45:21.084053: step 136460, loss = 0.52 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 16:45:33.165343: step 136470, loss = 0.54 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 16:45:45.342189: step 136480, loss = 0.49 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 16:45:57.481049: step 136490, loss = 0.56 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 16:46:09.646268: step 136500, loss = 0.49 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 16:46:23.770861: step 136510, loss = 0.49 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 16:46:35.948911: step 136520, loss = 0.51 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 16:46:48.063346: step 136530, loss = 0.39 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 16:47:00.121655: step 136540, loss = 0.46 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 16:47:12.269218: step 136550, loss = 0.44 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 16:47:24.418676: step 136560, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 16:47:36.544713: step 136570, loss = 0.42 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 16:47:48.729498: step 136580, loss = 0.60 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 16:48:00.895680: step 136590, loss = 0.54 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 16:48:13.025569: step 136600, loss = 0.53 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 16:48:27.570004: step 136610, loss = 0.42 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 16:48:39.698623: step 136620, loss = 0.49 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 16:48:51.838037: step 136630, loss = 0.45 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 16:49:03.988937: step 136640, loss = 0.48 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 16:49:16.185312: step 136650, loss = 0.51 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 16:49:28.256996: step 136660, loss = 0.51 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 16:49:40.228117: step 136670, loss = 0.50 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 16:49:52.369779: step 136680, loss = 0.50 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 16:50:04.527381: step 136690, loss = 0.46 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 16:50:16.681002: step 136700, loss = 0.50 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 16:50:30.684818: step 136710, loss = 0.45 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 16:50:42.818546: step 136720, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 16:50:54.907227: step 136730, loss = 0.46 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 16:51:07.057802: step 136740, loss = 0.48 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 16:51:19.238262: step 136750, loss = 0.53 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 16:51:31.379499: step 136760, loss = 0.48 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 16:51:43.548221: step 136770, loss = 0.50 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 16:51:55.701056: step 136780, loss = 0.57 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 16:52:07.897656: step 136790, loss = 0.50 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 16:52:20.113288: step 136800, loss = 0.47 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 16:52:34.166790: step 136810, loss = 0.46 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 16:52:46.325749: step 136820, loss = 0.56 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 16:52:58.513717: step 136830, loss = 0.53 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-22 16:53:10.622873: step 136840, loss = 0.49 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 16:53:22.775843: step 136850, loss = 0.47 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 16:53:34.944280: step 136860, loss = 0.47 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 16:53:47.085681: step 136870, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 16:53:59.232963: step 136880, loss = 0.49 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 16:54:11.475560: step 136890, loss = 0.53 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 16:54:23.582772: step 136900, loss = 0.45 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 16:54:37.756987: step 136910, loss = 0.56 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 16:54:49.707416: step 136920, loss = 0.48 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 16:55:01.788260: step 136930, loss = 0.39 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 16:55:13.965061: step 136940, loss = 0.55 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 16:55:26.154669: step 136950, loss = 0.49 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 16:55:38.317034: step 136960, loss = 0.44 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 16:55:50.472989: step 136970, loss = 0.49 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 16:56:02.670966: step 136980, loss = 0.48 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 16:56:14.894828: step 136990, loss = 0.54 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 16:56:27.052413: step 137000, loss = 0.54 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 16:56:41.090052: step 137010, loss = 0.52 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 16:56:53.284619: step 137020, loss = 0.51 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 16:57:05.416478: step 137030, loss = 0.47 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 16:57:17.495442: step 137040, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 16:57:29.647233: step 137050, loss = 0.57 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 16:57:41.856524: step 137060, loss = 0.54 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 16:57:54.014088: step 137070, loss = 0.51 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 16:58:06.228093: step 137080, loss = 0.45 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 16:58:18.373756: step 137090, loss = 0.63 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 16:58:30.550192: step 137100, loss = 0.54 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 16:58:44.553858: step 137110, loss = 0.44 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 16:58:56.762038: step 137120, loss = 0.49 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 16:59:08.934677: step 137130, loss = 0.49 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 16:59:21.096876: step 137140, loss = 0.45 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 16:59:33.150436: step 137150, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 16:59:45.314830: step 137160, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 16:59:57.380498: step 137170, loss = 0.52 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 17:00:09.487858: step 137180, loss = 0.46 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 17:00:21.634718: step 137190, loss = 0.50 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 17:00:33.816905: step 137200, loss = 0.53 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 17:00:47.811419: step 137210, loss = 0.62 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 17:00:59.991138: step 137220, loss = 0.55 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 17:01:12.167347: step 137230, loss = 0.55 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 17:01:24.424124: step 137240, loss = 0.47 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 17:01:36.605030: step 137250, loss = 0.52 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 17:01:48.782984: step 137260, loss = 0.50 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 17:02:00.980517: step 137270, loss = 0.54 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 17:02:13.121631: step 137280, loss = 0.49 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 17:02:25.324323: step 137290, loss = 0.47 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 17:02:37.506032: step 137300, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 17:02:51.528783: step 137310, loss = 0.43 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 17:03:03.651964: step 137320, loss = 0.46 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 17:03:15.822222: step 137330, loss = 0.56 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 17:03:28.025716: step 137340, loss = 0.45 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 17:03:40.193515: step 137350, loss = 0.50 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 17:03:52.286034: step 137360, loss = 0.42 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 17:04:04.390242: step 137370, loss = 0.60 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 17:04:16.530417: step 137380, loss = 0.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 17:04:28.627299: step 137390, loss = 0.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 17:04:40.663977: step 137400, loss = 0.50 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 17:04:54.842524: step 137410, loss = 0.50 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 17:05:06.944626: step 137420, loss = 0.48 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-22 17:05:18.935383: step 137430, loss = 0.54 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 17:05:31.105503: step 137440, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 17:05:43.225988: step 137450, loss = 0.58 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 17:05:55.406857: step 137460, loss = 0.46 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 17:06:07.566485: step 137470, loss = 0.54 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 17:06:19.704710: step 137480, loss = 0.47 (24.6 examples/sec; 1.218 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 17:06:31.868083: step 137490, loss = 0.48 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 17:06:44.049028: step 137500, loss = 0.41 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 17:06:58.159568: step 137510, loss = 0.43 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 17:07:10.297976: step 137520, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 17:07:22.486577: step 137530, loss = 0.49 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 17:07:34.640647: step 137540, loss = 0.45 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 17:07:46.799959: step 137550, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 17:07:58.929173: step 137560, loss = 0.47 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 17:08:11.061165: step 137570, loss = 0.53 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 17:08:23.229710: step 137580, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 17:08:35.388901: step 137590, loss = 0.55 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 17:08:47.588684: step 137600, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 17:09:01.906955: step 137610, loss = 0.45 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 17:09:14.101898: step 137620, loss = 0.50 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 17:09:26.265138: step 137630, loss = 0.49 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 17:09:38.416217: step 137640, loss = 0.52 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 17:09:50.493343: step 137650, loss = 0.51 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 17:10:02.636748: step 137660, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 17:10:14.838836: step 137670, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 17:10:26.834675: step 137680, loss = 0.45 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 17:10:38.990269: step 137690, loss = 0.61 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 17:10:51.150434: step 137700, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 17:11:05.244088: step 137710, loss = 0.51 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 17:11:17.397645: step 137720, loss = 0.39 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 17:11:29.533964: step 137730, loss = 0.46 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 17:11:41.675192: step 137740, loss = 0.47 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 17:11:53.885433: step 137750, loss = 0.46 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 17:12:06.067692: step 137760, loss = 0.40 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 17:12:18.233346: step 137770, loss = 0.43 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 17:12:30.404966: step 137780, loss = 0.59 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 17:12:42.550137: step 137790, loss = 0.44 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 17:12:54.762293: step 137800, loss = 0.43 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 17:13:09.132418: step 137810, loss = 0.45 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 17:13:21.220277: step 137820, loss = 0.54 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 17:13:33.282419: step 137830, loss = 0.55 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 17:13:45.392759: step 137840, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 17:13:57.556313: step 137850, loss = 0.52 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 17:14:09.728857: step 137860, loss = 0.60 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 17:14:21.852933: step 137870, loss = 0.60 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 17:14:34.040954: step 137880, loss = 0.49 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 17:14:46.154013: step 137890, loss = 0.55 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 17:14:58.238273: step 137900, loss = 0.47 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 17:15:12.467806: step 137910, loss = 0.50 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 17:15:24.605639: step 137920, loss = 0.48 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 17:15:36.605962: step 137930, loss = 0.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 17:15:48.805988: step 137940, loss = 0.51 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 17:16:00.984151: step 137950, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 17:16:13.174859: step 137960, loss = 0.47 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 17:16:25.315243: step 137970, loss = 0.38 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 17:16:37.488936: step 137980, loss = 0.44 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 17:16:49.667370: step 137990, loss = 0.46 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 17:17:01.820391: step 138000, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 17:17:16.007551: step 138010, loss = 0.54 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 17:17:28.117396: step 138020, loss = 0.44 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 17:17:40.288684: step 138030, loss = 0.57 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 17:17:52.414888: step 138040, loss = 0.47 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 17:18:04.575793: step 138050, loss = 0.55 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 17:18:16.763220: step 138060, loss = 0.43 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 17:18:29.015141: step 138070, loss = 0.48 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 17:18:41.174218: step 138080, loss = 0.50 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 17:18:53.322684: step 138090, loss = 0.46 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 17:19:05.473250: step 138100, loss = 0.50 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 17:19:19.535967: step 138110, loss = 0.55 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 17:19:31.693303: step 138120, loss = 0.53 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 17:19:43.921211: step 138130, loss = 0.44 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 17:19:55.990790: step 138140, loss = 0.46 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 17:20:08.149160: step 138150, loss = 0.51 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 17:20:20.307608: step 138160, loss = 0.60 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 17:20:32.536591: step 138170, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 17:20:44.501939: step 138180, loss = 0.47 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 17:20:56.613508: step 138190, loss = 0.44 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 17:21:08.745209: step 138200, loss = 0.46 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 17:21:22.938192: step 138210, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 17:21:35.082237: step 138220, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 17:21:47.272682: step 138230, loss = 0.49 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 17:21:59.421746: step 138240, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 17:22:11.572495: step 138250, loss = 0.49 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 17:22:23.770156: step 138260, loss = 0.47 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 17:22:35.902097: step 138270, loss = 0.54 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 17:22:48.004083: step 138280, loss = 0.45 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 17:23:00.074963: step 138290, loss = 0.50 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 17:23:12.191563: step 138300, loss = 0.51 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 17:23:26.223520: step 138310, loss = 0.53 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 17:23:38.230574: step 138320, loss = 0.56 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 17:23:50.275298: step 138330, loss = 0.51 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 17:24:02.389352: step 138340, loss = 0.39 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 17:24:14.491304: step 138350, loss = 0.50 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 17:24:26.686333: step 138360, loss = 0.44 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 17:24:38.811213: step 138370, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 17:24:51.027230: step 138380, loss = 0.58 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 17:25:03.136204: step 138390, loss = 0.53 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 17:25:15.318105: step 138400, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 17:25:29.401951: step 138410, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 17:25:41.607477: step 138420, loss = 0.59 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 17:25:53.635153: step 138430, loss = 0.45 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 17:26:05.706576: step 138440, loss = 0.47 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 17:26:17.813600: step 138450, loss = 0.45 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 17:26:29.909247: step 138460, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 17:26:41.918218: step 138470, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 17:26:54.132341: step 138480, loss = 0.55 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 17:27:06.226632: step 138490, loss = 0.49 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 17:27:18.308774: step 138500, loss = 0.47 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 17:27:32.376923: step 138510, loss = 0.46 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 17:27:44.467651: step 138520, loss = 0.55 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 17:27:56.595704: step 138530, loss = 0.53 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 17:28:08.812186: step 138540, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 17:28:20.994106: step 138550, loss = 0.57 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 17:28:33.148694: step 138560, loss = 0.45 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 17:28:45.317672: step 138570, loss = 0.55 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 17:28:57.463667: step 138580, loss = 0.53 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 17:29:09.645919: step 138590, loss = 0.46 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 17:29:21.851186: step 138600, loss = 0.50 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 17:29:36.169317: step 138610, loss = 0.45 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 17:29:48.348618: step 138620, loss = 0.58 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 17:30:00.503825: step 138630, loss = 0.48 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 17:30:12.558983: step 138640, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 17:30:24.674182: step 138650, loss = 0.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 17:30:36.863755: step 138660, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 17:30:49.004903: step 138670, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 17:31:01.111630: step 138680, loss = 0.42 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 17:31:13.188078: step 138690, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 17:31:25.321407: step 138700, loss = 0.56 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 17:31:39.396513: step 138710, loss = 0.53 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 17:31:51.583270: step 138720, loss = 0.54 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 17:32:03.754465: step 138730, loss = 0.45 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 17:32:15.961829: step 138740, loss = 0.47 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 17:32:28.145776: step 138750, loss = 0.65 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 17:32:40.296162: step 138760, loss = 0.41 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 17:32:52.503698: step 138770, loss = 0.45 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 17:33:04.708996: step 138780, loss = 0.46 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 17:33:16.960542: step 138790, loss = 0.51 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-22 17:33:29.134388: step 138800, loss = 0.54 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 17:33:43.564205: step 138810, loss = 0.47 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 17:33:55.765758: step 138820, loss = 0.46 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 17:34:07.936720: step 138830, loss = 0.51 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 17:34:20.124046: step 138840, loss = 0.51 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 17:34:32.316580: step 138850, loss = 0.51 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 17:34:44.480011: step 138860, loss = 0.50 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 17:34:56.631080: step 138870, loss = 0.46 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 17:35:08.709766: step 138880, loss = 0.61 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 17:35:20.840968: step 138890, loss = 0.60 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 17:35:33.075716: step 138900, loss = 0.51 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 17:35:47.051993: step 138910, loss = 0.43 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 17:35:59.108722: step 138920, loss = 0.57 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 17:36:11.150588: step 138930, loss = 0.52 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 17:36:23.055674: step 138940, loss = 0.51 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 17:36:35.027175: step 138950, loss = 0.45 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 17:36:47.260467: step 138960, loss = 0.53 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 17:36:59.485533: step 138970, loss = 0.48 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 17:37:11.727040: step 138980, loss = 0.46 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 17:37:23.866361: step 138990, loss = 0.58 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 17:37:36.062637: step 139000, loss = 0.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 17:37:50.184665: step 139010, loss = 0.51 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 17:38:02.395585: step 139020, loss = 0.55 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 17:38:14.594088: step 139030, loss = 0.47 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 17:38:26.831734: step 139040, loss = 0.47 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 17:38:38.969211: step 139050, loss = 0.48 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 17:38:51.181644: step 139060, loss = 0.58 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 17:39:03.342965: step 139070, loss = 0.49 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 17:39:15.536966: step 139080, loss = 0.48 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 17:39:27.736570: step 139090, loss = 0.52 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 17:39:39.942703: step 139100, loss = 0.52 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 17:39:54.101519: step 139110, loss = 0.40 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 17:40:06.297081: step 139120, loss = 0.42 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 17:40:18.389930: step 139130, loss = 0.56 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 17:40:30.635783: step 139140, loss = 0.53 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 17:40:42.835926: step 139150, loss = 0.47 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 17:40:54.989736: step 139160, loss = 0.55 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 17:41:07.137395: step 139170, loss = 0.46 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 17:41:19.301684: step 139180, loss = 0.56 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 17:41:31.418596: step 139190, loss = 0.50 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 17:41:43.595483: step 139200, loss = 0.49 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 17:41:57.733571: step 139210, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 17:42:09.889867: step 139220, loss = 0.49 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 17:42:22.107249: step 139230, loss = 0.43 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 17:42:34.342397: step 139240, loss = 0.52 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 17:42:46.549411: step 139250, loss = 0.64 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 17:42:58.787659: step 139260, loss = 0.52 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 17:43:10.998772: step 139270, loss = 0.40 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 17:43:23.232198: step 139280, loss = 0.46 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 17:43:35.437396: step 139290, loss = 0.52 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 17:43:47.624874: step 139300, loss = 0.50 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 17:44:02.089936: step 139310, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 17:44:14.283584: step 139320, loss = 0.53 (24.6 examples/sec; 1.220 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 17:44:26.456745: step 139330, loss = 0.52 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 17:44:38.650193: step 139340, loss = 0.51 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 17:44:50.863241: step 139350, loss = 0.48 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 17:45:03.014796: step 139360, loss = 0.47 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 17:45:15.187371: step 139370, loss = 0.50 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 17:45:27.284885: step 139380, loss = 0.50 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 17:45:39.369666: step 139390, loss = 0.50 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 17:45:51.449237: step 139400, loss = 0.56 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 17:46:05.610219: step 139410, loss = 0.46 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 17:46:17.752893: step 139420, loss = 0.47 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 17:46:29.978317: step 139430, loss = 0.49 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 17:46:41.968995: step 139440, loss = 0.50 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 17:46:54.169684: step 139450, loss = 0.55 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 17:47:06.313263: step 139460, loss = 0.56 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 17:47:18.478576: step 139470, loss = 0.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 17:47:30.613532: step 139480, loss = 0.51 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 17:47:42.774918: step 139490, loss = 0.51 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 17:47:54.968790: step 139500, loss = 0.54 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 17:48:09.535843: step 139510, loss = 0.55 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 17:48:21.711253: step 139520, loss = 0.45 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 17:48:33.856528: step 139530, loss = 0.54 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 17:48:46.102961: step 139540, loss = 0.51 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 17:48:58.348916: step 139550, loss = 0.47 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 17:49:10.507010: step 139560, loss = 0.40 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 17:49:22.674843: step 139570, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 17:49:34.851462: step 139580, loss = 0.48 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 17:49:46.977604: step 139590, loss = 0.48 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 17:49:59.144908: step 139600, loss = 0.43 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 17:50:13.608076: step 139610, loss = 0.48 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 17:50:25.745757: step 139620, loss = 0.50 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 17:50:37.856102: step 139630, loss = 0.53 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 17:50:50.078616: step 139640, loss = 0.54 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 17:51:02.253859: step 139650, loss = 0.55 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 17:51:14.420239: step 139660, loss = 0.45 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 17:51:26.636379: step 139670, loss = 0.49 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 17:51:38.859493: step 139680, loss = 0.51 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-22 17:51:50.881200: step 139690, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 17:52:03.091056: step 139700, loss = 0.47 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 17:52:17.165850: step 139710, loss = 0.53 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 17:52:29.348871: step 139720, loss = 0.58 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 17:52:41.512539: step 139730, loss = 0.48 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 17:52:53.715278: step 139740, loss = 0.52 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-22 17:53:05.911494: step 139750, loss = 0.47 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 17:53:18.088203: step 139760, loss = 0.56 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 17:53:30.260468: step 139770, loss = 0.50 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 17:53:42.458214: step 139780, loss = 0.53 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 17:53:54.642558: step 139790, loss = 0.48 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-22 17:54:06.899937: step 139800, loss = 0.47 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 17:54:21.004969: step 139810, loss = 0.57 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 17:54:33.148298: step 139820, loss = 0.48 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 17:54:45.345394: step 139830, loss = 0.45 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 17:54:57.561146: step 139840, loss = 0.46 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 17:55:09.680777: step 139850, loss = 0.40 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 17:55:21.844657: step 139860, loss = 0.44 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 17:55:33.927738: step 139870, loss = 0.49 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 17:55:46.055145: step 139880, loss = 0.47 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 17:55:58.215586: step 139890, loss = 0.54 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 17:56:10.490137: step 139900, loss = 0.63 (23.3 examples/sec; 1.289 sec/batch)\n",
      "2019-05-22 17:56:24.606799: step 139910, loss = 0.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 17:56:36.794845: step 139920, loss = 0.56 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 17:56:49.007463: step 139930, loss = 0.41 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 17:57:01.046582: step 139940, loss = 0.46 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 17:57:13.252293: step 139950, loss = 0.50 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 17:57:25.352816: step 139960, loss = 0.44 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 17:57:37.523640: step 139970, loss = 0.47 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 17:57:49.678449: step 139980, loss = 0.50 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-22 17:58:01.880746: step 139990, loss = 0.58 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 17:58:14.022666: step 140000, loss = 0.45 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 17:58:31.579205: step 140010, loss = 0.58 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 17:58:43.768540: step 140020, loss = 0.45 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 17:58:55.839129: step 140030, loss = 0.37 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 17:59:08.033732: step 140040, loss = 0.58 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 17:59:20.198737: step 140050, loss = 0.45 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 17:59:32.345892: step 140060, loss = 0.47 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 17:59:44.502062: step 140070, loss = 0.56 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 17:59:56.644282: step 140080, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 18:00:08.835988: step 140090, loss = 0.50 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 18:00:21.007203: step 140100, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 18:00:35.573766: step 140110, loss = 0.47 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 18:00:47.702389: step 140120, loss = 0.39 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 18:00:59.878981: step 140130, loss = 0.49 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 18:01:12.088029: step 140140, loss = 0.50 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 18:01:24.273258: step 140150, loss = 0.49 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 18:01:36.474059: step 140160, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 18:01:48.715771: step 140170, loss = 0.46 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 18:02:00.905779: step 140180, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 18:02:12.834503: step 140190, loss = 0.44 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 18:02:25.009760: step 140200, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 18:02:39.168825: step 140210, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 18:02:51.391087: step 140220, loss = 0.55 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 18:03:03.576727: step 140230, loss = 0.43 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 18:03:15.821342: step 140240, loss = 0.44 (24.4 examples/sec; 1.231 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 18:03:28.005646: step 140250, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 18:03:40.207953: step 140260, loss = 0.46 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 18:03:52.398562: step 140270, loss = 0.44 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 18:04:04.610203: step 140280, loss = 0.50 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 18:04:16.815107: step 140290, loss = 0.41 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 18:04:29.036825: step 140300, loss = 0.51 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 18:04:43.062850: step 140310, loss = 0.51 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 18:04:55.136755: step 140320, loss = 0.49 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 18:05:07.220768: step 140330, loss = 0.52 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 18:05:19.356916: step 140340, loss = 0.42 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 18:05:31.446634: step 140350, loss = 0.50 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 18:05:43.569249: step 140360, loss = 0.51 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 18:05:55.708360: step 140370, loss = 0.43 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 18:06:07.892254: step 140380, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 18:06:20.067712: step 140390, loss = 0.45 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 18:06:32.245346: step 140400, loss = 0.54 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 18:06:46.316329: step 140410, loss = 0.49 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 18:06:58.459538: step 140420, loss = 0.41 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 18:07:10.635088: step 140430, loss = 0.58 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 18:07:22.668759: step 140440, loss = 0.45 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 18:07:34.803526: step 140450, loss = 0.46 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 18:07:46.976798: step 140460, loss = 0.42 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 18:07:59.094592: step 140470, loss = 0.47 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 18:08:11.371068: step 140480, loss = 0.47 (23.0 examples/sec; 1.304 sec/batch)\n",
      "2019-05-22 18:08:23.547958: step 140490, loss = 0.46 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 18:08:35.697692: step 140500, loss = 0.56 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 18:08:49.883844: step 140510, loss = 0.48 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 18:09:02.104495: step 140520, loss = 0.49 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 18:09:14.329786: step 140530, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 18:09:26.536368: step 140540, loss = 0.56 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 18:09:38.694457: step 140550, loss = 0.56 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 18:09:50.883877: step 140560, loss = 0.53 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 18:10:03.043899: step 140570, loss = 0.45 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 18:10:15.258008: step 140580, loss = 0.61 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 18:10:27.398648: step 140590, loss = 0.51 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 18:10:39.635156: step 140600, loss = 0.50 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 18:10:53.668191: step 140610, loss = 0.43 (23.2 examples/sec; 1.292 sec/batch)\n",
      "2019-05-22 18:11:05.817308: step 140620, loss = 0.57 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 18:11:17.991076: step 140630, loss = 0.51 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 18:11:30.117906: step 140640, loss = 0.47 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 18:11:42.237451: step 140650, loss = 0.51 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 18:11:54.368864: step 140660, loss = 0.50 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 18:12:06.567101: step 140670, loss = 0.53 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 18:12:18.743132: step 140680, loss = 0.45 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 18:12:30.763605: step 140690, loss = 0.59 (25.3 examples/sec; 1.187 sec/batch)\n",
      "2019-05-22 18:12:42.946524: step 140700, loss = 0.49 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 18:12:57.022254: step 140710, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 18:13:09.189298: step 140720, loss = 0.40 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 18:13:21.392747: step 140730, loss = 0.54 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 18:13:33.509382: step 140740, loss = 0.45 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 18:13:45.646938: step 140750, loss = 0.50 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 18:13:57.808162: step 140760, loss = 0.53 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 18:14:09.993510: step 140770, loss = 0.43 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 18:14:22.147820: step 140780, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 18:14:34.258793: step 140790, loss = 0.53 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 18:14:46.410575: step 140800, loss = 0.51 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 18:15:00.423433: step 140810, loss = 0.48 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 18:15:12.605587: step 140820, loss = 0.44 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 18:15:24.778658: step 140830, loss = 0.44 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 18:15:36.978715: step 140840, loss = 0.56 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 18:15:49.188531: step 140850, loss = 0.45 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 18:16:01.241505: step 140860, loss = 0.56 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 18:16:13.434574: step 140870, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 18:16:25.674583: step 140880, loss = 0.45 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 18:16:37.820822: step 140890, loss = 0.57 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 18:16:50.013648: step 140900, loss = 0.62 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 18:17:04.195803: step 140910, loss = 0.58 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 18:17:16.384223: step 140920, loss = 0.47 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 18:17:28.569695: step 140930, loss = 0.43 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 18:17:40.627271: step 140940, loss = 0.50 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 18:17:52.745492: step 140950, loss = 0.48 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 18:18:04.927492: step 140960, loss = 0.45 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 18:18:17.139335: step 140970, loss = 0.53 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 18:18:29.360835: step 140980, loss = 0.47 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 18:18:41.535356: step 140990, loss = 0.52 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 18:18:53.754035: step 141000, loss = 0.45 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 18:19:07.792889: step 141010, loss = 0.55 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 18:19:19.941745: step 141020, loss = 0.45 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 18:19:32.132085: step 141030, loss = 0.46 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 18:19:44.324399: step 141040, loss = 0.45 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 18:19:56.530353: step 141050, loss = 0.43 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 18:20:08.738079: step 141060, loss = 0.47 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 18:20:20.886127: step 141070, loss = 0.46 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 18:20:33.167845: step 141080, loss = 0.51 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 18:20:45.370185: step 141090, loss = 0.59 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 18:20:57.562217: step 141100, loss = 0.47 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 18:21:11.525043: step 141110, loss = 0.47 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 18:21:23.680859: step 141120, loss = 0.46 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 18:21:35.817988: step 141130, loss = 0.45 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 18:21:47.939301: step 141140, loss = 0.50 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 18:22:00.111010: step 141150, loss = 0.45 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 18:22:12.312539: step 141160, loss = 0.51 (24.4 examples/sec; 1.231 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 18:22:24.632096: step 141170, loss = 0.51 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 18:22:36.753230: step 141180, loss = 0.48 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 18:22:48.861364: step 141190, loss = 0.47 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 18:23:00.937433: step 141200, loss = 0.44 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 18:23:15.090369: step 141210, loss = 0.59 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 18:23:27.250800: step 141220, loss = 0.45 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 18:23:39.467555: step 141230, loss = 0.43 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 18:23:51.781330: step 141240, loss = 0.46 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 18:24:03.910586: step 141250, loss = 0.45 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 18:24:16.020444: step 141260, loss = 0.54 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 18:24:28.116864: step 141270, loss = 0.55 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 18:24:40.260999: step 141280, loss = 0.51 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 18:24:52.482013: step 141290, loss = 0.53 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 18:25:04.674730: step 141300, loss = 0.43 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 18:25:19.202023: step 141310, loss = 0.61 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 18:25:31.364025: step 141320, loss = 0.57 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 18:25:43.545311: step 141330, loss = 0.45 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 18:25:55.740876: step 141340, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 18:26:07.859105: step 141350, loss = 0.45 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 18:26:19.999214: step 141360, loss = 0.43 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 18:26:32.241114: step 141370, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 18:26:44.396999: step 141380, loss = 0.50 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-22 18:26:56.585812: step 141390, loss = 0.57 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 18:27:08.731854: step 141400, loss = 0.51 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 18:27:22.781375: step 141410, loss = 0.57 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 18:27:35.034333: step 141420, loss = 0.49 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 18:27:47.200873: step 141430, loss = 0.53 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 18:27:59.251032: step 141440, loss = 0.44 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 18:28:11.350259: step 141450, loss = 0.53 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 18:28:23.544752: step 141460, loss = 0.61 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 18:28:35.778553: step 141470, loss = 0.47 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 18:28:47.948866: step 141480, loss = 0.48 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 18:29:00.092642: step 141490, loss = 0.59 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 18:29:12.295120: step 141500, loss = 0.44 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 18:29:26.740911: step 141510, loss = 0.48 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 18:29:38.932320: step 141520, loss = 0.44 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 18:29:51.138739: step 141530, loss = 0.50 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-22 18:30:03.321445: step 141540, loss = 0.49 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 18:30:15.530359: step 141550, loss = 0.47 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 18:30:27.770999: step 141560, loss = 0.60 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 18:30:39.955545: step 141570, loss = 0.55 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 18:30:52.116723: step 141580, loss = 0.51 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 18:31:04.272627: step 141590, loss = 0.55 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 18:31:16.323464: step 141600, loss = 0.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 18:31:30.428273: step 141610, loss = 0.49 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 18:31:42.550373: step 141620, loss = 0.53 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 18:31:54.764924: step 141630, loss = 0.55 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 18:32:06.920022: step 141640, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 18:32:19.109395: step 141650, loss = 0.44 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 18:32:31.317892: step 141660, loss = 0.53 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 18:32:43.546896: step 141670, loss = 0.56 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-22 18:32:55.750662: step 141680, loss = 0.43 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 18:33:07.931387: step 141690, loss = 0.47 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 18:33:19.980344: step 141700, loss = 0.51 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 18:33:34.222949: step 141710, loss = 0.48 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 18:33:46.297528: step 141720, loss = 0.39 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 18:33:58.458952: step 141730, loss = 0.51 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 18:34:10.617563: step 141740, loss = 0.48 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 18:34:22.804150: step 141750, loss = 0.43 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 18:34:34.996689: step 141760, loss = 0.49 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 18:34:47.164187: step 141770, loss = 0.50 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 18:34:59.313882: step 141780, loss = 0.47 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 18:35:11.516783: step 141790, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 18:35:23.704542: step 141800, loss = 0.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 18:35:38.192571: step 141810, loss = 0.49 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 18:35:50.324981: step 141820, loss = 0.57 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 18:36:02.511414: step 141830, loss = 0.53 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 18:36:14.700646: step 141840, loss = 0.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 18:36:26.837430: step 141850, loss = 0.52 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 18:36:38.987969: step 141860, loss = 0.45 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 18:36:51.218424: step 141870, loss = 0.44 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 18:37:03.380996: step 141880, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 18:37:15.564951: step 141890, loss = 0.48 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 18:37:27.774877: step 141900, loss = 0.59 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 18:37:42.089108: step 141910, loss = 0.52 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 18:37:54.205059: step 141920, loss = 0.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 18:38:06.381046: step 141930, loss = 0.46 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 18:38:18.546801: step 141940, loss = 0.50 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 18:38:30.612680: step 141950, loss = 0.53 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 18:38:42.789716: step 141960, loss = 0.47 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 18:38:54.958698: step 141970, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 18:39:07.198009: step 141980, loss = 0.47 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 18:39:19.369916: step 141990, loss = 0.46 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 18:39:31.618483: step 142000, loss = 0.46 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 18:39:45.710742: step 142010, loss = 0.43 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 18:39:57.860264: step 142020, loss = 0.46 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 18:40:10.055610: step 142030, loss = 0.45 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 18:40:22.233340: step 142040, loss = 0.51 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 18:40:34.450467: step 142050, loss = 0.44 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 18:40:46.671080: step 142060, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 18:40:58.867745: step 142070, loss = 0.47 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 18:41:11.060385: step 142080, loss = 0.49 (24.8 examples/sec; 1.211 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 18:41:23.200543: step 142090, loss = 0.54 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 18:41:35.339672: step 142100, loss = 0.44 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 18:41:49.416852: step 142110, loss = 0.46 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 18:42:01.607190: step 142120, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 18:42:13.840718: step 142130, loss = 0.41 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 18:42:26.066893: step 142140, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 18:42:38.295428: step 142150, loss = 0.49 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 18:42:50.493975: step 142160, loss = 0.58 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 18:43:02.639431: step 142170, loss = 0.51 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 18:43:14.786076: step 142180, loss = 0.57 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 18:43:26.882225: step 142190, loss = 0.54 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 18:43:38.909942: step 142200, loss = 0.44 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 18:43:52.971428: step 142210, loss = 0.53 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 18:44:05.106637: step 142220, loss = 0.47 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 18:44:17.270528: step 142230, loss = 0.55 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 18:44:29.451431: step 142240, loss = 0.47 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 18:44:41.695083: step 142250, loss = 0.53 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 18:44:53.893341: step 142260, loss = 0.56 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 18:45:06.126147: step 142270, loss = 0.45 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-22 18:45:18.366692: step 142280, loss = 0.46 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 18:45:30.546410: step 142290, loss = 0.45 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 18:45:42.685947: step 142300, loss = 0.51 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 18:45:56.744375: step 142310, loss = 0.55 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 18:46:08.961848: step 142320, loss = 0.41 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 18:46:21.162040: step 142330, loss = 0.49 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 18:46:33.238831: step 142340, loss = 0.51 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 18:46:45.384196: step 142350, loss = 0.50 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 18:46:57.541114: step 142360, loss = 0.44 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 18:47:09.708172: step 142370, loss = 0.52 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 18:47:21.891707: step 142380, loss = 0.41 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 18:47:34.130920: step 142390, loss = 0.46 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 18:47:46.338344: step 142400, loss = 0.48 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 18:48:00.483162: step 142410, loss = 0.43 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 18:48:12.678510: step 142420, loss = 0.45 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 18:48:24.848323: step 142430, loss = 0.55 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 18:48:37.014592: step 142440, loss = 0.65 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 18:48:49.002866: step 142450, loss = 0.45 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 18:49:01.203259: step 142460, loss = 0.48 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 18:49:13.445818: step 142470, loss = 0.47 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 18:49:25.638204: step 142480, loss = 0.49 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 18:49:37.892699: step 142490, loss = 0.50 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 18:49:50.060178: step 142500, loss = 0.51 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 18:50:04.457468: step 142510, loss = 0.54 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 18:50:16.692987: step 142520, loss = 0.56 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 18:50:28.920277: step 142530, loss = 0.55 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 18:50:41.112363: step 142540, loss = 0.49 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 18:50:53.314734: step 142550, loss = 0.47 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 18:51:05.555898: step 142560, loss = 0.51 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 18:51:17.726997: step 142570, loss = 0.49 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 18:51:29.913680: step 142580, loss = 0.49 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 18:51:42.010437: step 142590, loss = 0.52 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 18:51:54.213998: step 142600, loss = 0.46 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 18:52:08.306233: step 142610, loss = 0.47 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 18:52:20.487041: step 142620, loss = 0.45 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 18:52:32.651785: step 142630, loss = 0.51 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 18:52:44.734760: step 142640, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 18:52:56.890887: step 142650, loss = 0.41 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 18:53:09.044174: step 142660, loss = 0.51 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 18:53:21.199439: step 142670, loss = 0.59 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 18:53:33.357036: step 142680, loss = 0.53 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 18:53:45.568422: step 142690, loss = 0.53 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 18:53:57.563950: step 142700, loss = 0.48 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 18:54:11.683408: step 142710, loss = 0.48 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 18:54:23.874114: step 142720, loss = 0.49 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 18:54:36.052859: step 142730, loss = 0.40 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 18:54:48.230400: step 142740, loss = 0.49 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 18:55:00.437416: step 142750, loss = 0.40 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 18:55:12.651447: step 142760, loss = 0.53 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 18:55:24.849010: step 142770, loss = 0.46 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 18:55:37.062531: step 142780, loss = 0.50 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 18:55:49.300191: step 142790, loss = 0.60 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 18:56:01.493049: step 142800, loss = 0.47 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 18:56:16.026038: step 142810, loss = 0.50 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 18:56:28.195474: step 142820, loss = 0.56 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 18:56:40.329240: step 142830, loss = 0.42 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 18:56:52.444005: step 142840, loss = 0.48 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 18:57:04.631183: step 142850, loss = 0.53 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 18:57:16.816854: step 142860, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 18:57:29.009993: step 142870, loss = 0.39 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 18:57:41.195157: step 142880, loss = 0.47 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 18:57:53.378084: step 142890, loss = 0.49 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 18:58:05.557654: step 142900, loss = 0.53 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 18:58:20.048371: step 142910, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 18:58:32.256028: step 142920, loss = 0.51 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 18:58:44.440789: step 142930, loss = 0.46 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 18:58:56.650380: step 142940, loss = 0.47 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 18:59:08.653098: step 142950, loss = 0.41 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 18:59:20.867537: step 142960, loss = 0.51 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 18:59:33.110142: step 142970, loss = 0.43 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 18:59:45.369724: step 142980, loss = 0.44 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 18:59:57.548298: step 142990, loss = 0.48 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 19:00:09.694934: step 143000, loss = 0.48 (24.6 examples/sec; 1.220 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 19:00:23.778857: step 143010, loss = 0.46 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 19:00:35.976901: step 143020, loss = 0.42 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 19:00:48.197418: step 143030, loss = 0.53 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 19:01:00.402002: step 143040, loss = 0.49 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 19:01:12.673844: step 143050, loss = 0.43 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 19:01:24.874836: step 143060, loss = 0.54 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 19:01:37.080534: step 143070, loss = 0.55 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 19:01:49.167854: step 143080, loss = 0.49 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 19:02:01.326071: step 143090, loss = 0.47 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 19:02:13.378553: step 143100, loss = 0.45 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 19:02:27.390999: step 143110, loss = 0.49 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 19:02:39.489438: step 143120, loss = 0.53 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 19:02:51.622046: step 143130, loss = 0.51 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 19:03:03.809983: step 143140, loss = 0.44 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 19:03:16.086773: step 143150, loss = 0.49 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 19:03:28.264648: step 143160, loss = 0.45 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 19:03:40.399011: step 143170, loss = 0.46 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 19:03:52.559081: step 143180, loss = 0.56 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 19:04:04.755085: step 143190, loss = 0.55 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 19:04:16.831129: step 143200, loss = 0.55 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 19:04:31.361362: step 143210, loss = 0.57 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 19:04:43.607413: step 143220, loss = 0.47 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-22 19:04:55.811012: step 143230, loss = 0.51 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 19:05:07.989629: step 143240, loss = 0.49 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 19:05:20.244608: step 143250, loss = 0.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 19:05:32.430309: step 143260, loss = 0.53 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 19:05:44.619818: step 143270, loss = 0.49 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 19:05:56.784192: step 143280, loss = 0.48 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 19:06:08.908826: step 143290, loss = 0.49 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 19:06:21.099645: step 143300, loss = 0.55 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 19:06:35.172327: step 143310, loss = 0.56 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 19:06:47.388766: step 143320, loss = 0.49 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 19:06:59.491048: step 143330, loss = 0.55 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 19:07:11.702163: step 143340, loss = 0.49 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 19:07:23.894579: step 143350, loss = 0.47 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 19:07:36.116740: step 143360, loss = 0.42 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 19:07:48.330679: step 143370, loss = 0.49 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 19:08:00.516560: step 143380, loss = 0.52 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 19:08:12.654250: step 143390, loss = 0.41 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 19:08:24.863031: step 143400, loss = 0.49 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 19:08:39.151018: step 143410, loss = 0.44 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 19:08:51.376831: step 143420, loss = 0.50 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 19:09:03.585193: step 143430, loss = 0.49 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 19:09:15.771197: step 143440, loss = 0.45 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 19:09:27.784944: step 143450, loss = 0.52 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 19:09:39.853857: step 143460, loss = 0.46 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 19:09:52.044243: step 143470, loss = 0.43 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 19:10:04.196747: step 143480, loss = 0.44 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 19:10:16.349047: step 143490, loss = 0.53 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 19:10:28.512914: step 143500, loss = 0.47 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 19:10:42.563829: step 143510, loss = 0.56 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 19:10:54.772605: step 143520, loss = 0.46 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 19:11:06.936471: step 143530, loss = 0.55 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 19:11:19.117752: step 143540, loss = 0.52 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 19:11:31.353755: step 143550, loss = 0.48 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 19:11:43.443264: step 143560, loss = 0.54 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 19:11:55.574250: step 143570, loss = 0.50 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 19:12:07.614653: step 143580, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 19:12:19.724948: step 143590, loss = 0.58 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 19:12:31.866479: step 143600, loss = 0.43 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-22 19:12:46.033253: step 143610, loss = 0.53 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 19:12:58.227191: step 143620, loss = 0.57 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 19:13:10.356529: step 143630, loss = 0.50 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 19:13:22.582579: step 143640, loss = 0.48 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 19:13:34.746554: step 143650, loss = 0.54 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 19:13:46.920720: step 143660, loss = 0.45 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 19:13:59.163287: step 143670, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 19:14:11.411804: step 143680, loss = 0.52 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 19:14:23.624855: step 143690, loss = 0.50 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 19:14:35.689241: step 143700, loss = 0.42 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 19:14:49.771416: step 143710, loss = 0.55 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 19:15:01.958814: step 143720, loss = 0.44 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 19:15:14.135801: step 143730, loss = 0.46 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 19:15:26.365926: step 143740, loss = 0.54 (24.0 examples/sec; 1.249 sec/batch)\n",
      "2019-05-22 19:15:38.520147: step 143750, loss = 0.47 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 19:15:50.702458: step 143760, loss = 0.48 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 19:16:02.847009: step 143770, loss = 0.60 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 19:16:14.974382: step 143780, loss = 0.50 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 19:16:27.152893: step 143790, loss = 0.54 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 19:16:39.377195: step 143800, loss = 0.59 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 19:16:53.426454: step 143810, loss = 0.55 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 19:17:05.506582: step 143820, loss = 0.51 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 19:17:17.672084: step 143830, loss = 0.55 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 19:17:29.814466: step 143840, loss = 0.50 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 19:17:42.007820: step 143850, loss = 0.50 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 19:17:54.233965: step 143860, loss = 0.42 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 19:18:06.366656: step 143870, loss = 0.46 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 19:18:18.492231: step 143880, loss = 0.47 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 19:18:30.698993: step 143890, loss = 0.50 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 19:18:42.900298: step 143900, loss = 0.46 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 19:18:57.141442: step 143910, loss = 0.56 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 19:19:09.278564: step 143920, loss = 0.48 (24.8 examples/sec; 1.209 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 19:19:21.498254: step 143930, loss = 0.51 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 19:19:33.656985: step 143940, loss = 0.60 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 19:19:45.809866: step 143950, loss = 0.51 (25.5 examples/sec; 1.178 sec/batch)\n",
      "2019-05-22 19:19:57.905289: step 143960, loss = 0.50 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 19:20:10.039103: step 143970, loss = 0.44 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 19:20:22.167734: step 143980, loss = 0.46 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 19:20:34.428472: step 143990, loss = 0.56 (22.9 examples/sec; 1.311 sec/batch)\n",
      "2019-05-22 19:20:46.623571: step 144000, loss = 0.51 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 19:21:01.162339: step 144010, loss = 0.55 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 19:21:13.338851: step 144020, loss = 0.44 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 19:21:25.456606: step 144030, loss = 0.44 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 19:21:37.562939: step 144040, loss = 0.55 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 19:21:49.685827: step 144050, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 19:22:01.892083: step 144060, loss = 0.54 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 19:22:13.985419: step 144070, loss = 0.53 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 19:22:26.144344: step 144080, loss = 0.47 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 19:22:38.340475: step 144090, loss = 0.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 19:22:50.573414: step 144100, loss = 0.43 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 19:23:04.689848: step 144110, loss = 0.53 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 19:23:16.920360: step 144120, loss = 0.53 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 19:23:29.090404: step 144130, loss = 0.55 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 19:23:41.297501: step 144140, loss = 0.65 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 19:23:53.465354: step 144150, loss = 0.54 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 19:24:05.723016: step 144160, loss = 0.46 (24.0 examples/sec; 1.250 sec/batch)\n",
      "2019-05-22 19:24:17.839834: step 144170, loss = 0.44 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 19:24:29.986845: step 144180, loss = 0.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 19:24:42.186520: step 144190, loss = 0.50 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 19:24:54.325413: step 144200, loss = 0.45 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 19:25:08.305079: step 144210, loss = 0.53 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 19:25:20.496050: step 144220, loss = 0.48 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 19:25:32.678636: step 144230, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 19:25:44.918519: step 144240, loss = 0.49 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 19:25:57.107694: step 144250, loss = 0.49 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 19:26:09.386595: step 144260, loss = 0.47 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 19:26:21.537770: step 144270, loss = 0.51 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 19:26:33.771257: step 144280, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 19:26:45.964114: step 144290, loss = 0.47 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 19:26:58.160146: step 144300, loss = 0.48 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 19:27:12.201889: step 144310, loss = 0.45 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 19:27:24.276182: step 144320, loss = 0.52 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 19:27:36.451126: step 144330, loss = 0.58 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 19:27:48.730919: step 144340, loss = 0.44 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 19:28:00.943285: step 144350, loss = 0.51 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 19:28:13.105364: step 144360, loss = 0.53 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 19:28:25.366408: step 144370, loss = 0.56 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 19:28:37.517888: step 144380, loss = 0.44 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 19:28:49.758800: step 144390, loss = 0.45 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 19:29:01.953920: step 144400, loss = 0.45 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 19:29:16.045357: step 144410, loss = 0.48 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 19:29:28.211655: step 144420, loss = 0.61 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 19:29:40.379770: step 144430, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 19:29:52.590758: step 144440, loss = 0.51 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 19:30:04.786362: step 144450, loss = 0.53 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 19:30:16.800052: step 144460, loss = 0.53 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 19:30:28.968739: step 144470, loss = 0.50 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 19:30:41.176358: step 144480, loss = 0.41 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 19:30:53.314710: step 144490, loss = 0.55 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 19:31:05.481247: step 144500, loss = 0.41 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 19:31:19.586983: step 144510, loss = 0.51 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 19:31:31.676863: step 144520, loss = 0.53 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 19:31:43.882111: step 144530, loss = 0.50 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 19:31:56.064151: step 144540, loss = 0.46 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 19:32:08.251480: step 144550, loss = 0.51 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 19:32:20.358947: step 144560, loss = 0.52 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 19:32:32.544516: step 144570, loss = 0.56 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 19:32:44.718433: step 144580, loss = 0.62 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 19:32:56.880770: step 144590, loss = 0.45 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 19:33:09.056883: step 144600, loss = 0.48 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 19:33:23.245563: step 144610, loss = 0.48 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 19:33:35.417865: step 144620, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 19:33:47.554264: step 144630, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 19:33:59.841962: step 144640, loss = 0.42 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 19:34:12.076963: step 144650, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 19:34:24.262848: step 144660, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 19:34:36.478321: step 144670, loss = 0.56 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 19:34:48.634384: step 144680, loss = 0.50 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 19:35:00.826839: step 144690, loss = 0.54 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 19:35:12.994111: step 144700, loss = 0.55 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 19:35:26.965352: step 144710, loss = 0.47 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 19:35:39.108383: step 144720, loss = 0.43 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 19:35:51.365262: step 144730, loss = 0.46 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 19:36:03.531799: step 144740, loss = 0.46 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 19:36:15.734744: step 144750, loss = 0.52 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 19:36:27.876259: step 144760, loss = 0.51 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 19:36:40.035054: step 144770, loss = 0.50 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 19:36:52.219354: step 144780, loss = 0.48 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 19:37:04.393539: step 144790, loss = 0.48 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 19:37:16.633255: step 144800, loss = 0.49 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 19:37:30.689845: step 144810, loss = 0.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 19:37:42.855782: step 144820, loss = 0.53 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 19:37:55.116355: step 144830, loss = 0.51 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 19:38:07.320447: step 144840, loss = 0.49 (24.4 examples/sec; 1.228 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 19:38:19.435168: step 144850, loss = 0.46 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 19:38:31.597244: step 144860, loss = 0.52 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 19:38:43.752313: step 144870, loss = 0.52 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 19:38:55.966295: step 144880, loss = 0.54 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 19:39:08.145807: step 144890, loss = 0.41 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 19:39:20.360766: step 144900, loss = 0.58 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 19:39:34.630599: step 144910, loss = 0.47 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 19:39:46.804111: step 144920, loss = 0.49 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 19:39:58.962656: step 144930, loss = 0.47 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 19:40:11.135831: step 144940, loss = 0.49 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 19:40:23.324514: step 144950, loss = 0.46 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 19:40:35.271647: step 144960, loss = 0.38 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 19:40:47.415089: step 144970, loss = 0.49 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 19:40:59.510830: step 144980, loss = 0.51 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 19:41:11.649298: step 144990, loss = 0.40 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 19:41:23.827350: step 145000, loss = 0.47 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 19:41:42.009060: step 145010, loss = 0.53 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 19:41:54.145659: step 145020, loss = 0.60 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 19:42:06.358845: step 145030, loss = 0.40 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 19:42:18.528308: step 145040, loss = 0.47 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 19:42:30.640278: step 145050, loss = 0.59 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 19:42:42.799294: step 145060, loss = 0.44 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-22 19:42:55.016521: step 145070, loss = 0.46 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 19:43:07.230710: step 145080, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 19:43:19.439103: step 145090, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 19:43:31.682733: step 145100, loss = 0.53 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 19:43:46.053899: step 145110, loss = 0.50 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 19:43:58.297057: step 145120, loss = 0.49 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 19:44:10.476490: step 145130, loss = 0.53 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 19:44:22.685879: step 145140, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 19:44:34.851267: step 145150, loss = 0.44 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-22 19:44:47.018322: step 145160, loss = 0.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 19:44:59.319914: step 145170, loss = 0.60 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 19:45:11.550667: step 145180, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 19:45:23.752312: step 145190, loss = 0.46 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-22 19:45:35.928546: step 145200, loss = 0.47 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 19:45:49.843683: step 145210, loss = 0.47 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 19:46:01.968230: step 145220, loss = 0.53 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 19:46:14.119744: step 145230, loss = 0.53 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 19:46:26.377415: step 145240, loss = 0.53 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 19:46:38.554145: step 145250, loss = 0.54 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 19:46:50.688408: step 145260, loss = 0.48 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 19:47:02.888252: step 145270, loss = 0.47 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 19:47:15.103792: step 145280, loss = 0.58 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-22 19:47:27.303834: step 145290, loss = 0.52 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 19:47:39.417680: step 145300, loss = 0.48 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 19:47:53.420865: step 145310, loss = 0.43 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 19:48:05.718591: step 145320, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 19:48:17.882015: step 145330, loss = 0.41 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 19:48:30.066629: step 145340, loss = 0.41 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 19:48:42.253334: step 145350, loss = 0.50 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 19:48:54.499314: step 145360, loss = 0.53 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 19:49:06.681275: step 145370, loss = 0.47 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 19:49:18.874761: step 145380, loss = 0.47 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 19:49:31.116448: step 145390, loss = 0.55 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 19:49:43.274728: step 145400, loss = 0.44 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 19:49:57.447550: step 145410, loss = 0.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 19:50:09.547338: step 145420, loss = 0.45 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 19:50:21.703729: step 145430, loss = 0.57 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 19:50:33.853882: step 145440, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 19:50:45.955216: step 145450, loss = 0.54 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 19:50:57.908739: step 145460, loss = 0.47 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 19:51:10.105936: step 145470, loss = 0.52 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 19:51:22.280821: step 145480, loss = 0.48 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 19:51:34.499155: step 145490, loss = 0.50 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 19:51:46.681417: step 145500, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 19:52:01.112322: step 145510, loss = 0.45 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 19:52:13.319844: step 145520, loss = 0.52 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 19:52:25.531822: step 145530, loss = 0.52 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 19:52:37.690383: step 145540, loss = 0.46 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 19:52:49.770056: step 145550, loss = 0.52 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 19:53:01.988614: step 145560, loss = 0.49 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 19:53:14.155612: step 145570, loss = 0.46 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 19:53:26.339199: step 145580, loss = 0.48 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 19:53:38.527055: step 145590, loss = 0.50 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 19:53:50.692192: step 145600, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 19:54:04.747319: step 145610, loss = 0.45 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 19:54:16.939367: step 145620, loss = 0.45 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 19:54:29.122827: step 145630, loss = 0.50 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 19:54:41.253725: step 145640, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 19:54:53.470854: step 145650, loss = 0.49 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 19:55:05.676258: step 145660, loss = 0.50 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 19:55:17.886977: step 145670, loss = 0.54 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 19:55:30.110038: step 145680, loss = 0.54 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 19:55:42.265376: step 145690, loss = 0.45 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 19:55:54.451653: step 145700, loss = 0.46 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 19:56:08.366050: step 145710, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 19:56:20.563519: step 145720, loss = 0.47 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 19:56:32.715060: step 145730, loss = 0.46 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 19:56:44.919541: step 145740, loss = 0.45 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 19:56:57.108620: step 145750, loss = 0.53 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 19:57:09.306208: step 145760, loss = 0.48 (24.5 examples/sec; 1.222 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 19:57:21.498344: step 145770, loss = 0.57 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 19:57:33.733784: step 145780, loss = 0.49 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 19:57:45.945289: step 145790, loss = 0.52 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 19:57:58.008455: step 145800, loss = 0.48 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 19:58:12.552391: step 145810, loss = 0.52 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-05-22 19:58:24.725063: step 145820, loss = 0.49 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 19:58:36.884164: step 145830, loss = 0.54 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 19:58:49.111957: step 145840, loss = 0.52 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 19:59:01.366788: step 145850, loss = 0.55 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 19:59:13.539372: step 145860, loss = 0.59 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 19:59:25.714726: step 145870, loss = 0.57 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 19:59:37.935451: step 145880, loss = 0.39 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 19:59:50.069093: step 145890, loss = 0.49 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 20:00:02.227203: step 145900, loss = 0.44 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 20:00:16.254875: step 145910, loss = 0.56 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 20:00:28.432506: step 145920, loss = 0.54 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 20:00:40.576462: step 145930, loss = 0.51 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 20:00:52.714063: step 145940, loss = 0.56 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 20:01:04.861677: step 145950, loss = 0.45 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 20:01:16.815740: step 145960, loss = 0.52 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 20:01:28.974188: step 145970, loss = 0.41 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 20:01:41.122288: step 145980, loss = 0.49 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 20:01:53.305578: step 145990, loss = 0.50 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 20:02:05.473973: step 146000, loss = 0.51 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 20:02:19.761412: step 146010, loss = 0.48 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 20:02:31.909453: step 146020, loss = 0.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 20:02:43.993850: step 146030, loss = 0.56 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 20:02:56.074965: step 146040, loss = 0.49 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 20:03:08.230683: step 146050, loss = 0.58 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 20:03:20.412181: step 146060, loss = 0.54 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 20:03:32.585871: step 146070, loss = 0.42 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 20:03:44.789854: step 146080, loss = 0.55 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 20:03:56.958780: step 146090, loss = 0.45 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 20:04:09.188304: step 146100, loss = 0.49 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 20:04:23.421904: step 146110, loss = 0.48 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 20:04:35.599384: step 146120, loss = 0.42 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 20:04:47.753884: step 146130, loss = 0.44 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 20:04:59.914413: step 146140, loss = 0.48 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 20:05:12.134774: step 146150, loss = 0.49 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 20:05:24.343534: step 146160, loss = 0.42 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 20:05:36.504497: step 146170, loss = 0.50 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 20:05:48.678925: step 146180, loss = 0.51 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 20:06:00.890837: step 146190, loss = 0.43 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 20:06:13.143205: step 146200, loss = 0.62 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 20:06:27.067414: step 146210, loss = 0.47 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 20:06:39.185322: step 146220, loss = 0.49 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 20:06:51.370360: step 146230, loss = 0.53 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 20:07:03.537793: step 146240, loss = 0.56 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 20:07:15.686754: step 146250, loss = 0.56 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 20:07:27.970519: step 146260, loss = 0.46 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 20:07:40.184958: step 146270, loss = 0.51 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 20:07:52.354993: step 146280, loss = 0.47 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-22 20:08:04.408578: step 146290, loss = 0.63 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 20:08:16.628547: step 146300, loss = 0.46 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 20:08:30.880036: step 146310, loss = 0.44 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 20:08:43.052500: step 146320, loss = 0.50 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 20:08:55.245651: step 146330, loss = 0.46 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 20:09:07.366653: step 146340, loss = 0.44 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 20:09:19.543952: step 146350, loss = 0.56 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 20:09:31.678586: step 146360, loss = 0.48 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 20:09:43.799479: step 146370, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 20:09:55.957970: step 146380, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 20:10:08.114333: step 146390, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 20:10:20.271742: step 146400, loss = 0.44 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 20:10:34.906841: step 146410, loss = 0.47 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 20:10:47.040504: step 146420, loss = 0.47 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 20:10:59.220491: step 146430, loss = 0.42 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 20:11:11.426485: step 146440, loss = 0.43 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 20:11:23.609943: step 146450, loss = 0.52 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 20:11:35.674052: step 146460, loss = 0.55 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 20:11:47.841999: step 146470, loss = 0.44 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 20:11:59.985153: step 146480, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 20:12:12.200843: step 146490, loss = 0.43 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 20:12:24.350963: step 146500, loss = 0.47 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 20:12:38.829277: step 146510, loss = 0.44 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 20:12:51.001888: step 146520, loss = 0.44 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 20:13:03.150183: step 146530, loss = 0.49 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 20:13:15.186057: step 146540, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 20:13:27.268935: step 146550, loss = 0.46 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 20:13:39.432114: step 146560, loss = 0.47 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 20:13:51.613156: step 146570, loss = 0.50 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 20:14:03.798024: step 146580, loss = 0.47 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 20:14:15.932577: step 146590, loss = 0.45 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 20:14:28.122692: step 146600, loss = 0.48 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 20:14:42.096379: step 146610, loss = 0.41 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 20:14:54.218898: step 146620, loss = 0.46 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 20:15:06.383508: step 146630, loss = 0.53 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 20:15:18.647043: step 146640, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 20:15:30.808229: step 146650, loss = 0.44 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 20:15:42.947648: step 146660, loss = 0.42 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 20:15:55.114341: step 146670, loss = 0.42 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 20:16:07.305371: step 146680, loss = 0.51 (24.9 examples/sec; 1.205 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 20:16:19.489678: step 146690, loss = 0.42 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 20:16:31.671869: step 146700, loss = 0.42 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 20:16:46.006451: step 146710, loss = 0.49 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 20:16:58.157874: step 146720, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 20:17:10.348626: step 146730, loss = 0.48 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 20:17:22.618015: step 146740, loss = 0.45 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 20:17:34.910122: step 146750, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 20:17:47.107424: step 146760, loss = 0.47 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 20:17:59.262412: step 146770, loss = 0.45 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 20:18:11.358344: step 146780, loss = 0.54 (25.4 examples/sec; 1.183 sec/batch)\n",
      "2019-05-22 20:18:23.468441: step 146790, loss = 0.47 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-22 20:18:35.658372: step 146800, loss = 0.56 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 20:18:50.031061: step 146810, loss = 0.40 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 20:19:02.158141: step 146820, loss = 0.41 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 20:19:14.250948: step 146830, loss = 0.50 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 20:19:26.383881: step 146840, loss = 0.45 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 20:19:38.412415: step 146850, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 20:19:50.623529: step 146860, loss = 0.41 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 20:20:02.828127: step 146870, loss = 0.49 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 20:20:15.046829: step 146880, loss = 0.49 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-22 20:20:27.270298: step 146890, loss = 0.54 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 20:20:39.410260: step 146900, loss = 0.44 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 20:20:53.518046: step 146910, loss = 0.44 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 20:21:05.666048: step 146920, loss = 0.44 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 20:21:17.830563: step 146930, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 20:21:30.057014: step 146940, loss = 0.51 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 20:21:42.231109: step 146950, loss = 0.49 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 20:21:54.322387: step 146960, loss = 0.43 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 20:22:06.352861: step 146970, loss = 0.42 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 20:22:18.510128: step 146980, loss = 0.48 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 20:22:30.658882: step 146990, loss = 0.45 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 20:22:42.750736: step 147000, loss = 0.57 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 20:22:56.825026: step 147010, loss = 0.48 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 20:23:09.031519: step 147020, loss = 0.49 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 20:23:21.119610: step 147030, loss = 0.46 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 20:23:33.276206: step 147040, loss = 0.51 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 20:23:45.480979: step 147050, loss = 0.48 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 20:23:57.628486: step 147060, loss = 0.47 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 20:24:09.788870: step 147070, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 20:24:22.018779: step 147080, loss = 0.52 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 20:24:34.166395: step 147090, loss = 0.48 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 20:24:46.311555: step 147100, loss = 0.43 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 20:25:00.724166: step 147110, loss = 0.51 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 20:25:12.891781: step 147120, loss = 0.46 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 20:25:25.006075: step 147130, loss = 0.51 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 20:25:37.134441: step 147140, loss = 0.47 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 20:25:49.328592: step 147150, loss = 0.53 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 20:26:01.510537: step 147160, loss = 0.52 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 20:26:13.558968: step 147170, loss = 0.47 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 20:26:25.694854: step 147180, loss = 0.55 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 20:26:37.897616: step 147190, loss = 0.52 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 20:26:50.088151: step 147200, loss = 0.51 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 20:27:04.221138: step 147210, loss = 0.63 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 20:27:16.382563: step 147220, loss = 0.50 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 20:27:28.529894: step 147230, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 20:27:40.739205: step 147240, loss = 0.42 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 20:27:52.899963: step 147250, loss = 0.47 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 20:28:05.066923: step 147260, loss = 0.60 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 20:28:17.251974: step 147270, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 20:28:29.334120: step 147280, loss = 0.47 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 20:28:41.470963: step 147290, loss = 0.51 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 20:28:53.525372: step 147300, loss = 0.53 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 20:29:07.842907: step 147310, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 20:29:20.012295: step 147320, loss = 0.55 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 20:29:32.202391: step 147330, loss = 0.54 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 20:29:44.365949: step 147340, loss = 0.41 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 20:29:56.537281: step 147350, loss = 0.52 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 20:30:08.660152: step 147360, loss = 0.48 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 20:30:20.717627: step 147370, loss = 0.53 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 20:30:32.866926: step 147380, loss = 0.49 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 20:30:45.045849: step 147390, loss = 0.44 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 20:30:57.257811: step 147400, loss = 0.50 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 20:31:11.795783: step 147410, loss = 0.52 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 20:31:23.958057: step 147420, loss = 0.53 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 20:31:36.137530: step 147430, loss = 0.51 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 20:31:48.262859: step 147440, loss = 0.44 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 20:32:00.474542: step 147450, loss = 0.48 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 20:32:12.688992: step 147460, loss = 0.46 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 20:32:24.692944: step 147470, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 20:32:36.864441: step 147480, loss = 0.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 20:32:49.013747: step 147490, loss = 0.45 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 20:33:01.289568: step 147500, loss = 0.50 (23.1 examples/sec; 1.297 sec/batch)\n",
      "2019-05-22 20:33:15.320404: step 147510, loss = 0.49 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 20:33:27.402000: step 147520, loss = 0.48 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 20:33:39.492228: step 147530, loss = 0.59 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 20:33:51.733820: step 147540, loss = 0.58 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 20:34:03.863758: step 147550, loss = 0.53 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 20:34:16.067012: step 147560, loss = 0.52 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 20:34:28.228374: step 147570, loss = 0.45 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 20:34:40.389772: step 147580, loss = 0.59 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 20:34:52.544846: step 147590, loss = 0.53 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-22 20:35:04.719436: step 147600, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 20:35:18.755966: step 147610, loss = 0.41 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 20:35:30.995669: step 147620, loss = 0.43 (23.2 examples/sec; 1.294 sec/batch)\n",
      "2019-05-22 20:35:43.159975: step 147630, loss = 0.58 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 20:35:55.287181: step 147640, loss = 0.53 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 20:36:07.450295: step 147650, loss = 0.45 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 20:36:19.653976: step 147660, loss = 0.44 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 20:36:31.846620: step 147670, loss = 0.53 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 20:36:44.059050: step 147680, loss = 0.43 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 20:36:56.171686: step 147690, loss = 0.44 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 20:37:08.307611: step 147700, loss = 0.47 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 20:37:22.805894: step 147710, loss = 0.51 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 20:37:34.764575: step 147720, loss = 0.41 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 20:37:46.949933: step 147730, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 20:37:59.143764: step 147740, loss = 0.43 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 20:38:11.291945: step 147750, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 20:38:23.410484: step 147760, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 20:38:35.475044: step 147770, loss = 0.49 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 20:38:47.602108: step 147780, loss = 0.53 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 20:38:59.786843: step 147790, loss = 0.47 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 20:39:11.950972: step 147800, loss = 0.49 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 20:39:26.040128: step 147810, loss = 0.46 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 20:39:38.218250: step 147820, loss = 0.45 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 20:39:50.392215: step 147830, loss = 0.51 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 20:40:02.552844: step 147840, loss = 0.46 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 20:40:14.763136: step 147850, loss = 0.56 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 20:40:26.956422: step 147860, loss = 0.49 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 20:40:39.175623: step 147870, loss = 0.47 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 20:40:51.343972: step 147880, loss = 0.58 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 20:41:03.475812: step 147890, loss = 0.43 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 20:41:15.629248: step 147900, loss = 0.46 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 20:41:30.267105: step 147910, loss = 0.41 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 20:41:42.402057: step 147920, loss = 0.49 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 20:41:54.629643: step 147930, loss = 0.52 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-22 20:42:06.854052: step 147940, loss = 0.48 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 20:42:19.003009: step 147950, loss = 0.54 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 20:42:31.195386: step 147960, loss = 0.44 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 20:42:43.254016: step 147970, loss = 0.52 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 20:42:55.403536: step 147980, loss = 0.53 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 20:43:07.594139: step 147990, loss = 0.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 20:43:19.806016: step 148000, loss = 0.51 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 20:43:33.872804: step 148010, loss = 0.55 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 20:43:46.009613: step 148020, loss = 0.48 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 20:43:58.206194: step 148030, loss = 0.50 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 20:44:10.396323: step 148040, loss = 0.50 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 20:44:22.553665: step 148050, loss = 0.65 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 20:44:34.749632: step 148060, loss = 0.48 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 20:44:46.904334: step 148070, loss = 0.51 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 20:44:59.150429: step 148080, loss = 0.42 (23.0 examples/sec; 1.302 sec/batch)\n",
      "2019-05-22 20:45:11.338494: step 148090, loss = 0.51 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 20:45:23.557983: step 148100, loss = 0.57 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 20:45:37.574465: step 148110, loss = 0.51 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 20:45:49.853903: step 148120, loss = 0.41 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-22 20:46:02.022542: step 148130, loss = 0.52 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 20:46:14.203138: step 148140, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 20:46:26.400387: step 148150, loss = 0.49 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 20:46:38.612048: step 148160, loss = 0.46 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 20:46:50.782762: step 148170, loss = 0.46 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 20:47:02.954453: step 148180, loss = 0.50 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 20:47:15.125857: step 148190, loss = 0.54 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 20:47:27.354476: step 148200, loss = 0.45 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 20:47:41.619268: step 148210, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 20:47:53.549767: step 148220, loss = 0.53 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 20:48:05.653280: step 148230, loss = 0.51 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 20:48:17.793833: step 148240, loss = 0.50 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 20:48:29.892399: step 148250, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 20:48:42.050093: step 148260, loss = 0.55 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 20:48:54.127610: step 148270, loss = 0.47 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 20:49:06.284991: step 148280, loss = 0.53 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 20:49:18.519119: step 148290, loss = 0.53 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 20:49:30.702405: step 148300, loss = 0.56 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 20:49:44.760846: step 148310, loss = 0.54 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 20:49:56.930600: step 148320, loss = 0.46 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 20:50:09.099933: step 148330, loss = 0.44 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 20:50:21.267507: step 148340, loss = 0.63 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 20:50:33.481244: step 148350, loss = 0.46 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 20:50:45.611414: step 148360, loss = 0.54 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 20:50:57.746684: step 148370, loss = 0.56 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 20:51:09.868921: step 148380, loss = 0.46 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 20:51:22.110409: step 148390, loss = 0.49 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 20:51:34.255711: step 148400, loss = 0.45 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 20:51:48.630848: step 148410, loss = 0.46 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 20:52:00.834231: step 148420, loss = 0.46 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 20:52:12.976407: step 148430, loss = 0.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 20:52:25.171825: step 148440, loss = 0.50 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 20:52:37.382318: step 148450, loss = 0.48 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 20:52:49.515553: step 148460, loss = 0.48 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 20:53:01.564532: step 148470, loss = 0.46 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 20:53:13.716013: step 148480, loss = 0.42 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 20:53:25.905368: step 148490, loss = 0.41 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 20:53:38.122906: step 148500, loss = 0.49 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 20:53:52.369517: step 148510, loss = 0.52 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 20:54:04.606039: step 148520, loss = 0.48 (23.0 examples/sec; 1.304 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 20:54:16.744048: step 148530, loss = 0.54 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 20:54:28.907149: step 148540, loss = 0.46 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 20:54:41.080750: step 148550, loss = 0.50 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 20:54:53.220992: step 148560, loss = 0.45 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 20:55:05.377623: step 148570, loss = 0.48 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 20:55:17.530091: step 148580, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 20:55:29.687492: step 148590, loss = 0.57 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 20:55:41.826957: step 148600, loss = 0.48 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 20:55:55.902706: step 148610, loss = 0.45 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 20:56:08.030537: step 148620, loss = 0.45 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 20:56:20.202563: step 148630, loss = 0.54 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 20:56:32.439616: step 148640, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 20:56:44.616872: step 148650, loss = 0.47 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 20:56:56.817231: step 148660, loss = 0.48 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 20:57:08.936588: step 148670, loss = 0.54 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 20:57:21.074163: step 148680, loss = 0.54 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 20:57:33.122078: step 148690, loss = 0.45 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 20:57:45.251943: step 148700, loss = 0.49 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 20:57:59.232835: step 148710, loss = 0.46 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 20:58:11.299420: step 148720, loss = 0.49 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 20:58:23.390859: step 148730, loss = 0.55 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 20:58:35.534133: step 148740, loss = 0.51 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 20:58:47.675955: step 148750, loss = 0.46 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 20:58:59.759996: step 148760, loss = 0.46 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 20:59:11.970162: step 148770, loss = 0.39 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 20:59:24.187942: step 148780, loss = 0.49 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 20:59:36.448778: step 148790, loss = 0.61 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 20:59:48.662044: step 148800, loss = 0.47 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 21:00:03.124474: step 148810, loss = 0.52 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 21:00:15.356834: step 148820, loss = 0.52 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 21:00:27.525618: step 148830, loss = 0.53 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 21:00:39.715985: step 148840, loss = 0.49 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 21:00:51.894353: step 148850, loss = 0.48 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 21:01:04.078836: step 148860, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 21:01:16.258212: step 148870, loss = 0.46 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 21:01:28.454643: step 148880, loss = 0.55 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 21:01:40.644420: step 148890, loss = 0.44 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 21:01:52.824025: step 148900, loss = 0.55 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 21:02:07.438452: step 148910, loss = 0.54 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 21:02:19.632171: step 148920, loss = 0.58 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 21:02:31.861727: step 148930, loss = 0.52 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 21:02:44.077546: step 148940, loss = 0.49 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 21:02:56.234131: step 148950, loss = 0.44 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 21:03:08.392167: step 148960, loss = 0.46 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 21:03:20.498968: step 148970, loss = 0.46 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 21:03:32.601943: step 148980, loss = 0.47 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 21:03:44.864295: step 148990, loss = 0.46 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 21:03:57.092574: step 149000, loss = 0.48 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 21:04:11.269980: step 149010, loss = 0.50 (23.1 examples/sec; 1.300 sec/batch)\n",
      "2019-05-22 21:04:23.432611: step 149020, loss = 0.53 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 21:04:35.608840: step 149030, loss = 0.58 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 21:04:47.783046: step 149040, loss = 0.50 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 21:04:59.935905: step 149050, loss = 0.55 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 21:05:12.048254: step 149060, loss = 0.46 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 21:05:24.177843: step 149070, loss = 0.42 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 21:05:36.309272: step 149080, loss = 0.58 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 21:05:48.527497: step 149090, loss = 0.47 (24.1 examples/sec; 1.245 sec/batch)\n",
      "2019-05-22 21:06:00.726107: step 149100, loss = 0.49 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 21:06:14.812572: step 149110, loss = 0.44 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 21:06:27.032846: step 149120, loss = 0.46 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 21:06:39.246058: step 149130, loss = 0.47 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 21:06:51.500748: step 149140, loss = 0.42 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 21:07:03.667571: step 149150, loss = 0.56 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 21:07:15.774136: step 149160, loss = 0.54 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 21:07:27.951691: step 149170, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 21:07:40.090444: step 149180, loss = 0.46 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 21:07:52.322672: step 149190, loss = 0.51 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 21:08:04.478549: step 149200, loss = 0.60 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 21:08:18.597339: step 149210, loss = 0.42 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 21:08:30.735465: step 149220, loss = 0.58 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 21:08:42.804718: step 149230, loss = 0.51 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 21:08:55.047710: step 149240, loss = 0.50 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 21:09:07.191660: step 149250, loss = 0.43 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 21:09:19.376848: step 149260, loss = 0.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 21:09:31.565196: step 149270, loss = 0.51 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 21:09:43.744033: step 149280, loss = 0.54 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 21:09:55.942827: step 149290, loss = 0.51 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 21:10:08.096532: step 149300, loss = 0.47 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 21:10:22.418580: step 149310, loss = 0.57 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 21:10:34.638111: step 149320, loss = 0.54 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 21:10:46.826287: step 149330, loss = 0.46 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 21:10:59.028424: step 149340, loss = 0.47 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 21:11:11.161568: step 149350, loss = 0.48 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 21:11:23.361870: step 149360, loss = 0.46 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 21:11:35.531502: step 149370, loss = 0.47 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 21:11:47.723241: step 149380, loss = 0.47 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 21:11:59.980918: step 149390, loss = 0.46 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 21:12:12.213519: step 149400, loss = 0.45 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 21:12:26.411580: step 149410, loss = 0.55 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 21:12:38.580963: step 149420, loss = 0.48 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 21:12:50.831058: step 149430, loss = 0.46 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 21:13:02.988676: step 149440, loss = 0.54 (24.3 examples/sec; 1.235 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 21:13:15.176777: step 149450, loss = 0.53 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 21:13:27.359732: step 149460, loss = 0.46 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 21:13:39.554061: step 149470, loss = 0.50 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 21:13:51.559123: step 149480, loss = 0.50 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 21:14:03.754216: step 149490, loss = 0.55 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 21:14:15.809622: step 149500, loss = 0.51 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 21:14:30.112325: step 149510, loss = 0.58 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 21:14:42.321442: step 149520, loss = 0.51 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 21:14:54.502652: step 149530, loss = 0.51 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 21:15:06.526150: step 149540, loss = 0.46 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 21:15:18.544501: step 149550, loss = 0.52 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 21:15:30.541284: step 149560, loss = 0.50 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 21:15:42.606903: step 149570, loss = 0.56 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 21:15:54.689797: step 149580, loss = 0.42 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 21:16:06.889546: step 149590, loss = 0.54 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 21:16:19.065891: step 149600, loss = 0.42 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 21:16:33.155441: step 149610, loss = 0.43 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 21:16:45.291497: step 149620, loss = 0.48 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 21:16:57.426573: step 149630, loss = 0.52 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 21:17:09.549732: step 149640, loss = 0.46 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 21:17:21.725743: step 149650, loss = 0.56 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 21:17:33.915202: step 149660, loss = 0.43 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 21:17:46.121952: step 149670, loss = 0.44 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 21:17:58.313121: step 149680, loss = 0.48 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 21:18:10.547853: step 149690, loss = 0.51 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 21:18:22.729647: step 149700, loss = 0.51 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 21:18:36.858048: step 149710, loss = 0.52 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 21:18:49.024303: step 149720, loss = 0.58 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 21:19:01.099383: step 149730, loss = 0.46 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 21:19:13.323061: step 149740, loss = 0.48 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 21:19:25.381136: step 149750, loss = 0.55 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 21:19:37.611729: step 149760, loss = 0.46 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 21:19:49.777273: step 149770, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 21:20:01.988964: step 149780, loss = 0.47 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 21:20:14.197417: step 149790, loss = 0.52 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 21:20:26.393351: step 149800, loss = 0.52 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 21:20:40.470909: step 149810, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 21:20:52.657903: step 149820, loss = 0.45 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 21:21:04.851787: step 149830, loss = 0.47 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 21:21:17.133063: step 149840, loss = 0.48 (22.9 examples/sec; 1.312 sec/batch)\n",
      "2019-05-22 21:21:29.337528: step 149850, loss = 0.42 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 21:21:41.566115: step 149860, loss = 0.43 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 21:21:53.756198: step 149870, loss = 0.47 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 21:22:05.964403: step 149880, loss = 0.46 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 21:22:18.165841: step 149890, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 21:22:30.428389: step 149900, loss = 0.53 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-22 21:22:44.508757: step 149910, loss = 0.55 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 21:22:56.775848: step 149920, loss = 0.47 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 21:23:08.924399: step 149930, loss = 0.48 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 21:23:21.145430: step 149940, loss = 0.47 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 21:23:33.327797: step 149950, loss = 0.51 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 21:23:45.549299: step 149960, loss = 0.49 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 21:23:57.750651: step 149970, loss = 0.45 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 21:24:09.729641: step 149980, loss = 0.43 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 21:24:21.926686: step 149990, loss = 0.46 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 21:24:33.949985: step 150000, loss = 0.43 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 21:24:51.653774: step 150010, loss = 0.49 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 21:25:03.830797: step 150020, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 21:25:15.955996: step 150030, loss = 0.49 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 21:25:28.134133: step 150040, loss = 0.49 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 21:25:40.262269: step 150050, loss = 0.47 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 21:25:52.472401: step 150060, loss = 0.49 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 21:26:04.617715: step 150070, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 21:26:16.745297: step 150080, loss = 0.47 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 21:26:28.875459: step 150090, loss = 0.49 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 21:26:41.025517: step 150100, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 21:26:55.318935: step 150110, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 21:27:07.493955: step 150120, loss = 0.43 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 21:27:19.642975: step 150130, loss = 0.44 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 21:27:31.829954: step 150140, loss = 0.54 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 21:27:44.022446: step 150150, loss = 0.53 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 21:27:56.275316: step 150160, loss = 0.59 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 21:28:08.493543: step 150170, loss = 0.44 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 21:28:20.669911: step 150180, loss = 0.45 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 21:28:32.897617: step 150190, loss = 0.52 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 21:28:45.068987: step 150200, loss = 0.51 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 21:28:59.228277: step 150210, loss = 0.49 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 21:29:11.381593: step 150220, loss = 0.45 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 21:29:23.425063: step 150230, loss = 0.52 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 21:29:35.605945: step 150240, loss = 0.45 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 21:29:47.793210: step 150250, loss = 0.46 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 21:29:59.972594: step 150260, loss = 0.51 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 21:30:12.172115: step 150270, loss = 0.52 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 21:30:24.385506: step 150280, loss = 0.50 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 21:30:36.535965: step 150290, loss = 0.51 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 21:30:48.714163: step 150300, loss = 0.49 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 21:31:02.831175: step 150310, loss = 0.47 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 21:31:14.938531: step 150320, loss = 0.45 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 21:31:27.156793: step 150330, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 21:31:39.324259: step 150340, loss = 0.46 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 21:31:51.559185: step 150350, loss = 0.51 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 21:32:03.743370: step 150360, loss = 0.49 (24.4 examples/sec; 1.228 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 21:32:15.897888: step 150370, loss = 0.47 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 21:32:28.107789: step 150380, loss = 0.50 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 21:32:40.350032: step 150390, loss = 0.57 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 21:32:52.558035: step 150400, loss = 0.44 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 21:33:06.596486: step 150410, loss = 0.52 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 21:33:18.742927: step 150420, loss = 0.40 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 21:33:30.942735: step 150430, loss = 0.54 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 21:33:43.166734: step 150440, loss = 0.50 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 21:33:55.383264: step 150450, loss = 0.48 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 21:34:07.612161: step 150460, loss = 0.45 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 21:34:19.819788: step 150470, loss = 0.44 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 21:34:31.804831: step 150480, loss = 0.42 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 21:34:43.944461: step 150490, loss = 0.41 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 21:34:56.133644: step 150500, loss = 0.49 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 21:35:10.417337: step 150510, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 21:35:22.619653: step 150520, loss = 0.51 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 21:35:34.786932: step 150530, loss = 0.43 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 21:35:46.984543: step 150540, loss = 0.44 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 21:35:59.158358: step 150550, loss = 0.45 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 21:36:11.322919: step 150560, loss = 0.55 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 21:36:23.435271: step 150570, loss = 0.53 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 21:36:35.559127: step 150580, loss = 0.50 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 21:36:47.776925: step 150590, loss = 0.54 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 21:36:59.978928: step 150600, loss = 0.39 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 21:37:14.259705: step 150610, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 21:37:26.442691: step 150620, loss = 0.52 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 21:37:38.609604: step 150630, loss = 0.52 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 21:37:50.855646: step 150640, loss = 0.41 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 21:38:03.066350: step 150650, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 21:38:15.280184: step 150660, loss = 0.47 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 21:38:27.491004: step 150670, loss = 0.48 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 21:38:39.699150: step 150680, loss = 0.45 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 21:38:51.953618: step 150690, loss = 0.59 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 21:39:04.160355: step 150700, loss = 0.52 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 21:39:18.636308: step 150710, loss = 0.46 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 21:39:30.890726: step 150720, loss = 0.48 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 21:39:42.823167: step 150730, loss = 0.47 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-22 21:39:55.067364: step 150740, loss = 0.49 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 21:40:07.223024: step 150750, loss = 0.55 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 21:40:19.400836: step 150760, loss = 0.51 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 21:40:31.571285: step 150770, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 21:40:43.711909: step 150780, loss = 0.54 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 21:40:55.901966: step 150790, loss = 0.39 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 21:41:08.088747: step 150800, loss = 0.45 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 21:41:22.364965: step 150810, loss = 0.56 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 21:41:34.575768: step 150820, loss = 0.46 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 21:41:46.792158: step 150830, loss = 0.48 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 21:41:58.993100: step 150840, loss = 0.54 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 21:42:11.209527: step 150850, loss = 0.45 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 21:42:23.387051: step 150860, loss = 0.43 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 21:42:35.620268: step 150870, loss = 0.47 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 21:42:47.817683: step 150880, loss = 0.44 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 21:43:00.022932: step 150890, loss = 0.42 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 21:43:12.144171: step 150900, loss = 0.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 21:43:26.396181: step 150910, loss = 0.39 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 21:43:38.616395: step 150920, loss = 0.42 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 21:43:50.823002: step 150930, loss = 0.50 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 21:44:02.994375: step 150940, loss = 0.56 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 21:44:15.163286: step 150950, loss = 0.55 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 21:44:27.344810: step 150960, loss = 0.47 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 21:44:39.503153: step 150970, loss = 0.52 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 21:44:51.528940: step 150980, loss = 0.54 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 21:45:03.731726: step 150990, loss = 0.55 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 21:45:15.912104: step 151000, loss = 0.47 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 21:45:29.976757: step 151010, loss = 0.47 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 21:45:42.124062: step 151020, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 21:45:54.339463: step 151030, loss = 0.55 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 21:46:06.509707: step 151040, loss = 0.42 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 21:46:18.682036: step 151050, loss = 0.56 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 21:46:30.916697: step 151060, loss = 0.51 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 21:46:43.195943: step 151070, loss = 0.46 (24.2 examples/sec; 1.242 sec/batch)\n",
      "2019-05-22 21:46:55.347176: step 151080, loss = 0.47 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 21:47:07.575785: step 151090, loss = 0.46 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 21:47:19.760131: step 151100, loss = 0.54 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 21:47:34.100884: step 151110, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 21:47:46.321004: step 151120, loss = 0.51 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 21:47:58.568339: step 151130, loss = 0.59 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 21:48:10.760954: step 151140, loss = 0.55 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 21:48:22.914800: step 151150, loss = 0.53 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 21:48:35.165698: step 151160, loss = 0.47 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 21:48:47.383347: step 151170, loss = 0.47 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 21:48:59.538272: step 151180, loss = 0.44 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 21:49:11.753766: step 151190, loss = 0.45 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 21:49:23.918057: step 151200, loss = 0.52 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 21:49:38.138771: step 151210, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 21:49:50.480986: step 151220, loss = 0.43 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 21:50:02.446031: step 151230, loss = 0.42 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 21:50:14.662295: step 151240, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 21:50:26.857818: step 151250, loss = 0.43 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 21:50:39.046763: step 151260, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 21:50:51.254790: step 151270, loss = 0.55 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 21:51:03.490264: step 151280, loss = 0.49 (24.5 examples/sec; 1.224 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 21:51:15.659741: step 151290, loss = 0.41 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 21:51:27.840265: step 151300, loss = 0.56 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 21:51:42.038583: step 151310, loss = 0.51 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 21:51:54.235898: step 151320, loss = 0.48 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 21:52:06.445524: step 151330, loss = 0.47 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 21:52:18.655464: step 151340, loss = 0.50 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 21:52:30.866342: step 151350, loss = 0.46 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 21:52:43.012493: step 151360, loss = 0.51 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 21:52:55.183360: step 151370, loss = 0.49 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 21:53:07.389157: step 151380, loss = 0.47 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 21:53:19.619493: step 151390, loss = 0.51 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 21:53:31.807106: step 151400, loss = 0.53 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 21:53:46.205131: step 151410, loss = 0.56 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 21:53:58.402649: step 151420, loss = 0.54 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 21:54:10.588288: step 151430, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 21:54:22.754312: step 151440, loss = 0.59 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 21:54:34.955358: step 151450, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 21:54:47.173415: step 151460, loss = 0.52 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 21:54:59.359631: step 151470, loss = 0.56 (25.3 examples/sec; 1.184 sec/batch)\n",
      "2019-05-22 21:55:11.289202: step 151480, loss = 0.47 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 21:55:23.394397: step 151490, loss = 0.45 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 21:55:35.494792: step 151500, loss = 0.51 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 21:55:49.345627: step 151510, loss = 0.39 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 21:56:01.557656: step 151520, loss = 0.54 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 21:56:13.754404: step 151530, loss = 0.58 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 21:56:25.924610: step 151540, loss = 0.47 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 21:56:38.152720: step 151550, loss = 0.50 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 21:56:50.386211: step 151560, loss = 0.49 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 21:57:02.619849: step 151570, loss = 0.44 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 21:57:14.795912: step 151580, loss = 0.44 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 21:57:26.975350: step 151590, loss = 0.45 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 21:57:39.113555: step 151600, loss = 0.51 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 21:57:53.235463: step 151610, loss = 0.57 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 21:58:05.386087: step 151620, loss = 0.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 21:58:17.570351: step 151630, loss = 0.43 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 21:58:29.770900: step 151640, loss = 0.44 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 21:58:41.962544: step 151650, loss = 0.51 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 21:58:54.175354: step 151660, loss = 0.46 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 21:59:06.356959: step 151670, loss = 0.46 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 21:59:18.640143: step 151680, loss = 0.47 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 21:59:30.774454: step 151690, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 21:59:42.972337: step 151700, loss = 0.40 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 21:59:56.992775: step 151710, loss = 0.53 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 22:00:09.135429: step 151720, loss = 0.51 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 22:00:21.208468: step 151730, loss = 0.46 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 22:00:33.337205: step 151740, loss = 0.53 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 22:00:45.520104: step 151750, loss = 0.51 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 22:00:57.722162: step 151760, loss = 0.44 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 22:01:09.949128: step 151770, loss = 0.51 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 22:01:22.186984: step 151780, loss = 0.54 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-22 22:01:34.357613: step 151790, loss = 0.56 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 22:01:46.544794: step 151800, loss = 0.48 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 22:02:00.588767: step 151810, loss = 0.53 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 22:02:12.806874: step 151820, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 22:02:25.044244: step 151830, loss = 0.42 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 22:02:37.231889: step 151840, loss = 0.51 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 22:02:49.445918: step 151850, loss = 0.48 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 22:03:01.617375: step 151860, loss = 0.51 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 22:03:13.788367: step 151870, loss = 0.42 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 22:03:25.965356: step 151880, loss = 0.52 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 22:03:38.182669: step 151890, loss = 0.49 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 22:03:50.416911: step 151900, loss = 0.40 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 22:04:04.585792: step 151910, loss = 0.53 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 22:04:16.773867: step 151920, loss = 0.41 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 22:04:29.027987: step 151930, loss = 0.58 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 22:04:41.158491: step 151940, loss = 0.50 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 22:04:53.260429: step 151950, loss = 0.55 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 22:05:05.408499: step 151960, loss = 0.48 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 22:05:17.459837: step 151970, loss = 0.49 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-22 22:05:29.497281: step 151980, loss = 0.44 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 22:05:41.618567: step 151990, loss = 0.46 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 22:05:53.789424: step 152000, loss = 0.57 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 22:06:07.905618: step 152010, loss = 0.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 22:06:20.117261: step 152020, loss = 0.57 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 22:06:32.316250: step 152030, loss = 0.59 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 22:06:44.489913: step 152040, loss = 0.49 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 22:06:56.690321: step 152050, loss = 0.46 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 22:07:08.919723: step 152060, loss = 0.56 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 22:07:21.084241: step 152070, loss = 0.50 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 22:07:33.295192: step 152080, loss = 0.54 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 22:07:45.471427: step 152090, loss = 0.47 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 22:07:57.696526: step 152100, loss = 0.42 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-22 22:08:11.823204: step 152110, loss = 0.52 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 22:08:24.017883: step 152120, loss = 0.46 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-22 22:08:36.147310: step 152130, loss = 0.44 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 22:08:48.332598: step 152140, loss = 0.48 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 22:09:00.537114: step 152150, loss = 0.55 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 22:09:12.750576: step 152160, loss = 0.46 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 22:09:24.901895: step 152170, loss = 0.51 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 22:09:37.076163: step 152180, loss = 0.49 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 22:09:49.252845: step 152190, loss = 0.46 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 22:10:01.441045: step 152200, loss = 0.51 (24.4 examples/sec; 1.232 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 22:10:15.403454: step 152210, loss = 0.48 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 22:10:27.570402: step 152220, loss = 0.46 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 22:10:39.701912: step 152230, loss = 0.38 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 22:10:51.801346: step 152240, loss = 0.44 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 22:11:03.972131: step 152250, loss = 0.46 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 22:11:16.186468: step 152260, loss = 0.42 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 22:11:28.371667: step 152270, loss = 0.57 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 22:11:40.612236: step 152280, loss = 0.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 22:11:52.830893: step 152290, loss = 0.47 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 22:12:05.011241: step 152300, loss = 0.56 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 22:12:19.254510: step 152310, loss = 0.50 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 22:12:31.441187: step 152320, loss = 0.54 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 22:12:43.679297: step 152330, loss = 0.51 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 22:12:55.943448: step 152340, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 22:13:08.163624: step 152350, loss = 0.55 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 22:13:20.351835: step 152360, loss = 0.48 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 22:13:32.582703: step 152370, loss = 0.43 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 22:13:44.810550: step 152380, loss = 0.46 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 22:13:56.996775: step 152390, loss = 0.48 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 22:14:09.221883: step 152400, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 22:14:23.451342: step 152410, loss = 0.45 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 22:14:35.527510: step 152420, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 22:14:47.686297: step 152430, loss = 0.43 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 22:14:59.760409: step 152440, loss = 0.50 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 22:15:11.898247: step 152450, loss = 0.46 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 22:15:23.997041: step 152460, loss = 0.48 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 22:15:36.108227: step 152470, loss = 0.45 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 22:15:48.306949: step 152480, loss = 0.47 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 22:16:00.337514: step 152490, loss = 0.45 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 22:16:12.526186: step 152500, loss = 0.55 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 22:16:26.624633: step 152510, loss = 0.53 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 22:16:38.809407: step 152520, loss = 0.44 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 22:16:51.042890: step 152530, loss = 0.45 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 22:17:03.285133: step 152540, loss = 0.47 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 22:17:15.463076: step 152550, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 22:17:27.636978: step 152560, loss = 0.49 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 22:17:39.802403: step 152570, loss = 0.46 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 22:17:52.042182: step 152580, loss = 0.47 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 22:18:04.195658: step 152590, loss = 0.55 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 22:18:16.391527: step 152600, loss = 0.41 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 22:18:30.490160: step 152610, loss = 0.45 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 22:18:42.622475: step 152620, loss = 0.52 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 22:18:54.806850: step 152630, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 22:19:07.047826: step 152640, loss = 0.46 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 22:19:19.280053: step 152650, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 22:19:31.505126: step 152660, loss = 0.45 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 22:19:43.713040: step 152670, loss = 0.45 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 22:19:55.942221: step 152680, loss = 0.47 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 22:20:08.142398: step 152690, loss = 0.55 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 22:20:20.319022: step 152700, loss = 0.49 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 22:20:34.216783: step 152710, loss = 0.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 22:20:46.379034: step 152720, loss = 0.56 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 22:20:58.568719: step 152730, loss = 0.49 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 22:21:10.594987: step 152740, loss = 0.45 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 22:21:22.825143: step 152750, loss = 0.42 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 22:21:35.013343: step 152760, loss = 0.54 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 22:21:47.203943: step 152770, loss = 0.48 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 22:21:59.371749: step 152780, loss = 0.52 (24.1 examples/sec; 1.246 sec/batch)\n",
      "2019-05-22 22:22:11.496150: step 152790, loss = 0.43 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 22:22:23.691613: step 152800, loss = 0.51 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 22:22:37.976722: step 152810, loss = 0.50 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 22:22:50.189499: step 152820, loss = 0.53 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 22:23:02.408198: step 152830, loss = 0.46 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 22:23:14.700296: step 152840, loss = 0.47 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 22:23:26.856903: step 152850, loss = 0.52 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 22:23:39.074391: step 152860, loss = 0.52 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 22:23:51.235821: step 152870, loss = 0.50 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 22:24:03.305852: step 152880, loss = 0.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 22:24:15.426724: step 152890, loss = 0.53 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 22:24:27.575925: step 152900, loss = 0.47 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 22:24:41.784345: step 152910, loss = 0.50 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 22:24:54.073958: step 152920, loss = 0.44 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 22:25:06.338356: step 152930, loss = 0.46 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 22:25:18.520133: step 152940, loss = 0.42 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 22:25:30.715270: step 152950, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 22:25:42.836928: step 152960, loss = 0.47 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 22:25:55.026164: step 152970, loss = 0.46 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 22:26:07.254739: step 152980, loss = 0.44 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 22:26:19.259769: step 152990, loss = 0.43 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 22:26:31.483175: step 153000, loss = 0.47 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 22:26:45.598917: step 153010, loss = 0.43 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 22:26:57.740704: step 153020, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 22:27:09.908019: step 153030, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 22:27:22.116400: step 153040, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 22:27:34.283161: step 153050, loss = 0.54 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 22:27:46.477163: step 153060, loss = 0.45 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 22:27:58.717763: step 153070, loss = 0.49 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 22:28:10.913497: step 153080, loss = 0.53 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 22:28:23.255003: step 153090, loss = 0.51 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-22 22:28:35.411174: step 153100, loss = 0.55 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 22:28:49.588601: step 153110, loss = 0.51 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 22:29:01.771929: step 153120, loss = 0.47 (24.3 examples/sec; 1.233 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 22:29:13.965518: step 153130, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 22:29:26.122898: step 153140, loss = 0.48 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 22:29:38.349925: step 153150, loss = 0.60 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 22:29:50.555528: step 153160, loss = 0.58 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 22:30:02.844209: step 153170, loss = 0.42 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 22:30:15.026625: step 153180, loss = 0.59 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 22:30:27.298834: step 153190, loss = 0.50 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 22:30:39.448144: step 153200, loss = 0.44 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 22:30:53.653996: step 153210, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 22:31:05.853224: step 153220, loss = 0.49 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 22:31:18.059202: step 153230, loss = 0.55 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 22:31:30.070537: step 153240, loss = 0.54 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 22:31:42.178348: step 153250, loss = 0.45 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 22:31:54.312102: step 153260, loss = 0.50 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 22:32:06.495406: step 153270, loss = 0.51 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 22:32:18.712398: step 153280, loss = 0.44 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 22:32:30.909316: step 153290, loss = 0.56 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 22:32:43.239742: step 153300, loss = 0.46 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 22:32:57.420634: step 153310, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 22:33:09.641468: step 153320, loss = 0.50 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 22:33:21.850442: step 153330, loss = 0.46 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 22:33:33.996958: step 153340, loss = 0.54 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 22:33:46.111059: step 153350, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 22:33:58.225285: step 153360, loss = 0.56 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 22:34:10.398678: step 153370, loss = 0.41 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 22:34:22.604384: step 153380, loss = 0.50 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 22:34:34.795670: step 153390, loss = 0.47 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 22:34:46.951642: step 153400, loss = 0.47 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 22:35:01.504782: step 153410, loss = 0.43 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 22:35:13.678206: step 153420, loss = 0.51 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 22:35:25.882834: step 153430, loss = 0.51 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 22:35:38.034675: step 153440, loss = 0.46 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 22:35:50.080929: step 153450, loss = 0.54 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 22:36:02.280380: step 153460, loss = 0.57 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 22:36:14.491633: step 153470, loss = 0.35 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 22:36:26.703494: step 153480, loss = 0.43 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 22:36:38.759503: step 153490, loss = 0.43 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 22:36:50.935685: step 153500, loss = 0.50 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 22:37:04.993323: step 153510, loss = 0.56 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 22:37:17.207780: step 153520, loss = 0.48 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 22:37:29.430766: step 153530, loss = 0.48 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-22 22:37:41.610679: step 153540, loss = 0.52 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 22:37:53.792655: step 153550, loss = 0.53 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 22:38:05.973862: step 153560, loss = 0.45 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 22:38:18.153549: step 153570, loss = 0.47 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 22:38:30.346636: step 153580, loss = 0.47 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 22:38:42.543142: step 153590, loss = 0.53 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 22:38:54.765631: step 153600, loss = 0.47 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 22:39:08.995263: step 153610, loss = 0.48 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 22:39:21.198833: step 153620, loss = 0.43 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 22:39:33.393463: step 153630, loss = 0.46 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 22:39:45.589577: step 153640, loss = 0.48 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 22:39:57.820843: step 153650, loss = 0.54 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 22:40:10.052165: step 153660, loss = 0.44 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 22:40:22.243064: step 153670, loss = 0.43 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 22:40:34.417960: step 153680, loss = 0.54 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 22:40:46.571890: step 153690, loss = 0.41 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 22:40:58.623997: step 153700, loss = 0.53 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 22:41:12.687721: step 153710, loss = 0.52 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 22:41:24.890617: step 153720, loss = 0.57 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 22:41:37.050562: step 153730, loss = 0.54 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 22:41:48.999916: step 153740, loss = 0.52 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 22:42:01.192994: step 153750, loss = 0.53 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 22:42:13.401463: step 153760, loss = 0.46 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 22:42:25.625736: step 153770, loss = 0.54 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 22:42:37.794984: step 153780, loss = 0.48 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 22:42:49.981063: step 153790, loss = 0.40 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 22:43:02.146767: step 153800, loss = 0.56 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 22:43:16.393267: step 153810, loss = 0.57 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 22:43:28.546423: step 153820, loss = 0.54 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 22:43:40.595294: step 153830, loss = 0.54 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 22:43:52.725661: step 153840, loss = 0.57 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 22:44:04.907371: step 153850, loss = 0.49 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-22 22:44:17.071055: step 153860, loss = 0.56 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 22:44:29.256002: step 153870, loss = 0.56 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 22:44:41.387832: step 153880, loss = 0.48 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 22:44:53.600828: step 153890, loss = 0.48 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 22:45:05.796329: step 153900, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 22:45:20.061629: step 153910, loss = 0.58 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 22:45:32.262206: step 153920, loss = 0.41 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 22:45:44.443495: step 153930, loss = 0.49 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 22:45:56.510615: step 153940, loss = 0.49 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-22 22:46:08.777882: step 153950, loss = 0.48 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 22:46:20.919336: step 153960, loss = 0.42 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 22:46:33.097417: step 153970, loss = 0.48 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 22:46:45.293771: step 153980, loss = 0.47 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 22:46:57.407955: step 153990, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 22:47:09.561138: step 154000, loss = 0.48 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 22:47:23.622387: step 154010, loss = 0.48 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 22:47:35.838344: step 154020, loss = 0.47 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 22:47:48.210816: step 154030, loss = 0.45 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 22:48:00.462126: step 154040, loss = 0.46 (24.3 examples/sec; 1.234 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 22:48:12.645726: step 154050, loss = 0.50 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 22:48:24.831464: step 154060, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 22:48:37.008565: step 154070, loss = 0.43 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 22:48:49.187834: step 154080, loss = 0.51 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 22:49:01.361357: step 154090, loss = 0.57 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 22:49:13.550092: step 154100, loss = 0.50 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 22:49:27.686846: step 154110, loss = 0.50 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 22:49:39.865951: step 154120, loss = 0.52 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 22:49:52.053460: step 154130, loss = 0.46 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 22:50:04.276390: step 154140, loss = 0.42 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 22:50:16.462796: step 154150, loss = 0.53 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 22:50:28.655146: step 154160, loss = 0.51 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 22:50:40.823320: step 154170, loss = 0.49 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 22:50:53.002687: step 154180, loss = 0.53 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 22:51:05.073376: step 154190, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 22:51:17.353180: step 154200, loss = 0.55 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 22:51:31.477681: step 154210, loss = 0.53 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 22:51:43.672134: step 154220, loss = 0.44 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 22:51:55.841850: step 154230, loss = 0.52 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 22:52:07.936501: step 154240, loss = 0.52 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 22:52:20.018131: step 154250, loss = 0.49 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 22:52:32.162368: step 154260, loss = 0.53 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 22:52:44.352618: step 154270, loss = 0.41 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 22:52:56.420831: step 154280, loss = 0.47 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 22:53:08.542845: step 154290, loss = 0.38 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 22:53:20.706189: step 154300, loss = 0.43 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 22:53:34.936401: step 154310, loss = 0.44 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 22:53:47.092709: step 154320, loss = 0.52 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 22:53:59.271757: step 154330, loss = 0.48 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 22:54:11.489809: step 154340, loss = 0.44 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 22:54:23.703814: step 154350, loss = 0.52 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 22:54:36.007655: step 154360, loss = 0.46 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 22:54:48.247962: step 154370, loss = 0.53 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 22:55:00.385384: step 154380, loss = 0.43 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-22 22:55:12.587497: step 154390, loss = 0.47 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 22:55:24.771810: step 154400, loss = 0.53 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 22:55:38.818234: step 154410, loss = 0.45 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 22:55:50.937154: step 154420, loss = 0.48 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 22:56:03.107981: step 154430, loss = 0.48 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 22:56:15.186206: step 154440, loss = 0.47 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 22:56:27.335921: step 154450, loss = 0.43 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 22:56:39.537667: step 154460, loss = 0.53 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 22:56:51.749991: step 154470, loss = 0.46 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 22:57:03.949510: step 154480, loss = 0.49 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 22:57:16.088067: step 154490, loss = 0.54 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 22:57:28.215232: step 154500, loss = 0.54 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 22:57:42.321244: step 154510, loss = 0.51 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 22:57:54.541429: step 154520, loss = 0.52 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 22:58:06.691672: step 154530, loss = 0.51 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 22:58:18.906313: step 154540, loss = 0.49 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 22:58:31.095759: step 154550, loss = 0.47 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 22:58:43.328640: step 154560, loss = 0.47 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-22 22:58:55.492337: step 154570, loss = 0.46 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 22:59:07.786658: step 154580, loss = 0.49 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 22:59:19.987193: step 154590, loss = 0.55 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 22:59:32.221332: step 154600, loss = 0.45 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 22:59:46.335700: step 154610, loss = 0.48 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 22:59:58.571097: step 154620, loss = 0.54 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-22 23:00:10.839986: step 154630, loss = 0.53 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 23:00:22.953568: step 154640, loss = 0.48 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 23:00:35.170853: step 154650, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 23:00:47.367023: step 154660, loss = 0.55 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 23:00:59.584506: step 154670, loss = 0.50 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 23:01:11.775256: step 154680, loss = 0.44 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 23:01:23.897803: step 154690, loss = 0.48 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 23:01:36.111817: step 154700, loss = 0.54 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 23:01:50.143346: step 154710, loss = 0.55 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 23:02:02.332326: step 154720, loss = 0.42 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 23:02:14.573034: step 154730, loss = 0.41 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-22 23:02:26.694900: step 154740, loss = 0.54 (25.4 examples/sec; 1.181 sec/batch)\n",
      "2019-05-22 23:02:38.770784: step 154750, loss = 0.45 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 23:02:50.843850: step 154760, loss = 0.51 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 23:03:02.988140: step 154770, loss = 0.49 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 23:03:15.166230: step 154780, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 23:03:27.358392: step 154790, loss = 0.52 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 23:03:39.548783: step 154800, loss = 0.56 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 23:03:53.705494: step 154810, loss = 0.45 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 23:04:05.943761: step 154820, loss = 0.41 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 23:04:18.128889: step 154830, loss = 0.47 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 23:04:30.332733: step 154840, loss = 0.53 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 23:04:42.602555: step 154850, loss = 0.45 (22.9 examples/sec; 1.309 sec/batch)\n",
      "2019-05-22 23:04:54.752917: step 154860, loss = 0.40 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 23:05:06.966357: step 154870, loss = 0.53 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 23:05:19.146978: step 154880, loss = 0.52 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 23:05:31.372227: step 154890, loss = 0.42 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 23:05:43.586927: step 154900, loss = 0.46 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 23:05:57.781721: step 154910, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 23:06:09.932650: step 154920, loss = 0.46 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 23:06:22.071159: step 154930, loss = 0.47 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 23:06:34.244470: step 154940, loss = 0.50 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 23:06:46.492377: step 154950, loss = 0.42 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 23:06:58.697907: step 154960, loss = 0.48 (24.6 examples/sec; 1.221 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 23:07:10.918002: step 154970, loss = 0.41 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 23:07:23.123610: step 154980, loss = 0.54 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 23:07:35.284173: step 154990, loss = 0.50 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 23:07:47.339051: step 155000, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 23:08:05.010602: step 155010, loss = 0.55 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 23:08:17.246749: step 155020, loss = 0.47 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 23:08:29.405689: step 155030, loss = 0.58 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 23:08:41.555839: step 155040, loss = 0.59 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 23:08:53.720284: step 155050, loss = 0.43 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 23:09:05.917797: step 155060, loss = 0.44 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 23:09:18.099019: step 155070, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 23:09:30.255040: step 155080, loss = 0.52 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 23:09:42.445623: step 155090, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 23:09:54.649841: step 155100, loss = 0.50 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 23:10:08.816016: step 155110, loss = 0.47 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 23:10:21.055144: step 155120, loss = 0.53 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 23:10:33.262168: step 155130, loss = 0.45 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 23:10:45.448335: step 155140, loss = 0.46 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 23:10:57.641741: step 155150, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 23:11:09.778886: step 155160, loss = 0.48 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 23:11:21.999592: step 155170, loss = 0.49 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 23:11:34.085111: step 155180, loss = 0.49 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 23:11:46.292273: step 155190, loss = 0.46 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 23:11:58.493540: step 155200, loss = 0.50 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 23:12:12.639591: step 155210, loss = 0.49 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 23:12:24.765662: step 155220, loss = 0.65 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 23:12:36.904139: step 155230, loss = 0.47 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 23:12:48.969398: step 155240, loss = 0.54 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 23:13:01.100961: step 155250, loss = 0.48 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 23:13:13.313408: step 155260, loss = 0.55 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 23:13:25.504619: step 155270, loss = 0.47 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 23:13:37.704368: step 155280, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 23:13:49.844753: step 155290, loss = 0.49 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 23:14:02.067726: step 155300, loss = 0.42 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 23:14:16.271660: step 155310, loss = 0.47 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 23:14:28.461450: step 155320, loss = 0.51 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-22 23:14:40.658963: step 155330, loss = 0.45 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 23:14:52.850437: step 155340, loss = 0.53 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 23:15:05.037174: step 155350, loss = 0.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 23:15:17.267047: step 155360, loss = 0.58 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 23:15:29.485949: step 155370, loss = 0.50 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 23:15:41.587326: step 155380, loss = 0.44 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 23:15:53.733423: step 155390, loss = 0.39 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 23:16:05.947089: step 155400, loss = 0.44 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 23:16:20.229904: step 155410, loss = 0.55 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 23:16:32.280027: step 155420, loss = 0.49 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 23:16:44.574556: step 155430, loss = 0.42 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 23:16:56.767948: step 155440, loss = 0.52 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 23:17:08.931130: step 155450, loss = 0.51 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 23:17:21.157990: step 155460, loss = 0.41 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-22 23:17:33.413751: step 155470, loss = 0.48 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 23:17:45.593802: step 155480, loss = 0.62 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 23:17:57.798269: step 155490, loss = 0.52 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 23:18:09.874205: step 155500, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 23:18:24.085103: step 155510, loss = 0.46 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 23:18:36.297908: step 155520, loss = 0.43 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 23:18:48.529836: step 155530, loss = 0.52 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 23:19:00.727396: step 155540, loss = 0.41 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 23:19:13.017247: step 155550, loss = 0.60 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 23:19:25.270258: step 155560, loss = 0.52 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 23:19:37.440494: step 155570, loss = 0.43 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 23:19:49.648592: step 155580, loss = 0.42 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 23:20:01.833947: step 155590, loss = 0.54 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 23:20:13.990237: step 155600, loss = 0.46 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 23:20:28.200628: step 155610, loss = 0.49 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 23:20:40.390935: step 155620, loss = 0.55 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 23:20:52.461533: step 155630, loss = 0.55 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 23:21:04.625569: step 155640, loss = 0.45 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 23:21:16.720888: step 155650, loss = 0.51 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 23:21:28.820165: step 155660, loss = 0.45 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-22 23:21:40.877662: step 155670, loss = 0.54 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-22 23:21:52.962933: step 155680, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 23:22:04.989143: step 155690, loss = 0.57 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 23:22:17.095346: step 155700, loss = 0.50 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 23:22:31.482923: step 155710, loss = 0.64 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 23:22:43.608464: step 155720, loss = 0.44 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 23:22:55.789725: step 155730, loss = 0.49 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 23:23:07.834135: step 155740, loss = 0.57 (25.2 examples/sec; 1.188 sec/batch)\n",
      "2019-05-22 23:23:19.829073: step 155750, loss = 0.51 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 23:23:31.963557: step 155760, loss = 0.41 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 23:23:44.072390: step 155770, loss = 0.49 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 23:23:56.173762: step 155780, loss = 0.47 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 23:24:08.336538: step 155790, loss = 0.45 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 23:24:20.462892: step 155800, loss = 0.52 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 23:24:34.802461: step 155810, loss = 0.53 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 23:24:46.998236: step 155820, loss = 0.50 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 23:24:59.170226: step 155830, loss = 0.45 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 23:25:11.317790: step 155840, loss = 0.48 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 23:25:23.488158: step 155850, loss = 0.56 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 23:25:35.655493: step 155860, loss = 0.59 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 23:25:47.889025: step 155870, loss = 0.55 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 23:26:00.036263: step 155880, loss = 0.49 (24.8 examples/sec; 1.208 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 23:26:12.217113: step 155890, loss = 0.48 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 23:26:24.349099: step 155900, loss = 0.57 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 23:26:38.678832: step 155910, loss = 0.47 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-22 23:26:50.713050: step 155920, loss = 0.48 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 23:27:02.814089: step 155930, loss = 0.46 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 23:27:15.000293: step 155940, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 23:27:27.150419: step 155950, loss = 0.45 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 23:27:39.335838: step 155960, loss = 0.41 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 23:27:51.485522: step 155970, loss = 0.55 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 23:28:03.652798: step 155980, loss = 0.50 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 23:28:15.756864: step 155990, loss = 0.47 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-22 23:28:27.749759: step 156000, loss = 0.47 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 23:28:42.000579: step 156010, loss = 0.46 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 23:28:54.161225: step 156020, loss = 0.51 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-22 23:29:06.259588: step 156030, loss = 0.43 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 23:29:18.372694: step 156040, loss = 0.50 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-22 23:29:30.557875: step 156050, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 23:29:42.682048: step 156060, loss = 0.44 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 23:29:54.802790: step 156070, loss = 0.50 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 23:30:06.928505: step 156080, loss = 0.42 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 23:30:19.075591: step 156090, loss = 0.48 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 23:30:31.220523: step 156100, loss = 0.48 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 23:30:45.442130: step 156110, loss = 0.47 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 23:30:57.567114: step 156120, loss = 0.54 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-22 23:31:09.744209: step 156130, loss = 0.45 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 23:31:21.867997: step 156140, loss = 0.48 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 23:31:33.946806: step 156150, loss = 0.55 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 23:31:46.039003: step 156160, loss = 0.46 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 23:31:58.083198: step 156170, loss = 0.48 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 23:32:10.069397: step 156180, loss = 0.50 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-22 23:32:22.020144: step 156190, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 23:32:34.075036: step 156200, loss = 0.47 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 23:32:48.513459: step 156210, loss = 0.57 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 23:33:00.654689: step 156220, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 23:33:12.763310: step 156230, loss = 0.65 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 23:33:24.938711: step 156240, loss = 0.45 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 23:33:36.957671: step 156250, loss = 0.48 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 23:33:49.069078: step 156260, loss = 0.64 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 23:34:01.216326: step 156270, loss = 0.45 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 23:34:13.372558: step 156280, loss = 0.52 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 23:34:25.596827: step 156290, loss = 0.46 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 23:34:37.755683: step 156300, loss = 0.50 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 23:34:51.776765: step 156310, loss = 0.61 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 23:35:03.870205: step 156320, loss = 0.45 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 23:35:16.002707: step 156330, loss = 0.46 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 23:35:28.199727: step 156340, loss = 0.45 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 23:35:40.314834: step 156350, loss = 0.60 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 23:35:52.427155: step 156360, loss = 0.46 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 23:36:04.595196: step 156370, loss = 0.57 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 23:36:16.734425: step 156380, loss = 0.49 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 23:36:28.895638: step 156390, loss = 0.46 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 23:36:41.055370: step 156400, loss = 0.53 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 23:36:55.131531: step 156410, loss = 0.49 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 23:37:07.249958: step 156420, loss = 0.51 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 23:37:19.375381: step 156430, loss = 0.47 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 23:37:31.468394: step 156440, loss = 0.45 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 23:37:43.558052: step 156450, loss = 0.48 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 23:37:55.688796: step 156460, loss = 0.55 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 23:38:07.812504: step 156470, loss = 0.46 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 23:38:19.966602: step 156480, loss = 0.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 23:38:32.132593: step 156490, loss = 0.47 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-22 23:38:44.196495: step 156500, loss = 0.52 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 23:38:58.169357: step 156510, loss = 0.54 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-22 23:39:10.330342: step 156520, loss = 0.40 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 23:39:22.483599: step 156530, loss = 0.39 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 23:39:34.596956: step 156540, loss = 0.48 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 23:39:46.732166: step 156550, loss = 0.53 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 23:39:58.877151: step 156560, loss = 0.49 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 23:40:11.014300: step 156570, loss = 0.42 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 23:40:23.165029: step 156580, loss = 0.43 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 23:40:35.291139: step 156590, loss = 0.55 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 23:40:47.412720: step 156600, loss = 0.49 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 23:41:01.555105: step 156610, loss = 0.51 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 23:41:13.729939: step 156620, loss = 0.46 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 23:41:25.842791: step 156630, loss = 0.46 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 23:41:37.955347: step 156640, loss = 0.44 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 23:41:50.136646: step 156650, loss = 0.54 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 23:42:02.271588: step 156660, loss = 0.48 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 23:42:14.432972: step 156670, loss = 0.51 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 23:42:26.559824: step 156680, loss = 0.47 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 23:42:38.718079: step 156690, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 23:42:50.906232: step 156700, loss = 0.47 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 23:43:04.992612: step 156710, loss = 0.41 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 23:43:17.156486: step 156720, loss = 0.53 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 23:43:29.293996: step 156730, loss = 0.46 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 23:43:41.434639: step 156740, loss = 0.45 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 23:43:53.461711: step 156750, loss = 0.48 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-22 23:44:05.564021: step 156760, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 23:44:17.691416: step 156770, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 23:44:29.876264: step 156780, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 23:44:42.068651: step 156790, loss = 0.55 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 23:44:54.148855: step 156800, loss = 0.55 (24.8 examples/sec; 1.212 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-22 23:45:08.403626: step 156810, loss = 0.51 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 23:45:20.522095: step 156820, loss = 0.45 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 23:45:32.611888: step 156830, loss = 0.44 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 23:45:44.759202: step 156840, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 23:45:56.954261: step 156850, loss = 0.59 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-22 23:46:09.078770: step 156860, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 23:46:21.287641: step 156870, loss = 0.43 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 23:46:33.429012: step 156880, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 23:46:45.580812: step 156890, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 23:46:57.721099: step 156900, loss = 0.51 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-22 23:47:11.811863: step 156910, loss = 0.48 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-22 23:47:23.943172: step 156920, loss = 0.43 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 23:47:36.095949: step 156930, loss = 0.43 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-22 23:47:48.201983: step 156940, loss = 0.48 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-22 23:48:00.356630: step 156950, loss = 0.45 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 23:48:12.526305: step 156960, loss = 0.45 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 23:48:24.628683: step 156970, loss = 0.45 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 23:48:36.776293: step 156980, loss = 0.56 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 23:48:48.908439: step 156990, loss = 0.54 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-22 23:49:00.959485: step 157000, loss = 0.53 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-22 23:49:15.047467: step 157010, loss = 0.59 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 23:49:27.146451: step 157020, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 23:49:39.289185: step 157030, loss = 0.45 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 23:49:51.373287: step 157040, loss = 0.47 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 23:50:03.537663: step 157050, loss = 0.53 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-22 23:50:15.618437: step 157060, loss = 0.61 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 23:50:27.707920: step 157070, loss = 0.46 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 23:50:39.814827: step 157080, loss = 0.44 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-22 23:50:51.992699: step 157090, loss = 0.60 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 23:51:04.124656: step 157100, loss = 0.50 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 23:51:18.298994: step 157110, loss = 0.69 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 23:51:30.436517: step 157120, loss = 0.54 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 23:51:42.601415: step 157130, loss = 0.47 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 23:51:54.812619: step 157140, loss = 0.47 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-22 23:52:06.986364: step 157150, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 23:52:19.008863: step 157160, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 23:52:31.120703: step 157170, loss = 0.48 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 23:52:43.269985: step 157180, loss = 0.56 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 23:52:55.448804: step 157190, loss = 0.46 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 23:53:07.602732: step 157200, loss = 0.47 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-22 23:53:21.787704: step 157210, loss = 0.41 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 23:53:33.921924: step 157220, loss = 0.44 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 23:53:46.058174: step 157230, loss = 0.53 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-22 23:53:58.216502: step 157240, loss = 0.52 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 23:54:10.334610: step 157250, loss = 0.46 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 23:54:22.360836: step 157260, loss = 0.49 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 23:54:34.425683: step 157270, loss = 0.49 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 23:54:46.569298: step 157280, loss = 0.39 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 23:54:58.741277: step 157290, loss = 0.47 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 23:55:10.891687: step 157300, loss = 0.44 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-22 23:55:25.147460: step 157310, loss = 0.57 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-22 23:55:37.288103: step 157320, loss = 0.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 23:55:49.438932: step 157330, loss = 0.51 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 23:56:01.560551: step 157340, loss = 0.44 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-22 23:56:13.698005: step 157350, loss = 0.45 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-22 23:56:25.811465: step 157360, loss = 0.45 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-22 23:56:37.955691: step 157370, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-22 23:56:50.098785: step 157380, loss = 0.44 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-22 23:57:02.257139: step 157390, loss = 0.43 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-22 23:57:14.395851: step 157400, loss = 0.49 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-22 23:57:28.462647: step 157410, loss = 0.42 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-22 23:57:40.564606: step 157420, loss = 0.44 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-22 23:57:52.690895: step 157430, loss = 0.44 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-22 23:58:04.808621: step 157440, loss = 0.58 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-22 23:58:16.939721: step 157450, loss = 0.41 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-22 23:58:29.064029: step 157460, loss = 0.48 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-22 23:58:41.191619: step 157470, loss = 0.53 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-22 23:58:53.347157: step 157480, loss = 0.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-22 23:59:05.490916: step 157490, loss = 0.53 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-22 23:59:17.613228: step 157500, loss = 0.46 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-22 23:59:31.885660: step 157510, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-22 23:59:44.023873: step 157520, loss = 0.56 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-22 23:59:56.146477: step 157530, loss = 0.50 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 00:00:08.260592: step 157540, loss = 0.51 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 00:00:20.391643: step 157550, loss = 0.44 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-23 00:00:32.517959: step 157560, loss = 0.48 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 00:00:44.660028: step 157570, loss = 0.50 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 00:00:56.726425: step 157580, loss = 0.46 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 00:01:08.890868: step 157590, loss = 0.47 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 00:01:21.032996: step 157600, loss = 0.41 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-23 00:01:35.016212: step 157610, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 00:01:47.125680: step 157620, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 00:01:59.265504: step 157630, loss = 0.46 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-23 00:02:11.390120: step 157640, loss = 0.52 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 00:02:23.479967: step 157650, loss = 0.48 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 00:02:35.563121: step 157660, loss = 0.42 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 00:02:47.668132: step 157670, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 00:02:59.814301: step 157680, loss = 0.66 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 00:03:11.931873: step 157690, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 00:03:24.036664: step 157700, loss = 0.52 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 00:03:38.061654: step 157710, loss = 0.45 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 00:03:50.209751: step 157720, loss = 0.55 (24.8 examples/sec; 1.210 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23 00:04:02.353586: step 157730, loss = 0.43 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-23 00:04:14.470774: step 157740, loss = 0.42 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 00:04:26.593094: step 157750, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 00:04:38.634631: step 157760, loss = 0.44 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 00:04:50.771515: step 157770, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 00:05:02.911068: step 157780, loss = 0.45 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 00:05:15.025523: step 157790, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 00:05:27.178260: step 157800, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 00:05:41.342605: step 157810, loss = 0.43 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 00:05:53.445554: step 157820, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 00:06:05.532520: step 157830, loss = 0.45 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 00:06:17.693389: step 157840, loss = 0.44 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 00:06:29.872356: step 157850, loss = 0.59 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 00:06:42.068351: step 157860, loss = 0.55 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 00:06:54.159644: step 157870, loss = 0.48 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 00:07:06.271964: step 157880, loss = 0.47 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 00:07:18.478881: step 157890, loss = 0.48 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 00:07:30.543720: step 157900, loss = 0.43 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 00:07:44.597728: step 157910, loss = 0.48 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 00:07:56.764526: step 157920, loss = 0.40 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-23 00:08:08.986506: step 157930, loss = 0.46 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 00:08:21.112577: step 157940, loss = 0.43 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 00:08:33.231387: step 157950, loss = 0.49 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 00:08:45.362508: step 157960, loss = 0.40 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 00:08:57.492202: step 157970, loss = 0.52 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 00:09:09.660272: step 157980, loss = 0.48 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 00:09:21.805544: step 157990, loss = 0.50 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 00:09:33.909870: step 158000, loss = 0.45 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 00:09:48.130467: step 158010, loss = 0.50 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-23 00:10:00.217672: step 158020, loss = 0.47 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 00:10:12.318040: step 158030, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 00:10:24.448985: step 158040, loss = 0.47 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 00:10:36.562509: step 158050, loss = 0.45 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 00:10:48.718715: step 158060, loss = 0.48 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 00:11:00.890404: step 158070, loss = 0.51 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 00:11:13.098618: step 158080, loss = 0.44 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 00:11:25.265468: step 158090, loss = 0.47 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 00:11:37.385575: step 158100, loss = 0.51 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 00:11:51.831415: step 158110, loss = 0.47 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 00:12:03.931219: step 158120, loss = 0.48 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 00:12:16.103712: step 158130, loss = 0.44 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 00:12:28.200528: step 158140, loss = 0.45 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 00:12:40.263153: step 158150, loss = 0.47 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 00:12:52.350935: step 158160, loss = 0.46 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 00:13:04.491257: step 158170, loss = 0.45 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 00:13:16.627227: step 158180, loss = 0.60 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 00:13:28.718737: step 158190, loss = 0.44 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 00:13:40.857358: step 158200, loss = 0.44 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 00:13:55.173869: step 158210, loss = 0.51 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 00:14:07.314484: step 158220, loss = 0.43 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 00:14:19.491629: step 158230, loss = 0.42 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 00:14:31.559839: step 158240, loss = 0.54 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 00:14:43.683337: step 158250, loss = 0.50 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 00:14:55.741168: step 158260, loss = 0.50 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 00:15:07.793240: step 158270, loss = 0.51 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-23 00:15:19.998023: step 158280, loss = 0.54 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-23 00:15:32.153671: step 158290, loss = 0.50 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 00:15:44.237254: step 158300, loss = 0.49 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-23 00:15:58.328199: step 158310, loss = 0.49 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 00:16:10.497715: step 158320, loss = 0.57 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 00:16:22.615387: step 158330, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 00:16:34.745278: step 158340, loss = 0.40 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-23 00:16:46.894574: step 158350, loss = 0.43 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 00:16:59.016569: step 158360, loss = 0.42 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 00:17:11.130722: step 158370, loss = 0.47 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 00:17:23.256708: step 158380, loss = 0.37 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 00:17:35.392822: step 158390, loss = 0.47 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 00:17:47.395994: step 158400, loss = 0.50 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 00:18:01.558787: step 158410, loss = 0.47 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 00:18:13.688450: step 158420, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 00:18:25.862003: step 158430, loss = 0.51 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 00:18:37.970045: step 158440, loss = 0.50 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 00:18:50.139460: step 158450, loss = 0.54 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 00:19:02.260190: step 158460, loss = 0.53 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 00:19:14.385960: step 158470, loss = 0.52 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 00:19:26.509456: step 158480, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 00:19:38.625218: step 158490, loss = 0.47 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 00:19:50.756848: step 158500, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 00:20:05.317278: step 158510, loss = 0.56 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-23 00:20:17.384469: step 158520, loss = 0.49 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 00:20:29.543192: step 158530, loss = 0.45 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 00:20:41.691219: step 158540, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 00:20:53.844780: step 158550, loss = 0.52 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 00:21:05.974186: step 158560, loss = 0.50 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 00:21:18.114023: step 158570, loss = 0.49 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 00:21:30.195120: step 158580, loss = 0.48 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 00:21:42.332294: step 158590, loss = 0.45 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 00:21:54.473822: step 158600, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 00:22:08.576328: step 158610, loss = 0.45 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 00:22:20.718129: step 158620, loss = 0.43 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 00:22:32.822611: step 158630, loss = 0.41 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 00:22:44.914492: step 158640, loss = 0.48 (25.0 examples/sec; 1.198 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23 00:22:57.025860: step 158650, loss = 0.54 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 00:23:09.164337: step 158660, loss = 0.52 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 00:23:21.329891: step 158670, loss = 0.46 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 00:23:33.443177: step 158680, loss = 0.53 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 00:23:45.572828: step 158690, loss = 0.46 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 00:23:57.727483: step 158700, loss = 0.47 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 00:24:11.930536: step 158710, loss = 0.48 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 00:24:24.014993: step 158720, loss = 0.45 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 00:24:36.172948: step 158730, loss = 0.58 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 00:24:48.337601: step 158740, loss = 0.44 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 00:25:00.456797: step 158750, loss = 0.54 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 00:25:12.578792: step 158760, loss = 0.48 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 00:25:24.610125: step 158770, loss = 0.52 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 00:25:36.714319: step 158780, loss = 0.54 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 00:25:48.807026: step 158790, loss = 0.45 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 00:26:00.930248: step 158800, loss = 0.43 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 00:26:15.260297: step 158810, loss = 0.57 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 00:26:27.373807: step 158820, loss = 0.48 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 00:26:39.458635: step 158830, loss = 0.43 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 00:26:51.559027: step 158840, loss = 0.46 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 00:27:03.689118: step 158850, loss = 0.47 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 00:27:15.773403: step 158860, loss = 0.38 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 00:27:27.884777: step 158870, loss = 0.55 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-23 00:27:40.016586: step 158880, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 00:27:52.093093: step 158890, loss = 0.49 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 00:28:04.179472: step 158900, loss = 0.47 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 00:28:18.288740: step 158910, loss = 0.40 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 00:28:30.411769: step 158920, loss = 0.43 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 00:28:42.531136: step 158930, loss = 0.62 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 00:28:54.723340: step 158940, loss = 0.41 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-23 00:29:06.884241: step 158950, loss = 0.50 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 00:29:19.017389: step 158960, loss = 0.41 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 00:29:31.136262: step 158970, loss = 0.41 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 00:29:43.282951: step 158980, loss = 0.40 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 00:29:55.464023: step 158990, loss = 0.48 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 00:30:07.572926: step 159000, loss = 0.44 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 00:30:21.608062: step 159010, loss = 0.54 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 00:30:33.614980: step 159020, loss = 0.51 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-23 00:30:45.737929: step 159030, loss = 0.38 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 00:30:57.871953: step 159040, loss = 0.55 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 00:31:09.981917: step 159050, loss = 0.49 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 00:31:22.113454: step 159060, loss = 0.49 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 00:31:34.307804: step 159070, loss = 0.48 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 00:31:46.456697: step 159080, loss = 0.38 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 00:31:58.558842: step 159090, loss = 0.51 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-23 00:32:10.718039: step 159100, loss = 0.41 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 00:32:24.926866: step 159110, loss = 0.49 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 00:32:37.029962: step 159120, loss = 0.51 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 00:32:49.179733: step 159130, loss = 0.38 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 00:33:01.275909: step 159140, loss = 0.49 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 00:33:13.470884: step 159150, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 00:33:25.669548: step 159160, loss = 0.55 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 00:33:37.814676: step 159170, loss = 0.53 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-23 00:33:50.008318: step 159180, loss = 0.54 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-23 00:34:02.171531: step 159190, loss = 0.57 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 00:34:14.356613: step 159200, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 00:34:28.376910: step 159210, loss = 0.57 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 00:34:40.500215: step 159220, loss = 0.54 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 00:34:52.654776: step 159230, loss = 0.46 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 00:35:04.810181: step 159240, loss = 0.46 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 00:35:16.946031: step 159250, loss = 0.52 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 00:35:29.073530: step 159260, loss = 0.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 00:35:41.131178: step 159270, loss = 0.53 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 00:35:53.203472: step 159280, loss = 0.49 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 00:36:05.356766: step 159290, loss = 0.63 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 00:36:17.501788: step 159300, loss = 0.55 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 00:36:31.725963: step 159310, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 00:36:43.860681: step 159320, loss = 0.54 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 00:36:55.957158: step 159330, loss = 0.43 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 00:37:08.114244: step 159340, loss = 0.43 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 00:37:20.294235: step 159350, loss = 0.45 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 00:37:32.479897: step 159360, loss = 0.38 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 00:37:44.599833: step 159370, loss = 0.52 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 00:37:56.709533: step 159380, loss = 0.59 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 00:38:08.792830: step 159390, loss = 0.49 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 00:38:20.893862: step 159400, loss = 0.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 00:38:35.108461: step 159410, loss = 0.52 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 00:38:47.264806: step 159420, loss = 0.50 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 00:38:59.355457: step 159430, loss = 0.47 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-23 00:39:11.483916: step 159440, loss = 0.52 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 00:39:23.618406: step 159450, loss = 0.54 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 00:39:35.751628: step 159460, loss = 0.46 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 00:39:47.903620: step 159470, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 00:40:00.014252: step 159480, loss = 0.44 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 00:40:12.191212: step 159490, loss = 0.52 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 00:40:24.319561: step 159500, loss = 0.59 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 00:40:38.396415: step 159510, loss = 0.45 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 00:40:50.434988: step 159520, loss = 0.53 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 00:41:02.455668: step 159530, loss = 0.42 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-23 00:41:14.615368: step 159540, loss = 0.54 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 00:41:26.771222: step 159550, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 00:41:38.929613: step 159560, loss = 0.44 (24.8 examples/sec; 1.209 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23 00:41:51.119002: step 159570, loss = 0.46 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 00:42:03.217708: step 159580, loss = 0.40 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 00:42:15.366157: step 159590, loss = 0.47 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 00:42:27.469697: step 159600, loss = 0.52 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 00:42:41.738473: step 159610, loss = 0.52 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 00:42:53.875520: step 159620, loss = 0.42 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 00:43:06.012152: step 159630, loss = 0.45 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 00:43:18.122599: step 159640, loss = 0.53 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 00:43:30.313303: step 159650, loss = 0.49 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-23 00:43:42.465612: step 159660, loss = 0.43 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 00:43:54.599796: step 159670, loss = 0.47 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 00:44:06.766411: step 159680, loss = 0.46 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 00:44:18.889873: step 159690, loss = 0.47 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 00:44:31.017449: step 159700, loss = 0.50 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 00:44:45.234233: step 159710, loss = 0.46 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 00:44:57.410441: step 159720, loss = 0.44 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 00:45:09.616594: step 159730, loss = 0.50 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 00:45:21.766707: step 159740, loss = 0.55 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 00:45:33.935535: step 159750, loss = 0.43 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 00:45:46.097181: step 159760, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 00:45:58.258628: step 159770, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 00:46:10.299150: step 159780, loss = 0.39 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 00:46:22.420513: step 159790, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 00:46:34.569716: step 159800, loss = 0.45 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 00:46:49.077113: step 159810, loss = 0.48 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 00:47:01.193744: step 159820, loss = 0.42 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 00:47:13.319229: step 159830, loss = 0.46 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 00:47:25.460885: step 159840, loss = 0.50 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 00:47:37.595306: step 159850, loss = 0.49 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 00:47:49.739693: step 159860, loss = 0.47 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 00:48:01.896572: step 159870, loss = 0.50 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 00:48:14.001798: step 159880, loss = 0.42 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-23 00:48:26.105523: step 159890, loss = 0.46 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 00:48:38.226463: step 159900, loss = 0.48 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 00:48:52.342184: step 159910, loss = 0.44 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-23 00:49:04.492651: step 159920, loss = 0.54 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 00:49:16.638562: step 159930, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 00:49:28.750123: step 159940, loss = 0.54 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 00:49:40.833023: step 159950, loss = 0.36 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 00:49:52.984131: step 159960, loss = 0.52 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 00:50:05.139638: step 159970, loss = 0.52 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 00:50:17.296307: step 159980, loss = 0.47 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 00:50:29.431196: step 159990, loss = 0.44 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 00:50:41.560170: step 160000, loss = 0.45 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 00:50:59.557621: step 160010, loss = 0.54 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 00:51:11.648430: step 160020, loss = 0.46 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-23 00:51:23.671014: step 160030, loss = 0.52 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 00:51:35.798328: step 160040, loss = 0.45 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 00:51:47.971892: step 160050, loss = 0.49 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 00:52:00.125172: step 160060, loss = 0.47 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-23 00:52:12.232822: step 160070, loss = 0.52 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 00:52:24.427022: step 160080, loss = 0.48 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 00:52:36.569691: step 160090, loss = 0.46 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 00:52:48.706235: step 160100, loss = 0.54 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 00:53:03.166569: step 160110, loss = 0.52 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 00:53:15.308721: step 160120, loss = 0.58 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 00:53:27.380747: step 160130, loss = 0.53 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 00:53:39.520591: step 160140, loss = 0.55 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 00:53:51.651259: step 160150, loss = 0.47 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-23 00:54:03.804072: step 160160, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 00:54:15.991445: step 160170, loss = 0.53 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 00:54:28.166700: step 160180, loss = 0.47 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-23 00:54:40.326591: step 160190, loss = 0.46 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 00:54:52.482323: step 160200, loss = 0.45 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 00:55:07.109120: step 160210, loss = 0.42 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 00:55:19.238791: step 160220, loss = 0.44 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 00:55:31.281537: step 160230, loss = 0.44 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-23 00:55:43.315106: step 160240, loss = 0.44 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-23 00:55:55.437768: step 160250, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 00:56:07.563228: step 160260, loss = 0.48 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 00:56:19.644773: step 160270, loss = 0.48 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-23 00:56:31.672365: step 160280, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 00:56:43.798891: step 160290, loss = 0.46 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 00:56:55.934887: step 160300, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 00:57:09.887147: step 160310, loss = 0.40 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 00:57:21.986524: step 160320, loss = 0.45 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 00:57:34.083413: step 160330, loss = 0.46 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 00:57:46.215346: step 160340, loss = 0.44 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 00:57:58.366596: step 160350, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 00:58:10.452999: step 160360, loss = 0.44 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 00:58:22.581955: step 160370, loss = 0.39 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 00:58:34.689965: step 160380, loss = 0.55 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 00:58:46.847007: step 160390, loss = 0.46 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 00:58:59.013217: step 160400, loss = 0.43 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 00:59:13.183743: step 160410, loss = 0.44 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 00:59:25.261703: step 160420, loss = 0.58 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 00:59:37.411101: step 160430, loss = 0.50 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 00:59:49.548931: step 160440, loss = 0.50 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 01:00:01.697016: step 160450, loss = 0.49 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 01:00:13.859426: step 160460, loss = 0.55 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 01:00:26.006878: step 160470, loss = 0.49 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 01:00:38.138723: step 160480, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23 01:00:50.287254: step 160490, loss = 0.50 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 01:01:02.395696: step 160500, loss = 0.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 01:01:16.457077: step 160510, loss = 0.51 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 01:01:28.551397: step 160520, loss = 0.40 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-23 01:01:40.602292: step 160530, loss = 0.48 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-23 01:01:52.757343: step 160540, loss = 0.48 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 01:02:04.903418: step 160550, loss = 0.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 01:02:17.072811: step 160560, loss = 0.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 01:02:29.203603: step 160570, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 01:02:41.319502: step 160580, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 01:02:53.443660: step 160590, loss = 0.45 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 01:03:05.522629: step 160600, loss = 0.50 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-23 01:03:19.484368: step 160610, loss = 0.50 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 01:03:31.580442: step 160620, loss = 0.50 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 01:03:43.761392: step 160630, loss = 0.49 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 01:03:55.942320: step 160640, loss = 0.47 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 01:04:08.062929: step 160650, loss = 0.50 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 01:04:20.235338: step 160660, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 01:04:32.424222: step 160670, loss = 0.47 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 01:04:44.553374: step 160680, loss = 0.44 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 01:04:56.733369: step 160690, loss = 0.51 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 01:05:08.899481: step 160700, loss = 0.40 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 01:05:23.138914: step 160710, loss = 0.43 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-23 01:05:35.277669: step 160720, loss = 0.56 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 01:05:47.406941: step 160730, loss = 0.49 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 01:05:59.568904: step 160740, loss = 0.47 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 01:06:11.720054: step 160750, loss = 0.51 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 01:06:23.850055: step 160760, loss = 0.55 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 01:06:36.021072: step 160770, loss = 0.47 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 01:06:47.999944: step 160780, loss = 0.58 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-23 01:07:00.080678: step 160790, loss = 0.45 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 01:07:12.214322: step 160800, loss = 0.52 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 01:07:26.501974: step 160810, loss = 0.50 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 01:07:38.617397: step 160820, loss = 0.56 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 01:07:50.739488: step 160830, loss = 0.44 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 01:08:02.867118: step 160840, loss = 0.45 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 01:08:14.991886: step 160850, loss = 0.48 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 01:08:27.164464: step 160860, loss = 0.53 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 01:08:39.222968: step 160870, loss = 0.55 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 01:08:51.323698: step 160880, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 01:09:03.442379: step 160890, loss = 0.49 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 01:09:15.585518: step 160900, loss = 0.52 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 01:09:30.088735: step 160910, loss = 0.49 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 01:09:42.199112: step 160920, loss = 0.43 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 01:09:54.273986: step 160930, loss = 0.50 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 01:10:06.392140: step 160940, loss = 0.46 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 01:10:18.521219: step 160950, loss = 0.45 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 01:10:30.667315: step 160960, loss = 0.51 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-23 01:10:42.784711: step 160970, loss = 0.48 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 01:10:54.936880: step 160980, loss = 0.57 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 01:11:07.087301: step 160990, loss = 0.54 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 01:11:19.194962: step 161000, loss = 0.45 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 01:11:33.258187: step 161010, loss = 0.49 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 01:11:45.404863: step 161020, loss = 0.57 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 01:11:57.428080: step 161030, loss = 0.43 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 01:12:09.526322: step 161040, loss = 0.49 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 01:12:21.698692: step 161050, loss = 0.48 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-23 01:12:33.826953: step 161060, loss = 0.59 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 01:12:45.949390: step 161070, loss = 0.55 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 01:12:58.064639: step 161080, loss = 0.46 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 01:13:10.179110: step 161090, loss = 0.52 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 01:13:22.357504: step 161100, loss = 0.45 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-23 01:13:36.373841: step 161110, loss = 0.51 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 01:13:48.460171: step 161120, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 01:14:00.600149: step 161130, loss = 0.43 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 01:14:12.739646: step 161140, loss = 0.41 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 01:14:24.878401: step 161150, loss = 0.46 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 01:14:36.979033: step 161160, loss = 0.45 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 01:14:49.103871: step 161170, loss = 0.46 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 01:15:01.220376: step 161180, loss = 0.46 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 01:15:13.348348: step 161190, loss = 0.43 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 01:15:25.501989: step 161200, loss = 0.44 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 01:15:39.803796: step 161210, loss = 0.48 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 01:15:51.908646: step 161220, loss = 0.57 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 01:16:04.001055: step 161230, loss = 0.50 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 01:16:16.143135: step 161240, loss = 0.47 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-23 01:16:28.270255: step 161250, loss = 0.51 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 01:16:40.363042: step 161260, loss = 0.50 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-23 01:16:52.500377: step 161270, loss = 0.55 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 01:17:04.550669: step 161280, loss = 0.47 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 01:17:16.600536: step 161290, loss = 0.52 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 01:17:28.760810: step 161300, loss = 0.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 01:17:42.936989: step 161310, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 01:17:55.033843: step 161320, loss = 0.38 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-23 01:18:07.121160: step 161330, loss = 0.53 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 01:18:19.242591: step 161340, loss = 0.46 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 01:18:31.358286: step 161350, loss = 0.48 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 01:18:43.510912: step 161360, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 01:18:55.582336: step 161370, loss = 0.48 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 01:19:07.733882: step 161380, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 01:19:19.892232: step 161390, loss = 0.52 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-23 01:19:32.016034: step 161400, loss = 0.44 (24.9 examples/sec; 1.206 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23 01:19:46.069421: step 161410, loss = 0.48 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-23 01:19:58.192780: step 161420, loss = 0.45 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 01:20:10.366128: step 161430, loss = 0.52 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 01:20:22.506516: step 161440, loss = 0.46 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 01:20:34.659317: step 161450, loss = 0.49 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 01:20:46.826099: step 161460, loss = 0.48 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 01:20:58.991259: step 161470, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 01:21:11.142139: step 161480, loss = 0.54 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 01:21:23.274411: step 161490, loss = 0.47 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 01:21:35.407174: step 161500, loss = 0.46 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 01:21:49.501806: step 161510, loss = 0.40 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 01:22:01.624263: step 161520, loss = 0.45 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 01:22:13.775305: step 161530, loss = 0.44 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 01:22:25.751730: step 161540, loss = 0.48 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 01:22:37.887928: step 161550, loss = 0.54 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 01:22:50.043199: step 161560, loss = 0.57 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 01:23:02.162064: step 161570, loss = 0.50 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 01:23:14.252103: step 161580, loss = 0.57 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 01:23:26.435823: step 161590, loss = 0.48 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 01:23:38.551909: step 161600, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 01:23:52.587574: step 161610, loss = 0.47 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-23 01:24:04.681208: step 161620, loss = 0.49 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 01:24:16.854273: step 161630, loss = 0.53 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 01:24:28.961903: step 161640, loss = 0.47 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-23 01:24:41.056875: step 161650, loss = 0.43 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 01:24:53.223674: step 161660, loss = 0.56 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 01:25:05.318554: step 161670, loss = 0.42 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 01:25:17.453721: step 161680, loss = 0.50 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 01:25:29.591904: step 161690, loss = 0.44 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 01:25:41.707142: step 161700, loss = 0.42 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 01:25:55.992191: step 161710, loss = 0.55 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 01:26:08.092690: step 161720, loss = 0.45 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 01:26:20.239249: step 161730, loss = 0.50 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 01:26:32.362304: step 161740, loss = 0.43 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 01:26:44.510732: step 161750, loss = 0.53 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-23 01:26:56.682936: step 161760, loss = 0.45 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-23 01:27:08.813277: step 161770, loss = 0.49 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 01:27:21.002085: step 161780, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 01:27:33.026791: step 161790, loss = 0.54 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 01:27:45.133194: step 161800, loss = 0.53 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 01:27:59.704439: step 161810, loss = 0.43 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 01:28:11.849051: step 161820, loss = 0.55 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 01:28:23.966423: step 161830, loss = 0.53 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 01:28:36.132945: step 161840, loss = 0.51 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 01:28:48.313973: step 161850, loss = 0.43 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 01:29:00.379970: step 161860, loss = 0.46 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 01:29:12.564318: step 161870, loss = 0.42 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 01:29:24.700423: step 161880, loss = 0.44 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 01:29:36.868495: step 161890, loss = 0.45 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 01:29:49.030060: step 161900, loss = 0.46 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-23 01:30:03.228137: step 161910, loss = 0.49 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 01:30:15.360152: step 161920, loss = 0.42 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 01:30:27.465093: step 161930, loss = 0.48 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-23 01:30:39.589388: step 161940, loss = 0.53 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 01:30:51.737775: step 161950, loss = 0.47 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 01:31:03.881403: step 161960, loss = 0.54 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 01:31:16.023235: step 161970, loss = 0.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 01:31:28.175647: step 161980, loss = 0.55 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 01:31:40.317153: step 161990, loss = 0.44 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 01:31:52.429582: step 162000, loss = 0.44 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 01:32:06.973943: step 162010, loss = 0.50 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 01:32:19.093258: step 162020, loss = 0.56 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 01:32:31.209253: step 162030, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 01:32:43.215549: step 162040, loss = 0.50 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 01:32:55.333462: step 162050, loss = 0.44 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 01:33:07.402316: step 162060, loss = 0.49 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 01:33:19.577840: step 162070, loss = 0.43 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 01:33:31.737190: step 162080, loss = 0.45 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-23 01:33:43.924173: step 162090, loss = 0.45 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 01:33:56.054386: step 162100, loss = 0.52 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 01:34:10.208166: step 162110, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 01:34:22.292979: step 162120, loss = 0.60 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 01:34:34.461707: step 162130, loss = 0.45 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 01:34:46.621224: step 162140, loss = 0.59 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 01:34:58.746542: step 162150, loss = 0.48 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 01:35:10.879574: step 162160, loss = 0.48 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 01:35:23.041063: step 162170, loss = 0.58 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 01:35:35.164832: step 162180, loss = 0.51 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 01:35:47.321937: step 162190, loss = 0.51 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 01:35:59.444511: step 162200, loss = 0.47 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 01:36:13.874818: step 162210, loss = 0.56 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 01:36:26.036841: step 162220, loss = 0.46 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 01:36:38.209390: step 162230, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 01:36:50.403453: step 162240, loss = 0.48 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 01:37:02.532549: step 162250, loss = 0.42 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 01:37:14.660586: step 162260, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 01:37:26.824538: step 162270, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 01:37:38.973100: step 162280, loss = 0.42 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-23 01:37:50.990808: step 162290, loss = 0.47 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-23 01:38:03.043016: step 162300, loss = 0.47 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 01:38:17.519400: step 162310, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 01:38:29.664486: step 162320, loss = 0.53 (24.6 examples/sec; 1.217 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23 01:38:41.824910: step 162330, loss = 0.45 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 01:38:53.973978: step 162340, loss = 0.44 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 01:39:06.156817: step 162350, loss = 0.49 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 01:39:18.259635: step 162360, loss = 0.50 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 01:39:30.365050: step 162370, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 01:39:42.501830: step 162380, loss = 0.48 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 01:39:54.654592: step 162390, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 01:40:06.813446: step 162400, loss = 0.58 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 01:40:21.027181: step 162410, loss = 0.47 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 01:40:33.108335: step 162420, loss = 0.51 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 01:40:45.209679: step 162430, loss = 0.43 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 01:40:57.335797: step 162440, loss = 0.48 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 01:41:09.474221: step 162450, loss = 0.43 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 01:41:21.624364: step 162460, loss = 0.51 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 01:41:33.747760: step 162470, loss = 0.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 01:41:45.844805: step 162480, loss = 0.51 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-23 01:41:57.995912: step 162490, loss = 0.41 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 01:42:10.158866: step 162500, loss = 0.43 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-23 01:42:24.386243: step 162510, loss = 0.56 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 01:42:36.507481: step 162520, loss = 0.49 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 01:42:48.631523: step 162530, loss = 0.50 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 01:43:00.701939: step 162540, loss = 0.48 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-23 01:43:12.788600: step 162550, loss = 0.55 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 01:43:24.942625: step 162560, loss = 0.55 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 01:43:37.121578: step 162570, loss = 0.57 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-23 01:43:49.253105: step 162580, loss = 0.48 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-23 01:44:01.491403: step 162590, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 01:44:13.622291: step 162600, loss = 0.47 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 01:44:27.773091: step 162610, loss = 0.52 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 01:44:39.949780: step 162620, loss = 0.48 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 01:44:52.059572: step 162630, loss = 0.51 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 01:45:04.197881: step 162640, loss = 0.54 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 01:45:16.372300: step 162650, loss = 0.55 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 01:45:28.507864: step 162660, loss = 0.46 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 01:45:40.620810: step 162670, loss = 0.50 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 01:45:52.781152: step 162680, loss = 0.48 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-23 01:46:04.948056: step 162690, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 01:46:17.063686: step 162700, loss = 0.58 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 01:46:31.348629: step 162710, loss = 0.52 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 01:46:43.484417: step 162720, loss = 0.51 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 01:46:55.627443: step 162730, loss = 0.48 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 01:47:07.808216: step 162740, loss = 0.56 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 01:47:19.960378: step 162750, loss = 0.48 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 01:47:32.073943: step 162760, loss = 0.49 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 01:47:44.235162: step 162770, loss = 0.48 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 01:47:56.386022: step 162780, loss = 0.45 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 01:48:08.496319: step 162790, loss = 0.47 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-23 01:48:20.509200: step 162800, loss = 0.48 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 01:48:35.081891: step 162810, loss = 0.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 01:48:47.194790: step 162820, loss = 0.49 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 01:48:59.288356: step 162830, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 01:49:11.474294: step 162840, loss = 0.57 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 01:49:23.542472: step 162850, loss = 0.58 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 01:49:35.720138: step 162860, loss = 0.50 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 01:49:47.862138: step 162870, loss = 0.50 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 01:50:00.050151: step 162880, loss = 0.56 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 01:50:12.195080: step 162890, loss = 0.47 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 01:50:24.384687: step 162900, loss = 0.48 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 01:50:38.914779: step 162910, loss = 0.51 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 01:50:51.049396: step 162920, loss = 0.46 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 01:51:03.180772: step 162930, loss = 0.49 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 01:51:15.328717: step 162940, loss = 0.45 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 01:51:27.478229: step 162950, loss = 0.42 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 01:51:39.627492: step 162960, loss = 0.42 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 01:51:51.791580: step 162970, loss = 0.49 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 01:52:03.898209: step 162980, loss = 0.44 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 01:52:16.027897: step 162990, loss = 0.60 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 01:52:28.155258: step 163000, loss = 0.47 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 01:52:42.278547: step 163010, loss = 0.47 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 01:52:54.386614: step 163020, loss = 0.49 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 01:53:06.593696: step 163030, loss = 0.50 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 01:53:18.672078: step 163040, loss = 0.47 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-23 01:53:30.750779: step 163050, loss = 0.47 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 01:53:42.868437: step 163060, loss = 0.57 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 01:53:54.969788: step 163070, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 01:54:07.106588: step 163080, loss = 0.50 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 01:54:19.207944: step 163090, loss = 0.47 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 01:54:31.246619: step 163100, loss = 0.47 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 01:54:45.327561: step 163110, loss = 0.54 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 01:54:57.482667: step 163120, loss = 0.60 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 01:55:09.604952: step 163130, loss = 0.53 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 01:55:21.789070: step 163140, loss = 0.48 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 01:55:33.863451: step 163150, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 01:55:46.032580: step 163160, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 01:55:58.184446: step 163170, loss = 0.49 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 01:56:10.308050: step 163180, loss = 0.53 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 01:56:22.445594: step 163190, loss = 0.48 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 01:56:34.565184: step 163200, loss = 0.48 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 01:56:48.682377: step 163210, loss = 0.54 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 01:57:00.818487: step 163220, loss = 0.54 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 01:57:12.981415: step 163230, loss = 0.43 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 01:57:25.121254: step 163240, loss = 0.48 (24.9 examples/sec; 1.203 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23 01:57:37.303424: step 163250, loss = 0.51 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 01:57:49.441772: step 163260, loss = 0.48 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 01:58:01.552917: step 163270, loss = 0.53 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 01:58:13.682370: step 163280, loss = 0.54 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 01:58:25.793135: step 163290, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 01:58:37.793779: step 163300, loss = 0.43 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 01:58:52.352304: step 163310, loss = 0.46 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-23 01:59:04.474037: step 163320, loss = 0.47 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 01:59:16.641659: step 163330, loss = 0.53 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-23 01:59:28.820589: step 163340, loss = 0.52 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 01:59:40.913757: step 163350, loss = 0.45 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 01:59:53.047709: step 163360, loss = 0.36 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 02:00:05.164563: step 163370, loss = 0.39 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 02:00:17.300886: step 163380, loss = 0.52 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 02:00:29.419048: step 163390, loss = 0.51 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 02:00:41.523176: step 163400, loss = 0.50 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 02:00:56.038233: step 163410, loss = 0.47 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 02:01:08.169637: step 163420, loss = 0.52 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 02:01:20.282866: step 163430, loss = 0.51 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 02:01:32.367594: step 163440, loss = 0.46 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 02:01:44.541438: step 163450, loss = 0.44 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 02:01:56.643686: step 163460, loss = 0.52 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 02:02:08.848694: step 163470, loss = 0.47 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-23 02:02:20.965133: step 163480, loss = 0.47 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 02:02:33.133745: step 163490, loss = 0.43 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 02:02:45.310323: step 163500, loss = 0.58 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 02:02:59.908634: step 163510, loss = 0.49 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 02:03:12.056027: step 163520, loss = 0.53 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 02:03:24.205429: step 163530, loss = 0.55 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 02:03:36.343935: step 163540, loss = 0.47 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 02:03:48.342373: step 163550, loss = 0.58 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 02:04:00.424921: step 163560, loss = 0.45 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 02:04:12.626190: step 163570, loss = 0.48 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 02:04:24.802030: step 163580, loss = 0.44 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 02:04:36.909971: step 163590, loss = 0.54 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-23 02:04:48.956580: step 163600, loss = 0.48 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 02:05:03.064816: step 163610, loss = 0.55 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 02:05:15.177995: step 163620, loss = 0.57 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 02:05:27.316405: step 163630, loss = 0.42 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 02:05:39.473823: step 163640, loss = 0.54 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 02:05:51.618236: step 163650, loss = 0.44 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 02:06:03.761625: step 163660, loss = 0.51 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 02:06:15.882266: step 163670, loss = 0.49 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 02:06:28.055815: step 163680, loss = 0.43 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 02:06:40.184091: step 163690, loss = 0.48 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 02:06:52.334442: step 163700, loss = 0.44 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 02:07:06.546543: step 163710, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 02:07:18.718263: step 163720, loss = 0.49 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 02:07:30.863294: step 163730, loss = 0.47 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 02:07:42.997012: step 163740, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 02:07:55.132727: step 163750, loss = 0.48 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 02:08:07.252213: step 163760, loss = 0.52 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 02:08:19.421942: step 163770, loss = 0.46 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 02:08:31.585385: step 163780, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 02:08:43.769097: step 163790, loss = 0.45 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-23 02:08:55.824388: step 163800, loss = 0.48 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-23 02:09:10.376415: step 163810, loss = 0.47 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 02:09:22.499822: step 163820, loss = 0.46 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 02:09:34.663831: step 163830, loss = 0.57 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 02:09:46.701784: step 163840, loss = 0.52 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 02:09:58.787938: step 163850, loss = 0.54 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 02:10:10.914836: step 163860, loss = 0.48 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 02:10:23.075330: step 163870, loss = 0.45 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 02:10:35.135953: step 163880, loss = 0.54 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 02:10:47.308098: step 163890, loss = 0.51 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 02:10:59.461048: step 163900, loss = 0.49 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 02:11:13.677223: step 163910, loss = 0.56 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 02:11:25.730693: step 163920, loss = 0.54 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-23 02:11:37.750069: step 163930, loss = 0.50 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 02:11:49.744931: step 163940, loss = 0.55 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-23 02:12:01.705287: step 163950, loss = 0.42 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 02:12:13.722321: step 163960, loss = 0.57 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 02:12:25.789648: step 163970, loss = 0.49 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 02:12:37.861707: step 163980, loss = 0.53 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 02:12:49.948331: step 163990, loss = 0.41 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 02:13:02.022759: step 164000, loss = 0.47 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 02:13:16.020165: step 164010, loss = 0.45 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-23 02:13:28.118870: step 164020, loss = 0.55 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 02:13:40.209900: step 164030, loss = 0.46 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 02:13:52.307810: step 164040, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 02:14:04.366774: step 164050, loss = 0.54 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 02:14:16.383238: step 164060, loss = 0.47 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 02:14:28.467963: step 164070, loss = 0.48 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 02:14:40.570323: step 164080, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 02:14:52.640399: step 164090, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 02:15:04.727500: step 164100, loss = 0.45 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 02:15:18.709445: step 164110, loss = 0.48 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 02:15:30.805177: step 164120, loss = 0.53 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 02:15:42.854217: step 164130, loss = 0.46 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 02:15:54.960848: step 164140, loss = 0.50 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 02:16:07.028457: step 164150, loss = 0.42 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 02:16:19.183191: step 164160, loss = 0.49 (24.5 examples/sec; 1.227 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23 02:16:31.312554: step 164170, loss = 0.46 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 02:16:43.473984: step 164180, loss = 0.49 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 02:16:55.620920: step 164190, loss = 0.44 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 02:17:07.727898: step 164200, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 02:17:22.124755: step 164210, loss = 0.52 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 02:17:34.213736: step 164220, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 02:17:46.294473: step 164230, loss = 0.64 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 02:17:58.423058: step 164240, loss = 0.54 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 02:18:10.468487: step 164250, loss = 0.40 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 02:18:22.583052: step 164260, loss = 0.44 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 02:18:34.690729: step 164270, loss = 0.45 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 02:18:46.837562: step 164280, loss = 0.55 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 02:18:58.982881: step 164290, loss = 0.47 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 02:19:11.097035: step 164300, loss = 0.50 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-23 02:19:25.086027: step 164310, loss = 0.48 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 02:19:37.204514: step 164320, loss = 0.55 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 02:19:49.369053: step 164330, loss = 0.48 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 02:20:01.412256: step 164340, loss = 0.49 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 02:20:13.561951: step 164350, loss = 0.50 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-23 02:20:25.697930: step 164360, loss = 0.45 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 02:20:37.851457: step 164370, loss = 0.56 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 02:20:50.026693: step 164380, loss = 0.48 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 02:21:02.174097: step 164390, loss = 0.48 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 02:21:14.358494: step 164400, loss = 0.43 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 02:21:28.636256: step 164410, loss = 0.46 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 02:21:40.752260: step 164420, loss = 0.54 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 02:21:52.814808: step 164430, loss = 0.61 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 02:22:04.948407: step 164440, loss = 0.52 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 02:22:16.994248: step 164450, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 02:22:29.052444: step 164460, loss = 0.44 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 02:22:41.091924: step 164470, loss = 0.54 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 02:22:53.108934: step 164480, loss = 0.45 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 02:23:05.126757: step 164490, loss = 0.58 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 02:23:17.167190: step 164500, loss = 0.45 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-23 02:23:31.363412: step 164510, loss = 0.47 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-23 02:23:43.349749: step 164520, loss = 0.46 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 02:23:55.374762: step 164530, loss = 0.53 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 02:24:07.429555: step 164540, loss = 0.53 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 02:24:19.479785: step 164550, loss = 0.42 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 02:24:31.444744: step 164560, loss = 0.43 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-23 02:24:43.528356: step 164570, loss = 0.53 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 02:24:55.610959: step 164580, loss = 0.54 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 02:25:07.635128: step 164590, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 02:25:19.682819: step 164600, loss = 0.44 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-23 02:25:33.629096: step 164610, loss = 0.47 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 02:25:45.687075: step 164620, loss = 0.44 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 02:25:57.751493: step 164630, loss = 0.49 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 02:26:09.806715: step 164640, loss = 0.50 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 02:26:21.873590: step 164650, loss = 0.48 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 02:26:33.958823: step 164660, loss = 0.47 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 02:26:46.018101: step 164670, loss = 0.47 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 02:26:58.038595: step 164680, loss = 0.44 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 02:27:10.161285: step 164690, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 02:27:22.310537: step 164700, loss = 0.44 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-23 02:27:36.639984: step 164710, loss = 0.46 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 02:27:48.748469: step 164720, loss = 0.46 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-23 02:28:00.871486: step 164730, loss = 0.47 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 02:28:12.984292: step 164740, loss = 0.43 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 02:28:25.111977: step 164750, loss = 0.52 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 02:28:37.223316: step 164760, loss = 0.41 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 02:28:49.333246: step 164770, loss = 0.40 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 02:29:01.456878: step 164780, loss = 0.61 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 02:29:13.624577: step 164790, loss = 0.41 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 02:29:25.745024: step 164800, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 02:29:39.877937: step 164810, loss = 0.55 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-23 02:29:51.911722: step 164820, loss = 0.53 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 02:30:04.044986: step 164830, loss = 0.42 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-23 02:30:16.073193: step 164840, loss = 0.47 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 02:30:28.270788: step 164850, loss = 0.48 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 02:30:40.416834: step 164860, loss = 0.56 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 02:30:52.521636: step 164870, loss = 0.49 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 02:31:04.674352: step 164880, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 02:31:16.827253: step 164890, loss = 0.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 02:31:28.967632: step 164900, loss = 0.43 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 02:31:43.037086: step 164910, loss = 0.44 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 02:31:55.136442: step 164920, loss = 0.43 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 02:32:07.268760: step 164930, loss = 0.46 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-23 02:32:19.414923: step 164940, loss = 0.47 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 02:32:31.551873: step 164950, loss = 0.45 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 02:32:43.713430: step 164960, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 02:32:55.823298: step 164970, loss = 0.49 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 02:33:07.925682: step 164980, loss = 0.53 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 02:33:20.096990: step 164990, loss = 0.42 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 02:33:32.253352: step 165000, loss = 0.52 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 02:33:50.288366: step 165010, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 02:34:02.350482: step 165020, loss = 0.46 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 02:34:14.411524: step 165030, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 02:34:26.752282: step 165040, loss = 0.46 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-23 02:34:38.758680: step 165050, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 02:34:50.739753: step 165060, loss = 0.46 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 02:35:02.711316: step 165070, loss = 0.63 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 02:35:14.694181: step 165080, loss = 0.53 (25.1 examples/sec; 1.196 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23 02:35:26.808644: step 165090, loss = 0.52 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 02:35:38.920576: step 165100, loss = 0.43 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 02:35:53.449930: step 165110, loss = 0.44 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 02:36:05.485562: step 165120, loss = 0.45 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 02:36:17.521736: step 165130, loss = 0.40 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 02:36:29.587588: step 165140, loss = 0.51 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 02:36:41.726108: step 165150, loss = 0.41 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 02:36:53.825726: step 165160, loss = 0.57 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 02:37:05.871016: step 165170, loss = 0.44 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 02:37:17.874690: step 165180, loss = 0.49 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 02:37:29.871937: step 165190, loss = 0.47 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 02:37:41.880527: step 165200, loss = 0.49 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 02:37:56.166690: step 165210, loss = 0.41 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 02:38:08.208855: step 165220, loss = 0.48 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-23 02:38:20.189108: step 165230, loss = 0.51 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 02:38:32.248918: step 165240, loss = 0.44 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 02:38:44.331687: step 165250, loss = 0.49 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 02:38:56.332127: step 165260, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 02:39:08.322633: step 165270, loss = 0.43 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-23 02:39:20.336158: step 165280, loss = 0.50 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-23 02:39:32.347716: step 165290, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 02:39:44.367439: step 165300, loss = 0.42 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-23 02:39:58.364067: step 165310, loss = 0.55 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-23 02:40:10.389339: step 165320, loss = 0.53 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 02:40:22.404315: step 165330, loss = 0.57 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 02:40:34.490059: step 165340, loss = 0.46 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 02:40:46.497276: step 165350, loss = 0.56 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 02:40:58.501447: step 165360, loss = 0.46 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-23 02:41:10.522956: step 165370, loss = 0.48 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 02:41:22.611351: step 165380, loss = 0.49 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 02:41:34.647183: step 165390, loss = 0.50 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 02:41:46.818395: step 165400, loss = 0.49 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 02:42:01.361854: step 165410, loss = 0.44 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 02:42:13.486578: step 165420, loss = 0.40 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 02:42:25.634345: step 165430, loss = 0.43 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 02:42:37.780305: step 165440, loss = 0.48 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 02:42:49.951686: step 165450, loss = 0.52 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 02:43:02.114461: step 165460, loss = 0.51 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 02:43:14.283129: step 165470, loss = 0.50 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-23 02:43:26.459486: step 165480, loss = 0.47 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-23 02:43:38.559377: step 165490, loss = 0.41 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 02:43:50.733917: step 165500, loss = 0.45 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 02:44:04.880483: step 165510, loss = 0.45 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 02:44:17.050556: step 165520, loss = 0.50 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 02:44:29.177097: step 165530, loss = 0.42 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 02:44:41.325103: step 165540, loss = 0.50 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 02:44:53.484960: step 165550, loss = 0.45 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 02:45:05.535699: step 165560, loss = 0.43 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-23 02:45:17.515064: step 165570, loss = 0.44 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 02:45:29.543095: step 165580, loss = 0.40 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 02:45:41.719118: step 165590, loss = 0.53 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 02:45:53.855532: step 165600, loss = 0.46 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 02:46:08.402736: step 165610, loss = 0.51 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 02:46:20.546275: step 165620, loss = 0.50 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 02:46:32.680384: step 165630, loss = 0.58 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 02:46:44.799709: step 165640, loss = 0.50 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 02:46:56.948539: step 165650, loss = 0.47 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 02:47:09.106158: step 165660, loss = 0.51 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 02:47:21.262866: step 165670, loss = 0.48 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 02:47:33.391901: step 165680, loss = 0.44 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 02:47:45.545125: step 165690, loss = 0.47 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 02:47:57.751857: step 165700, loss = 0.42 (24.3 examples/sec; 1.237 sec/batch)\n",
      "2019-05-23 02:48:12.076671: step 165710, loss = 0.56 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 02:48:24.217408: step 165720, loss = 0.55 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 02:48:36.296540: step 165730, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 02:48:48.448873: step 165740, loss = 0.58 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 02:49:00.592231: step 165750, loss = 0.46 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 02:49:12.754174: step 165760, loss = 0.45 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 02:49:24.892546: step 165770, loss = 0.42 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 02:49:37.049720: step 165780, loss = 0.56 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 02:49:49.240025: step 165790, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 02:50:01.362663: step 165800, loss = 0.43 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 02:50:15.918410: step 165810, loss = 0.51 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 02:50:27.942518: step 165820, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 02:50:40.040159: step 165830, loss = 0.54 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 02:50:52.122998: step 165840, loss = 0.43 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 02:51:04.268098: step 165850, loss = 0.39 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 02:51:16.428241: step 165860, loss = 0.50 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 02:51:28.522297: step 165870, loss = 0.47 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 02:51:40.667639: step 165880, loss = 0.54 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 02:51:52.816828: step 165890, loss = 0.54 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 02:52:04.950165: step 165900, loss = 0.52 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 02:52:19.119486: step 165910, loss = 0.45 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 02:52:31.298359: step 165920, loss = 0.49 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 02:52:43.427273: step 165930, loss = 0.48 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 02:52:55.586119: step 165940, loss = 0.45 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 02:53:07.660126: step 165950, loss = 0.39 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 02:53:19.818186: step 165960, loss = 0.50 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 02:53:31.868409: step 165970, loss = 0.53 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 02:53:43.901625: step 165980, loss = 0.41 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 02:53:56.038379: step 165990, loss = 0.46 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 02:54:08.095045: step 166000, loss = 0.46 (25.0 examples/sec; 1.198 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23 02:54:22.217906: step 166010, loss = 0.51 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 02:54:34.278035: step 166020, loss = 0.53 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 02:54:46.391552: step 166030, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 02:54:58.474556: step 166040, loss = 0.55 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 02:55:10.631002: step 166050, loss = 0.42 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 02:55:22.734679: step 166060, loss = 0.55 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 02:55:34.756956: step 166070, loss = 0.56 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 02:55:46.819396: step 166080, loss = 0.50 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 02:55:58.915863: step 166090, loss = 0.65 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 02:56:11.057832: step 166100, loss = 0.50 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 02:56:25.156892: step 166110, loss = 0.45 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 02:56:37.298059: step 166120, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 02:56:49.436528: step 166130, loss = 0.47 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 02:57:01.597047: step 166140, loss = 0.37 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 02:57:13.750880: step 166150, loss = 0.41 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 02:57:25.907283: step 166160, loss = 0.55 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 02:57:38.063999: step 166170, loss = 0.55 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 02:57:50.183173: step 166180, loss = 0.52 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 02:58:02.330108: step 166190, loss = 0.46 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 02:58:14.457909: step 166200, loss = 0.45 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 02:58:28.796892: step 166210, loss = 0.46 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 02:58:40.905891: step 166220, loss = 0.46 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 02:58:53.042347: step 166230, loss = 0.48 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 02:59:05.162113: step 166240, loss = 0.45 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 02:59:17.297872: step 166250, loss = 0.44 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 02:59:29.402338: step 166260, loss = 0.57 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 02:59:41.521593: step 166270, loss = 0.50 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 02:59:53.656223: step 166280, loss = 0.53 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 03:00:05.815432: step 166290, loss = 0.42 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 03:00:17.969354: step 166300, loss = 0.54 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 03:00:32.107987: step 166310, loss = 0.56 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 03:00:44.127790: step 166320, loss = 0.42 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 03:00:56.214605: step 166330, loss = 0.43 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 03:01:08.295312: step 166340, loss = 0.50 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 03:01:20.431353: step 166350, loss = 0.49 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 03:01:32.582761: step 166360, loss = 0.54 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 03:01:44.746998: step 166370, loss = 0.57 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 03:01:56.870925: step 166380, loss = 0.38 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 03:02:08.985440: step 166390, loss = 0.56 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 03:02:21.097332: step 166400, loss = 0.42 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 03:02:35.254826: step 166410, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 03:02:47.402258: step 166420, loss = 0.56 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 03:02:59.578394: step 166430, loss = 0.43 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 03:03:11.738997: step 166440, loss = 0.46 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 03:03:23.866898: step 166450, loss = 0.49 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 03:03:35.985267: step 166460, loss = 0.46 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 03:03:48.152889: step 166470, loss = 0.49 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 03:04:00.330117: step 166480, loss = 0.44 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 03:04:12.492486: step 166490, loss = 0.48 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 03:04:24.618810: step 166500, loss = 0.43 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 03:04:38.884496: step 166510, loss = 0.52 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 03:04:51.012147: step 166520, loss = 0.47 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 03:05:03.180171: step 166530, loss = 0.53 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 03:05:15.360861: step 166540, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 03:05:27.529841: step 166550, loss = 0.41 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 03:05:39.695385: step 166560, loss = 0.46 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 03:05:51.705080: step 166570, loss = 0.50 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 03:06:03.768325: step 166580, loss = 0.49 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 03:06:15.931586: step 166590, loss = 0.51 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 03:06:28.080080: step 166600, loss = 0.43 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 03:06:42.324343: step 166610, loss = 0.42 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 03:06:54.446603: step 166620, loss = 0.53 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 03:07:06.617548: step 166630, loss = 0.45 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 03:07:18.793754: step 166640, loss = 0.48 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 03:07:30.960191: step 166650, loss = 0.54 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 03:07:43.109213: step 166660, loss = 0.46 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 03:07:55.312607: step 166670, loss = 0.49 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 03:08:07.445872: step 166680, loss = 0.49 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 03:08:19.628632: step 166690, loss = 0.42 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 03:08:31.780814: step 166700, loss = 0.46 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 03:08:46.140708: step 166710, loss = 0.44 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 03:08:58.301812: step 166720, loss = 0.46 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 03:09:10.496642: step 166730, loss = 0.53 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 03:09:22.651896: step 166740, loss = 0.47 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 03:09:34.763147: step 166750, loss = 0.49 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 03:09:46.945587: step 166760, loss = 0.57 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 03:09:59.140585: step 166770, loss = 0.46 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 03:10:11.305336: step 166780, loss = 0.52 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 03:10:23.442276: step 166790, loss = 0.45 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 03:10:35.608405: step 166800, loss = 0.46 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 03:10:50.186065: step 166810, loss = 0.60 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-23 03:11:02.148649: step 166820, loss = 0.52 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-23 03:11:14.253181: step 166830, loss = 0.49 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 03:11:26.381134: step 166840, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 03:11:38.507218: step 166850, loss = 0.53 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 03:11:50.644007: step 166860, loss = 0.47 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 03:12:02.818432: step 166870, loss = 0.46 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 03:12:15.011081: step 166880, loss = 0.49 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 03:12:27.162889: step 166890, loss = 0.49 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 03:12:39.319765: step 166900, loss = 0.49 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 03:12:53.494019: step 166910, loss = 0.54 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 03:13:05.663463: step 166920, loss = 0.43 (24.6 examples/sec; 1.220 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23 03:13:17.820581: step 166930, loss = 0.45 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 03:13:29.952535: step 166940, loss = 0.42 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 03:13:42.068353: step 166950, loss = 0.60 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 03:13:54.195203: step 166960, loss = 0.48 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 03:14:06.310136: step 166970, loss = 0.44 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 03:14:18.470853: step 166980, loss = 0.45 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 03:14:30.560981: step 166990, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 03:14:42.677019: step 167000, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 03:14:56.822924: step 167010, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 03:15:08.948928: step 167020, loss = 0.43 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 03:15:21.037445: step 167030, loss = 0.54 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 03:15:33.172362: step 167040, loss = 0.45 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 03:15:45.326213: step 167050, loss = 0.48 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-23 03:15:57.467523: step 167060, loss = 0.46 (25.4 examples/sec; 1.182 sec/batch)\n",
      "2019-05-23 03:16:09.552188: step 167070, loss = 0.47 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-23 03:16:21.611198: step 167080, loss = 0.52 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 03:16:33.716380: step 167090, loss = 0.50 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 03:16:45.796350: step 167100, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 03:16:59.875444: step 167110, loss = 0.48 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 03:17:11.986014: step 167120, loss = 0.49 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 03:17:24.135923: step 167130, loss = 0.48 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 03:17:36.252198: step 167140, loss = 0.52 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 03:17:48.375951: step 167150, loss = 0.46 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 03:18:00.498154: step 167160, loss = 0.51 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 03:18:12.648210: step 167170, loss = 0.44 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-23 03:18:24.859185: step 167180, loss = 0.45 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 03:18:37.024268: step 167190, loss = 0.56 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 03:18:49.257836: step 167200, loss = 0.45 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 03:19:03.670461: step 167210, loss = 0.44 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 03:19:15.852886: step 167220, loss = 0.51 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 03:19:28.035846: step 167230, loss = 0.50 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 03:19:40.167918: step 167240, loss = 0.41 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 03:19:52.389005: step 167250, loss = 0.54 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 03:20:04.599489: step 167260, loss = 0.52 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 03:20:16.822401: step 167270, loss = 0.40 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 03:20:29.046390: step 167280, loss = 0.45 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 03:20:41.269570: step 167290, loss = 0.46 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 03:20:53.478145: step 167300, loss = 0.54 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 03:21:07.803313: step 167310, loss = 0.46 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 03:21:19.996235: step 167320, loss = 0.47 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 03:21:32.059414: step 167330, loss = 0.45 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 03:21:44.208899: step 167340, loss = 0.47 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 03:21:56.444631: step 167350, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 03:22:08.622185: step 167360, loss = 0.57 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 03:22:20.812822: step 167370, loss = 0.48 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 03:22:32.959183: step 167380, loss = 0.50 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 03:22:45.130258: step 167390, loss = 0.47 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 03:22:57.350018: step 167400, loss = 0.51 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 03:23:11.558387: step 167410, loss = 0.49 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 03:23:23.802628: step 167420, loss = 0.48 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 03:23:35.972298: step 167430, loss = 0.49 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 03:23:48.169331: step 167440, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 03:24:00.383624: step 167450, loss = 0.46 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 03:24:12.562987: step 167460, loss = 0.56 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 03:24:24.782407: step 167470, loss = 0.55 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-23 03:24:36.983016: step 167480, loss = 0.43 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 03:24:49.175341: step 167490, loss = 0.52 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 03:25:01.339920: step 167500, loss = 0.45 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-23 03:25:15.528899: step 167510, loss = 0.55 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 03:25:27.772587: step 167520, loss = 0.46 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 03:25:39.983189: step 167530, loss = 0.54 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-23 03:25:52.129214: step 167540, loss = 0.43 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 03:26:04.345711: step 167550, loss = 0.53 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-23 03:26:16.496010: step 167560, loss = 0.52 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 03:26:28.736125: step 167570, loss = 0.47 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 03:26:40.798747: step 167580, loss = 0.50 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 03:26:52.933954: step 167590, loss = 0.41 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 03:27:05.142808: step 167600, loss = 0.54 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-23 03:27:19.686455: step 167610, loss = 0.48 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 03:27:31.858157: step 167620, loss = 0.52 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 03:27:44.064082: step 167630, loss = 0.49 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 03:27:56.231408: step 167640, loss = 0.46 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-23 03:28:08.416568: step 167650, loss = 0.51 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 03:28:20.636698: step 167660, loss = 0.48 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 03:28:32.808629: step 167670, loss = 0.43 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 03:28:45.096237: step 167680, loss = 0.43 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-23 03:28:57.269112: step 167690, loss = 0.43 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 03:29:09.480906: step 167700, loss = 0.45 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 03:29:23.905220: step 167710, loss = 0.47 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 03:29:36.100025: step 167720, loss = 0.43 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 03:29:48.226374: step 167730, loss = 0.53 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 03:30:00.395697: step 167740, loss = 0.48 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 03:30:12.629507: step 167750, loss = 0.46 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-23 03:30:24.812568: step 167760, loss = 0.49 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 03:30:36.931308: step 167770, loss = 0.47 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 03:30:49.122242: step 167780, loss = 0.48 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 03:31:01.300973: step 167790, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 03:31:13.469177: step 167800, loss = 0.56 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 03:31:27.598113: step 167810, loss = 0.46 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 03:31:39.810930: step 167820, loss = 0.51 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 03:31:51.895332: step 167830, loss = 0.47 (24.1 examples/sec; 1.243 sec/batch)\n",
      "2019-05-23 03:32:04.038480: step 167840, loss = 0.49 (24.5 examples/sec; 1.225 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23 03:32:16.213580: step 167850, loss = 0.48 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 03:32:28.437888: step 167860, loss = 0.47 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 03:32:40.609408: step 167870, loss = 0.55 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 03:32:52.820985: step 167880, loss = 0.52 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-23 03:33:05.060436: step 167890, loss = 0.57 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 03:33:17.252185: step 167900, loss = 0.49 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 03:33:31.388782: step 167910, loss = 0.42 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 03:33:43.593308: step 167920, loss = 0.42 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 03:33:55.846145: step 167930, loss = 0.52 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-23 03:34:08.035338: step 167940, loss = 0.45 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 03:34:20.259580: step 167950, loss = 0.67 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 03:34:32.392731: step 167960, loss = 0.52 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 03:34:44.584184: step 167970, loss = 0.60 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-23 03:34:56.750152: step 167980, loss = 0.49 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 03:35:08.931060: step 167990, loss = 0.48 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-23 03:35:21.122888: step 168000, loss = 0.44 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 03:35:35.308100: step 168010, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 03:35:47.511927: step 168020, loss = 0.47 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-23 03:35:59.706664: step 168030, loss = 0.49 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-23 03:36:11.877167: step 168040, loss = 0.47 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 03:36:23.996093: step 168050, loss = 0.59 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 03:36:36.215191: step 168060, loss = 0.48 (24.2 examples/sec; 1.237 sec/batch)\n",
      "2019-05-23 03:36:48.379963: step 168070, loss = 0.52 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 03:37:00.451828: step 168080, loss = 0.49 (25.3 examples/sec; 1.185 sec/batch)\n",
      "2019-05-23 03:37:12.630933: step 168090, loss = 0.47 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 03:37:24.807462: step 168100, loss = 0.57 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 03:37:39.245524: step 168110, loss = 0.46 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 03:37:51.507353: step 168120, loss = 0.48 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-23 03:38:03.726614: step 168130, loss = 0.54 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 03:38:15.901540: step 168140, loss = 0.46 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 03:38:28.077611: step 168150, loss = 0.44 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 03:38:40.223890: step 168160, loss = 0.56 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 03:38:52.440460: step 168170, loss = 0.45 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 03:39:04.594931: step 168180, loss = 0.58 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 03:39:16.824077: step 168190, loss = 0.47 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 03:39:29.034630: step 168200, loss = 0.46 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 03:39:43.241904: step 168210, loss = 0.51 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 03:39:55.422218: step 168220, loss = 0.43 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 03:40:07.610832: step 168230, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 03:40:19.780239: step 168240, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 03:40:31.970733: step 168250, loss = 0.46 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 03:40:44.183298: step 168260, loss = 0.55 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 03:40:56.385775: step 168270, loss = 0.55 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 03:41:08.604679: step 168280, loss = 0.60 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 03:41:20.843725: step 168290, loss = 0.58 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-23 03:41:32.980610: step 168300, loss = 0.51 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 03:41:47.522075: step 168310, loss = 0.48 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 03:41:59.730028: step 168320, loss = 0.52 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 03:42:11.814974: step 168330, loss = 0.54 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 03:42:24.008658: step 168340, loss = 0.54 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 03:42:36.205065: step 168350, loss = 0.56 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 03:42:48.415617: step 168360, loss = 0.38 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 03:43:00.630104: step 168370, loss = 0.50 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 03:43:12.834382: step 168380, loss = 0.40 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 03:43:25.026162: step 168390, loss = 0.45 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-23 03:43:37.168795: step 168400, loss = 0.62 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 03:43:51.665376: step 168410, loss = 0.57 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 03:44:03.808133: step 168420, loss = 0.42 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 03:44:15.986640: step 168430, loss = 0.47 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 03:44:28.162623: step 168440, loss = 0.53 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 03:44:40.257676: step 168450, loss = 0.57 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 03:44:52.340359: step 168460, loss = 0.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 03:45:04.512799: step 168470, loss = 0.42 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 03:45:16.692229: step 168480, loss = 0.49 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-23 03:45:28.778523: step 168490, loss = 0.42 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 03:45:40.953326: step 168500, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 03:45:55.074158: step 168510, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 03:46:07.169622: step 168520, loss = 0.44 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 03:46:19.334302: step 168530, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 03:46:31.441144: step 168540, loss = 0.50 (25.4 examples/sec; 1.180 sec/batch)\n",
      "2019-05-23 03:46:43.492691: step 168550, loss = 0.56 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 03:46:55.647976: step 168560, loss = 0.42 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 03:47:07.753691: step 168570, loss = 0.49 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 03:47:19.806383: step 168580, loss = 0.53 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 03:47:31.943155: step 168590, loss = 0.45 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 03:47:44.084819: step 168600, loss = 0.48 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 03:47:58.410710: step 168610, loss = 0.54 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 03:48:10.576199: step 168620, loss = 0.53 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 03:48:22.716417: step 168630, loss = 0.49 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 03:48:34.867918: step 168640, loss = 0.46 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 03:48:47.025508: step 168650, loss = 0.47 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 03:48:59.162871: step 168660, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 03:49:11.320926: step 168670, loss = 0.46 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 03:49:23.420597: step 168680, loss = 0.56 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 03:49:35.521707: step 168690, loss = 0.40 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 03:49:47.648162: step 168700, loss = 0.45 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 03:50:01.701087: step 168710, loss = 0.57 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 03:50:13.890741: step 168720, loss = 0.45 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 03:50:25.972167: step 168730, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 03:50:38.082017: step 168740, loss = 0.43 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 03:50:50.207890: step 168750, loss = 0.46 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 03:51:02.330765: step 168760, loss = 0.55 (25.0 examples/sec; 1.200 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23 03:51:14.494546: step 168770, loss = 0.49 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-23 03:51:26.660292: step 168780, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 03:51:38.724002: step 168790, loss = 0.43 (25.3 examples/sec; 1.188 sec/batch)\n",
      "2019-05-23 03:51:50.844340: step 168800, loss = 0.43 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 03:52:05.127967: step 168810, loss = 0.46 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 03:52:17.406121: step 168820, loss = 0.44 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 03:52:29.530482: step 168830, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 03:52:41.680511: step 168840, loss = 0.43 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 03:52:53.843750: step 168850, loss = 0.46 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 03:53:06.026011: step 168860, loss = 0.48 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 03:53:18.207246: step 168870, loss = 0.49 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 03:53:30.392536: step 168880, loss = 0.49 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 03:53:42.586683: step 168890, loss = 0.41 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 03:53:54.798470: step 168900, loss = 0.44 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-23 03:54:09.192012: step 168910, loss = 0.50 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 03:54:21.397783: step 168920, loss = 0.47 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 03:54:33.576200: step 168930, loss = 0.60 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 03:54:45.815547: step 168940, loss = 0.53 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 03:54:58.004043: step 168950, loss = 0.40 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 03:55:10.265579: step 168960, loss = 0.54 (24.2 examples/sec; 1.241 sec/batch)\n",
      "2019-05-23 03:55:22.497533: step 168970, loss = 0.51 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 03:55:34.699572: step 168980, loss = 0.45 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 03:55:46.835089: step 168990, loss = 0.49 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 03:55:59.019073: step 169000, loss = 0.45 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 03:56:13.238464: step 169010, loss = 0.45 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 03:56:25.428274: step 169020, loss = 0.42 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 03:56:37.594442: step 169030, loss = 0.51 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 03:56:49.696259: step 169040, loss = 0.43 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 03:57:01.879609: step 169050, loss = 0.62 (24.3 examples/sec; 1.234 sec/batch)\n",
      "2019-05-23 03:57:14.060085: step 169060, loss = 0.55 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 03:57:26.243626: step 169070, loss = 0.51 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 03:57:38.418435: step 169080, loss = 0.51 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 03:57:50.548396: step 169090, loss = 0.48 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-23 03:58:02.739928: step 169100, loss = 0.44 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 03:58:16.897284: step 169110, loss = 0.52 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 03:58:29.026233: step 169120, loss = 0.47 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 03:58:41.190328: step 169130, loss = 0.45 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 03:58:53.397599: step 169140, loss = 0.50 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 03:59:05.631174: step 169150, loss = 0.49 (24.2 examples/sec; 1.240 sec/batch)\n",
      "2019-05-23 03:59:17.857144: step 169160, loss = 0.48 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 03:59:29.989386: step 169170, loss = 0.44 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 03:59:42.133442: step 169180, loss = 0.58 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 03:59:54.244507: step 169190, loss = 0.44 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 04:00:06.398351: step 169200, loss = 0.44 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 04:00:20.497002: step 169210, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 04:00:32.640581: step 169220, loss = 0.42 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 04:00:44.759121: step 169230, loss = 0.52 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 04:00:56.853309: step 169240, loss = 0.60 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 04:01:09.053009: step 169250, loss = 0.43 (24.3 examples/sec; 1.233 sec/batch)\n",
      "2019-05-23 04:01:21.170424: step 169260, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 04:01:33.314623: step 169270, loss = 0.51 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 04:01:45.492591: step 169280, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 04:01:57.558734: step 169290, loss = 0.45 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 04:02:09.702497: step 169300, loss = 0.47 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 04:02:23.703716: step 169310, loss = 0.48 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 04:02:35.814893: step 169320, loss = 0.53 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 04:02:47.885662: step 169330, loss = 0.54 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-23 04:02:59.915942: step 169340, loss = 0.41 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 04:03:12.074607: step 169350, loss = 0.48 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 04:03:24.251569: step 169360, loss = 0.52 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 04:03:36.455800: step 169370, loss = 0.52 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 04:03:48.631958: step 169380, loss = 0.44 (24.3 examples/sec; 1.236 sec/batch)\n",
      "2019-05-23 04:04:00.804596: step 169390, loss = 0.48 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 04:04:13.048560: step 169400, loss = 0.51 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 04:04:27.546685: step 169410, loss = 0.51 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 04:04:39.795893: step 169420, loss = 0.50 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 04:04:51.965803: step 169430, loss = 0.49 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 04:05:04.086288: step 169440, loss = 0.53 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 04:05:16.273899: step 169450, loss = 0.51 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-23 04:05:28.427210: step 169460, loss = 0.42 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 04:05:40.562360: step 169470, loss = 0.34 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 04:05:52.757403: step 169480, loss = 0.56 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 04:06:04.906069: step 169490, loss = 0.49 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 04:06:17.032158: step 169500, loss = 0.54 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 04:06:31.437978: step 169510, loss = 0.42 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 04:06:43.555414: step 169520, loss = 0.50 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 04:06:55.677192: step 169530, loss = 0.53 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 04:07:07.922524: step 169540, loss = 0.48 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-23 04:07:20.112671: step 169550, loss = 0.51 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 04:07:32.378946: step 169560, loss = 0.45 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 04:07:44.578579: step 169570, loss = 0.42 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 04:07:56.826816: step 169580, loss = 0.45 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 04:08:08.930380: step 169590, loss = 0.52 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 04:08:21.118147: step 169600, loss = 0.40 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 04:08:35.382252: step 169610, loss = 0.55 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 04:08:47.580871: step 169620, loss = 0.41 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 04:08:59.798019: step 169630, loss = 0.50 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-23 04:09:12.006732: step 169640, loss = 0.55 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 04:09:24.213199: step 169650, loss = 0.46 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 04:09:36.383348: step 169660, loss = 0.48 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 04:09:48.592366: step 169670, loss = 0.48 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 04:10:00.798408: step 169680, loss = 0.43 (24.5 examples/sec; 1.227 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23 04:10:13.006689: step 169690, loss = 0.46 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 04:10:25.214220: step 169700, loss = 0.52 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-23 04:10:39.367130: step 169710, loss = 0.55 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 04:10:51.587643: step 169720, loss = 0.45 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 04:11:03.743698: step 169730, loss = 0.49 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 04:11:15.955158: step 169740, loss = 0.51 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 04:11:28.135466: step 169750, loss = 0.44 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 04:11:40.372602: step 169760, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 04:11:52.563235: step 169770, loss = 0.44 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 04:12:04.694673: step 169780, loss = 0.51 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 04:12:16.896348: step 169790, loss = 0.49 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 04:12:29.087310: step 169800, loss = 0.46 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 04:12:43.207821: step 169810, loss = 0.50 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 04:12:55.438745: step 169820, loss = 0.51 (24.3 examples/sec; 1.232 sec/batch)\n",
      "2019-05-23 04:13:07.571254: step 169830, loss = 0.57 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-23 04:13:19.606263: step 169840, loss = 0.42 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-23 04:13:31.830125: step 169850, loss = 0.48 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-23 04:13:44.034385: step 169860, loss = 0.52 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-23 04:13:56.254888: step 169870, loss = 0.47 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 04:14:08.433214: step 169880, loss = 0.41 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 04:14:20.623339: step 169890, loss = 0.49 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-23 04:14:32.873597: step 169900, loss = 0.52 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 04:14:47.129490: step 169910, loss = 0.56 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-23 04:14:59.305085: step 169920, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 04:15:11.472692: step 169930, loss = 0.55 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 04:15:23.618081: step 169940, loss = 0.54 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 04:15:35.865303: step 169950, loss = 0.46 (24.1 examples/sec; 1.244 sec/batch)\n",
      "2019-05-23 04:15:48.069817: step 169960, loss = 0.42 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 04:16:00.280102: step 169970, loss = 0.45 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 04:16:12.503314: step 169980, loss = 0.46 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 04:16:24.775662: step 169990, loss = 0.56 (24.2 examples/sec; 1.239 sec/batch)\n",
      "2019-05-23 04:16:36.963006: step 170000, loss = 0.52 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 04:16:54.890708: step 170010, loss = 0.49 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-23 04:17:07.052589: step 170020, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 04:17:19.168016: step 170030, loss = 0.45 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 04:17:31.386787: step 170040, loss = 0.47 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 04:17:43.607995: step 170050, loss = 0.43 (24.2 examples/sec; 1.238 sec/batch)\n",
      "2019-05-23 04:17:55.814614: step 170060, loss = 0.46 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 04:18:07.964747: step 170070, loss = 0.55 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 04:18:20.029438: step 170080, loss = 0.44 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 04:18:32.021175: step 170090, loss = 0.52 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 04:18:44.186471: step 170100, loss = 0.48 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 04:18:58.827023: step 170110, loss = 0.58 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 04:19:11.008272: step 170120, loss = 0.46 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 04:19:23.154531: step 170130, loss = 0.50 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 04:19:35.295210: step 170140, loss = 0.44 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 04:19:47.460646: step 170150, loss = 0.43 (24.4 examples/sec; 1.230 sec/batch)\n",
      "2019-05-23 04:19:59.590278: step 170160, loss = 0.50 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 04:20:11.744509: step 170170, loss = 0.43 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 04:20:23.896083: step 170180, loss = 0.45 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 04:20:36.020763: step 170190, loss = 0.50 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 04:20:48.168316: step 170200, loss = 0.41 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 04:21:02.374406: step 170210, loss = 0.57 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 04:21:14.516388: step 170220, loss = 0.46 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 04:21:26.635383: step 170230, loss = 0.51 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 04:21:38.775670: step 170240, loss = 0.46 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 04:21:50.901676: step 170250, loss = 0.47 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-23 04:22:03.051985: step 170260, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 04:22:15.115457: step 170270, loss = 0.51 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 04:22:27.231231: step 170280, loss = 0.48 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 04:22:39.330549: step 170290, loss = 0.51 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 04:22:51.560553: step 170300, loss = 0.53 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 04:23:05.621013: step 170310, loss = 0.45 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 04:23:17.728859: step 170320, loss = 0.53 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 04:23:29.816181: step 170330, loss = 0.44 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 04:23:41.902999: step 170340, loss = 0.43 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 04:23:53.884309: step 170350, loss = 0.46 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-23 04:24:05.964101: step 170360, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 04:24:18.037375: step 170370, loss = 0.51 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 04:24:30.135401: step 170380, loss = 0.49 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 04:24:42.208481: step 170390, loss = 0.40 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 04:24:54.286255: step 170400, loss = 0.49 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 04:25:08.300778: step 170410, loss = 0.46 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 04:25:20.364247: step 170420, loss = 0.57 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 04:25:32.427234: step 170430, loss = 0.54 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 04:25:44.489988: step 170440, loss = 0.52 (25.1 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 04:25:56.541866: step 170450, loss = 0.45 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 04:26:08.638461: step 170460, loss = 0.42 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 04:26:20.707352: step 170470, loss = 0.46 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 04:26:32.762306: step 170480, loss = 0.40 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 04:26:44.813165: step 170490, loss = 0.42 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 04:26:56.895375: step 170500, loss = 0.51 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 04:27:10.797206: step 170510, loss = 0.53 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-23 04:27:22.831257: step 170520, loss = 0.47 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 04:27:34.893820: step 170530, loss = 0.42 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 04:27:46.998239: step 170540, loss = 0.47 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 04:27:59.085277: step 170550, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 04:28:11.136548: step 170560, loss = 0.52 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 04:28:23.203248: step 170570, loss = 0.50 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 04:28:35.313196: step 170580, loss = 0.44 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-23 04:28:47.286741: step 170590, loss = 0.52 (25.3 examples/sec; 1.186 sec/batch)\n",
      "2019-05-23 04:28:59.324791: step 170600, loss = 0.41 (24.9 examples/sec; 1.204 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23 04:29:13.761968: step 170610, loss = 0.56 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 04:29:25.836528: step 170620, loss = 0.44 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 04:29:37.853459: step 170630, loss = 0.49 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-23 04:29:49.929425: step 170640, loss = 0.45 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 04:30:02.036488: step 170650, loss = 0.50 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 04:30:14.106990: step 170660, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 04:30:26.171942: step 170670, loss = 0.47 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 04:30:38.270864: step 170680, loss = 0.43 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 04:30:50.361063: step 170690, loss = 0.54 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 04:31:02.424728: step 170700, loss = 0.51 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 04:31:16.481862: step 170710, loss = 0.51 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 04:31:28.523652: step 170720, loss = 0.49 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 04:31:40.609673: step 170730, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 04:31:52.670680: step 170740, loss = 0.45 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 04:32:04.726429: step 170750, loss = 0.46 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 04:32:16.805060: step 170760, loss = 0.44 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 04:32:28.836731: step 170770, loss = 0.46 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 04:32:40.943347: step 170780, loss = 0.45 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-23 04:32:53.074230: step 170790, loss = 0.41 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 04:33:05.115848: step 170800, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 04:33:19.469286: step 170810, loss = 0.43 (25.2 examples/sec; 1.190 sec/batch)\n",
      "2019-05-23 04:33:31.554523: step 170820, loss = 0.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 04:33:43.636863: step 170830, loss = 0.54 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 04:33:55.626202: step 170840, loss = 0.48 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 04:34:07.704933: step 170850, loss = 0.54 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 04:34:19.760978: step 170860, loss = 0.55 (25.2 examples/sec; 1.189 sec/batch)\n",
      "2019-05-23 04:34:31.925202: step 170870, loss = 0.48 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 04:34:44.093765: step 170880, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 04:34:56.198291: step 170890, loss = 0.46 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 04:35:08.314184: step 170900, loss = 0.46 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 04:35:22.346350: step 170910, loss = 0.41 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 04:35:34.442244: step 170920, loss = 0.48 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 04:35:46.643625: step 170930, loss = 0.55 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 04:35:58.796017: step 170940, loss = 0.50 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-23 04:36:10.908473: step 170950, loss = 0.48 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 04:36:23.084612: step 170960, loss = 0.49 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 04:36:35.252114: step 170970, loss = 0.52 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 04:36:47.367167: step 170980, loss = 0.48 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 04:36:59.513861: step 170990, loss = 0.44 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 04:37:11.666901: step 171000, loss = 0.42 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-23 04:37:26.185687: step 171010, loss = 0.40 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 04:37:38.232058: step 171020, loss = 0.54 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-23 04:37:50.411767: step 171030, loss = 0.44 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 04:38:02.543510: step 171040, loss = 0.51 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 04:38:14.658051: step 171050, loss = 0.41 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 04:38:26.778786: step 171060, loss = 0.54 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 04:38:38.925519: step 171070, loss = 0.49 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 04:38:51.056869: step 171080, loss = 0.49 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 04:39:03.179444: step 171090, loss = 0.49 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 04:39:15.293870: step 171100, loss = 0.47 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 04:39:29.691556: step 171110, loss = 0.45 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 04:39:41.813709: step 171120, loss = 0.52 (24.9 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 04:39:53.921348: step 171130, loss = 0.53 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 04:40:05.989058: step 171140, loss = 0.51 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 04:40:18.148925: step 171150, loss = 0.48 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 04:40:30.328279: step 171160, loss = 0.56 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 04:40:42.504771: step 171170, loss = 0.41 (24.6 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 04:40:54.653704: step 171180, loss = 0.52 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 04:41:06.766820: step 171190, loss = 0.46 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 04:41:18.913465: step 171200, loss = 0.50 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 04:41:32.907548: step 171210, loss = 0.43 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 04:41:45.027508: step 171220, loss = 0.58 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 04:41:57.177142: step 171230, loss = 0.47 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 04:42:09.314337: step 171240, loss = 0.44 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 04:42:21.445905: step 171250, loss = 0.44 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 04:42:33.622485: step 171260, loss = 0.53 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 04:42:45.729086: step 171270, loss = 0.42 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 04:42:57.832300: step 171280, loss = 0.45 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 04:43:09.991490: step 171290, loss = 0.45 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 04:43:22.127093: step 171300, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 04:43:36.475457: step 171310, loss = 0.46 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 04:43:48.569117: step 171320, loss = 0.45 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 04:44:00.698303: step 171330, loss = 0.51 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 04:44:12.809432: step 171340, loss = 0.49 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 04:44:24.924009: step 171350, loss = 0.48 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 04:44:37.053243: step 171360, loss = 0.57 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 04:44:49.176905: step 171370, loss = 0.53 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 04:45:01.333530: step 171380, loss = 0.45 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 04:45:13.464057: step 171390, loss = 0.46 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 04:45:25.580622: step 171400, loss = 0.46 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 04:45:39.690596: step 171410, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 04:45:51.835531: step 171420, loss = 0.43 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 04:46:04.019183: step 171430, loss = 0.45 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 04:46:16.201261: step 171440, loss = 0.55 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 04:46:28.348043: step 171450, loss = 0.47 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 04:46:40.438405: step 171460, loss = 0.46 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 04:46:52.566531: step 171470, loss = 0.48 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 04:47:04.707745: step 171480, loss = 0.52 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 04:47:16.844027: step 171490, loss = 0.52 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 04:47:28.989350: step 171500, loss = 0.44 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 04:47:42.927857: step 171510, loss = 0.52 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 04:47:55.046375: step 171520, loss = 0.49 (24.6 examples/sec; 1.217 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23 04:48:07.151265: step 171530, loss = 0.43 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 04:48:19.277880: step 171540, loss = 0.48 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 04:48:31.419559: step 171550, loss = 0.49 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 04:48:43.519109: step 171560, loss = 0.53 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 04:48:55.646809: step 171570, loss = 0.59 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 04:49:07.766603: step 171580, loss = 0.50 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 04:49:19.967269: step 171590, loss = 0.42 (24.3 examples/sec; 1.235 sec/batch)\n",
      "2019-05-23 04:49:32.043367: step 171600, loss = 0.43 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 04:49:46.213274: step 171610, loss = 0.49 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 04:49:58.339978: step 171620, loss = 0.59 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 04:50:10.419189: step 171630, loss = 0.43 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 04:50:22.583038: step 171640, loss = 0.48 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 04:50:34.711485: step 171650, loss = 0.44 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 04:50:46.845831: step 171660, loss = 0.43 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 04:50:59.014993: step 171670, loss = 0.47 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 04:51:11.101513: step 171680, loss = 0.53 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 04:51:23.299065: step 171690, loss = 0.52 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 04:51:35.481492: step 171700, loss = 0.48 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 04:51:49.777743: step 171710, loss = 0.47 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 04:52:01.916512: step 171720, loss = 0.45 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 04:52:14.030296: step 171730, loss = 0.50 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 04:52:26.083102: step 171740, loss = 0.49 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 04:52:38.222586: step 171750, loss = 0.45 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 04:52:50.265565: step 171760, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 04:53:02.432994: step 171770, loss = 0.44 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 04:53:14.560633: step 171780, loss = 0.42 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 04:53:26.706892: step 171790, loss = 0.50 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-23 04:53:38.846015: step 171800, loss = 0.44 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 04:53:52.888717: step 171810, loss = 0.50 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-23 04:54:05.011864: step 171820, loss = 0.48 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 04:54:17.133682: step 171830, loss = 0.49 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 04:54:29.216682: step 171840, loss = 0.40 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 04:54:41.267333: step 171850, loss = 0.49 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 04:54:53.384703: step 171860, loss = 0.47 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 04:55:05.540403: step 171870, loss = 0.44 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 04:55:17.723270: step 171880, loss = 0.49 (24.4 examples/sec; 1.229 sec/batch)\n",
      "2019-05-23 04:55:29.886755: step 171890, loss = 0.46 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 04:55:41.989686: step 171900, loss = 0.47 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-23 04:55:56.395739: step 171910, loss = 0.56 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 04:56:08.529595: step 171920, loss = 0.53 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 04:56:20.655708: step 171930, loss = 0.50 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 04:56:32.813445: step 171940, loss = 0.51 (24.5 examples/sec; 1.224 sec/batch)\n",
      "2019-05-23 04:56:44.931297: step 171950, loss = 0.53 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 04:56:56.984652: step 171960, loss = 0.48 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 04:57:09.091475: step 171970, loss = 0.45 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 04:57:21.203086: step 171980, loss = 0.56 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 04:57:33.277425: step 171990, loss = 0.41 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 04:57:45.418646: step 172000, loss = 0.45 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 04:57:59.706328: step 172010, loss = 0.41 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 04:58:11.838195: step 172020, loss = 0.51 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 04:58:23.928791: step 172030, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 04:58:36.076615: step 172040, loss = 0.48 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 04:58:48.195786: step 172050, loss = 0.45 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 04:59:00.328453: step 172060, loss = 0.41 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 04:59:12.473573: step 172070, loss = 0.48 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 04:59:24.583042: step 172080, loss = 0.41 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-23 04:59:36.729547: step 172090, loss = 0.53 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 04:59:48.805249: step 172100, loss = 0.48 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 05:00:03.258013: step 172110, loss = 0.48 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 05:00:15.392656: step 172120, loss = 0.55 (24.5 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 05:00:27.493500: step 172130, loss = 0.55 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 05:00:39.594633: step 172140, loss = 0.48 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 05:00:51.707965: step 172150, loss = 0.45 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-23 05:01:03.807053: step 172160, loss = 0.44 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 05:01:15.944113: step 172170, loss = 0.46 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 05:01:28.103926: step 172180, loss = 0.48 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 05:01:40.204195: step 172190, loss = 0.43 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 05:01:52.381774: step 172200, loss = 0.52 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 05:02:06.457802: step 172210, loss = 0.48 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-23 05:02:18.587346: step 172220, loss = 0.57 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 05:02:30.718794: step 172230, loss = 0.46 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 05:02:42.827261: step 172240, loss = 0.43 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 05:02:54.930726: step 172250, loss = 0.46 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 05:03:07.006276: step 172260, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 05:03:19.122043: step 172270, loss = 0.51 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 05:03:31.243158: step 172280, loss = 0.51 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 05:03:43.365980: step 172290, loss = 0.48 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-23 05:03:55.544160: step 172300, loss = 0.46 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 05:04:09.726814: step 172310, loss = 0.58 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 05:04:21.858958: step 172320, loss = 0.44 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 05:04:34.018874: step 172330, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 05:04:46.108277: step 172340, loss = 0.43 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 05:04:58.196466: step 172350, loss = 0.47 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 05:05:10.261546: step 172360, loss = 0.49 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 05:05:22.319302: step 172370, loss = 0.50 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-23 05:05:34.475709: step 172380, loss = 0.47 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 05:05:46.602566: step 172390, loss = 0.52 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 05:05:58.741443: step 172400, loss = 0.41 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 05:06:12.798341: step 172410, loss = 0.47 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 05:06:24.888597: step 172420, loss = 0.39 (25.2 examples/sec; 1.191 sec/batch)\n",
      "2019-05-23 05:06:37.050345: step 172430, loss = 0.46 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 05:06:49.194427: step 172440, loss = 0.53 (24.6 examples/sec; 1.219 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23 05:07:01.280558: step 172450, loss = 0.44 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 05:07:13.398413: step 172460, loss = 0.47 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 05:07:25.471285: step 172470, loss = 0.56 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 05:07:37.542459: step 172480, loss = 0.49 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 05:07:49.627634: step 172490, loss = 0.48 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 05:08:01.702574: step 172500, loss = 0.52 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-23 05:08:15.863804: step 172510, loss = 0.46 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 05:08:28.016307: step 172520, loss = 0.46 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-23 05:08:40.100175: step 172530, loss = 0.46 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-23 05:08:52.214410: step 172540, loss = 0.49 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 05:09:04.326536: step 172550, loss = 0.46 (24.5 examples/sec; 1.222 sec/batch)\n",
      "2019-05-23 05:09:16.381986: step 172560, loss = 0.50 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-23 05:09:28.540732: step 172570, loss = 0.45 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 05:09:40.697454: step 172580, loss = 0.38 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 05:09:52.898382: step 172590, loss = 0.48 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 05:10:05.040852: step 172600, loss = 0.52 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 05:10:19.457215: step 172610, loss = 0.52 (25.1 examples/sec; 1.196 sec/batch)\n",
      "2019-05-23 05:10:31.583135: step 172620, loss = 0.42 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 05:10:43.736081: step 172630, loss = 0.44 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 05:10:55.856879: step 172640, loss = 0.43 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 05:11:07.947688: step 172650, loss = 0.45 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 05:11:20.052555: step 172660, loss = 0.43 (25.1 examples/sec; 1.193 sec/batch)\n",
      "2019-05-23 05:11:32.181344: step 172670, loss = 0.44 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 05:11:44.319953: step 172680, loss = 0.43 (24.6 examples/sec; 1.221 sec/batch)\n",
      "2019-05-23 05:11:56.433627: step 172690, loss = 0.55 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 05:12:08.552013: step 172700, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 05:12:22.770949: step 172710, loss = 0.50 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 05:12:34.904646: step 172720, loss = 0.49 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 05:12:47.032671: step 172730, loss = 0.47 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 05:12:59.162260: step 172740, loss = 0.45 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 05:13:11.202142: step 172750, loss = 0.53 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-23 05:13:23.334019: step 172760, loss = 0.49 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 05:13:35.435049: step 172770, loss = 0.51 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 05:13:47.556790: step 172780, loss = 0.45 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 05:13:59.635487: step 172790, loss = 0.53 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 05:14:11.753974: step 172800, loss = 0.49 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-23 05:14:25.728393: step 172810, loss = 0.50 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 05:14:37.815959: step 172820, loss = 0.50 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 05:14:49.937188: step 172830, loss = 0.48 (24.1 examples/sec; 1.247 sec/batch)\n",
      "2019-05-23 05:15:01.999256: step 172840, loss = 0.49 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 05:15:14.070450: step 172850, loss = 0.46 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 05:15:26.106556: step 172860, loss = 0.46 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 05:15:38.199986: step 172870, loss = 0.40 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 05:15:50.280805: step 172880, loss = 0.53 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 05:16:02.427880: step 172890, loss = 0.57 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-23 05:16:14.521306: step 172900, loss = 0.43 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 05:16:28.551679: step 172910, loss = 0.47 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 05:16:40.665706: step 172920, loss = 0.53 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 05:16:52.776213: step 172930, loss = 0.47 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 05:17:04.873375: step 172940, loss = 0.46 (25.1 examples/sec; 1.194 sec/batch)\n",
      "2019-05-23 05:17:16.959719: step 172950, loss = 0.47 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 05:17:29.064492: step 172960, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 05:17:41.215041: step 172970, loss = 0.50 (24.4 examples/sec; 1.231 sec/batch)\n",
      "2019-05-23 05:17:53.331807: step 172980, loss = 0.53 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 05:18:05.448455: step 172990, loss = 0.59 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 05:18:17.526839: step 173000, loss = 0.45 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 05:18:31.485271: step 173010, loss = 0.43 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 05:18:43.567084: step 173020, loss = 0.46 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 05:18:55.704121: step 173030, loss = 0.47 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 05:19:07.757784: step 173040, loss = 0.48 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 05:19:19.915050: step 173050, loss = 0.45 (24.4 examples/sec; 1.232 sec/batch)\n",
      "2019-05-23 05:19:32.067803: step 173060, loss = 0.49 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 05:19:44.206912: step 173070, loss = 0.41 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 05:19:56.346243: step 173080, loss = 0.51 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 05:20:08.412202: step 173090, loss = 0.49 (24.5 examples/sec; 1.223 sec/batch)\n",
      "2019-05-23 05:20:20.528687: step 173100, loss = 0.47 (24.4 examples/sec; 1.227 sec/batch)\n",
      "2019-05-23 05:20:34.534992: step 173110, loss = 0.57 (25.2 examples/sec; 1.193 sec/batch)\n",
      "2019-05-23 05:20:46.607628: step 173120, loss = 0.51 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 05:20:58.736957: step 173130, loss = 0.47 (24.9 examples/sec; 1.203 sec/batch)\n",
      "2019-05-23 05:21:10.879415: step 173140, loss = 0.44 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 05:21:23.041936: step 173150, loss = 0.48 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 05:21:35.162545: step 173160, loss = 0.63 (24.9 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 05:21:47.320451: step 173170, loss = 0.52 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 05:21:59.454349: step 173180, loss = 0.50 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 05:22:11.607404: step 173190, loss = 0.45 (24.4 examples/sec; 1.228 sec/batch)\n",
      "2019-05-23 05:22:23.710497: step 173200, loss = 0.43 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 05:22:38.133845: step 173210, loss = 0.45 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 05:22:50.288115: step 173220, loss = 0.52 (25.0 examples/sec; 1.200 sec/batch)\n",
      "2019-05-23 05:23:02.465247: step 173230, loss = 0.44 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 05:23:14.606461: step 173240, loss = 0.48 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 05:23:26.695690: step 173250, loss = 0.51 (24.8 examples/sec; 1.208 sec/batch)\n",
      "2019-05-23 05:23:38.851484: step 173260, loss = 0.40 (24.6 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 05:23:50.991449: step 173270, loss = 0.52 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 05:24:03.153387: step 173280, loss = 0.41 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 05:24:15.290452: step 173290, loss = 0.50 (24.6 examples/sec; 1.219 sec/batch)\n",
      "2019-05-23 05:24:27.402357: step 173300, loss = 0.49 (25.0 examples/sec; 1.201 sec/batch)\n",
      "2019-05-23 05:24:41.797774: step 173310, loss = 0.55 (24.9 examples/sec; 1.204 sec/batch)\n",
      "2019-05-23 05:24:53.916239: step 173320, loss = 0.49 (24.7 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 05:25:05.981884: step 173330, loss = 0.56 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 05:25:18.063692: step 173340, loss = 0.39 (24.9 examples/sec; 1.206 sec/batch)\n",
      "2019-05-23 05:25:30.173626: step 173350, loss = 0.37 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 05:25:42.207027: step 173360, loss = 0.46 (25.2 examples/sec; 1.188 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23 05:25:54.264822: step 173370, loss = 0.45 (24.9 examples/sec; 1.205 sec/batch)\n",
      "2019-05-23 05:26:06.315235: step 173380, loss = 0.52 (24.5 examples/sec; 1.225 sec/batch)\n",
      "2019-05-23 05:26:18.390164: step 173390, loss = 0.43 (24.7 examples/sec; 1.213 sec/batch)\n",
      "2019-05-23 05:26:30.498106: step 173400, loss = 0.48 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 05:26:45.161634: step 173410, loss = 0.50 (25.1 examples/sec; 1.195 sec/batch)\n",
      "2019-05-23 05:26:57.245913: step 173420, loss = 0.38 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 05:27:09.364936: step 173430, loss = 0.54 (24.6 examples/sec; 1.218 sec/batch)\n",
      "2019-05-23 05:27:21.460410: step 173440, loss = 0.50 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 05:27:33.527010: step 173450, loss = 0.59 (24.7 examples/sec; 1.215 sec/batch)\n",
      "2019-05-23 05:27:45.671863: step 173460, loss = 0.50 (24.8 examples/sec; 1.207 sec/batch)\n",
      "2019-05-23 05:27:57.793639: step 173470, loss = 0.49 (24.5 examples/sec; 1.226 sec/batch)\n",
      "2019-05-23 05:28:09.877358: step 173480, loss = 0.46 (24.8 examples/sec; 1.211 sec/batch)\n",
      "2019-05-23 05:28:21.984585: step 173490, loss = 0.47 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 05:28:34.016579: step 173500, loss = 0.51 (24.7 examples/sec; 1.217 sec/batch)\n",
      "2019-05-23 05:28:47.976590: step 173510, loss = 0.43 (25.2 examples/sec; 1.192 sec/batch)\n",
      "2019-05-23 05:29:00.037906: step 173520, loss = 0.48 (25.0 examples/sec; 1.198 sec/batch)\n",
      "2019-05-23 05:29:12.126886: step 173530, loss = 0.47 (24.8 examples/sec; 1.212 sec/batch)\n",
      "2019-05-23 05:29:24.231055: step 173540, loss = 0.43 (24.8 examples/sec; 1.209 sec/batch)\n",
      "2019-05-23 05:29:36.313992: step 173550, loss = 0.42 (25.0 examples/sec; 1.199 sec/batch)\n",
      "2019-05-23 05:29:48.327559: step 173560, loss = 0.41 (25.0 examples/sec; 1.202 sec/batch)\n",
      "2019-05-23 05:30:00.440148: step 173570, loss = 0.45 (24.7 examples/sec; 1.214 sec/batch)\n",
      "2019-05-23 05:30:12.544479: step 173580, loss = 0.43 (24.8 examples/sec; 1.210 sec/batch)\n",
      "2019-05-23 05:30:24.707435: step 173590, loss = 0.57 (24.6 examples/sec; 1.220 sec/batch)\n",
      "2019-05-23 05:30:36.829730: step 173600, loss = 0.50 (25.1 examples/sec; 1.197 sec/batch)\n",
      "2019-05-23 05:30:50.718106: step 173610, loss = 0.50 (25.5 examples/sec; 1.176 sec/batch)\n",
      "2019-05-23 05:31:02.756777: step 173620, loss = 0.52 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 05:31:14.880455: step 173630, loss = 0.50 (24.7 examples/sec; 1.216 sec/batch)\n",
      "2019-05-23 05:31:27.032627: step 173640, loss = 0.52 (24.6 examples/sec; 1.218 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "function = join(deeppath_code,'01_training/xClasses/bazel-bin/inception/imagenet_train' )\n",
    "root = function.rsplit('/',1)[1]\n",
    "# Note that the --train_dir directory appears to be deleted and recreated\n",
    "!python $function \\\n",
    "    --num_gpus=1 --batch_size=30 --train_dir=$intermediate_checkpoints --data_dir=$trainValid_records \\\n",
    "    --ClassNumber=$class_number --mode=$training_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "Validation is ideally performed while training is in progress to help decide when sufficient training has been performed. On Jupyter we can't run training and validation concurrently, and so have to stop training in order to validate, then do more training if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training checkpoints every 5000 batches. The training_checkpoint parameter to eval_all.sh controls which\n",
    "# checkpoints are validated:\n",
    "# -1: Validate the maximal checkpoint\n",
    "# n*5000, n>=0: Validate all checkpoints from n*5000\n",
    "# \n",
    "\n",
    "training_count = -1\n",
    "try:\n",
    "    shutil.rmtree(eval_results)\n",
    "except:\n",
    "    pass\n",
    "os.mkdir(eval_results)\n",
    "\n",
    "eval = join(os.environ[\"HOME\"],'jupyter_DeepPATH','eval_all.sh')\n",
    "!$eval \\\n",
    "    $intermediate_checkpoints $eval_results $trainValid_records $hugo_symbols_path \\\n",
    "    $deeppath_code/02_testing/xClasses/nc_imagenet_eval.py \\\n",
    "    $deeppath_code/03_postprocessing/0h_ROC_MultiOutput_BootStrap.py $training_logs $training_mode $training_count $class_number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hugo_symbols_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    shutil.rmtree(test_results)\n",
    "except:\n",
    "    pass\n",
    "os.mkdir(test_results)\n",
    "\n",
    "function = join(deeppath_code,'02_testing/xClasses/nc_imagenet_eval.py' )\n",
    "root = function.rsplit('/',1)[1].split('.')[0]\n",
    "\n",
    "#Redirect output because Jupyter has a limit on test output\n",
    "out_log_file = join(training_logs,root+'.test.out.log')\n",
    "err_log_file = join(training_logs,root+'.test.err.log')\n",
    "oldstderr = sys.stderr\n",
    "sys.stderr = open(err_log_file, 'w')\n",
    "oldstdout = sys.stdout\n",
    "sys.stdout = open(out_log_file, 'w')\n",
    "\n",
    "!python $function --checkpoint_dir=$intermediate_checkpoints --eval_dir=$test_results \\\n",
    "    --data_dir=$test_records\n",
    "  --batch_size 30 --ImageSet_basename='test_' --run_once --ClassNumber=$class_number \\\n",
    "    --mode=$training_mode --TVmode='test'\n",
    "\n",
    "sys.stderr = oldstderr\n",
    "sys.stdout = oldstdout\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze results\n",
    "The following generates heatmaps for specified slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "function = join(deeppath_code,'03_postprocessing/0f_HeatMap_nClasses.py' )\n",
    "root = function.rsplit('/',1)[1].split('.')[0]\n",
    "try:\n",
    "    shutil.rmtree(heatmaps)\n",
    "except:\n",
    "    pass\n",
    "os.mkdir(heatmaps)\n",
    "\n",
    "\n",
    "try:\n",
    "    os.mkdir(output)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tile_stats = join(test_results, 'out_filename_Stats.txt')\n",
    "!python $function  --image_file=$sorted_tiles --tiles_overlap 0 --output_dir=$heatmaps \\\n",
    "    --tiles_stats=$tile_stats --resample_factor 10  --filter_tile '' \\\n",
    "    --Cmap 'CancerType' --tiles_size $tile_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"/home/bcliffor/tmp/j2.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(join(tiling, \n",
    "  'px299/heatmaps/heatmap_CancerType_test_TCGA-NC-A5HR-01A-02-TS2.1B2A21A9-E685-461D-A3FF-42A0D9D7FC23_TCGA-LUSC.jpg'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate ROC curve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:\n",
      "123907\n",
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "123907\n",
      "[[0.12360078 0.45603293 0.27976966 ... 0.14261308 0.13428485 0.624903  ]\n",
      " [0.22959703 0.04028478 0.13573992 ... 0.01772705 0.27107644 0.28194022]\n",
      " [0.16719195 0.08152965 0.44461897 ... 0.05313322 0.02481815 0.29360414]\n",
      " ...\n",
      " [0.03063962 0.16208029 0.02026311 ... 0.00974926 0.5687011  0.04259276]\n",
      " [0.00869015 0.179678   0.01802507 ... 0.01322573 0.61355925 0.00470623]\n",
      " [0.0341638  0.7314836  0.03733882 ... 0.00669071 0.8765054  0.04443452]]\n",
      "123907\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "[0. 0. 0. ... 0. 0. 0.] [0.12360078 0.22959703 0.16719195 ... 0.03063962 0.00869015 0.0341638 ] [0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.12360078 0.22959703 0.16719195 ... 0.03063962 0.00869015 0.0341638 ]\n",
      "0.5009312110360793\n",
      "0.5126002965267155\n",
      "Confidence interval for the score: [0.501 - 0.513]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "0.5169878928538563\n",
      "0.5225989706230029\n",
      "Confidence interval for the score: [0.517 - 0.523]\n",
      "[0. 0. 0. ... 0. 0. 0.] [0.45603293 0.04028478 0.08152965 ... 0.16208029 0.179678   0.7314836 ] [0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.45603293 0.04028478 0.08152965 ... 0.16208029 0.179678   0.7314836 ]\n",
      "0.44908636040272687\n",
      "0.4577385839142213\n",
      "Confidence interval for the score: [0.449 - 0.458]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "0.48441986867353987\n",
      "0.48640847560129946\n",
      "Confidence interval for the score: [0.484 - 0.486]\n",
      "[0. 0. 0. ... 0. 0. 0.] [0.27976966 0.13573992 0.44461897 ... 0.02026311 0.01802507 0.03733882] [0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.27976966 0.13573992 0.44461897 ... 0.02026311 0.01802507 0.03733882]\n",
      "0.5237351121936153\n",
      "0.530373295395084\n",
      "Confidence interval for the score: [0.524 - 0.53]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "0.5094889645950437\n",
      "0.5122668370001129\n",
      "Confidence interval for the score: [0.509 - 0.512]\n",
      "[1. 1. 1. ... 0. 0. 0.] [0.12478167 0.07213926 0.1318799  ... 0.27469683 0.510818   0.7959899 ] [0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "[0.12478167 0.07213926 0.1318799  ... 0.27469683 0.510818   0.7959899 ]\n",
      "0.5074996891453913\n",
      "0.5160729193056058\n",
      "Confidence interval for the score: [0.507 - 0.516]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "0.5008440859972952\n",
      "0.5036280517817017\n",
      "Confidence interval for the score: [0.501 - 0.504]\n",
      "[0. 0. 0. ... 1. 1. 1.] [0.1276429  0.17313701 0.17378601 ... 0.19880223 0.53985786 0.64822674] [0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 1. 1.]\n",
      "[0.1276429  0.17313701 0.17378601 ... 0.19880223 0.53985786 0.64822674]\n",
      "0.5063541520840513\n",
      "0.5144067508331781\n",
      "Confidence interval for the score: [0.506 - 0.514]\n",
      "[0. 0. 0. ... 1. 1. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "0.4879640697920085\n",
      "0.49220582720589784\n",
      "Confidence interval for the score: [0.488 - 0.492]\n",
      "[0. 0. 0. ... 1. 1. 1.] [0.66563356 0.05315253 0.53623796 ... 0.15814707 0.01989511 0.06986234] [1. 0. 1. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 1. 1.]\n",
      "[0.66563356 0.05315253 0.53623796 ... 0.15814707 0.01989511 0.06986234]\n",
      "0.48398928048297923\n",
      "0.49051319757162104\n",
      "Confidence interval for the score: [0.484 - 0.491]\n",
      "[0. 0. 0. ... 1. 1. 1.]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "0.49047413444671717\n",
      "0.49516611126634735\n",
      "Confidence interval for the score: [0.490 - 0.495]\n",
      "[1. 1. 1. ... 0. 0. 0.] [0.14507648 0.03177398 0.08124909 ... 0.01765504 0.0081788  0.00902694] [0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "[0.14507648 0.03177398 0.08124909 ... 0.01765504 0.0081788  0.00902694]\n",
      "0.5243959172753345\n",
      "0.53177636157101\n",
      "Confidence interval for the score: [0.524 - 0.532]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "0.5020755590472272\n",
      "0.5043616347017081\n",
      "Confidence interval for the score: [0.502 - 0.504]\n",
      "[0. 0. 0. ... 0. 0. 0.] [0.14261308 0.01772705 0.05313322 ... 0.00974926 0.01322573 0.00669071] [0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.14261308 0.01772705 0.05313322 ... 0.00974926 0.01322573 0.00669071]\n",
      "0.5056215274390008\n",
      "0.5135921039151414\n",
      "Confidence interval for the score: [0.506 - 0.514]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "0.5013065017465602\n",
      "0.5033882994456919\n",
      "Confidence interval for the score: [0.501 - 0.503]\n",
      "[0. 0. 0. ... 1. 1. 1.] [0.13428485 0.27107644 0.02481815 ... 0.5687011  0.61355925 0.8765054 ] [0. 0. 0. ... 1. 1. 1.]\n",
      "[0. 0. 0. ... 1. 1. 1.]\n",
      "[0.13428485 0.27107644 0.02481815 ... 0.5687011  0.61355925 0.8765054 ]\n",
      "0.5067925631794091\n",
      "0.5181552138678944\n",
      "Confidence interval for the score: [0.507 - 0.518]\n",
      "[0. 0. 0. ... 1. 1. 1.]\n",
      "[0. 0. 0. ... 1. 1. 1.]\n",
      "0.496975860338755\n",
      "0.5004950272289685\n",
      "Confidence interval for the score: [0.497 - 0.5]\n",
      "[1. 1. 1. ... 0. 0. 0.] [0.624903   0.28194022 0.29360414 ... 0.04259276 0.00470623 0.04443452] [0. 1. 0. ... 0. 0. 0.]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "[0.624903   0.28194022 0.29360414 ... 0.04259276 0.00470623 0.04443452]\n",
      "0.5613017468861092\n",
      "0.5678716826253428\n",
      "Confidence interval for the score: [0.561 - 0.568]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "0.5176100933755508\n",
      "0.5230737219894196\n",
      "Confidence interval for the score: [0.518 - 0.523]\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "[0.12360078 0.45603293 0.27976966 ... 0.00669071 0.8765054  0.04443452]\n",
      "0.5940274089545489\n",
      "0.5964171093974855\n",
      "Confidence interval for the score: [0.594 - 0.596]\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "0.5433510385250255\n",
      "0.5447892007203673\n",
      "Confidence interval for the score: [0.543 - 0.545]\n",
      "y_ref_PerTile.ravel(), y_score_PcS_PerTile.ravel()\n",
      "[0. 0. 0. ... 0. 1. 0.] [0. 0. 0. ... 0. 1. 0.]\n",
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "[[0.12360078 0.45603293 0.27976966 ... 0.14261308 0.13428485 0.624903  ]\n",
      " [0.22959703 0.04028478 0.13573992 ... 0.01772705 0.27107644 0.28194022]\n",
      " [0.16719195 0.08152965 0.44461897 ... 0.05313322 0.02481815 0.29360414]\n",
      " ...\n",
      " [0.03063962 0.16208029 0.02026311 ... 0.00974926 0.5687011  0.04259276]\n",
      " [0.00869015 0.179678   0.01802507 ... 0.01322573 0.61355925 0.00470623]\n",
      " [0.0341638  0.7314836  0.03733882 ... 0.00669071 0.8765054  0.04443452]]\n",
      "0.5096990365406374\n",
      "0.5125991254869496\n",
      "Confidence interval for the score: [0.510 - 0.513]\n",
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "0.5019873753165924\n",
      "0.5032007070995106\n",
      "Confidence interval for the score: [0.502 - 0.503]\n",
      "******* FP / TP for average probability\n",
      "{0: array([0.00000000e+00, 8.99806542e-06, 8.09825887e-05, ...,\n",
      "       9.99982004e-01, 1.00000000e+00, 1.00000000e+00]), 1: array([0.00000000e+00, 9.52353742e-06, 4.76176871e-05, ...,\n",
      "       9.99952382e-01, 9.99952382e-01, 1.00000000e+00]), 2: array([0.        , 0.        , 0.        , ..., 0.99980615, 0.99980615,\n",
      "       1.        ]), 3: array([0.00000000e+00, 9.65763678e-06, 9.65763678e-06, ...,\n",
      "       9.99323965e-01, 9.99323965e-01, 1.00000000e+00]), 4: array([0.00000000e+00, 9.95857234e-06, 4.48135755e-04, ...,\n",
      "       9.99830704e-01, 9.99830704e-01, 1.00000000e+00]), 5: array([0.00000000e+00, 1.15465441e-05, 2.30930882e-05, ...,\n",
      "       9.99491952e-01, 9.99491952e-01, 1.00000000e+00]), 6: array([0.00000000e+00, 0.00000000e+00, 6.25091159e-05, ...,\n",
      "       9.99833309e-01, 9.99833309e-01, 1.00000000e+00]), 7: array([0.00000000e+00, 0.00000000e+00, 3.04909035e-05, ...,\n",
      "       9.99989836e-01, 9.99989836e-01, 1.00000000e+00]), 8: array([0.00000000e+00, 8.88683504e-06, 1.06642021e-04, ...,\n",
      "       9.99617866e-01, 9.99617866e-01, 1.00000000e+00]), 9: array([0.        , 0.        , 0.        , ..., 0.99987343, 0.99987343,\n",
      "       1.        ]), 'macro': array([0.00000000e+00, 8.88683504e-06, 8.99806542e-06, ...,\n",
      "       9.99982004e-01, 9.99989836e-01, 1.00000000e+00]), 'micro': array([0.00000000e+00, 1.05982725e-06, 4.87520534e-05, ...,\n",
      "       9.99990462e-01, 9.99990462e-01, 1.00000000e+00])}\n",
      "{0: array([0.       , 0.       , 0.       , ..., 0.9999217, 0.9999217,\n",
      "       1.       ]), 1: array([0.       , 0.       , 0.       , ..., 0.9998942, 1.       ,\n",
      "       1.       ]), 2: array([0.00000000e+00, 2.41721054e-05, 7.25163162e-05, ...,\n",
      "       9.99975828e-01, 1.00000000e+00, 1.00000000e+00]), 3: array([0.00000000e+00, 0.00000000e+00, 4.91110893e-05, ...,\n",
      "       9.99950889e-01, 1.00000000e+00, 1.00000000e+00]), 4: array([0.        , 0.        , 0.        , ..., 0.99995743, 1.        ,\n",
      "       1.        ]), 5: array([0.        , 0.        , 0.        , ..., 0.99997319, 1.        ,\n",
      "       1.        ]), 6: array([0.00000000e+00, 3.58153361e-05, 3.58153361e-05, ...,\n",
      "       9.99964185e-01, 1.00000000e+00, 1.00000000e+00]), 7: array([0.00000000e+00, 3.91895599e-05, 3.91895599e-05, ...,\n",
      "       9.99960810e-01, 1.00000000e+00, 1.00000000e+00]), 8: array([0.        , 0.        , 0.        , ..., 0.99991213, 1.        ,\n",
      "       1.        ]), 9: array([0.00000000e+00, 1.30717246e-05, 6.53586228e-05, ...,\n",
      "       9.99986928e-01, 1.00000000e+00, 1.00000000e+00]), 'macro': array([2.12879835e-05, 2.12879835e-05, 2.12879835e-05, ...,\n",
      "       9.99988251e-01, 9.99992170e-01, 1.00000000e+00]), 'micro': array([0.        , 0.        , 0.        , ..., 0.99999662, 1.        ,\n",
      "       1.        ])}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* FP / TP for percent selected\n",
      "{0: array([0.        , 0.07286633, 1.        ]), 1: array([0.        , 0.04138929, 1.        ]), 2: array([0.        , 0.04305948, 1.        ]), 3: array([0.        , 0.02945579, 1.        ]), 4: array([0.        , 0.11603728, 1.        ]), 5: array([0.        , 0.19442071, 1.        ]), 6: array([0.        , 0.02422228, 1.        ]), 7: array([0.        , 0.01877223, 1.        ]), 8: array([0.        , 0.03504968, 1.        ]), 9: array([0.        , 0.39781884, 1.        ]), 'macro': array([0.        , 0.01877223, 0.02422228, 0.02945579, 0.03504968,\n",
      "       0.04138929, 0.04305948, 0.07286633, 0.11603728, 0.19442071,\n",
      "       0.39781884, 1.        ]), 'micro': array([0.        , 0.07897091, 1.        ])}\n",
      "{0: array([0.        , 0.11243345, 1.        ]), 1: array([0.        , 0.01221964, 1.        ]), 2: array([0.        , 0.06487793, 1.        ]), 3: array([0.        , 0.03393576, 1.        ]), 4: array([0.        , 0.09633477, 1.        ]), 5: array([0.        , 0.18002198, 1.        ]), 6: array([0.        , 0.03065793, 1.        ]), 7: array([0.        , 0.02339617, 1.        ]), 8: array([0.        , 0.03259819, 1.        ]), 9: array([0.        , 0.43855636, 1.        ]), 'macro': array([0.        , 0.02026961, 0.02601757, 0.03139466, 0.03705435,\n",
      "       0.04351449, 0.04533922, 0.07632584, 0.11867704, 0.19707772,\n",
      "       0.40239051, 1.        ]), 'micro': array([0.        , 0.16714266, 1.        ])}\n",
      "n_classes\n",
      "10\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.1834793  0.12727461 0.07674763 0.11342989 0.02490004 0.28490944\n",
      " 0.11323259 0.24056134 0.45829196 0.05210948 0.03237392 0.13751553\n",
      " 0.1831491  0.07755558 0.05099603 0.07465974 0.0333491  0.1127335\n",
      " 0.1548868  0.05637225 0.15642062 0.28963594 0.23625555 0.05004109\n",
      " 0.28078943 0.02631155 0.06908604 0.02028918 0.01973469 0.1072136\n",
      " 0.07957213 0.21935106 0.19264599 0.0579674  0.08610469 0.07690155\n",
      " 0.02120946 0.34285566 0.08642419 0.14705805 0.06958643 0.1214668\n",
      " 0.06156694 0.0662856  0.10106042 0.15972184 0.01617754 0.03800871\n",
      " 0.31826522 0.04365566 0.06814366 0.13806117 0.19996868 0.39097104\n",
      " 0.05881345 0.01775426 0.07593039 0.07935314 0.40331845 0.08034389\n",
      " 0.0910722  0.10319098 0.10030537 0.0592981  0.2454626  0.09163264\n",
      " 0.37659946 0.1851374  0.16034599 0.1517908  0.07749984 0.17389542\n",
      " 0.10977373 0.01431553 0.31934383 0.05936572 0.15049991 0.14880654\n",
      " 0.03208146 0.16211417 0.09115902 0.14279001 0.03456031 0.09715535\n",
      " 0.16649602 0.18186015 0.11473827 0.03590542 0.06574772 0.07953351\n",
      " 0.12112202 0.17692234 0.1144433  0.07791554 0.17801989 0.08329685\n",
      " 0.07055042 0.18181858 0.11637852 0.1411051  0.02822144 0.25405817\n",
      " 0.34130238 0.08473322 0.1516479  0.30152499 0.04000515 0.02959007\n",
      " 0.02987935 0.16647724 0.08309629 0.04233904 0.19130348 0.22926431\n",
      " 0.15061763 0.04308866 0.1221662 ]\n",
      "0.36620926243567753\n",
      "0.7581699346405228\n",
      "Confidence interval for the score: [0.366 - 0.758]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[9.09090909e-02 1.24401914e-01 0.00000000e+00 2.00892857e-02\n",
      " 0.00000000e+00 6.78733032e-02 1.93704600e-02 2.04373048e-01\n",
      " 6.06349206e-01 2.88461538e-03 6.92520776e-04 0.00000000e+00\n",
      " 1.21057986e-01 3.39805825e-02 7.90166813e-03 2.56191289e-03\n",
      " 1.34770889e-03 2.38473768e-02 1.11248966e-01 0.00000000e+00\n",
      " 4.72440945e-03 1.22448980e-01 1.66666667e-01 9.22882427e-02\n",
      " 2.33980012e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 7.54098361e-02 1.00214746e-02 2.79097387e-02\n",
      " 4.54545455e-03 6.45161290e-03 0.00000000e+00 1.92090395e-02\n",
      " 3.78787879e-03 5.53359684e-02 6.19469027e-02 6.41435242e-02\n",
      " 2.58823529e-02 0.00000000e+00 6.33937083e-02 1.47058824e-03\n",
      " 1.56250000e-02 1.19815668e-01 0.00000000e+00 0.00000000e+00\n",
      " 2.61635220e-01 1.40712946e-02 0.00000000e+00 1.00490196e-01\n",
      " 2.14873078e-01 7.78210117e-01 7.89473684e-02 0.00000000e+00\n",
      " 1.20048019e-03 2.75614140e-02 2.72727273e-01 2.68138801e-02\n",
      " 1.83827947e-02 1.22362869e-01 1.66944908e-03 0.00000000e+00\n",
      " 3.77763537e-01 5.03355705e-03 2.51732102e-01 7.05521472e-02\n",
      " 1.35968379e-01 1.30580357e-01 1.40845070e-02 1.01769912e-01\n",
      " 9.24214418e-03 1.39275766e-02 1.49310873e-01 3.25732899e-03\n",
      " 4.52830189e-02 6.77506775e-02 0.00000000e+00 2.81350482e-02\n",
      " 9.80163361e-02 9.15094340e-02 1.88087774e-03 1.18043845e-02\n",
      " 3.84615385e-02 5.69620253e-02 1.05263158e-02 2.89645185e-03\n",
      " 2.27848101e-02 3.24675325e-02 1.05411103e-02 4.44444444e-02\n",
      " 5.74293528e-02 3.16561146e-03 8.48343986e-02 3.46774194e-02\n",
      " 1.08597285e-02 1.52371342e-01 1.59489633e-02 3.55731225e-02\n",
      " 1.38504155e-02 2.25733634e-03 8.89621087e-02 2.22717149e-03\n",
      " 3.47826087e-02 2.32967033e-01 1.97183099e-02 0.00000000e+00\n",
      " 0.00000000e+00 1.10248447e-01 5.62645012e-02 9.55414013e-03\n",
      " 9.89887203e-02 2.10608424e-02 5.65217391e-02 0.00000000e+00\n",
      " 2.29357798e-02]\n",
      "0.4403292181069959\n",
      "0.7830065359477124\n",
      "Confidence interval for the score: [0.440 - 0.783]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.]\n",
      "[0.13905807 0.05011522 0.22106199 0.06084622 0.17887107 0.20067534\n",
      " 0.09686675 0.02302952 0.03209819 0.10336063 0.02120209 0.06846181\n",
      " 0.07497977 0.07228695 0.0285399  0.11999197 0.11597822 0.10624329\n",
      " 0.05361601 0.14116889 0.05132462 0.05005929 0.0482371  0.03028021\n",
      " 0.21475658 0.0743014  0.10625229 0.07388864 0.06009812 0.0577423\n",
      " 0.25381477 0.04768328 0.09161507 0.07609477 0.03281745 0.08669134\n",
      " 0.17179155 0.1375406  0.15034239 0.06936581 0.01609427 0.14030051\n",
      " 0.01683757 0.03817858 0.08720612 0.1270133  0.13166338 0.07522826\n",
      " 0.02420851 0.26906041 0.06497458 0.05944753 0.1262521  0.00994791\n",
      " 0.03959254 0.13852344 0.25243413 0.03462924 0.18405824 0.12931423\n",
      " 0.56661323 0.04385647 0.03298303 0.03725871 0.01514153 0.11133491\n",
      " 0.17956114 0.17873336 0.0821133  0.07786604 0.03429414 0.03463768\n",
      " 0.02534857 0.10023999 0.0474921  0.05636539 0.07596608 0.06655008\n",
      " 0.05099599 0.06154307 0.00694006 0.06727034 0.02914353 0.12362468\n",
      " 0.27062983 0.04526753 0.03953558 0.036105   0.22399551 0.07520264\n",
      " 0.22515337 0.076378   0.05682675 0.04151505 0.05003969 0.12049103\n",
      " 0.05200899 0.05247166 0.09640234 0.12526408 0.02306577 0.11081475\n",
      " 0.13359579 0.05998283 0.21373467 0.0237432  0.14063396 0.07668791\n",
      " 0.24044373 0.08169081 0.03127555 0.07996445 0.03774398 0.14385371\n",
      " 0.04092614 0.04806811 0.0187031 ]\n",
      "0.41054091539528437\n",
      "0.6817817014446228\n",
      "Confidence interval for the score: [0.411 - 0.682]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.]\n",
      "[0.         0.         0.         0.         0.02857143 0.00904977\n",
      " 0.01937046 0.00089246 0.0047619  0.01346154 0.00069252 0.\n",
      " 0.00915565 0.00832178 0.00087796 0.         0.01078167 0.00317965\n",
      " 0.0153019  0.01183432 0.         0.         0.00157233 0.00126422\n",
      " 0.08348031 0.02283654 0.         0.01923077 0.         0.01092896\n",
      " 0.12168933 0.00059382 0.00113636 0.03870968 0.         0.01016949\n",
      " 0.26136364 0.00395257 0.03893805 0.01179651 0.00117647 0.01298701\n",
      " 0.         0.00073529 0.015625   0.         0.03076923 0.\n",
      " 0.         0.19371482 0.         0.00980392 0.09545942 0.\n",
      " 0.         0.02235772 0.01620648 0.00479329 0.02392344 0.02839117\n",
      " 0.29184188 0.         0.         0.         0.00032041 0.\n",
      " 0.00692841 0.03067485 0.01818182 0.00558036 0.         0.\n",
      " 0.         0.20612813 0.         0.00325733 0.         0.00406504\n",
      " 0.00396825 0.00562701 0.         0.00660377 0.00062696 0.00168634\n",
      " 0.01282051 0.00090416 0.         0.00289645 0.13670886 0.02651515\n",
      " 0.03092059 0.         0.00820419 0.00066644 0.00116212 0.00241935\n",
      " 0.         0.00605449 0.         0.01844532 0.00277008 0.01128668\n",
      " 0.01317957 0.         0.0173913  0.         0.04319249 0.0112782\n",
      " 0.01694915 0.01708075 0.00232019 0.03397028 0.00136134 0.02808112\n",
      " 0.00652174 0.         0.        ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.346031746031746\n",
      "0.6231435643564356\n",
      "Confidence interval for the score: [0.346 - 0.623]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0.20650883 0.09150949 0.33132609 0.09019743 0.12375225 0.26627027\n",
      " 0.1508883  0.36613784 0.06301545 0.12371436 0.30976267 0.16367999\n",
      " 0.15288538 0.15788587 0.18492072 0.23478542 0.65657749 0.27147274\n",
      " 0.06962692 0.1569554  0.36758215 0.09596379 0.27092627 0.01771367\n",
      " 0.15168449 0.23924101 0.23879938 0.07830307 0.31124176 0.18209437\n",
      " 0.19023226 0.1750389  0.16009734 0.1765595  0.26955736 0.11822071\n",
      " 0.09730593 0.13339286 0.14163945 0.31464062 0.2442975  0.30955705\n",
      " 0.01220315 0.21736151 0.13334568 0.12375701 0.13076075 0.3385259\n",
      " 0.13056945 0.10290817 0.13068305 0.16544579 0.13102591 0.07961341\n",
      " 0.0643436  0.18101884 0.13527812 0.27340357 0.16022166 0.36401822\n",
      " 0.07473103 0.08662126 0.2183417  0.1309359  0.09426907 0.41588144\n",
      " 0.18674957 0.36636729 0.26725808 0.22050578 0.38619498 0.41897272\n",
      " 0.20292526 0.03911881 0.09437888 0.16107744 0.10145189 0.14383733\n",
      " 0.10165922 0.25692551 0.05062394 0.0859272  0.26315748 0.21420155\n",
      " 0.29531948 0.13608701 0.19491827 0.26445995 0.07547826 0.04307186\n",
      " 0.13944035 0.24990315 0.15840099 0.24281626 0.07308163 0.12140782\n",
      " 0.26748632 0.0494445  0.44986388 0.25630435 0.24025513 0.16550726\n",
      " 0.08408243 0.35496238 0.19253213 0.06611278 0.15616788 0.14875861\n",
      " 0.28513057 0.23976564 0.06943954 0.09350734 0.31676753 0.14569895\n",
      " 0.08665833 0.07916911 0.56217899]\n",
      "0.47446236559139787\n",
      "0.7065047021943573\n",
      "Confidence interval for the score: [0.474 - 0.707]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0.06060606 0.         0.05063291 0.00669643 0.02857143 0.0361991\n",
      " 0.07990315 0.12985274 0.00952381 0.01730769 0.03947368 0.\n",
      " 0.06917599 0.05755895 0.05443371 0.02561913 0.21293801 0.04928458\n",
      " 0.01406121 0.04733728 0.01417323 0.00709849 0.04559748 0.00252845\n",
      " 0.01881246 0.09134615 0.03571429 0.         0.00284091 0.06994536\n",
      " 0.04080172 0.0219715  0.         0.03096774 0.02673797 0.01468927\n",
      " 0.00757576 0.         0.01415929 0.12951585 0.10411765 0.15584416\n",
      " 0.00047664 0.03676471 0.03125    0.00691244 0.         0.\n",
      " 0.01132075 0.05018762 0.         0.04166667 0.02717197 0.07726515\n",
      " 0.         0.         0.00180072 0.1312163  0.01435407 0.08832808\n",
      " 0.0291962  0.01265823 0.00834725 0.         0.01602051 0.14261745\n",
      " 0.01847575 0.26993865 0.08458498 0.11216518 0.02816901 0.15486726\n",
      " 0.09242144 0.00278552 0.01148545 0.01465798 0.00754717 0.07317073\n",
      " 0.01190476 0.07395498 0.08051342 0.01037736 0.05454545 0.03709949\n",
      " 0.07692308 0.01537071 0.02105263 0.02317161 0.00379747 0.0021645\n",
      " 0.00210822 0.03333333 0.05834093 0.02265911 0.00232423 0.00564516\n",
      " 0.0199095  0.01715439 0.07814992 0.06060606 0.12650046 0.02708804\n",
      " 0.00164745 0.0935412  0.         0.00659341 0.08450704 0.\n",
      " 0.07118644 0.07298137 0.01218097 0.01167728 0.126021   0.00936037\n",
      " 0.03043478 0.01470588 0.25840979]\n",
      "0.49956521739130444\n",
      "0.7269317329332333\n",
      "Confidence interval for the score: [0.500 - 0.727]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
      "[0.09335179 0.42982502 0.16426035 0.11317439 0.3463525  0.18673301\n",
      " 0.11533123 0.03355324 0.10286665 0.10858778 0.24571442 0.07345552\n",
      " 0.09865079 0.23519129 0.04336583 0.19254745 0.06452145 0.22948089\n",
      " 0.23393756 0.10322476 0.01384575 0.05741793 0.04819529 0.01527391\n",
      " 0.26355224 0.04365533 0.21456249 0.01296406 0.20075397 0.16289244\n",
      " 0.04815523 0.19091949 0.06140194 0.07325769 0.04169368 0.06026002\n",
      " 0.1206699  0.02529181 0.09164187 0.12589118 0.1599496  0.38145014\n",
      " 0.0117685  0.03817338 0.06465966 0.10462676 0.22398833 0.07679331\n",
      " 0.12812058 0.06809977 0.16980285 0.12754483 0.0500238  0.04371245\n",
      " 0.16390069 0.17573037 0.08713931 0.09876798 0.10087183 0.15817703\n",
      " 0.05773329 0.15533213 0.04287934 0.03730654 0.02097692 0.09836276\n",
      " 0.093842   0.05679588 0.28638619 0.17867136 0.04014803 0.06464429\n",
      " 0.10188118 0.03316068 0.06284506 0.26472453 0.192848   0.0505418\n",
      " 0.50526896 0.08547525 0.04881158 0.0785201  0.42536856 0.33048866\n",
      " 0.27210715 0.07796565 0.04564547 0.13086913 0.04101196 0.06013103\n",
      " 0.12061362 0.09438007 0.29664339 0.13344188 0.114491   0.13302227\n",
      " 0.24254087 0.0439467  0.12722503 0.12343475 0.31829998 0.29653668\n",
      " 0.03622011 0.23221956 0.31699917 0.0205729  0.15136654 0.13590677\n",
      " 0.19300629 0.14508543 0.07086384 0.06561813 0.11442462 0.24130608\n",
      " 0.07556379 0.05296768 0.07302483]\n",
      "0.434156378600823\n",
      "0.7240099009900991\n",
      "Confidence interval for the score: [0.434 - 0.724]\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
      "[0.00000000e+00 2.15311005e-01 2.53164557e-02 2.23214286e-03\n",
      " 1.42857143e-02 4.52488688e-02 2.90556901e-02 3.56983490e-03\n",
      " 3.65079365e-02 6.15384615e-02 8.31024931e-03 1.92307692e-02\n",
      " 8.13835198e-03 2.84327323e-01 0.00000000e+00 1.45175064e-02\n",
      " 8.08625337e-03 9.22098569e-02 1.15798180e-02 0.00000000e+00\n",
      " 0.00000000e+00 5.32386868e-03 3.14465409e-03 0.00000000e+00\n",
      " 9.64138742e-02 6.00961538e-03 7.14285714e-02 0.00000000e+00\n",
      " 8.52272727e-03 2.67759563e-02 3.57909807e-03 1.84085511e-02\n",
      " 0.00000000e+00 1.61290323e-03 0.00000000e+00 3.38983051e-03\n",
      " 1.06060606e-01 0.00000000e+00 1.41592920e-02 4.54657164e-02\n",
      " 9.35294118e-02 1.68831169e-01 4.76644423e-04 6.61764706e-03\n",
      " 5.20833333e-03 2.30414747e-03 0.00000000e+00 0.00000000e+00\n",
      " 3.01886792e-02 5.62851782e-03 5.88235294e-02 2.20588235e-02\n",
      " 1.43010368e-03 1.44524736e-02 5.26315789e-02 1.21951220e-02\n",
      " 7.80312125e-03 2.03714799e-02 0.00000000e+00 4.02208202e-02\n",
      " 7.44923705e-03 3.79746835e-02 0.00000000e+00 0.00000000e+00\n",
      " 5.12656200e-03 6.71140940e-03 0.00000000e+00 4.60122699e-03\n",
      " 7.98418972e-02 1.11607143e-01 0.00000000e+00 8.84955752e-03\n",
      " 2.77264325e-02 2.22841226e-02 8.42266462e-03 1.48208469e-01\n",
      " 2.26415094e-02 1.21951220e-02 6.19047619e-01 2.65273312e-02\n",
      " 9.33488915e-03 2.35849057e-02 1.05329154e-01 1.68634064e-01\n",
      " 8.97435897e-02 5.42495479e-03 0.00000000e+00 2.38957277e-02\n",
      " 6.32911392e-03 7.03463203e-03 1.33520731e-02 0.00000000e+00\n",
      " 1.22151322e-01 1.83272243e-03 3.13771063e-02 4.43548387e-02\n",
      " 2.35294118e-02 2.01816347e-03 6.37958533e-03 1.77865613e-02\n",
      " 3.48107110e-01 2.25733634e-03 0.00000000e+00 3.11804009e-02\n",
      " 9.56521739e-02 2.19780220e-03 4.31924883e-02 3.75939850e-03\n",
      " 6.10169492e-02 1.47515528e-02 1.16009281e-02 3.18471338e-03\n",
      " 3.26721120e-02 1.01404056e-02 1.44927536e-02 0.00000000e+00\n",
      " 6.11620795e-03]\n",
      "0.472680412371134\n",
      "0.7379032258064515\n",
      "Confidence interval for the score: [0.473 - 0.738]\n",
      "[0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0.35241318 0.20316152 0.37713916 0.52153444 0.27699224 0.17584229\n",
      " 0.17329679 0.12531174 0.0965192  0.2146228  0.0774751  0.06979218\n",
      " 0.35670372 0.26857624 0.20967221 0.21762999 0.20631876 0.33336755\n",
      " 0.65806676 0.49258951 0.20393028 0.06749801 0.12613512 0.01810591\n",
      " 0.1261617  0.09708116 0.32963409 0.34705495 0.51291924 0.2451161\n",
      " 0.0958239  0.39769318 0.10842692 0.10774496 0.04052621 0.45553089\n",
      " 0.21373215 0.20757432 0.26767744 0.14587755 0.06395217 0.2862795\n",
      " 0.0187557  0.25287291 0.24171073 0.32254821 0.23373615 0.42926402\n",
      " 0.07498983 0.16728634 0.09872255 0.41826195 0.1130201  0.02881864\n",
      " 0.09863556 0.26546275 0.13636143 0.31071642 0.31589545 0.21492266\n",
      " 0.04732183 0.05042344 0.04383801 0.31203853 0.02390427 0.54427944\n",
      " 0.31636231 0.10864517 0.19504821 0.10564155 0.12217952 0.14984262\n",
      " 0.21097266 0.10950972 0.13334885 0.31525718 0.21659959 0.08137649\n",
      " 0.31952995 0.25065169 0.04353959 0.15979597 0.21367459 0.29878886\n",
      " 0.2990873  0.35447543 0.05712613 0.04111172 0.2475012  0.52700638\n",
      " 0.26970864 0.1868748  0.24735911 0.07755054 0.17494589 0.29032937\n",
      " 0.26929474 0.13757654 0.07852331 0.19753807 0.20024382 0.12177147\n",
      " 0.08615616 0.06235777 0.26048391 0.16448776 0.21781431 0.32385943\n",
      " 0.11246576 0.40183417 0.43352537 0.14283587 0.12832068 0.11319213\n",
      " 0.07923302 0.26519795 0.24418399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39173913043478265\n",
      "0.6729879740980573\n",
      "Confidence interval for the score: [0.392 - 0.673]\n",
      "[0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0.12121212 0.10047847 0.06329114 0.35491071 0.04285714 0.03167421\n",
      " 0.09443099 0.07853637 0.06031746 0.16346154 0.00623269 0.01923077\n",
      " 0.23397762 0.22884882 0.18612818 0.02476516 0.02560647 0.12082671\n",
      " 0.66873449 0.30769231 0.0015748  0.00177462 0.06603774 0.00126422\n",
      " 0.01410935 0.02403846 0.21428571 0.56730769 0.32102273 0.23060109\n",
      " 0.04795991 0.35154394 0.03295455 0.04548387 0.         0.51977401\n",
      " 0.28409091 0.02766798 0.21061947 0.01818629 0.02411765 0.09090909\n",
      " 0.00381316 0.12132353 0.06770833 0.14516129 0.15384615 0.04689864\n",
      " 0.01257862 0.09005629 0.         0.49754902 0.08401859 0.01667593\n",
      " 0.02631579 0.05487805 0.02160864 0.3487118  0.19138756 0.06072555\n",
      " 0.00804998 0.02109705 0.0033389  0.04651163 0.00192246 0.38087248\n",
      " 0.16859122 0.05751534 0.15019763 0.0546875  0.08450704 0.03982301\n",
      " 0.06099815 0.13370474 0.03905054 0.10749186 0.10188679 0.05284553\n",
      " 0.20634921 0.17524116 0.03267211 0.1509434  0.03260188 0.1079258\n",
      " 0.05128205 0.26311031 0.         0.00144823 0.18227848 0.63203463\n",
      " 0.17217147 0.07777778 0.08751139 0.00733089 0.14933178 0.16048387\n",
      " 0.03529412 0.0332997  0.00478469 0.07048748 0.18374885 0.01354402\n",
      " 0.01482702 0.01113586 0.07826087 0.16483516 0.16619718 0.31203008\n",
      " 0.00677966 0.38043478 0.58990719 0.05626327 0.02236484 0.0374415\n",
      " 0.03478261 0.13970588 0.1116208 ]\n",
      "0.37544802867383514\n",
      "0.6408529741863076\n",
      "Confidence interval for the score: [0.375 - 0.641]\n",
      "[1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0.\n",
      " 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "[0.47653974 0.45213632 0.58843816 0.44345216 0.18498882 0.50959859\n",
      " 0.14871243 0.39308235 0.10745902 0.22670289 0.30653565 0.36173174\n",
      " 0.29022558 0.25695296 0.28413192 0.60620864 0.78601017 0.27909554\n",
      " 0.41216781 0.37800117 0.64179791 0.23665985 0.45701325 0.52354683\n",
      " 0.34577393 0.40412222 0.31458557 0.18885841 0.33476911 0.28529203\n",
      " 0.52257971 0.28989255 0.34768644 0.43330094 0.20059561 0.27268246\n",
      " 0.17740311 0.34505736 0.27851519 0.18304052 0.1881611  0.39003714\n",
      " 0.6280574  0.34955448 0.17614689 0.26020757 0.50493811 0.8715614\n",
      " 0.38955072 0.47750588 0.29394275 0.2777524  0.25300628 0.0355297\n",
      " 0.11781923 0.47416899 0.53347647 0.37114117 0.36518002 0.40671996\n",
      " 0.63083652 0.2798468  0.31221463 0.55939391 0.10641772 0.46274648\n",
      " 0.36574637 0.43962629 0.16902081 0.22634526 0.4933522  0.51976195\n",
      " 0.22004591 0.02331305 0.57718114 0.42041098 0.28451892 0.38886874\n",
      " 0.25659312 0.34092506 0.36639007 0.22931331 0.72644896 0.38553693\n",
      " 0.40957104 0.21131801 0.54307871 0.09833569 0.18646926 0.04475885\n",
      " 0.39337353 0.25365073 0.4575336  0.52603655 0.20449272 0.41987298\n",
      " 0.59556878 0.16795264 0.5334504  0.5445544  0.06829888 0.59431993\n",
      " 0.19938244 0.39831873 0.35298352 0.06219031 0.12417784 0.33312758\n",
      " 0.42616249 0.38179641 0.25866076 0.35239261 0.1797355  0.57487452\n",
      " 0.36734447 0.29111332 0.63837822]\n",
      "0.44370419720186544\n",
      "0.6591070163004962\n",
      "Confidence interval for the score: [0.444 - 0.659]\n",
      "[1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0.\n",
      " 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      "[0.06060606 0.215311   0.37974684 0.18303571 0.02857143 0.2081448\n",
      " 0.031477   0.14859438 0.06507937 0.04038462 0.07548476 0.03846154\n",
      " 0.08138352 0.19764216 0.16856892 0.27583262 0.39218329 0.05564388\n",
      " 0.08933002 0.13609467 0.16535433 0.06743567 0.3081761  0.87610619\n",
      " 0.23633157 0.16646635 0.03571429 0.04807692 0.01136364 0.32240437\n",
      " 0.56621331 0.07125891 0.03636364 0.24419355 0.00534759 0.09152542\n",
      " 0.03787879 0.09090909 0.08141593 0.04915213 0.06882353 0.18181818\n",
      " 0.89180172 0.21470588 0.0625     0.05990783 0.30769231 0.81543116\n",
      " 0.28176101 0.40337711 0.35294118 0.1372549  0.22881659 0.03057254\n",
      " 0.18421053 0.26422764 0.2605042  0.31276213 0.04784689 0.20741325\n",
      " 0.20641596 0.092827   0.07512521 0.43023256 0.03684716 0.17785235\n",
      " 0.12933025 0.34125767 0.04901186 0.14508929 0.26760563 0.30530973\n",
      " 0.07948244 0.00278552 0.17075038 0.13029316 0.03773585 0.31842818\n",
      " 0.04365079 0.10691318 0.58693116 0.11698113 0.6815047  0.10118044\n",
      " 0.18589744 0.04159132 0.25263158 0.00506879 0.02658228 0.0021645\n",
      " 0.16022488 0.07777778 0.39927074 0.14661779 0.07553748 0.1766129\n",
      " 0.24253394 0.11099899 0.18660287 0.49670619 0.02031394 0.16478555\n",
      " 0.00494234 0.13808463 0.07826087 0.         0.03192488 0.10150376\n",
      " 0.16949153 0.17468944 0.16415313 0.16242038 0.04122909 0.28549142\n",
      " 0.1826087  0.21323529 0.38379205]\n",
      "0.4528475199020209\n",
      "0.6552482715273412\n",
      "Confidence interval for the score: [0.453 - 0.655]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      "[0.14020275 0.06822929 0.07762845 0.075179   0.07703696 0.07385473\n",
      " 0.06278613 0.07571171 0.02432854 0.04664078 0.35211058 0.46384297\n",
      " 0.13203796 0.02745295 0.02655978 0.16981534 0.04010097 0.07396029\n",
      " 0.03923639 0.06401853 0.09192114 0.12396238 0.05059532 0.02396292\n",
      " 0.01769918 0.07167179 0.17074634 0.15090638 0.07396216 0.04839013\n",
      " 0.08490179 0.07625967 0.34765534 0.14524203 0.32489716 0.1248081\n",
      " 0.05779109 0.24065111 0.13709816 0.11409202 0.05699785 0.08081274\n",
      " 0.03138009 0.2372393  0.02024058 0.05144189 0.17625266 0.14952485\n",
      " 0.0379504  0.04792103 0.14220253 0.03914263 0.05292635 0.03280996\n",
      " 0.115495   0.09490569 0.18251539 0.0828782  0.06450738 0.12478981\n",
      " 0.0556248  0.01785325 0.27079683 0.13411531 0.1325201  0.02818697\n",
      " 0.05303786 0.04668748 0.03159449 0.05198797 0.11736687 0.11086239\n",
      " 0.06039335 0.0677824  0.02699712 0.09494257 0.07578193 0.04414815\n",
      " 0.03455169 0.104277   0.19079195 0.06118459 0.06597695 0.06084919\n",
      " 0.05545562 0.01325922 0.43333316 0.30577122 0.03110823 0.16685912\n",
      " 0.13524094 0.14346698 0.0610837  0.25963202 0.05148019 0.05657419\n",
      " 0.11767345 0.03663812 0.08965315 0.12138361 0.02267168 0.06195519\n",
      " 0.26758543 0.10855595 0.15649507 0.23553922 0.04500237 0.12712783\n",
      " 0.07974623 0.03736729 0.04628382 0.07075408 0.09181908 0.11452803\n",
      " 0.03939119 0.11072206 0.06043737]\n",
      "0.33333333333333337\n",
      "0.623444976076555\n",
      "Confidence interval for the score: [0.333 - 0.623]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      "[0.         0.01435407 0.01265823 0.         0.05714286 0.00452489\n",
      " 0.01210654 0.00178492 0.         0.00096154 0.09833795 0.19230769\n",
      " 0.02136317 0.         0.00087796 0.01964133 0.00134771 0.00794913\n",
      " 0.00124069 0.         0.01574803 0.02750665 0.         0.00252845\n",
      " 0.         0.0078125  0.         0.16346154 0.00568182 0.00273224\n",
      " 0.01360057 0.00356295 0.07159091 0.03483871 0.1657754  0.01694915\n",
      " 0.01893939 0.05928854 0.03893805 0.0693045  0.03117647 0.02597403\n",
      " 0.01811249 0.12279412 0.         0.         0.10769231 0.\n",
      " 0.02389937 0.00422139 0.17647059 0.         0.00357526 0.00444691\n",
      " 0.21052632 0.         0.0060024  0.02696225 0.         0.00394322\n",
      " 0.00756939 0.00421941 0.04006678 0.         0.06856777 0.\n",
      " 0.00230947 0.01150307 0.00237154 0.0078125  0.02816901 0.\n",
      " 0.01848429 0.22841226 0.         0.         0.         0.01219512\n",
      " 0.         0.01848875 0.04317386 0.01415094 0.00125392 0.00168634\n",
      " 0.00641026 0.         0.10526316 0.07168718 0.00126582 0.15530303\n",
      " 0.01756852 0.06666667 0.00546946 0.07764079 0.00813481 0.00967742\n",
      " 0.00271493 0.00201816 0.00478469 0.01976285 0.01108033 0.00677201\n",
      " 0.09555189 0.01781737 0.         0.18241758 0.02159624 0.05639098\n",
      " 0.02711864 0.         0.00348028 0.00424628 0.02781019 0.04290172\n",
      " 0.01086957 0.         0.00152905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4014336917562724\n",
      "0.6794258373205742\n",
      "Confidence interval for the score: [0.401 - 0.679]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0.12201863 0.03404547 0.35480504 0.12670434 0.03025061 0.10947427\n",
      " 0.10288379 0.01842884 0.02786264 0.07867705 0.04479572 0.04540278\n",
      " 0.08335484 0.02727605 0.1130535  0.439397   0.03125061 0.31017346\n",
      " 0.23615088 0.16247722 0.16968008 0.11905526 0.13274846 0.00918072\n",
      " 0.04196566 0.10418805 0.06162455 0.12283204 0.01520883 0.0084075\n",
      " 0.10503008 0.03544306 0.0613114  0.07083667 0.05119744 0.1122107\n",
      " 0.01046987 0.1236725  0.07481319 0.10526616 0.09661186 0.16802033\n",
      " 0.01714818 0.23130913 0.03694158 0.12390181 0.01723303 0.49109287\n",
      " 0.05831394 0.10369449 0.02780435 0.09106368 0.10679011 0.03715844\n",
      " 0.0397373  0.02526617 0.14677941 0.04818135 0.25554815 0.03826143\n",
      " 0.02164806 0.34506682 0.082734   0.08335818 0.14840648 0.17232399\n",
      " 0.18513249 0.13791789 0.22898807 0.12858776 0.03106839 0.02878454\n",
      " 0.0980946  0.00683647 0.01732967 0.11931913 0.24419417 0.06030171\n",
      " 0.04542121 0.2307281  0.02737055 0.06170099 0.01883957 0.0963033\n",
      " 0.3521623  0.07071216 0.06354707 0.13006149 0.05810389 0.06428852\n",
      " 0.08083086 0.14714918 0.07833324 0.07055454 0.07701361 0.37874606\n",
      " 0.21201646 0.02749394 0.12878526 0.13222887 0.16862186 0.43233643\n",
      " 0.0541546  0.15976466 0.23416352 0.0388898  0.02430024 0.03585606\n",
      " 0.05425508 0.05909777 0.13105982 0.06977496 0.12624146 0.3249709\n",
      " 0.05927495 0.09309426 0.02564978]\n",
      "0.4253859348198971\n",
      "0.6718106995884774\n",
      "Confidence interval for the score: [0.425 - 0.672]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0.00000000e+00 0.00000000e+00 8.86075949e-02 2.23214286e-03\n",
      " 0.00000000e+00 4.52488688e-03 1.93704600e-02 0.00000000e+00\n",
      " 1.58730159e-03 4.80769231e-03 6.92520776e-04 0.00000000e+00\n",
      " 2.03458800e-03 6.93481276e-04 5.17998244e-02 2.01537148e-01\n",
      " 0.00000000e+00 1.28775835e-01 2.93631100e-02 3.55029586e-02\n",
      " 0.00000000e+00 1.86335404e-02 1.88679245e-02 0.00000000e+00\n",
      " 2.93944738e-03 2.10336538e-02 0.00000000e+00 1.92307692e-02\n",
      " 0.00000000e+00 0.00000000e+00 1.14531138e-02 0.00000000e+00\n",
      " 0.00000000e+00 1.35483871e-02 5.34759358e-03 1.80790960e-02\n",
      " 0.00000000e+00 0.00000000e+00 1.76991150e-03 1.37625952e-02\n",
      " 4.58823529e-02 2.59740260e-02 9.53288847e-04 5.73529412e-02\n",
      " 0.00000000e+00 4.60829493e-03 0.00000000e+00 2.72314675e-02\n",
      " 5.03144654e-03 6.09756098e-03 0.00000000e+00 2.20588235e-02\n",
      " 4.18305327e-02 1.00055586e-02 0.00000000e+00 0.00000000e+00\n",
      " 1.08043217e-02 0.00000000e+00 9.56937799e-03 0.00000000e+00\n",
      " 1.20148985e-04 3.24894515e-01 3.33889816e-03 0.00000000e+00\n",
      " 6.98494072e-02 3.35570470e-03 2.30946882e-03 1.76380368e-02\n",
      " 7.58893281e-02 3.12500000e-02 0.00000000e+00 0.00000000e+00\n",
      " 5.54528651e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 4.15094340e-02 9.48509485e-03 0.00000000e+00 5.38585209e-02\n",
      " 1.16686114e-03 9.43396226e-04 6.26959248e-04 1.68634064e-03\n",
      " 1.98717949e-01 9.04159132e-04 0.00000000e+00 7.24112962e-03\n",
      " 7.59493671e-03 4.32900433e-03 4.21644413e-03 2.22222222e-02\n",
      " 2.73473108e-02 3.83205598e-03 5.81057525e-04 2.37096774e-01\n",
      " 2.08144796e-02 0.00000000e+00 1.59489633e-03 2.43741765e-02\n",
      " 4.06278855e-02 3.61173815e-02 1.64744646e-03 4.45434298e-02\n",
      " 7.82608696e-02 2.19780220e-03 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 3.88198758e-03 4.06032483e-02 8.49256900e-03\n",
      " 2.68378063e-02 2.57410296e-02 5.79710145e-03 7.35294118e-03\n",
      " 1.52905199e-03]\n",
      "0.4876543209876543\n",
      "0.7374260355029586\n",
      "Confidence interval for the score: [0.488 - 0.737]\n",
      "[1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0.10788667 0.0927681  0.10331939 0.25498421 0.47400342 0.06983023\n",
      " 0.19950531 0.02115801 0.13667843 0.08196519 0.20681979 0.16153799\n",
      " 0.27096276 0.02737862 0.123383   0.15130775 0.02585163 0.15897046\n",
      " 0.23180222 0.23991103 0.48411687 0.30546172 0.05966272 0.02627213\n",
      " 0.3384915  0.09002143 0.18252306 0.07703283 0.062976   0.11003738\n",
      " 0.02743539 0.09383251 0.21973443 0.11606552 0.24163558 0.14700972\n",
      " 0.0914658  0.16459533 0.13965307 0.056187   0.05722558 0.09339974\n",
      " 0.0131633  0.29621662 0.03344023 0.07768436 0.10952016 0.34379381\n",
      " 0.08236814 0.03114514 0.1903328  0.09301691 0.07856843 0.03584138\n",
      " 0.15248756 0.26917229 0.0441875  0.03747051 0.06853591 0.09648332\n",
      " 0.04036885 0.02789571 0.15354979 0.35137297 0.14102646 0.04506151\n",
      " 0.04892434 0.02574297 0.31041474 0.13851194 0.07274941 0.05227568\n",
      " 0.07417995 0.10375545 0.03697402 0.09695201 0.13815001 0.02303596\n",
      " 0.09584112 0.09448482 0.03848471 0.05458812 0.08049723 0.13146481\n",
      " 0.12703457 0.03865522 0.07745335 0.05558296 0.03695787 0.05481078\n",
      " 0.09236121 0.17718855 0.26837854 0.15457788 0.05650014 0.03304661\n",
      " 0.11947046 0.10064577 0.0920136  0.05624821 0.21372714 0.06668605\n",
      " 0.24964154 0.32055997 0.11804776 0.05477607 0.17921427 0.19578401\n",
      " 0.27317511 0.1048519  0.05927961 0.08203032 0.06365158 0.05599793\n",
      " 0.03008304 0.31690647 0.07603687]\n",
      "0.44500561167227837\n",
      "0.700277520814061\n",
      "Confidence interval for the score: [0.445 - 0.7]\n",
      "[1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0.         0.00478469 0.01265823 0.05133929 0.57142857 0.00452489\n",
      " 0.15254237 0.00133869 0.0984127  0.04903846 0.02354571 0.05769231\n",
      " 0.15971516 0.         0.04741001 0.00256191 0.         0.00953895\n",
      " 0.00661704 0.10650888 0.01102362 0.11801242 0.01257862 0.01643489\n",
      " 0.25514403 0.02764423 0.03571429 0.02884615 0.         0.01092896\n",
      " 0.00071582 0.0023753  0.03863636 0.02903226 0.02673797 0.01581921\n",
      " 0.04166667 0.00395257 0.05486726 0.00565249 0.02117647 0.\n",
      " 0.         0.28897059 0.         0.         0.01538462 0.00151286\n",
      " 0.01383648 0.00703565 0.11764706 0.00245098 0.06649982 0.03390773\n",
      " 0.18421053 0.11788618 0.00120048 0.00239664 0.         0.01498423\n",
      " 0.00841043 0.00421941 0.01001669 0.08139535 0.16052547 0.\n",
      " 0.         0.00076687 0.20948617 0.04575893 0.         0.\n",
      " 0.00739372 0.30362117 0.00306279 0.01140065 0.03018868 0.00271003\n",
      " 0.00396825 0.04501608 0.00466744 0.01509434 0.00188088 0.01854975\n",
      " 0.         0.00542495 0.01052632 0.0166546  0.00886076 0.0275974\n",
      " 0.00632467 0.05555556 0.13947129 0.01632789 0.01104009 0.00080645\n",
      " 0.00633484 0.03733602 0.00318979 0.00329381 0.06001847 0.01128668\n",
      " 0.02306425 0.21380846 0.00869565 0.0021978  0.05633803 0.04511278\n",
      " 0.1559322  0.0015528  0.0075406  0.05732484 0.00991832 0.00390016\n",
      " 0.00289855 0.30882353 0.0030581 ]\n",
      "0.3354735152487961\n",
      "0.6265673981191223\n",
      "Confidence interval for the score: [0.335 - 0.627]\n",
      "[1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1.\n",
      " 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0.]\n",
      "[0.60435443 0.46729642 0.50550957 0.51143611 0.2102201  0.64058905\n",
      " 0.38141072 0.55832979 0.17507959 0.44553988 0.74155078 0.62804659\n",
      " 0.40313297 0.23606391 0.41127179 0.62614055 0.66918877 0.50126522\n",
      " 0.09059698 0.46543954 0.81466706 0.57685975 0.49152574 0.03108417\n",
      " 0.15235875 0.56926519 0.49441219 0.16208678 0.63750964 0.23735233\n",
      " 0.36842311 0.46652002 0.6961607  0.54943773 0.67166535 0.34840115\n",
      " 0.2748823  0.6770278  0.41578819 0.51265542 0.43868805 0.46418607\n",
      " 0.04051313 0.38137678 0.61542984 0.52119257 0.50403685 0.80405601\n",
      " 0.31667451 0.41376914 0.24960337 0.21063373 0.26914908 0.07154965\n",
      " 0.18984287 0.52556607 0.7128134  0.24662216 0.59190276 0.56812312\n",
      " 0.6830471  0.36196869 0.70616378 0.605429   0.2846984  0.3960887\n",
      " 0.5436593  0.31552066 0.25326699 0.29363061 0.54419916 0.58705439\n",
      " 0.46530721 0.09820245 0.75615685 0.56504785 0.52012405 0.44847291\n",
      " 0.23030998 0.46829461 0.26527634 0.42066767 0.49183362 0.63273696\n",
      " 0.50677432 0.54539561 0.63434181 0.72784946 0.48116599 0.17139465\n",
      " 0.5756615  0.51375703 0.23219544 0.70340132 0.47958681 0.447153\n",
      " 0.72513531 0.43270033 0.67153528 0.42466579 0.18316543 0.74337765\n",
      " 0.65004322 0.50489035 0.54282377 0.39033223 0.38563188 0.41346653\n",
      " 0.56224653 0.28119124 0.15124993 0.53807784 0.5423973  0.61418006\n",
      " 0.56776539 0.30399846 0.51903651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4213909994155465\n",
      "0.6338398597311513\n",
      "Confidence interval for the score: [0.421 - 0.634]\n",
      "[1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1.\n",
      " 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0.]\n",
      "[0.66666667 0.32535885 0.36708861 0.37946429 0.22857143 0.58823529\n",
      " 0.54237288 0.43105756 0.11746032 0.64615385 0.7465374  0.67307692\n",
      " 0.29399797 0.18862691 0.48200176 0.43296328 0.34770889 0.50874404\n",
      " 0.05252275 0.35502959 0.78740157 0.63176575 0.37735849 0.00758534\n",
      " 0.05878895 0.6328125  0.60714286 0.15384615 0.65056818 0.25027322\n",
      " 0.18396564 0.5023753  0.81477273 0.55516129 0.77005348 0.29039548\n",
      " 0.23863636 0.75889328 0.48318584 0.5930204  0.58411765 0.33766234\n",
      " 0.02097235 0.14926471 0.80208333 0.66129032 0.38461538 0.10892587\n",
      " 0.35974843 0.22560976 0.29411765 0.16666667 0.23632463 0.03446359\n",
      " 0.26315789 0.52845528 0.67286915 0.12522469 0.44019139 0.52917981\n",
      " 0.42256398 0.37974684 0.85809683 0.44186047 0.26305671 0.28355705\n",
      " 0.42032333 0.19555215 0.1944664  0.35546875 0.57746479 0.38938053\n",
      " 0.6987061  0.08635097 0.6179173  0.58143322 0.71320755 0.44715447\n",
      " 0.11111111 0.46623794 0.14352392 0.56981132 0.11974922 0.54974705\n",
      " 0.33974359 0.61030741 0.6        0.84503983 0.60379747 0.11038961\n",
      " 0.58257203 0.62222222 0.09480401 0.71992669 0.63567693 0.32822581\n",
      " 0.63800905 0.63874874 0.69856459 0.25296443 0.19298246 0.72460497\n",
      " 0.75617792 0.44766147 0.60869565 0.40659341 0.53333333 0.46992481\n",
      " 0.49152542 0.22437888 0.11194896 0.65286624 0.61279658 0.53588144\n",
      " 0.65507246 0.31617647 0.21100917]\n",
      "0.3855386416861826\n",
      "0.6038461538461538\n",
      "Confidence interval for the score: [0.386 - 0.604]\n",
      "[0. 1. 1. ... 0. 1. 0.]\n",
      "[0.1834793  0.13905807 0.20650883 ... 0.02564978 0.07603687 0.51903651]\n",
      "0.6005472752678156\n",
      "0.6808358835058281\n",
      "Confidence interval for the score: [0.601 - 0.681]\n",
      "1170 1170\n",
      "[0. 1. 1. ... 0. 1. 0.]\n",
      "[0.09090909 0.         0.06060606 ... 0.00152905 0.0030581  0.21100917]\n",
      "0.5990879718271335\n",
      "0.6799735245449531\n",
      "Confidence interval for the score: [0.599 - 0.68]\n",
      "[[0. 1. 1. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 1. 0.]]\n",
      "[[0.1834793  0.13905807 0.20650883 ... 0.12201863 0.10788667 0.60435443]\n",
      " [0.12727461 0.05011522 0.09150949 ... 0.03404547 0.0927681  0.46729642]\n",
      " [0.07674763 0.22106199 0.33132609 ... 0.35480504 0.10331939 0.50550957]\n",
      " ...\n",
      " [0.15061763 0.04092614 0.08665833 ... 0.05927495 0.03008304 0.56776539]\n",
      " [0.04308866 0.04806811 0.07916911 ... 0.09309426 0.31690647 0.30399846]\n",
      " [0.1221662  0.0187031  0.56217899 ... 0.02564978 0.07603687 0.51903651]]\n",
      "0.5039151005918298\n",
      "0.5943943437302681\n",
      "Confidence interval for the score: [0.504 - 0.594]\n",
      "[[0. 1. 1. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 1. 0.]]\n",
      "[[0.09090909 0.         0.06060606 ... 0.         0.         0.66666667]\n",
      " [0.12440191 0.         0.         ... 0.         0.00478469 0.32535885]\n",
      " [0.         0.         0.05063291 ... 0.08860759 0.01265823 0.36708861]\n",
      " ...\n",
      " [0.05652174 0.00652174 0.03043478 ... 0.0057971  0.00289855 0.65507246]\n",
      " [0.         0.         0.01470588 ... 0.00735294 0.30882353 0.31617647]\n",
      " [0.02293578 0.         0.25840979 ... 0.00152905 0.0030581  0.21100917]]\n",
      "0.5069148198810017\n",
      "0.5943172739816703\n",
      "Confidence interval for the score: [0.507 - 0.594]\n",
      "******* FP / TP for average probability\n",
      "{0: array([0.        , 0.        , 0.        , 0.00980392, 0.00980392,\n",
      "       0.06862745, 0.06862745, 0.18627451, 0.18627451, 0.25490196,\n",
      "       0.25490196, 0.54901961, 0.54901961, 0.59803922, 0.59803922,\n",
      "       0.7254902 , 0.7254902 , 0.8627451 , 0.8627451 , 0.87254902,\n",
      "       0.87254902, 0.88235294, 0.88235294, 0.96078431, 0.96078431,\n",
      "       1.        ]), 1: array([0.        , 0.01041667, 0.01041667, 0.09375   , 0.09375   ,\n",
      "       0.13541667, 0.13541667, 0.14583333, 0.14583333, 0.15625   ,\n",
      "       0.15625   , 0.16666667, 0.16666667, 0.21875   , 0.21875   ,\n",
      "       0.30208333, 0.30208333, 0.32291667, 0.32291667, 0.52083333,\n",
      "       0.52083333, 0.54166667, 0.54166667, 0.60416667, 0.60416667,\n",
      "       0.66666667, 0.66666667, 0.70833333, 0.70833333, 0.71875   ,\n",
      "       0.71875   , 0.75      , 0.75      , 0.78125   , 0.78125   ,\n",
      "       0.8125    , 0.8125    , 0.94791667, 0.94791667, 1.        ]), 2: array([0.        , 0.        , 0.01123596, 0.01123596, 0.07865169,\n",
      "       0.07865169, 0.11235955, 0.11235955, 0.12359551, 0.12359551,\n",
      "       0.14606742, 0.14606742, 0.31460674, 0.31460674, 0.3258427 ,\n",
      "       0.3258427 , 0.35955056, 0.35955056, 0.37078652, 0.37078652,\n",
      "       0.38202247, 0.38202247, 0.39325843, 0.39325843, 0.40449438,\n",
      "       0.40449438, 0.41573034, 0.41573034, 0.43820225, 0.43820225,\n",
      "       0.56179775, 0.56179775, 0.61797753, 0.61797753, 0.70786517,\n",
      "       0.70786517, 0.71910112, 0.71910112, 0.76404494, 0.76404494,\n",
      "       0.80898876, 0.80898876, 0.87640449, 0.87640449, 0.98876404,\n",
      "       0.98876404, 1.        ]), 3: array([0.        , 0.        , 0.        , 0.03157895, 0.03157895,\n",
      "       0.04210526, 0.04210526, 0.14736842, 0.14736842, 0.21052632,\n",
      "       0.21052632, 0.22105263, 0.22105263, 0.24210526, 0.24210526,\n",
      "       0.26315789, 0.26315789, 0.34736842, 0.34736842, 0.38947368,\n",
      "       0.38947368, 0.4       , 0.4       , 0.57894737, 0.57894737,\n",
      "       0.67368421, 0.67368421, 0.75789474, 0.75789474, 0.76842105,\n",
      "       0.76842105, 0.85263158, 0.85263158, 0.89473684, 0.89473684,\n",
      "       0.96842105, 0.96842105, 1.        ]), 4: array([0.        , 0.01075269, 0.04301075, 0.04301075, 0.07526882,\n",
      "       0.07526882, 0.08602151, 0.08602151, 0.24731183, 0.24731183,\n",
      "       0.25806452, 0.25806452, 0.27956989, 0.27956989, 0.29032258,\n",
      "       0.29032258, 0.30107527, 0.30107527, 0.35483871, 0.35483871,\n",
      "       0.53763441, 0.53763441, 0.75268817, 0.75268817, 0.77419355,\n",
      "       0.77419355, 0.79569892, 0.79569892, 0.86021505, 0.86021505,\n",
      "       0.89247312, 0.89247312, 0.90322581, 0.90322581, 0.91397849,\n",
      "       0.91397849, 1.        , 1.        ]), 5: array([0.        , 0.01315789, 0.02631579, 0.02631579, 0.09210526,\n",
      "       0.09210526, 0.10526316, 0.10526316, 0.13157895, 0.13157895,\n",
      "       0.14473684, 0.14473684, 0.19736842, 0.19736842, 0.22368421,\n",
      "       0.22368421, 0.23684211, 0.23684211, 0.34210526, 0.34210526,\n",
      "       0.40789474, 0.40789474, 0.42105263, 0.42105263, 0.43421053,\n",
      "       0.43421053, 0.48684211, 0.48684211, 0.52631579, 0.52631579,\n",
      "       0.55263158, 0.55263158, 0.59210526, 0.59210526, 0.61842105,\n",
      "       0.61842105, 0.64473684, 0.64473684, 0.65789474, 0.65789474,\n",
      "       0.68421053, 0.68421053, 0.71052632, 0.71052632, 0.78947368,\n",
      "       0.78947368, 0.80263158, 0.80263158, 0.90789474, 0.90789474,\n",
      "       0.92105263, 0.92105263, 1.        ]), 6: array([0.  , 0.01, 0.03, 0.03, 0.06, 0.06, 0.26, 0.26, 0.31, 0.31, 0.35,\n",
      "       0.35, 0.41, 0.41, 0.5 , 0.5 , 0.55, 0.55, 0.63, 0.63, 0.66, 0.66,\n",
      "       0.67, 0.67, 0.7 , 0.7 , 0.76, 0.76, 0.79, 0.79, 0.91, 0.91, 0.93,\n",
      "       0.93, 1.  ]), 7: array([0.        , 0.00990099, 0.11881188, 0.11881188, 0.22772277,\n",
      "       0.22772277, 0.24752475, 0.24752475, 0.28712871, 0.28712871,\n",
      "       0.30693069, 0.30693069, 0.38613861, 0.38613861, 0.44554455,\n",
      "       0.44554455, 0.45544554, 0.45544554, 0.4950495 , 0.4950495 ,\n",
      "       0.61386139, 0.61386139, 0.62376238, 0.62376238, 0.63366337,\n",
      "       0.63366337, 0.66336634, 0.66336634, 0.91089109, 0.91089109,\n",
      "       1.        ]), 8: array([0.        , 0.01041667, 0.01041667, 0.10416667, 0.10416667,\n",
      "       0.19791667, 0.19791667, 0.21875   , 0.21875   , 0.22916667,\n",
      "       0.22916667, 0.27083333, 0.27083333, 0.33333333, 0.33333333,\n",
      "       0.39583333, 0.39583333, 0.51041667, 0.51041667, 0.60416667,\n",
      "       0.60416667, 0.61458333, 0.61458333, 0.625     , 0.625     ,\n",
      "       0.78125   , 0.78125   , 0.80208333, 0.80208333, 0.98958333,\n",
      "       0.98958333, 1.        ]), 9: array([0.        , 0.01666667, 0.01666667, 0.03333333, 0.03333333,\n",
      "       0.06666667, 0.06666667, 0.08333333, 0.08333333, 0.1       ,\n",
      "       0.1       , 0.11666667, 0.11666667, 0.15      , 0.15      ,\n",
      "       0.18333333, 0.18333333, 0.2       , 0.2       , 0.25      ,\n",
      "       0.25      , 0.28333333, 0.28333333, 0.3       , 0.3       ,\n",
      "       0.31666667, 0.31666667, 0.33333333, 0.33333333, 0.38333333,\n",
      "       0.38333333, 0.43333333, 0.43333333, 0.45      , 0.45      ,\n",
      "       0.55      , 0.55      , 0.56666667, 0.56666667, 0.58333333,\n",
      "       0.58333333, 0.63333333, 0.63333333, 0.73333333, 0.73333333,\n",
      "       0.76666667, 0.76666667, 0.78333333, 0.78333333, 0.81666667,\n",
      "       0.81666667, 0.83333333, 0.83333333, 0.93333333, 0.93333333,\n",
      "       0.95      , 0.95      , 1.        , 1.        ]), 'macro': array([0.        , 0.00980392, 0.00990099, 0.01      , 0.01041667,\n",
      "       0.01075269, 0.01123596, 0.01315789, 0.01666667, 0.02631579,\n",
      "       0.03      , 0.03157895, 0.03333333, 0.04210526, 0.04301075,\n",
      "       0.06      , 0.06666667, 0.06862745, 0.07526882, 0.07865169,\n",
      "       0.08333333, 0.08602151, 0.09210526, 0.09375   , 0.1       ,\n",
      "       0.10416667, 0.10526316, 0.11235955, 0.11666667, 0.11881188,\n",
      "       0.12359551, 0.13157895, 0.13541667, 0.14473684, 0.14583333,\n",
      "       0.14606742, 0.14736842, 0.15      , 0.15625   , 0.16666667,\n",
      "       0.18333333, 0.18627451, 0.19736842, 0.19791667, 0.2       ,\n",
      "       0.21052632, 0.21875   , 0.22105263, 0.22368421, 0.22772277,\n",
      "       0.22916667, 0.23684211, 0.24210526, 0.24731183, 0.24752475,\n",
      "       0.25      , 0.25490196, 0.25806452, 0.26      , 0.26315789,\n",
      "       0.27083333, 0.27956989, 0.28333333, 0.28712871, 0.29032258,\n",
      "       0.3       , 0.30107527, 0.30208333, 0.30693069, 0.31      ,\n",
      "       0.31460674, 0.31666667, 0.32291667, 0.3258427 , 0.33333333,\n",
      "       0.34210526, 0.34736842, 0.35      , 0.35483871, 0.35955056,\n",
      "       0.37078652, 0.38202247, 0.38333333, 0.38613861, 0.38947368,\n",
      "       0.39325843, 0.39583333, 0.4       , 0.40449438, 0.40789474,\n",
      "       0.41      , 0.41573034, 0.42105263, 0.43333333, 0.43421053,\n",
      "       0.43820225, 0.44554455, 0.45      , 0.45544554, 0.48684211,\n",
      "       0.4950495 , 0.5       , 0.51041667, 0.52083333, 0.52631579,\n",
      "       0.53763441, 0.54166667, 0.54901961, 0.55      , 0.55263158,\n",
      "       0.56179775, 0.56666667, 0.57894737, 0.58333333, 0.59210526,\n",
      "       0.59803922, 0.60416667, 0.61386139, 0.61458333, 0.61797753,\n",
      "       0.61842105, 0.62376238, 0.625     , 0.63      , 0.63333333,\n",
      "       0.63366337, 0.64473684, 0.65789474, 0.66      , 0.66336634,\n",
      "       0.66666667, 0.67      , 0.67368421, 0.68421053, 0.7       ,\n",
      "       0.70786517, 0.70833333, 0.71052632, 0.71875   , 0.71910112,\n",
      "       0.7254902 , 0.73333333, 0.75      , 0.75268817, 0.75789474,\n",
      "       0.76      , 0.76404494, 0.76666667, 0.76842105, 0.77419355,\n",
      "       0.78125   , 0.78333333, 0.78947368, 0.79      , 0.79569892,\n",
      "       0.80208333, 0.80263158, 0.80898876, 0.8125    , 0.81666667,\n",
      "       0.83333333, 0.85263158, 0.86021505, 0.8627451 , 0.87254902,\n",
      "       0.87640449, 0.88235294, 0.89247312, 0.89473684, 0.90322581,\n",
      "       0.90789474, 0.91      , 0.91089109, 0.91397849, 0.92105263,\n",
      "       0.93      , 0.93333333, 0.94791667, 0.95      , 0.96078431,\n",
      "       0.96842105, 0.98876404, 0.98958333, 1.        ]), 'micro': array([0.        , 0.00110132, 0.00220264, 0.00220264, 0.00330396,\n",
      "       0.00330396, 0.00440529, 0.00440529, 0.00550661, 0.00550661,\n",
      "       0.00660793, 0.00660793, 0.00770925, 0.00770925, 0.00881057,\n",
      "       0.00881057, 0.00991189, 0.00991189, 0.01211454, 0.01211454,\n",
      "       0.01431718, 0.01431718, 0.01762115, 0.01762115, 0.02092511,\n",
      "       0.02092511, 0.0253304 , 0.0253304 , 0.02753304, 0.02753304,\n",
      "       0.02863436, 0.02863436, 0.02973568, 0.02973568, 0.030837  ,\n",
      "       0.030837  , 0.03414097, 0.03414097, 0.03744493, 0.03744493,\n",
      "       0.03854626, 0.03854626, 0.0407489 , 0.0407489 , 0.04405286,\n",
      "       0.04405286, 0.04845815, 0.04845815, 0.04955947, 0.04955947,\n",
      "       0.05176211, 0.05176211, 0.05396476, 0.05396476, 0.05726872,\n",
      "       0.05726872, 0.05947137, 0.05947137, 0.06497797, 0.06497797,\n",
      "       0.0660793 , 0.0660793 , 0.06718062, 0.06718062, 0.06938326,\n",
      "       0.06938326, 0.07048458, 0.07048458, 0.07378855, 0.07378855,\n",
      "       0.07929515, 0.07929515, 0.0814978 , 0.0814978 , 0.08590308,\n",
      "       0.08590308, 0.08920705, 0.08920705, 0.09030837, 0.09030837,\n",
      "       0.09251101, 0.09251101, 0.09361233, 0.09361233, 0.10132159,\n",
      "       0.10132159, 0.10462555, 0.10462555, 0.11013216, 0.11013216,\n",
      "       0.11123348, 0.11123348, 0.11674009, 0.11674009, 0.1222467 ,\n",
      "       0.1222467 , 0.12334802, 0.12334802, 0.12665198, 0.12665198,\n",
      "       0.1277533 , 0.1277533 , 0.13436123, 0.13436123, 0.13546256,\n",
      "       0.13546256, 0.14207048, 0.14207048, 0.1530837 , 0.1530837 ,\n",
      "       0.15418502, 0.15418502, 0.15638767, 0.15638767, 0.15748899,\n",
      "       0.15748899, 0.16299559, 0.16299559, 0.16409692, 0.16409692,\n",
      "       0.16960352, 0.16960352, 0.17621145, 0.17621145, 0.17731278,\n",
      "       0.17731278, 0.18171806, 0.18171806, 0.18281938, 0.18281938,\n",
      "       0.1839207 , 0.1839207 , 0.18502203, 0.18502203, 0.18722467,\n",
      "       0.18722467, 0.18942731, 0.18942731, 0.19162996, 0.19162996,\n",
      "       0.1938326 , 0.1938326 , 0.19713656, 0.19713656, 0.19823789,\n",
      "       0.19823789, 0.19933921, 0.19933921, 0.20374449, 0.20374449,\n",
      "       0.2092511 , 0.2092511 , 0.21145374, 0.21145374, 0.22136564,\n",
      "       0.22136564, 0.22246696, 0.22246696, 0.22797357, 0.22797357,\n",
      "       0.24559471, 0.24559471, 0.25110132, 0.25110132, 0.25440529,\n",
      "       0.25440529, 0.25550661, 0.25550661, 0.26431718, 0.26431718,\n",
      "       0.26872247, 0.26872247, 0.28303965, 0.28303965, 0.29515419,\n",
      "       0.29515419, 0.29955947, 0.29955947, 0.30506608, 0.30506608,\n",
      "       0.31387665, 0.31387665, 0.3215859 , 0.3215859 , 0.32268722,\n",
      "       0.32268722, 0.32599119, 0.32599119, 0.33259912, 0.33259912,\n",
      "       0.33370044, 0.33370044, 0.33480176, 0.33480176, 0.33920705,\n",
      "       0.33920705, 0.34251101, 0.34251101, 0.3469163 , 0.3469163 ,\n",
      "       0.34911894, 0.34911894, 0.35462555, 0.35462555, 0.35682819,\n",
      "       0.35682819, 0.36784141, 0.36784141, 0.37334802, 0.37334802,\n",
      "       0.37555066, 0.37555066, 0.3777533 , 0.3777533 , 0.39537445,\n",
      "       0.39537445, 0.39647577, 0.39647577, 0.40198238, 0.40198238,\n",
      "       0.40638767, 0.40638767, 0.41189427, 0.41189427, 0.42731278,\n",
      "       0.42731278, 0.43061674, 0.43061674, 0.4339207 , 0.4339207 ,\n",
      "       0.43832599, 0.43832599, 0.45814978, 0.45814978, 0.46145374,\n",
      "       0.46145374, 0.46255507, 0.46255507, 0.46365639, 0.46365639,\n",
      "       0.469163  , 0.469163  , 0.47797357, 0.47797357, 0.48017621,\n",
      "       0.48017621, 0.49669604, 0.49669604, 0.50550661, 0.50550661,\n",
      "       0.50770925, 0.50770925, 0.51321586, 0.51321586, 0.51431718,\n",
      "       0.51431718, 0.52753304, 0.52753304, 0.53854626, 0.53854626,\n",
      "       0.54295154, 0.54295154, 0.54735683, 0.54735683, 0.54955947,\n",
      "       0.54955947, 0.55176211, 0.55176211, 0.56828194, 0.56828194,\n",
      "       0.57378855, 0.57378855, 0.57709251, 0.57709251, 0.58370044,\n",
      "       0.58370044, 0.59030837, 0.59030837, 0.59911894, 0.59911894,\n",
      "       0.60682819, 0.60682819, 0.60903084, 0.60903084, 0.61453744,\n",
      "       0.61453744, 0.61563877, 0.61563877, 0.62114537, 0.62114537,\n",
      "       0.62334802, 0.62334802, 0.6277533 , 0.6277533 , 0.63436123,\n",
      "       0.63436123, 0.64647577, 0.64647577, 0.65528634, 0.65528634,\n",
      "       0.66740088, 0.66740088, 0.66960352, 0.66960352, 0.68061674,\n",
      "       0.68061674, 0.6839207 , 0.6839207 , 0.69273128, 0.69273128,\n",
      "       0.70484581, 0.70484581, 0.71035242, 0.71035242, 0.71365639,\n",
      "       0.71365639, 0.719163  , 0.719163  , 0.7246696 , 0.7246696 ,\n",
      "       0.72687225, 0.72687225, 0.72907489, 0.72907489, 0.73348018,\n",
      "       0.73348018, 0.75      , 0.75      , 0.75991189, 0.75991189,\n",
      "       0.76431718, 0.76431718, 0.7753304 , 0.7753304 , 0.77753304,\n",
      "       0.77753304, 0.77863436, 0.77863436, 0.77973568, 0.77973568,\n",
      "       0.78524229, 0.78524229, 0.80286344, 0.80286344, 0.80396476,\n",
      "       0.80396476, 0.81277533, 0.81277533, 0.8160793 , 0.8160793 ,\n",
      "       0.82819383, 0.82819383, 0.82929515, 0.82929515, 0.8314978 ,\n",
      "       0.8314978 , 0.83259912, 0.83259912, 0.84361233, 0.84361233,\n",
      "       0.84911894, 0.84911894, 0.85022026, 0.85022026, 0.86013216,\n",
      "       0.86013216, 0.86123348, 0.86123348, 0.86453744, 0.86453744,\n",
      "       0.8777533 , 0.8777533 , 0.87995595, 0.87995595, 0.88325991,\n",
      "       0.88325991, 0.89096916, 0.89096916, 0.89977974, 0.89977974,\n",
      "       0.9030837 , 0.9030837 , 0.9185022 , 0.9185022 , 0.93061674,\n",
      "       0.93061674, 0.94162996, 0.94162996, 0.96696035, 0.96696035,\n",
      "       0.969163  , 0.969163  , 0.97136564, 0.97136564, 0.97246696,\n",
      "       0.97246696, 0.97356828, 0.97356828, 0.97577093, 0.97577093,\n",
      "       0.98348018, 0.98348018, 1.        ])}\n",
      "{0: array([0.        , 0.06666667, 0.13333333, 0.13333333, 0.26666667,\n",
      "       0.26666667, 0.33333333, 0.33333333, 0.4       , 0.4       ,\n",
      "       0.46666667, 0.46666667, 0.53333333, 0.53333333, 0.66666667,\n",
      "       0.66666667, 0.73333333, 0.73333333, 0.8       , 0.8       ,\n",
      "       0.86666667, 0.86666667, 0.93333333, 0.93333333, 1.        ,\n",
      "       1.        ]), 1: array([0.        , 0.        , 0.04761905, 0.04761905, 0.0952381 ,\n",
      "       0.0952381 , 0.14285714, 0.14285714, 0.19047619, 0.19047619,\n",
      "       0.23809524, 0.23809524, 0.33333333, 0.33333333, 0.38095238,\n",
      "       0.38095238, 0.42857143, 0.42857143, 0.47619048, 0.47619048,\n",
      "       0.52380952, 0.52380952, 0.57142857, 0.57142857, 0.61904762,\n",
      "       0.61904762, 0.66666667, 0.66666667, 0.71428571, 0.71428571,\n",
      "       0.76190476, 0.76190476, 0.80952381, 0.80952381, 0.9047619 ,\n",
      "       0.9047619 , 0.95238095, 0.95238095, 1.        , 1.        ]), 2: array([0.        , 0.03571429, 0.03571429, 0.07142857, 0.07142857,\n",
      "       0.10714286, 0.10714286, 0.17857143, 0.17857143, 0.21428571,\n",
      "       0.21428571, 0.25      , 0.25      , 0.35714286, 0.35714286,\n",
      "       0.39285714, 0.39285714, 0.46428571, 0.46428571, 0.5       ,\n",
      "       0.5       , 0.53571429, 0.53571429, 0.60714286, 0.60714286,\n",
      "       0.64285714, 0.64285714, 0.67857143, 0.67857143, 0.71428571,\n",
      "       0.71428571, 0.75      , 0.75      , 0.78571429, 0.78571429,\n",
      "       0.82142857, 0.82142857, 0.85714286, 0.85714286, 0.89285714,\n",
      "       0.89285714, 0.92857143, 0.92857143, 0.96428571, 0.96428571,\n",
      "       1.        , 1.        ]), 3: array([0.        , 0.04545455, 0.09090909, 0.09090909, 0.13636364,\n",
      "       0.13636364, 0.22727273, 0.22727273, 0.27272727, 0.27272727,\n",
      "       0.31818182, 0.31818182, 0.36363636, 0.36363636, 0.40909091,\n",
      "       0.40909091, 0.45454545, 0.45454545, 0.5       , 0.5       ,\n",
      "       0.54545455, 0.54545455, 0.59090909, 0.59090909, 0.68181818,\n",
      "       0.68181818, 0.72727273, 0.72727273, 0.77272727, 0.77272727,\n",
      "       0.81818182, 0.81818182, 0.90909091, 0.90909091, 0.95454545,\n",
      "       0.95454545, 1.        , 1.        ]), 4: array([0.        , 0.        , 0.        , 0.04166667, 0.04166667,\n",
      "       0.16666667, 0.16666667, 0.20833333, 0.20833333, 0.29166667,\n",
      "       0.29166667, 0.33333333, 0.33333333, 0.41666667, 0.41666667,\n",
      "       0.45833333, 0.45833333, 0.54166667, 0.54166667, 0.58333333,\n",
      "       0.58333333, 0.625     , 0.625     , 0.66666667, 0.66666667,\n",
      "       0.70833333, 0.70833333, 0.79166667, 0.79166667, 0.83333333,\n",
      "       0.83333333, 0.875     , 0.875     , 0.91666667, 0.91666667,\n",
      "       0.95833333, 0.95833333, 1.        ]), 5: array([0.        , 0.        , 0.        , 0.02439024, 0.02439024,\n",
      "       0.07317073, 0.07317073, 0.12195122, 0.12195122, 0.14634146,\n",
      "       0.14634146, 0.17073171, 0.17073171, 0.2195122 , 0.2195122 ,\n",
      "       0.29268293, 0.29268293, 0.31707317, 0.31707317, 0.34146341,\n",
      "       0.34146341, 0.3902439 , 0.3902439 , 0.43902439, 0.43902439,\n",
      "       0.48780488, 0.48780488, 0.56097561, 0.56097561, 0.58536585,\n",
      "       0.58536585, 0.65853659, 0.65853659, 0.68292683, 0.68292683,\n",
      "       0.70731707, 0.70731707, 0.80487805, 0.80487805, 0.82926829,\n",
      "       0.82926829, 0.85365854, 0.85365854, 0.90243902, 0.90243902,\n",
      "       0.92682927, 0.92682927, 0.95121951, 0.95121951, 0.97560976,\n",
      "       0.97560976, 1.        , 1.        ]), 6: array([0.        , 0.        , 0.        , 0.05882353, 0.05882353,\n",
      "       0.11764706, 0.11764706, 0.17647059, 0.17647059, 0.23529412,\n",
      "       0.23529412, 0.35294118, 0.35294118, 0.41176471, 0.41176471,\n",
      "       0.47058824, 0.47058824, 0.52941176, 0.52941176, 0.58823529,\n",
      "       0.58823529, 0.64705882, 0.64705882, 0.70588235, 0.70588235,\n",
      "       0.76470588, 0.76470588, 0.82352941, 0.82352941, 0.88235294,\n",
      "       0.88235294, 0.94117647, 0.94117647, 1.        , 1.        ]), 7: array([0.    , 0.    , 0.    , 0.0625, 0.0625, 0.125 , 0.125 , 0.1875,\n",
      "       0.1875, 0.25  , 0.25  , 0.3125, 0.3125, 0.5   , 0.5   , 0.5625,\n",
      "       0.5625, 0.625 , 0.625 , 0.6875, 0.6875, 0.75  , 0.75  , 0.8125,\n",
      "       0.8125, 0.875 , 0.875 , 0.9375, 0.9375, 1.    , 1.    ]), 8: array([0.        , 0.        , 0.04761905, 0.04761905, 0.14285714,\n",
      "       0.14285714, 0.19047619, 0.19047619, 0.23809524, 0.23809524,\n",
      "       0.28571429, 0.28571429, 0.42857143, 0.42857143, 0.47619048,\n",
      "       0.47619048, 0.57142857, 0.57142857, 0.66666667, 0.66666667,\n",
      "       0.71428571, 0.71428571, 0.76190476, 0.76190476, 0.85714286,\n",
      "       0.85714286, 0.9047619 , 0.9047619 , 0.95238095, 0.95238095,\n",
      "       1.        , 1.        ]), 9: array([0.        , 0.        , 0.03508772, 0.03508772, 0.05263158,\n",
      "       0.05263158, 0.07017544, 0.07017544, 0.0877193 , 0.0877193 ,\n",
      "       0.10526316, 0.10526316, 0.14035088, 0.14035088, 0.15789474,\n",
      "       0.15789474, 0.1754386 , 0.1754386 , 0.19298246, 0.19298246,\n",
      "       0.22807018, 0.22807018, 0.26315789, 0.26315789, 0.31578947,\n",
      "       0.31578947, 0.35087719, 0.35087719, 0.45614035, 0.45614035,\n",
      "       0.50877193, 0.50877193, 0.56140351, 0.56140351, 0.57894737,\n",
      "       0.57894737, 0.61403509, 0.61403509, 0.64912281, 0.64912281,\n",
      "       0.66666667, 0.66666667, 0.70175439, 0.70175439, 0.78947368,\n",
      "       0.78947368, 0.80701754, 0.80701754, 0.85964912, 0.85964912,\n",
      "       0.87719298, 0.87719298, 0.9122807 , 0.9122807 , 0.94736842,\n",
      "       0.94736842, 0.96491228, 0.96491228, 1.        ]), 'macro': array([0.02599567, 0.039329  , 0.039329  , 0.039329  , 0.04885281,\n",
      "       0.04885281, 0.05242424, 0.05242424, 0.05593301, 0.05837204,\n",
      "       0.06425439, 0.06879985, 0.07055423, 0.07964514, 0.08381181,\n",
      "       0.08969416, 0.09144855, 0.09811521, 0.11061521, 0.11418664,\n",
      "       0.11594103, 0.12010769, 0.12498574, 0.12974765, 0.13150203,\n",
      "       0.14102584, 0.14590389, 0.15304675, 0.15655552, 0.16280552,\n",
      "       0.16637695, 0.16881597, 0.17357788, 0.1760169 , 0.18077881,\n",
      "       0.18435024, 0.18889569, 0.19065008, 0.19541198, 0.20493579,\n",
      "       0.20669018, 0.21335684, 0.21823489, 0.2229968 , 0.22475118,\n",
      "       0.22929664, 0.23882045, 0.2433659 , 0.25068298, 0.25693298,\n",
      "       0.26169488, 0.2641339 , 0.26867936, 0.27701269, 0.28326269,\n",
      "       0.28677146, 0.29343813, 0.2976048 , 0.30348715, 0.30803261,\n",
      "       0.32231832, 0.33065165, 0.33416043, 0.34041043, 0.34457709,\n",
      "       0.34984025, 0.35817358, 0.36293549, 0.36918549, 0.37506784,\n",
      "       0.38578213, 0.3892909 , 0.3940528 , 0.39762423, 0.41291245,\n",
      "       0.41535148, 0.41989693, 0.43166164, 0.4358283 , 0.44297116,\n",
      "       0.44654259, 0.45011402, 0.45537718, 0.47412718, 0.47867263,\n",
      "       0.48581549, 0.4953393 , 0.49988475, 0.50345618, 0.50833423,\n",
      "       0.51421658, 0.51778801, 0.52266606, 0.52792922, 0.53280727,\n",
      "       0.53637869, 0.54262869, 0.54438308, 0.55063308, 0.55795015,\n",
      "       0.56420015, 0.57008251, 0.57960632, 0.58436822, 0.58680725,\n",
      "       0.59097391, 0.59573582, 0.60240248, 0.61179361, 0.61911068,\n",
      "       0.62268211, 0.62619088, 0.63528179, 0.63703618, 0.6394752 ,\n",
      "       0.65280853, 0.66233234, 0.66858234, 0.67334425, 0.67691568,\n",
      "       0.6793547 , 0.6856047 , 0.69512851, 0.70101086, 0.70451964,\n",
      "       0.71076964, 0.72052573, 0.72296476, 0.72884711, 0.73509711,\n",
      "       0.73985902, 0.74574137, 0.75028682, 0.75272585, 0.7586082 ,\n",
      "       0.76217963, 0.76694153, 0.77181958, 0.77658149, 0.78015292,\n",
      "       0.78681958, 0.79559151, 0.80035342, 0.80452008, 0.80906554,\n",
      "       0.81494789, 0.81851932, 0.82027371, 0.82481916, 0.82898583,\n",
      "       0.84327154, 0.8485347 , 0.85097372, 0.85685608, 0.86518941,\n",
      "       0.86995132, 0.87239034, 0.87596177, 0.88072367, 0.88247806,\n",
      "       0.88598683, 0.89507774, 0.89924441, 0.90591107, 0.91257774,\n",
      "       0.91614917, 0.92281583, 0.9269825 , 0.93152796, 0.93569462,\n",
      "       0.93813365, 0.944016  , 0.950266  , 0.95443267, 0.95687169,\n",
      "       0.96275404, 0.96626282, 0.97102472, 0.97277911, 0.97944577,\n",
      "       0.98399123, 0.98756266, 0.99232456, 1.        ]), 'micro': array([0.        , 0.        , 0.        , 0.00381679, 0.00381679,\n",
      "       0.00763359, 0.00763359, 0.01145038, 0.01145038, 0.01526718,\n",
      "       0.01526718, 0.01908397, 0.01908397, 0.02290076, 0.02290076,\n",
      "       0.02671756, 0.02671756, 0.03435115, 0.03435115, 0.03816794,\n",
      "       0.03816794, 0.04198473, 0.04198473, 0.04580153, 0.04580153,\n",
      "       0.04961832, 0.04961832, 0.0648855 , 0.0648855 , 0.06870229,\n",
      "       0.06870229, 0.08015267, 0.08015267, 0.08396947, 0.08396947,\n",
      "       0.09160305, 0.09160305, 0.09923664, 0.09923664, 0.11832061,\n",
      "       0.11832061, 0.1221374 , 0.1221374 , 0.1259542 , 0.1259542 ,\n",
      "       0.12977099, 0.12977099, 0.14885496, 0.14885496, 0.15648855,\n",
      "       0.15648855, 0.16412214, 0.16412214, 0.16793893, 0.16793893,\n",
      "       0.17175573, 0.17175573, 0.18320611, 0.18320611, 0.1870229 ,\n",
      "       0.1870229 , 0.19465649, 0.19465649, 0.19847328, 0.19847328,\n",
      "       0.20610687, 0.20610687, 0.20992366, 0.20992366, 0.21374046,\n",
      "       0.21374046, 0.22137405, 0.22137405, 0.22519084, 0.22519084,\n",
      "       0.22900763, 0.22900763, 0.23282443, 0.23282443, 0.24045802,\n",
      "       0.24045802, 0.24427481, 0.24427481, 0.2480916 , 0.2480916 ,\n",
      "       0.25572519, 0.25572519, 0.26335878, 0.26335878, 0.27862595,\n",
      "       0.27862595, 0.28244275, 0.28244275, 0.29007634, 0.29007634,\n",
      "       0.29389313, 0.29389313, 0.29770992, 0.29770992, 0.30534351,\n",
      "       0.30534351, 0.30916031, 0.30916031, 0.3129771 , 0.3129771 ,\n",
      "       0.31679389, 0.31679389, 0.32061069, 0.32061069, 0.32442748,\n",
      "       0.32442748, 0.33206107, 0.33206107, 0.33587786, 0.33587786,\n",
      "       0.34732824, 0.34732824, 0.35114504, 0.35114504, 0.35496183,\n",
      "       0.35496183, 0.36259542, 0.36259542, 0.36641221, 0.36641221,\n",
      "       0.37022901, 0.37022901, 0.3740458 , 0.3740458 , 0.3778626 ,\n",
      "       0.3778626 , 0.38167939, 0.38167939, 0.38549618, 0.38549618,\n",
      "       0.40076336, 0.40076336, 0.40458015, 0.40458015, 0.40839695,\n",
      "       0.40839695, 0.41221374, 0.41221374, 0.41984733, 0.41984733,\n",
      "       0.42366412, 0.42366412, 0.43129771, 0.43129771, 0.4389313 ,\n",
      "       0.4389313 , 0.44274809, 0.44274809, 0.44656489, 0.44656489,\n",
      "       0.45419847, 0.45419847, 0.45801527, 0.45801527, 0.46183206,\n",
      "       0.46183206, 0.47328244, 0.47328244, 0.47709924, 0.47709924,\n",
      "       0.48091603, 0.48091603, 0.49236641, 0.49236641, 0.5       ,\n",
      "       0.5       , 0.50381679, 0.50381679, 0.50763359, 0.50763359,\n",
      "       0.51145038, 0.51145038, 0.51526718, 0.51526718, 0.51908397,\n",
      "       0.51908397, 0.52671756, 0.52671756, 0.53053435, 0.53053435,\n",
      "       0.53816794, 0.53816794, 0.54198473, 0.54198473, 0.54580153,\n",
      "       0.54580153, 0.55343511, 0.55343511, 0.55725191, 0.55725191,\n",
      "       0.5610687 , 0.5610687 , 0.5648855 , 0.5648855 , 0.56870229,\n",
      "       0.56870229, 0.57251908, 0.57251908, 0.57633588, 0.57633588,\n",
      "       0.58015267, 0.58015267, 0.58396947, 0.58396947, 0.58778626,\n",
      "       0.58778626, 0.59160305, 0.59160305, 0.59923664, 0.59923664,\n",
      "       0.60305344, 0.60305344, 0.60687023, 0.60687023, 0.61450382,\n",
      "       0.61450382, 0.61832061, 0.61832061, 0.6221374 , 0.6221374 ,\n",
      "       0.62977099, 0.62977099, 0.63358779, 0.63358779, 0.63740458,\n",
      "       0.63740458, 0.64122137, 0.64122137, 0.64503817, 0.64503817,\n",
      "       0.64885496, 0.64885496, 0.65267176, 0.65267176, 0.65648855,\n",
      "       0.65648855, 0.66030534, 0.66030534, 0.66412214, 0.66412214,\n",
      "       0.66793893, 0.66793893, 0.67175573, 0.67175573, 0.67557252,\n",
      "       0.67557252, 0.67938931, 0.67938931, 0.68320611, 0.68320611,\n",
      "       0.6870229 , 0.6870229 , 0.69083969, 0.69083969, 0.69465649,\n",
      "       0.69465649, 0.70610687, 0.70610687, 0.71374046, 0.71374046,\n",
      "       0.72519084, 0.72519084, 0.72900763, 0.72900763, 0.73282443,\n",
      "       0.73282443, 0.73664122, 0.73664122, 0.74045802, 0.74045802,\n",
      "       0.74427481, 0.74427481, 0.7480916 , 0.7480916 , 0.7519084 ,\n",
      "       0.7519084 , 0.75954198, 0.75954198, 0.76335878, 0.76335878,\n",
      "       0.76717557, 0.76717557, 0.77099237, 0.77099237, 0.77480916,\n",
      "       0.77480916, 0.77862595, 0.77862595, 0.78244275, 0.78244275,\n",
      "       0.78625954, 0.78625954, 0.79007634, 0.79007634, 0.79389313,\n",
      "       0.79389313, 0.79770992, 0.79770992, 0.80152672, 0.80152672,\n",
      "       0.80534351, 0.80534351, 0.80916031, 0.80916031, 0.8129771 ,\n",
      "       0.8129771 , 0.81679389, 0.81679389, 0.82061069, 0.82061069,\n",
      "       0.82442748, 0.82442748, 0.82824427, 0.82824427, 0.83206107,\n",
      "       0.83206107, 0.83587786, 0.83587786, 0.83969466, 0.83969466,\n",
      "       0.84351145, 0.84351145, 0.84732824, 0.84732824, 0.85114504,\n",
      "       0.85114504, 0.85496183, 0.85496183, 0.85877863, 0.85877863,\n",
      "       0.86259542, 0.86259542, 0.87022901, 0.87022901, 0.8740458 ,\n",
      "       0.8740458 , 0.8778626 , 0.8778626 , 0.88549618, 0.88549618,\n",
      "       0.88931298, 0.88931298, 0.89312977, 0.89312977, 0.89694656,\n",
      "       0.89694656, 0.90076336, 0.90076336, 0.90458015, 0.90458015,\n",
      "       0.90839695, 0.90839695, 0.91221374, 0.91221374, 0.91984733,\n",
      "       0.91984733, 0.92366412, 0.92366412, 0.92748092, 0.92748092,\n",
      "       0.93129771, 0.93129771, 0.9351145 , 0.9351145 , 0.9389313 ,\n",
      "       0.9389313 , 0.94274809, 0.94274809, 0.94656489, 0.94656489,\n",
      "       0.95038168, 0.95038168, 0.95419847, 0.95419847, 0.95801527,\n",
      "       0.95801527, 0.96183206, 0.96183206, 0.96564885, 0.96564885,\n",
      "       0.96946565, 0.96946565, 0.97328244, 0.97328244, 0.97709924,\n",
      "       0.97709924, 0.98091603, 0.98091603, 0.98473282, 0.98473282,\n",
      "       0.98854962, 0.98854962, 0.99236641, 0.99236641, 0.99618321,\n",
      "       0.99618321, 1.        , 1.        ])}\n",
      "******* FP / TP for percent selected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([0.        , 0.00980392, 0.00980392, 0.01960784, 0.01960784,\r\n",
      "       0.02941176, 0.02941176, 0.08823529, 0.08823529, 0.19607843,\r\n",
      "       0.19607843, 0.33333333, 0.33333333, 0.34313725, 0.34313725,\r\n",
      "       0.45098039, 0.45098039, 0.50980392, 0.50980392, 0.74509804,\r\n",
      "       0.74509804, 0.81372549, 0.81372549, 0.83333333, 1.        ]), 1: array([0.        , 0.01041667, 0.02083333, 0.02083333, 0.08333333,\r\n",
      "       0.08333333, 0.125     , 0.125     , 0.23958333, 0.23958333,\r\n",
      "       0.3125    , 0.3125    , 0.33333333, 0.33333333, 0.42708333,\r\n",
      "       0.42708333, 0.47916667, 0.47916667, 0.57291667, 0.57291667,\r\n",
      "       0.70833333, 1.        ]), 2: array([0.        , 0.01123596, 0.02247191, 0.02247191, 0.07865169,\r\n",
      "       0.07865169, 0.08988764, 0.08988764, 0.11235955, 0.11235955,\r\n",
      "       0.15730337, 0.15730337, 0.20224719, 0.20224719, 0.21348315,\r\n",
      "       0.2247191 , 0.3258427 , 0.3258427 , 0.33707865, 0.33707865,\r\n",
      "       0.37078652, 0.37078652, 0.41573034, 0.41573034, 0.43820225,\r\n",
      "       0.43820225, 0.49438202, 0.49438202, 0.52808989, 0.52808989,\r\n",
      "       0.5505618 , 0.5505618 , 0.57303371, 0.57303371, 0.58426966,\r\n",
      "       0.58426966, 0.59550562, 0.59550562, 0.60674157, 0.60674157,\r\n",
      "       0.61797753, 0.61797753, 0.80898876, 0.80898876, 0.87640449,\r\n",
      "       1.        ]), 3: array([0.        , 0.        , 0.02105263, 0.02105263, 0.03157895,\r\n",
      "       0.03157895, 0.04210526, 0.04210526, 0.08421053, 0.08421053,\r\n",
      "       0.15789474, 0.15789474, 0.17894737, 0.17894737, 0.25263158,\r\n",
      "       0.25263158, 0.28421053, 0.28421053, 0.29473684, 0.29473684,\r\n",
      "       0.37894737, 0.37894737, 0.38947368, 0.38947368, 0.43157895,\r\n",
      "       0.43157895, 0.45263158, 0.55789474, 0.55789474, 0.58947368,\r\n",
      "       0.58947368, 0.62105263, 0.62105263, 0.64210526, 0.64210526,\r\n",
      "       0.71578947, 0.71578947, 0.82105263, 1.        ]), 4: array([0.        , 0.01075269, 0.05376344, 0.05376344, 0.06451613,\r\n",
      "       0.06451613, 0.09677419, 0.09677419, 0.16129032, 0.16129032,\r\n",
      "       0.17204301, 0.17204301, 0.22580645, 0.22580645, 0.27956989,\r\n",
      "       0.27956989, 0.32258065, 0.32258065, 0.35483871, 0.35483871,\r\n",
      "       0.51612903, 0.51612903, 0.62365591, 0.62365591, 0.6344086 ,\r\n",
      "       0.6344086 , 0.7311828 , 0.7311828 , 0.74193548, 0.74193548,\r\n",
      "       0.77419355, 0.77419355, 0.79569892, 0.79569892, 0.87096774,\r\n",
      "       0.87096774, 0.88172043, 0.88172043, 0.94623656, 0.94623656,\r\n",
      "       0.96774194, 0.96774194, 1.        ]), 5: array([0.        , 0.01315789, 0.03947368, 0.03947368, 0.21052632,\r\n",
      "       0.21052632, 0.22368421, 0.22368421, 0.25      , 0.25      ,\r\n",
      "       0.26315789, 0.26315789, 0.27631579, 0.27631579, 0.28947368,\r\n",
      "       0.28947368, 0.30263158, 0.30263158, 0.31578947, 0.31578947,\r\n",
      "       0.34210526, 0.34210526, 0.35526316, 0.35526316, 0.36842105,\r\n",
      "       0.36842105, 0.38157895, 0.38157895, 0.40789474, 0.40789474,\r\n",
      "       0.42105263, 0.42105263, 0.46052632, 0.46052632, 0.47368421,\r\n",
      "       0.47368421, 0.53947368, 0.53947368, 0.55263158, 0.55263158,\r\n",
      "       0.56578947, 0.56578947, 0.59210526, 0.59210526, 0.61842105,\r\n",
      "       0.61842105, 0.64473684, 0.64473684, 0.68421053, 0.68421053,\r\n",
      "       0.78947368, 0.78947368, 0.82894737, 0.82894737, 0.84210526,\r\n",
      "       0.84210526, 0.88157895, 0.88157895, 1.        ]), 6: array([0.  , 0.01, 0.12, 0.12, 0.14, 0.14, 0.25, 0.25, 0.32, 0.32, 0.33,\r\n",
      "       0.33, 0.44, 0.44, 0.45, 0.45, 0.47, 0.47, 0.49, 0.51, 0.56, 0.56,\r\n",
      "       0.59, 0.59, 0.62, 0.62, 0.7 , 0.7 , 0.77, 0.77, 1.  ]), 7: array([0.        , 0.00990099, 0.05940594, 0.05940594, 0.06930693,\r\n",
      "       0.06930693, 0.11881188, 0.11881188, 0.20792079, 0.20792079,\r\n",
      "       0.24752475, 0.24752475, 0.31683168, 0.31683168, 0.36633663,\r\n",
      "       0.36633663, 0.37623762, 0.37623762, 0.44554455, 0.44554455,\r\n",
      "       0.45544554, 0.45544554, 0.47524752, 0.47524752, 0.56435644,\r\n",
      "       0.56435644, 0.57425743, 0.57425743, 0.65346535, 0.65346535,\r\n",
      "       0.67326733, 1.        ]), 8: array([0.        , 0.        , 0.0625    , 0.0625    , 0.07291667,\r\n",
      "       0.07291667, 0.08333333, 0.08333333, 0.10416667, 0.10416667,\r\n",
      "       0.19791667, 0.19791667, 0.30208333, 0.30208333, 0.40625   ,\r\n",
      "       0.40625   , 0.44791667, 0.44791667, 0.55208333, 0.55208333,\r\n",
      "       0.65625   , 0.65625   , 0.67708333, 0.67708333, 0.6875    ,\r\n",
      "       0.6875    , 0.76041667, 0.76041667, 0.78125   , 0.78125   ,\r\n",
      "       0.85416667, 0.85416667, 0.88541667, 1.        ]), 9: array([0.        , 0.01666667, 0.08333333, 0.08333333, 0.11666667,\r\n",
      "       0.11666667, 0.13333333, 0.13333333, 0.16666667, 0.16666667,\r\n",
      "       0.18333333, 0.18333333, 0.2       , 0.2       , 0.25      ,\r\n",
      "       0.25      , 0.28333333, 0.28333333, 0.3       , 0.3       ,\r\n",
      "       0.33333333, 0.33333333, 0.4       , 0.4       , 0.41666667,\r\n",
      "       0.41666667, 0.43333333, 0.43333333, 0.46666667, 0.46666667,\r\n",
      "       0.48333333, 0.48333333, 0.5       , 0.5       , 0.55      ,\r\n",
      "       0.55      , 0.58333333, 0.58333333, 0.63333333, 0.63333333,\r\n",
      "       0.71666667, 0.71666667, 0.75      , 0.75      , 0.76666667,\r\n",
      "       0.76666667, 0.83333333, 0.83333333, 0.86666667, 0.86666667,\r\n",
      "       0.88333333, 0.88333333, 0.9       , 0.9       , 0.91666667,\r\n",
      "       0.91666667, 0.93333333, 0.93333333, 1.        , 1.        ]), 'macro': array([0.        , 0.00980392, 0.00990099, 0.01      , 0.01041667,\r\n",
      "       0.01075269, 0.01123596, 0.01315789, 0.01666667, 0.01960784,\r\n",
      "       0.02083333, 0.02105263, 0.02247191, 0.02941176, 0.03157895,\r\n",
      "       0.03947368, 0.04210526, 0.05376344, 0.05940594, 0.0625    ,\r\n",
      "       0.06451613, 0.06930693, 0.07291667, 0.07865169, 0.08333333,\r\n",
      "       0.08421053, 0.08823529, 0.08988764, 0.09677419, 0.10416667,\r\n",
      "       0.11235955, 0.11666667, 0.11881188, 0.12      , 0.125     ,\r\n",
      "       0.13333333, 0.14      , 0.15730337, 0.15789474, 0.16129032,\r\n",
      "       0.16666667, 0.17204301, 0.17894737, 0.18333333, 0.19607843,\r\n",
      "       0.19791667, 0.2       , 0.20224719, 0.20792079, 0.21052632,\r\n",
      "       0.21348315, 0.22368421, 0.2247191 , 0.22580645, 0.23958333,\r\n",
      "       0.24752475, 0.25      , 0.25263158, 0.26315789, 0.27631579,\r\n",
      "       0.27956989, 0.28333333, 0.28421053, 0.28947368, 0.29473684,\r\n",
      "       0.3       , 0.30208333, 0.30263158, 0.3125    , 0.31578947,\r\n",
      "       0.31683168, 0.32      , 0.32258065, 0.3258427 , 0.33      ,\r\n",
      "       0.33333333, 0.33707865, 0.34210526, 0.34313725, 0.35483871,\r\n",
      "       0.35526316, 0.36633663, 0.36842105, 0.37078652, 0.37623762,\r\n",
      "       0.37894737, 0.38157895, 0.38947368, 0.4       , 0.40625   ,\r\n",
      "       0.40789474, 0.41573034, 0.41666667, 0.42105263, 0.42708333,\r\n",
      "       0.43157895, 0.43333333, 0.43820225, 0.44      , 0.44554455,\r\n",
      "       0.44791667, 0.45      , 0.45098039, 0.45263158, 0.45544554,\r\n",
      "       0.46052632, 0.46666667, 0.47      , 0.47368421, 0.47524752,\r\n",
      "       0.47916667, 0.48333333, 0.49      , 0.49438202, 0.5       ,\r\n",
      "       0.50980392, 0.51      , 0.51612903, 0.52808989, 0.53947368,\r\n",
      "       0.55      , 0.5505618 , 0.55208333, 0.55263158, 0.55789474,\r\n",
      "       0.56      , 0.56435644, 0.56578947, 0.57291667, 0.57303371,\r\n",
      "       0.57425743, 0.58333333, 0.58426966, 0.58947368, 0.59      ,\r\n",
      "       0.59210526, 0.59550562, 0.60674157, 0.61797753, 0.61842105,\r\n",
      "       0.62      , 0.62105263, 0.62365591, 0.63333333, 0.6344086 ,\r\n",
      "       0.64210526, 0.64473684, 0.65346535, 0.65625   , 0.67326733,\r\n",
      "       0.67708333, 0.68421053, 0.6875    , 0.7       , 0.70833333,\r\n",
      "       0.71578947, 0.71666667, 0.7311828 , 0.74193548, 0.74509804,\r\n",
      "       0.75      , 0.76041667, 0.76666667, 0.77      , 0.77419355,\r\n",
      "       0.78125   , 0.78947368, 0.79569892, 0.80898876, 0.81372549,\r\n",
      "       0.82105263, 0.82894737, 0.83333333, 0.84210526, 0.85416667,\r\n",
      "       0.86666667, 0.87096774, 0.87640449, 0.88157895, 0.88172043,\r\n",
      "       0.88333333, 0.88541667, 0.9       , 0.91666667, 0.93333333,\r\n",
      "       0.94623656, 0.96774194, 1.        ]), 'micro': array([0.        , 0.00110132, 0.00991189, 0.00991189, 0.01211454,\r\n",
      "       0.01211454, 0.01321586, 0.01321586, 0.0154185 , 0.0154185 ,\r\n",
      "       0.01651982, 0.01651982, 0.01762115, 0.01762115, 0.01872247,\r\n",
      "       0.01872247, 0.02202643, 0.02202643, 0.0253304 , 0.0253304 ,\r\n",
      "       0.02643172, 0.02643172, 0.02753304, 0.02753304, 0.02863436,\r\n",
      "       0.02863436, 0.02973568, 0.02973568, 0.03303965, 0.03303965,\r\n",
      "       0.03744493, 0.03744493, 0.03854626, 0.03854626, 0.03964758,\r\n",
      "       0.03964758, 0.0407489 , 0.0407489 , 0.04185022, 0.04185022,\r\n",
      "       0.04405286, 0.04405286, 0.04515419, 0.04515419, 0.04735683,\r\n",
      "       0.04735683, 0.0561674 , 0.0561674 , 0.05726872, 0.05726872,\r\n",
      "       0.06057269, 0.06057269, 0.07048458, 0.07048458, 0.07488987,\r\n",
      "       0.07488987, 0.07709251, 0.07709251, 0.07929515, 0.07929515,\r\n",
      "       0.08039648, 0.08590308, 0.08590308, 0.08700441, 0.08700441,\r\n",
      "       0.09030837, 0.09030837, 0.09140969, 0.09140969, 0.09361233,\r\n",
      "       0.09361233, 0.09581498, 0.09581498, 0.10132159, 0.10132159,\r\n",
      "       0.10352423, 0.10352423, 0.10572687, 0.10572687, 0.10903084,\r\n",
      "       0.10903084, 0.11343612, 0.11453744, 0.11563877, 0.11563877,\r\n",
      "       0.11894273, 0.11894273, 0.12004405, 0.12004405, 0.1222467 ,\r\n",
      "       0.1222467 , 0.12555066, 0.12555066, 0.12885463, 0.12885463,\r\n",
      "       0.13436123, 0.13436123, 0.13546256, 0.13546256, 0.13546256,\r\n",
      "       0.13546256, 0.1376652 , 0.1376652 , 0.13986784, 0.13986784,\r\n",
      "       0.14207048, 0.14207048, 0.14317181, 0.14317181, 0.14537445,\r\n",
      "       0.14537445, 0.14647577, 0.14647577, 0.14867841, 0.15418502,\r\n",
      "       0.15418502, 0.15528634, 0.15748899, 0.15748899, 0.15859031,\r\n",
      "       0.15859031, 0.15969163, 0.15969163, 0.16409692, 0.16409692,\r\n",
      "       0.16409692, 0.17511013, 0.17511013, 0.17621145, 0.17621145,\r\n",
      "       0.1784141 , 0.1784141 , 0.18061674, 0.18061674, 0.18502203,\r\n",
      "       0.18502203, 0.18832599, 0.18832599, 0.19273128, 0.19273128,\r\n",
      "       0.1938326 , 0.1938326 , 0.19823789, 0.19823789, 0.19933921,\r\n",
      "       0.19933921, 0.20264317, 0.20264317, 0.20814978, 0.20814978,\r\n",
      "       0.2092511 , 0.2092511 , 0.21145374, 0.21145374, 0.22026432,\r\n",
      "       0.22026432, 0.22577093, 0.22577093, 0.22687225, 0.22687225,\r\n",
      "       0.22797357, 0.22797357, 0.23237885, 0.23237885, 0.2345815 ,\r\n",
      "       0.2345815 , 0.23568282, 0.23568282, 0.23898678, 0.24449339,\r\n",
      "       0.24449339, 0.24889868, 0.25110132, 0.25330396, 0.25330396,\r\n",
      "       0.26211454, 0.26321586, 0.26321586, 0.2654185 , 0.27092511,\r\n",
      "       0.27092511, 0.27202643, 0.27202643, 0.27422907, 0.27422907,\r\n",
      "       0.2753304 , 0.2753304 , 0.27643172, 0.27643172, 0.27863436,\r\n",
      "       0.27863436, 0.27973568, 0.27973568, 0.2907489 , 0.2907489 ,\r\n",
      "       0.29735683, 0.29845815, 0.30066079, 0.30066079, 0.30176211,\r\n",
      "       0.30176211, 0.31167401, 0.31167401, 0.31497797, 0.31497797,\r\n",
      "       0.31718062, 0.31718062, 0.34251101, 0.34251101, 0.3469163 ,\r\n",
      "       0.3469163 , 0.34911894, 0.34911894, 0.35572687, 0.35792952,\r\n",
      "       0.36013216, 0.36013216, 0.36123348, 0.36784141, 0.36784141,\r\n",
      "       0.37114537, 0.37114537, 0.3722467 , 0.37334802, 0.37555066,\r\n",
      "       0.3777533 , 0.38436123, 0.38436123, 0.38986784, 0.38986784,\r\n",
      "       0.39427313, 0.39647577, 0.40528634, 0.40748899, 0.41079295,\r\n",
      "       0.41079295, 0.41299559, 0.41519824, 0.41519824, 0.41960352,\r\n",
      "       0.41960352, 0.42180617, 0.42400881, 0.42511013, 0.42511013,\r\n",
      "       0.4284141 , 0.43061674, 0.43171806, 0.4339207 , 0.43722467,\r\n",
      "       0.43722467, 0.44603524, 0.44603524, 0.44933921, 0.44933921,\r\n",
      "       0.45154185, 0.45264317, 0.45264317, 0.45374449, 0.45374449,\r\n",
      "       0.45594714, 0.46475771, 0.46475771, 0.46696035, 0.46696035,\r\n",
      "       0.48348018, 0.4845815 , 0.4845815 , 0.48898678, 0.48898678,\r\n",
      "       0.49229075, 0.49229075, 0.49779736, 0.49779736, 0.49889868,\r\n",
      "       0.50220264, 0.50660793, 0.50770925, 0.50770925, 0.51101322,\r\n",
      "       0.51101322, 0.51321586, 0.51321586, 0.52202643, 0.52202643,\r\n",
      "       0.52312775, 0.52312775, 0.52643172, 0.52863436, 0.53414097,\r\n",
      "       0.53414097, 0.53744493, 0.53964758, 0.53964758, 0.54735683,\r\n",
      "       0.54735683, 0.54845815, 0.54845815, 0.55066079, 0.55176211,\r\n",
      "       0.55396476, 0.55506608, 0.56387665, 0.56387665, 0.56828194,\r\n",
      "       0.56828194, 0.56938326, 0.56938326, 0.5715859 , 0.57378855,\r\n",
      "       0.57599119, 0.57819383, 0.57819383, 0.57929515, 0.57929515,\r\n",
      "       0.58259912, 0.58259912, 0.58810573, 0.59030837, 0.59140969,\r\n",
      "       0.59140969, 0.59471366, 0.59581498, 0.60132159, 0.60242291,\r\n",
      "       0.60682819, 0.60682819, 0.61013216, 0.61013216, 0.61453744,\r\n",
      "       0.61453744, 0.63876652, 0.64096916, 0.64096916, 0.64867841,\r\n",
      "       0.64867841, 0.65969163, 0.65969163, 0.66299559, 0.66299559,\r\n",
      "       0.6685022 , 0.6685022 , 0.67290749, 0.67290749, 0.68281938,\r\n",
      "       0.68281938, 0.68722467, 0.68722467, 0.6938326 , 0.6938326 ,\r\n",
      "       0.69493392, 0.69713656, 0.70374449, 0.70484581, 0.70704846,\r\n",
      "       0.70704846, 0.70814978, 0.71035242, 0.71145374, 0.71145374,\r\n",
      "       0.71255507, 0.71475771, 0.71696035, 0.71696035, 0.72356828,\r\n",
      "       0.72356828, 0.72907489, 0.73127753, 0.73237885, 0.7345815 ,\r\n",
      "       0.73568282, 0.73568282, 0.74008811, 0.74008811, 0.74229075,\r\n",
      "       0.74339207, 0.74559471, 0.74779736, 0.75220264, 0.75440529,\r\n",
      "       0.76211454, 0.76431718, 0.7654185 , 0.76982379, 0.77092511,\r\n",
      "       0.77422907, 0.77643172, 0.77863436, 0.780837  , 0.78193833,\r\n",
      "       0.78414097, 0.78634361, 0.78634361, 0.78744493, 0.78964758,\r\n",
      "       0.7907489 , 0.79295154, 0.79405286, 0.79405286, 0.79845815,\r\n",
      "       0.80066079, 0.80506608, 0.80506608, 0.8061674 , 0.8061674 ,\r\n",
      "       0.80726872, 0.80837004, 0.81057269, 0.81167401, 0.81938326,\r\n",
      "       0.8215859 , 0.82268722, 0.82378855, 0.82929515, 0.83259912,\r\n",
      "       0.83370044, 0.83480176, 0.83700441, 0.83920705, 0.84140969,\r\n",
      "       1.        ])}\r\n",
      "{0: array([0.        , 0.        , 0.06666667, 0.06666667, 0.13333333,\r\n",
      "       0.13333333, 0.26666667, 0.26666667, 0.33333333, 0.33333333,\r\n",
      "       0.4       , 0.4       , 0.46666667, 0.46666667, 0.6       ,\r\n",
      "       0.6       , 0.66666667, 0.66666667, 0.73333333, 0.73333333,\r\n",
      "       0.8       , 0.8       , 0.86666667, 0.86666667, 1.        ]), 1: array([0.        , 0.        , 0.        , 0.04761905, 0.04761905,\r\n",
      "       0.0952381 , 0.0952381 , 0.14285714, 0.14285714, 0.19047619,\r\n",
      "       0.19047619, 0.38095238, 0.38095238, 0.42857143, 0.42857143,\r\n",
      "       0.47619048, 0.47619048, 0.57142857, 0.57142857, 0.61904762,\r\n",
      "       0.61904762, 1.        ]), 2: array([0.        , 0.        , 0.        , 0.03571429, 0.03571429,\r\n",
      "       0.07142857, 0.07142857, 0.14285714, 0.14285714, 0.25      ,\r\n",
      "       0.25      , 0.28571429, 0.28571429, 0.39285714, 0.39285714,\r\n",
      "       0.42857143, 0.42857143, 0.46428571, 0.46428571, 0.5       ,\r\n",
      "       0.5       , 0.53571429, 0.53571429, 0.57142857, 0.57142857,\r\n",
      "       0.60714286, 0.60714286, 0.64285714, 0.64285714, 0.67857143,\r\n",
      "       0.67857143, 0.71428571, 0.71428571, 0.75      , 0.75      ,\r\n",
      "       0.78571429, 0.78571429, 0.82142857, 0.82142857, 0.85714286,\r\n",
      "       0.85714286, 0.89285714, 0.89285714, 0.92857143, 0.92857143,\r\n",
      "       1.        ]), 3: array([0.        , 0.04545455, 0.04545455, 0.09090909, 0.09090909,\r\n",
      "       0.13636364, 0.13636364, 0.18181818, 0.18181818, 0.22727273,\r\n",
      "       0.22727273, 0.27272727, 0.27272727, 0.31818182, 0.31818182,\r\n",
      "       0.36363636, 0.36363636, 0.45454545, 0.45454545, 0.5       ,\r\n",
      "       0.5       , 0.54545455, 0.54545455, 0.59090909, 0.59090909,\r\n",
      "       0.63636364, 0.63636364, 0.63636364, 0.68181818, 0.68181818,\r\n",
      "       0.72727273, 0.72727273, 0.77272727, 0.77272727, 0.81818182,\r\n",
      "       0.81818182, 0.86363636, 0.86363636, 1.        ]), 4: array([0.        , 0.        , 0.        , 0.04166667, 0.04166667,\r\n",
      "       0.08333333, 0.08333333, 0.125     , 0.125     , 0.16666667,\r\n",
      "       0.16666667, 0.20833333, 0.20833333, 0.25      , 0.25      ,\r\n",
      "       0.33333333, 0.33333333, 0.45833333, 0.45833333, 0.5       ,\r\n",
      "       0.5       , 0.54166667, 0.54166667, 0.58333333, 0.58333333,\r\n",
      "       0.66666667, 0.66666667, 0.70833333, 0.70833333, 0.75      ,\r\n",
      "       0.75      , 0.79166667, 0.79166667, 0.83333333, 0.83333333,\r\n",
      "       0.875     , 0.875     , 0.91666667, 0.91666667, 0.95833333,\r\n",
      "       0.95833333, 1.        , 1.        ]), 5: array([0.        , 0.        , 0.        , 0.02439024, 0.02439024,\r\n",
      "       0.09756098, 0.09756098, 0.14634146, 0.14634146, 0.17073171,\r\n",
      "       0.17073171, 0.19512195, 0.19512195, 0.2195122 , 0.2195122 ,\r\n",
      "       0.24390244, 0.24390244, 0.26829268, 0.26829268, 0.29268293,\r\n",
      "       0.29268293, 0.3902439 , 0.3902439 , 0.46341463, 0.46341463,\r\n",
      "       0.51219512, 0.51219512, 0.53658537, 0.53658537, 0.56097561,\r\n",
      "       0.56097561, 0.58536585, 0.58536585, 0.6097561 , 0.6097561 ,\r\n",
      "       0.63414634, 0.63414634, 0.65853659, 0.65853659, 0.68292683,\r\n",
      "       0.68292683, 0.70731707, 0.70731707, 0.73170732, 0.73170732,\r\n",
      "       0.75609756, 0.75609756, 0.85365854, 0.85365854, 0.90243902,\r\n",
      "       0.90243902, 0.92682927, 0.92682927, 0.95121951, 0.95121951,\r\n",
      "       0.97560976, 0.97560976, 1.        , 1.        ]), 6: array([0.        , 0.        , 0.        , 0.05882353, 0.05882353,\r\n",
      "       0.23529412, 0.23529412, 0.29411765, 0.29411765, 0.35294118,\r\n",
      "       0.35294118, 0.41176471, 0.41176471, 0.47058824, 0.47058824,\r\n",
      "       0.52941176, 0.52941176, 0.58823529, 0.58823529, 0.58823529,\r\n",
      "       0.58823529, 0.64705882, 0.64705882, 0.70588235, 0.70588235,\r\n",
      "       0.76470588, 0.76470588, 0.82352941, 0.82352941, 0.88235294,\r\n",
      "       1.        ]), 7: array([0.    , 0.    , 0.    , 0.0625, 0.0625, 0.125 , 0.125 , 0.1875,\r\n",
      "       0.1875, 0.25  , 0.25  , 0.3125, 0.3125, 0.4375, 0.4375, 0.5   ,\r\n",
      "       0.5   , 0.5625, 0.5625, 0.625 , 0.625 , 0.6875, 0.6875, 0.75  ,\r\n",
      "       0.75  , 0.8125, 0.8125, 0.875 , 0.875 , 0.9375, 0.9375, 1.    ]), 8: array([0.        , 0.04761905, 0.04761905, 0.0952381 , 0.0952381 ,\r\n",
      "       0.14285714, 0.14285714, 0.19047619, 0.19047619, 0.23809524,\r\n",
      "       0.23809524, 0.28571429, 0.28571429, 0.33333333, 0.33333333,\r\n",
      "       0.38095238, 0.38095238, 0.42857143, 0.42857143, 0.52380952,\r\n",
      "       0.52380952, 0.57142857, 0.57142857, 0.61904762, 0.61904762,\r\n",
      "       0.66666667, 0.66666667, 0.71428571, 0.71428571, 0.76190476,\r\n",
      "       0.76190476, 0.85714286, 0.85714286, 1.        ]), 9: array([0.        , 0.        , 0.        , 0.01754386, 0.01754386,\r\n",
      "       0.03508772, 0.03508772, 0.05263158, 0.05263158, 0.07017544,\r\n",
      "       0.07017544, 0.15789474, 0.15789474, 0.1754386 , 0.1754386 ,\r\n",
      "       0.19298246, 0.19298246, 0.29824561, 0.29824561, 0.38596491,\r\n",
      "       0.38596491, 0.42105263, 0.42105263, 0.45614035, 0.45614035,\r\n",
      "       0.47368421, 0.47368421, 0.50877193, 0.50877193, 0.52631579,\r\n",
      "       0.52631579, 0.56140351, 0.56140351, 0.59649123, 0.59649123,\r\n",
      "       0.63157895, 0.63157895, 0.64912281, 0.64912281, 0.68421053,\r\n",
      "       0.68421053, 0.70175439, 0.70175439, 0.73684211, 0.73684211,\r\n",
      "       0.77192982, 0.77192982, 0.78947368, 0.78947368, 0.8245614 ,\r\n",
      "       0.8245614 , 0.84210526, 0.84210526, 0.89473684, 0.89473684,\r\n",
      "       0.9122807 , 0.9122807 , 0.96491228, 0.96491228, 1.        ]), 'macro': array([0.00930736, 0.01597403, 0.01597403, 0.01597403, 0.01597403,\r\n",
      "       0.01597403, 0.01597403, 0.01597403, 0.01597403, 0.02264069,\r\n",
      "       0.0274026 , 0.03194805, 0.03551948, 0.04885281, 0.05339827,\r\n",
      "       0.05583729, 0.06038275, 0.06454941, 0.07079941, 0.07556132,\r\n",
      "       0.07972799, 0.08597799, 0.09073989, 0.09431132, 0.10558951,\r\n",
      "       0.11013497, 0.11680164, 0.12394449, 0.12811116, 0.13287306,\r\n",
      "       0.14358735, 0.14534174, 0.15159174, 0.15747409, 0.16223599,\r\n",
      "       0.16399038, 0.18163744, 0.18520887, 0.18975432, 0.19392099,\r\n",
      "       0.19567537, 0.19984204, 0.2043875 , 0.21315942, 0.21982609,\r\n",
      "       0.224588  , 0.22634238, 0.23705667, 0.24330667, 0.25062374,\r\n",
      "       0.25062374, 0.25874427, 0.25907322, 0.26323989, 0.26800179,\r\n",
      "       0.27425179, 0.28432755, 0.28887301, 0.29131203, 0.29375106,\r\n",
      "       0.30208439, 0.31261071, 0.32170161, 0.32414064, 0.32868609,\r\n",
      "       0.33745802, 0.34221993, 0.34465895, 0.36370657, 0.3661456 ,\r\n",
      "       0.3786456 , 0.38452795, 0.39702795, 0.40059938, 0.40648173,\r\n",
      "       0.42141907, 0.4249905 , 0.4347466 , 0.44807993, 0.4522466 ,\r\n",
      "       0.45956367, 0.46581367, 0.47069172, 0.47426315, 0.48051315,\r\n",
      "       0.48505861, 0.48749763, 0.49204308, 0.49555186, 0.50031376,\r\n",
      "       0.50275279, 0.50632421, 0.5080786 , 0.51051762, 0.51527953,\r\n",
      "       0.51982498, 0.52333376, 0.52690518, 0.53278754, 0.53903754,\r\n",
      "       0.54379944, 0.54968179, 0.55634846, 0.55634846, 0.56259846,\r\n",
      "       0.56503749, 0.56679187, 0.57267422, 0.57511325, 0.58136325,\r\n",
      "       0.59088706, 0.59439583, 0.59439583, 0.59796726, 0.60147603,\r\n",
      "       0.6081427 , 0.6081427 , 0.61230936, 0.61588079, 0.61831982,\r\n",
      "       0.62182859, 0.62540002, 0.63492383, 0.63736285, 0.64190831,\r\n",
      "       0.64779066, 0.65404066, 0.65647968, 0.66124159, 0.66481302,\r\n",
      "       0.67106302, 0.6728174 , 0.67638883, 0.68093429, 0.68681664,\r\n",
      "       0.68925566, 0.69282709, 0.69639852, 0.69996995, 0.70240897,\r\n",
      "       0.70829133, 0.71283678, 0.71700345, 0.72051222, 0.72884555,\r\n",
      "       0.73339101, 0.7431471 , 0.7493971 , 0.75415901, 0.75415901,\r\n",
      "       0.75899391, 0.76400829, 0.76883312, 0.77495458, 0.77511399,\r\n",
      "       0.78077594, 0.78266167, 0.789002  , 0.79477879, 0.80191902,\r\n",
      "       0.80616181, 0.81248352, 0.81692817, 0.82330966, 0.82831878,\r\n",
      "       0.83449827, 0.83858937, 0.84400663, 0.85024788, 0.85786612,\r\n",
      "       0.85933808, 0.8639647 , 0.86693442, 0.87250585, 0.88633673,\r\n",
      "       0.89430919, 0.90001175, 0.90195319, 0.90653903, 0.9107644 ,\r\n",
      "       0.91318796, 0.9140523 , 0.92718408, 0.93793118, 0.95218705,\r\n",
      "       0.96331582, 0.97908598, 1.        ]), 'micro': array([0.        , 0.        , 0.        , 0.00381679, 0.00381679,\r\n",
      "       0.00763359, 0.00763359, 0.01145038, 0.01145038, 0.01908397,\r\n",
      "       0.01908397, 0.02290076, 0.02290076, 0.03816794, 0.03816794,\r\n",
      "       0.04198473, 0.04198473, 0.04580153, 0.04580153, 0.07633588,\r\n",
      "       0.07633588, 0.08015267, 0.08015267, 0.08396947, 0.08396947,\r\n",
      "       0.09541985, 0.09541985, 0.09923664, 0.09923664, 0.10687023,\r\n",
      "       0.10687023, 0.11068702, 0.11068702, 0.11450382, 0.11450382,\r\n",
      "       0.11832061, 0.11832061, 0.1221374 , 0.1221374 , 0.12977099,\r\n",
      "       0.12977099, 0.13358779, 0.13358779, 0.14122137, 0.14122137,\r\n",
      "       0.14885496, 0.14885496, 0.15267176, 0.15648855, 0.16030534,\r\n",
      "       0.16030534, 0.16412214, 0.16412214, 0.17175573, 0.17175573,\r\n",
      "       0.17557252, 0.17557252, 0.17938931, 0.17938931, 0.18320611,\r\n",
      "       0.1870229 , 0.1870229 , 0.19083969, 0.19083969, 0.19465649,\r\n",
      "       0.19465649, 0.19847328, 0.19847328, 0.20229008, 0.20229008,\r\n",
      "       0.20610687, 0.20610687, 0.20992366, 0.20992366, 0.22137405,\r\n",
      "       0.22137405, 0.22900763, 0.22900763, 0.23664122, 0.23664122,\r\n",
      "       0.24427481, 0.24427481, 0.2480916 , 0.2480916 , 0.2519084 ,\r\n",
      "       0.2519084 , 0.25572519, 0.25572519, 0.25954198, 0.25954198,\r\n",
      "       0.26335878, 0.26335878, 0.26717557, 0.26717557, 0.27480916,\r\n",
      "       0.27480916, 0.28244275, 0.28244275, 0.28625954, 0.29389313,\r\n",
      "       0.29770992, 0.29770992, 0.30152672, 0.30152672, 0.30916031,\r\n",
      "       0.30916031, 0.31679389, 0.31679389, 0.32061069, 0.32061069,\r\n",
      "       0.32442748, 0.32442748, 0.32824427, 0.32824427, 0.32824427,\r\n",
      "       0.33206107, 0.33206107, 0.33206107, 0.33587786, 0.33587786,\r\n",
      "       0.33969466, 0.33969466, 0.34351145, 0.34351145, 0.35114504,\r\n",
      "       0.35496183, 0.35496183, 0.36259542, 0.36259542, 0.36641221,\r\n",
      "       0.36641221, 0.37022901, 0.37022901, 0.3778626 , 0.3778626 ,\r\n",
      "       0.38167939, 0.38167939, 0.38931298, 0.38931298, 0.39312977,\r\n",
      "       0.39312977, 0.39694656, 0.39694656, 0.40076336, 0.40076336,\r\n",
      "       0.40458015, 0.40458015, 0.41221374, 0.41221374, 0.41603053,\r\n",
      "       0.41603053, 0.42366412, 0.42366412, 0.42748092, 0.42748092,\r\n",
      "       0.43129771, 0.43129771, 0.4351145 , 0.4351145 , 0.44274809,\r\n",
      "       0.44274809, 0.44656489, 0.44656489, 0.45038168, 0.45038168,\r\n",
      "       0.45419847, 0.45419847, 0.45801527, 0.45801527, 0.45801527,\r\n",
      "       0.46183206, 0.46183206, 0.46183206, 0.46183206, 0.46564885,\r\n",
      "       0.46564885, 0.47328244, 0.47709924, 0.47709924, 0.47709924,\r\n",
      "       0.48091603, 0.48091603, 0.48473282, 0.48473282, 0.48854962,\r\n",
      "       0.48854962, 0.49236641, 0.49236641, 0.5       , 0.5       ,\r\n",
      "       0.51145038, 0.51145038, 0.51908397, 0.51908397, 0.52290076,\r\n",
      "       0.52290076, 0.53053435, 0.53053435, 0.53435115, 0.53435115,\r\n",
      "       0.53816794, 0.53816794, 0.54198473, 0.54198473, 0.54580153,\r\n",
      "       0.54580153, 0.55343511, 0.55343511, 0.55725191, 0.55725191,\r\n",
      "       0.5610687 , 0.5610687 , 0.5648855 , 0.5648855 , 0.5648855 ,\r\n",
      "       0.5648855 , 0.57251908, 0.57633588, 0.57633588, 0.58015267,\r\n",
      "       0.58015267, 0.58396947, 0.58396947, 0.58778626, 0.58778626,\r\n",
      "       0.58778626, 0.58778626, 0.59160305, 0.59160305, 0.60305344,\r\n",
      "       0.60305344, 0.60305344, 0.60305344, 0.60305344, 0.60305344,\r\n",
      "       0.60687023, 0.60687023, 0.60687023, 0.61068702, 0.61068702,\r\n",
      "       0.61450382, 0.61450382, 0.61450382, 0.61450382, 0.61832061,\r\n",
      "       0.61832061, 0.6221374 , 0.6221374 , 0.6221374 , 0.6221374 ,\r\n",
      "       0.6259542 , 0.6259542 , 0.62977099, 0.62977099, 0.64122137,\r\n",
      "       0.64122137, 0.64122137, 0.64503817, 0.64503817, 0.64885496,\r\n",
      "       0.64885496, 0.64885496, 0.65267176, 0.65267176, 0.65648855,\r\n",
      "       0.65648855, 0.66030534, 0.66412214, 0.66412214, 0.66793893,\r\n",
      "       0.66793893, 0.67175573, 0.67175573, 0.67938931, 0.67938931,\r\n",
      "       0.67938931, 0.67938931, 0.67938931, 0.68320611, 0.68320611,\r\n",
      "       0.6870229 , 0.6870229 , 0.69083969, 0.69083969, 0.69847328,\r\n",
      "       0.69847328, 0.70229008, 0.70229008, 0.70229008, 0.70229008,\r\n",
      "       0.70610687, 0.70610687, 0.70610687, 0.70992366, 0.70992366,\r\n",
      "       0.71374046, 0.71374046, 0.71755725, 0.71755725, 0.72137405,\r\n",
      "       0.72137405, 0.72519084, 0.72519084, 0.72900763, 0.72900763,\r\n",
      "       0.73282443, 0.73282443, 0.74045802, 0.74427481, 0.74427481,\r\n",
      "       0.7480916 , 0.7480916 , 0.75572519, 0.75572519, 0.75954198,\r\n",
      "       0.75954198, 0.76335878, 0.76335878, 0.76335878, 0.76335878,\r\n",
      "       0.76717557, 0.76717557, 0.77099237, 0.77099237, 0.77480916,\r\n",
      "       0.77480916, 0.77862595, 0.77862595, 0.78244275, 0.78244275,\r\n",
      "       0.78625954, 0.78625954, 0.78625954, 0.79770992, 0.79770992,\r\n",
      "       0.80152672, 0.80152672, 0.80534351, 0.80534351, 0.80916031,\r\n",
      "       0.80916031, 0.8129771 , 0.8129771 , 0.81679389, 0.81679389,\r\n",
      "       0.82061069, 0.82061069, 0.82442748, 0.82442748, 0.82824427,\r\n",
      "       0.82824427, 0.83206107, 0.83206107, 0.83969466, 0.83969466,\r\n",
      "       0.84351145, 0.84351145, 0.84351145, 0.84351145, 0.84732824,\r\n",
      "       0.85114504, 0.85114504, 0.85114504, 0.85496183, 0.85496183,\r\n",
      "       0.85877863, 0.85877863, 0.85877863, 0.85877863, 0.85877863,\r\n",
      "       0.85877863, 0.86259542, 0.86259542, 0.86641221, 0.86641221,\r\n",
      "       0.87022901, 0.87022901, 0.87022901, 0.87022901, 0.8778626 ,\r\n",
      "       0.8778626 , 0.8778626 , 0.8778626 , 0.8778626 , 0.8778626 ,\r\n",
      "       0.8778626 , 0.8778626 , 0.8778626 , 0.8778626 , 0.8778626 ,\r\n",
      "       0.8778626 , 0.8778626 , 0.88549618, 0.88549618, 0.88931298,\r\n",
      "       0.88931298, 0.88931298, 0.88931298, 0.89312977, 0.89312977,\r\n",
      "       0.89312977, 0.89312977, 0.89694656, 0.90076336, 0.90458015,\r\n",
      "       0.90458015, 0.90839695, 0.90839695, 0.91221374, 0.91221374,\r\n",
      "       0.91221374, 0.91221374, 0.91603053, 0.91603053, 0.91603053,\r\n",
      "       0.91603053, 0.91984733, 0.91984733, 0.91984733, 0.91984733,\r\n",
      "       1.        ])}\r\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(roc_curves)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "function = join(deeppath_code, '03_postprocessing/0h_ROC_MultiOutput_BootStrap.py')\n",
    "root = function.rsplit('/',1)[1].split('.')[0]\n",
    "\n",
    "file_stats = join(test_results, 'out_filename_Stats.txt')\n",
    "!python $function  --file_stats=$file_stats --output_dir=$roc_curves  --labels_names=$hugo_symbols_path \\\n",
    "    --ref_stats=''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optionally save training results to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///mnt/disks/deeppath-data/Data/Px512Ol0Bg25Mg5_Tile/So3_Sort/Cl3FtTrue_Train/data.tar.gz [Content-Type=application/x-tar]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run\n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "- [1/1 files][  2.2 GiB/  2.2 GiB] 100% Done  62.3 MiB/s ETA 00:00:00           \n",
      "Operation completed over 1 objects/2.2 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "# Populate the tar file\n",
    "# Note we don't save validation results.\n",
    "loc = join(training,'data.tar.gz')\n",
    "with tarfile.open(loc, \"w:gz\") as tar:\n",
    "    for name in [intermediate_checkpoints, training_logs,  roc_curves,  test_results, heatmaps]:\n",
    "        tar.add(name)\n",
    "\n",
    "# Copy it to GCS\n",
    "gcs = join('gs://', deeppath_data_bucket,tiling_params, sorting_params, training_params, 'data.tar.gz')\n",
    "!gsutil -m cp $loc $gcs \n",
    "\n",
    "# Delete the tar file\n",
    "!rm $loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "lw=2\n",
    "def draw_rocs(roc_base):\n",
    "    plt.figure()\n",
    "    #colors = cycle(['deeppink', 'navy', 'aqua', 'darkorange', 'cornflowerblue'])\n",
    "    colors = cycle(['deeppink', 'navy', 'C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9'])\n",
    "    lws = [4, 4, 2, 2, 2, 2, 2, 2 ,2 ,2 ,2 ,2]\n",
    "    curves = ['micro', 'macro', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n",
    "    linestyles = [':', ':', '-', '-', '-', '-', '-', '-', '-', '-', '-']\n",
    "    for curve, color, lw, linestyle in zip(curves, colors, lws, linestyles):\n",
    "        f = [x for x in os.listdir(roc_curves) if x.find(roc_base + curve)==0][0]\n",
    "        lines = np.loadtxt(join(roc_curves,f), comments=\"#\", delimiter=\"\\t\", unpack=True)\n",
    "        plt.plot(lines[0], lines[1], color = color, lw=lw, linestyle=linestyle,\n",
    "             label = '{0} ROC curve (area = {1:0.2f})'.format(curve,float(f[f.find('auc_')+4:f.find('auc_')+10])))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roc_base = 'out2_roc_data_AvPb_'\n",
    "draw_rocs(roc_base)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roc_base = 'out2_roc_data_PcSel_'\n",
    "draw_rocs(roc_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
